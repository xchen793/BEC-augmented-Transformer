---------------------------------------
Begin Slurm Prolog: Aug-12-2025 19:56:12
Job ID:    6981877
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Tue Aug 12 07:56:12 PM EDT 2025
atl1-1-02-009-35-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Tue Aug 12 19:56:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   44C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 107673.22it/s] 28%|██▊       | 38194/135842 [00:00<00:00, 158264.64it/s] 41%|████      | 55194/135842 [00:00<00:00, 120244.41it/s] 51%|█████     | 69378/135842 [00:00<00:00, 94859.84it/s]  66%|██████▌   | 89082/135842 [00:00<00:00, 119639.60it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90942.96it/s] 92%|█████████▏| 125235/135842 [00:01<00:00, 108781.48it/s]100%|██████████| 135842/135842 [00:01<00:00, 112014.90it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47750.54it/s] 32%|███▏      | 35278/108673 [00:00<00:00, 84673.09it/s] 49%|████▉     | 53085/108673 [00:00<00:00, 111470.78it/s] 66%|██████▌   | 71182/108673 [00:00<00:00, 131673.51it/s] 80%|████████  | 87353/108673 [00:01<00:00, 70798.08it/s]  97%|█████████▋| 105266/108673 [00:01<00:00, 89691.21it/s]100%|██████████| 108673/108673 [00:01<00:00, 89209.93it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 65%|██████▌   | 17686/27169 [00:00<00:00, 176846.25it/s]100%|██████████| 27169/27169 [00:00<00:00, 178733.67it/s]
tensor([  3,  27, 243, 911,  18, 864,  35,  12,  15, 239,   9,  20,  35,  12,
         11,  25, 108,   7,  37,   4,   2,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,   12,   21,   41,    6,    9,   59,  294,   24, 3864,   22,  159,
          95,   25, 2138,    7,   92,  366, 4130,    5,   46,   35,  287,    7,
         117,  883,   15,  130, 1027,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,   36,  140,  428,   25,  252,   10,  171,  117,   21,    8, 2431,
          17,   46,  593,  375,    5,   22,   11,   38,  114,   17,   28,  867,
           7,   34, 2788,    4,    2,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_12 19:57:06
******** [step = 50] loss: 9.402, acc: 0.006
******** [step = 100] loss: 8.977, acc: 0.104
******** [step = 150] loss: 8.600, acc: 0.151
******** [step = 200] loss: 8.260, acc: 0.175
******** [step = 250] loss: 7.950, acc: 0.189
******** [step = 300] loss: 7.646, acc: 0.200
******** [step = 350] loss: 7.354, acc: 0.208
******** [step = 400] loss: 7.082, acc: 0.217
******** [step = 450] loss: 6.839, acc: 0.225
******** [step = 500] loss: 6.625, acc: 0.232
******** [step = 550] loss: 6.435, acc: 0.241
******** [step = 600] loss: 6.268, acc: 0.248
******** [step = 650] loss: 6.119, acc: 0.254
******** [step = 700] loss: 5.984, acc: 0.260
******** [step = 750] loss: 5.860, acc: 0.266
******** [step = 800] loss: 5.750, acc: 0.271
******** [step = 850] loss: 5.647, acc: 0.276
EPOCH = 1 loss: 5.647, acc: 0.276, val_loss: 3.904, val_acc: 0.367

================================================================================2025-08_12 19:59:09
******** [step = 50] loss: 3.955, acc: 0.358
******** [step = 100] loss: 3.926, acc: 0.360
******** [step = 150] loss: 3.898, acc: 0.361
******** [step = 200] loss: 3.871, acc: 0.363
******** [step = 250] loss: 3.849, acc: 0.364
******** [step = 300] loss: 3.829, acc: 0.365
******** [step = 350] loss: 3.809, acc: 0.367
******** [step = 400] loss: 3.792, acc: 0.368
******** [step = 450] loss: 3.776, acc: 0.369
******** [step = 500] loss: 3.764, acc: 0.369
******** [step = 550] loss: 3.750, acc: 0.370
******** [step = 600] loss: 3.739, acc: 0.371
******** [step = 650] loss: 3.727, acc: 0.372
******** [step = 700] loss: 3.716, acc: 0.372
******** [step = 750] loss: 3.703, acc: 0.373
******** [step = 800] loss: 3.694, acc: 0.374
******** [step = 850] loss: 3.681, acc: 0.375
EPOCH = 2 loss: 3.681, acc: 0.375, val_loss: 3.414, val_acc: 0.403

================================================================================2025-08_12 20:01:12
******** [step = 50] loss: 3.482, acc: 0.387
******** [step = 100] loss: 3.446, acc: 0.391
******** [step = 150] loss: 3.432, acc: 0.392
******** [step = 200] loss: 3.419, acc: 0.394
******** [step = 250] loss: 3.407, acc: 0.395
******** [step = 300] loss: 3.399, acc: 0.396
******** [step = 350] loss: 3.394, acc: 0.397
******** [step = 400] loss: 3.383, acc: 0.398
******** [step = 450] loss: 3.376, acc: 0.399
******** [step = 500] loss: 3.370, acc: 0.400
******** [step = 550] loss: 3.366, acc: 0.400
******** [step = 600] loss: 3.362, acc: 0.401
******** [step = 650] loss: 3.357, acc: 0.402
******** [step = 700] loss: 3.351, acc: 0.402
******** [step = 750] loss: 3.348, acc: 0.403
******** [step = 800] loss: 3.344, acc: 0.404
******** [step = 850] loss: 3.340, acc: 0.404
EPOCH = 3 loss: 3.340, acc: 0.404, val_loss: 3.194, val_acc: 0.422

================================================================================2025-08_12 20:03:14
******** [step = 50] loss: 3.203, acc: 0.411
******** [step = 100] loss: 3.188, acc: 0.416
******** [step = 150] loss: 3.182, acc: 0.418
******** [step = 200] loss: 3.178, acc: 0.419
******** [step = 250] loss: 3.181, acc: 0.419
******** [step = 300] loss: 3.182, acc: 0.419
******** [step = 350] loss: 3.173, acc: 0.420
******** [step = 400] loss: 3.173, acc: 0.420
******** [step = 450] loss: 3.171, acc: 0.420
******** [step = 500] loss: 3.169, acc: 0.421
******** [step = 550] loss: 3.168, acc: 0.421
******** [step = 600] loss: 3.164, acc: 0.422
******** [step = 650] loss: 3.162, acc: 0.422
******** [step = 700] loss: 3.159, acc: 0.423
******** [step = 750] loss: 3.155, acc: 0.423
******** [step = 800] loss: 3.152, acc: 0.424
******** [step = 850] loss: 3.150, acc: 0.424
EPOCH = 4 loss: 3.150, acc: 0.424, val_loss: 3.068, val_acc: 0.438

================================================================================2025-08_12 20:05:17
******** [step = 50] loss: 3.030, acc: 0.431
******** [step = 100] loss: 3.010, acc: 0.434
******** [step = 150] loss: 3.004, acc: 0.436
******** [step = 200] loss: 3.005, acc: 0.437
******** [step = 250] loss: 3.008, acc: 0.437
******** [step = 300] loss: 3.014, acc: 0.436
******** [step = 350] loss: 3.018, acc: 0.436
******** [step = 400] loss: 3.022, acc: 0.436
******** [step = 450] loss: 3.022, acc: 0.436
******** [step = 500] loss: 3.026, acc: 0.436
******** [step = 550] loss: 3.029, acc: 0.436
******** [step = 600] loss: 3.031, acc: 0.435
******** [step = 650] loss: 3.034, acc: 0.435
******** [step = 700] loss: 3.036, acc: 0.435
******** [step = 750] loss: 3.039, acc: 0.435
******** [step = 800] loss: 3.040, acc: 0.435
******** [step = 850] loss: 3.044, acc: 0.435
EPOCH = 5 loss: 3.044, acc: 0.435, val_loss: 3.034, val_acc: 0.444

================================================================================2025-08_12 20:07:18
******** [step = 50] loss: 3.000, acc: 0.436
******** [step = 100] loss: 2.976, acc: 0.438
******** [step = 150] loss: 2.975, acc: 0.439
******** [step = 200] loss: 2.974, acc: 0.439
******** [step = 250] loss: 2.973, acc: 0.440
******** [step = 300] loss: 2.972, acc: 0.440
******** [step = 350] loss: 2.967, acc: 0.441
******** [step = 400] loss: 2.966, acc: 0.441
******** [step = 450] loss: 2.964, acc: 0.442
******** [step = 500] loss: 2.959, acc: 0.442
******** [step = 550] loss: 2.955, acc: 0.443
******** [step = 600] loss: 2.952, acc: 0.443
******** [step = 650] loss: 2.950, acc: 0.444
******** [step = 700] loss: 2.947, acc: 0.444
******** [step = 750] loss: 2.945, acc: 0.445
******** [step = 800] loss: 2.942, acc: 0.445
******** [step = 850] loss: 2.940, acc: 0.445
EPOCH = 6 loss: 2.940, acc: 0.445, val_loss: 2.872, val_acc: 0.466

================================================================================2025-08_12 20:09:20
******** [step = 50] loss: 2.797, acc: 0.454
******** [step = 100] loss: 2.801, acc: 0.456
******** [step = 150] loss: 2.800, acc: 0.457
******** [step = 200] loss: 2.803, acc: 0.458
******** [step = 250] loss: 2.802, acc: 0.458
******** [step = 300] loss: 2.804, acc: 0.458
******** [step = 350] loss: 2.805, acc: 0.458
******** [step = 400] loss: 2.810, acc: 0.458
******** [step = 450] loss: 2.819, acc: 0.457
******** [step = 500] loss: 2.826, acc: 0.457
******** [step = 550] loss: 2.833, acc: 0.456
******** [step = 600] loss: 2.835, acc: 0.456
******** [step = 650] loss: 2.838, acc: 0.455
******** [step = 700] loss: 2.839, acc: 0.455
******** [step = 750] loss: 2.843, acc: 0.455
******** [step = 800] loss: 2.847, acc: 0.455
******** [step = 850] loss: 2.848, acc: 0.455
EPOCH = 7 loss: 2.848, acc: 0.455, val_loss: 2.875, val_acc: 0.463

================================================================================2025-08_12 20:11:21
******** [step = 50] loss: 2.773, acc: 0.456
******** [step = 100] loss: 2.773, acc: 0.457
******** [step = 150] loss: 2.776, acc: 0.459
******** [step = 200] loss: 2.782, acc: 0.458
******** [step = 250] loss: 2.781, acc: 0.459
******** [step = 300] loss: 2.782, acc: 0.459
******** [step = 350] loss: 2.784, acc: 0.459
******** [step = 400] loss: 2.784, acc: 0.459
******** [step = 450] loss: 2.784, acc: 0.459
******** [step = 500] loss: 2.781, acc: 0.460
******** [step = 550] loss: 2.781, acc: 0.460
******** [step = 600] loss: 2.784, acc: 0.460
******** [step = 650] loss: 2.786, acc: 0.460
******** [step = 700] loss: 2.787, acc: 0.460
******** [step = 750] loss: 2.788, acc: 0.460
******** [step = 800] loss: 2.787, acc: 0.461
******** [step = 850] loss: 2.787, acc: 0.461
EPOCH = 8 loss: 2.787, acc: 0.461, val_loss: 2.781, val_acc: 0.478

================================================================================2025-08_12 20:13:21
******** [step = 50] loss: 2.658, acc: 0.471
******** [step = 100] loss: 2.663, acc: 0.471
******** [step = 150] loss: 2.660, acc: 0.471
******** [step = 200] loss: 2.662, acc: 0.472
******** [step = 250] loss: 2.664, acc: 0.472
******** [step = 300] loss: 2.674, acc: 0.471
******** [step = 350] loss: 2.680, acc: 0.470
******** [step = 400] loss: 2.684, acc: 0.470
******** [step = 450] loss: 2.686, acc: 0.471
******** [step = 500] loss: 2.689, acc: 0.471
******** [step = 550] loss: 2.692, acc: 0.471
******** [step = 600] loss: 2.693, acc: 0.471
******** [step = 650] loss: 2.696, acc: 0.471
******** [step = 700] loss: 2.697, acc: 0.471
******** [step = 750] loss: 2.697, acc: 0.472
******** [step = 800] loss: 2.700, acc: 0.472
******** [step = 850] loss: 2.701, acc: 0.472
EPOCH = 9 loss: 2.701, acc: 0.472, val_loss: 2.746, val_acc: 0.485

================================================================================2025-08_12 20:15:24
******** [step = 50] loss: 2.597, acc: 0.482
******** [step = 100] loss: 2.598, acc: 0.483
******** [step = 150] loss: 2.595, acc: 0.483
******** [step = 200] loss: 2.606, acc: 0.482
******** [step = 250] loss: 2.608, acc: 0.482
******** [step = 300] loss: 2.610, acc: 0.482
******** [step = 350] loss: 2.616, acc: 0.482
******** [step = 400] loss: 2.618, acc: 0.481
******** [step = 450] loss: 2.619, acc: 0.481
******** [step = 500] loss: 2.623, acc: 0.481
******** [step = 550] loss: 2.630, acc: 0.480
******** [step = 600] loss: 2.631, acc: 0.480
******** [step = 650] loss: 2.636, acc: 0.479
******** [step = 700] loss: 2.639, acc: 0.479
******** [step = 750] loss: 2.640, acc: 0.479
******** [step = 800] loss: 2.643, acc: 0.479
******** [step = 850] loss: 2.644, acc: 0.478
EPOCH = 10 loss: 2.644, acc: 0.478, val_loss: 2.715, val_acc: 0.487

================================================================================2025-08_12 20:17:25
******** [step = 50] loss: 2.557, acc: 0.484
******** [step = 100] loss: 2.562, acc: 0.484
******** [step = 150] loss: 2.559, acc: 0.486
******** [step = 200] loss: 2.559, acc: 0.487
******** [step = 250] loss: 2.565, acc: 0.486
******** [step = 300] loss: 2.572, acc: 0.486
******** [step = 350] loss: 2.576, acc: 0.485
******** [step = 400] loss: 2.576, acc: 0.485
******** [step = 450] loss: 2.577, acc: 0.485
******** [step = 500] loss: 2.580, acc: 0.485
******** [step = 550] loss: 2.582, acc: 0.485
******** [step = 600] loss: 2.585, acc: 0.485
******** [step = 650] loss: 2.588, acc: 0.485
******** [step = 700] loss: 2.591, acc: 0.484
******** [step = 750] loss: 2.592, acc: 0.485
******** [step = 800] loss: 2.593, acc: 0.485
******** [step = 850] loss: 2.593, acc: 0.485
EPOCH = 11 loss: 2.593, acc: 0.485, val_loss: 2.689, val_acc: 0.491

================================================================================2025-08_12 20:19:27
******** [step = 50] loss: 2.475, acc: 0.496
******** [step = 100] loss: 2.482, acc: 0.496
******** [step = 150] loss: 2.494, acc: 0.495
******** [step = 200] loss: 2.501, acc: 0.494
******** [step = 250] loss: 2.507, acc: 0.493
******** [step = 300] loss: 2.516, acc: 0.493
******** [step = 350] loss: 2.522, acc: 0.493
******** [step = 400] loss: 2.527, acc: 0.493
******** [step = 450] loss: 2.532, acc: 0.493
******** [step = 500] loss: 2.535, acc: 0.493
******** [step = 550] loss: 2.537, acc: 0.493
******** [step = 600] loss: 2.539, acc: 0.493
******** [step = 650] loss: 2.541, acc: 0.493
******** [step = 700] loss: 2.543, acc: 0.493
******** [step = 750] loss: 2.544, acc: 0.493
******** [step = 800] loss: 2.547, acc: 0.493
******** [step = 850] loss: 2.551, acc: 0.493
EPOCH = 12 loss: 2.551, acc: 0.493, val_loss: 2.662, val_acc: 0.498

================================================================================2025-08_12 20:21:28
******** [step = 50] loss: 2.478, acc: 0.499
******** [step = 100] loss: 2.461, acc: 0.501
******** [step = 150] loss: 2.464, acc: 0.501
******** [step = 200] loss: 2.473, acc: 0.501
******** [step = 250] loss: 2.478, acc: 0.500
******** [step = 300] loss: 2.479, acc: 0.501
******** [step = 350] loss: 2.482, acc: 0.501
******** [step = 400] loss: 2.485, acc: 0.500
******** [step = 450] loss: 2.490, acc: 0.500
******** [step = 500] loss: 2.493, acc: 0.500
******** [step = 550] loss: 2.494, acc: 0.500
******** [step = 600] loss: 2.497, acc: 0.500
******** [step = 650] loss: 2.500, acc: 0.499
******** [step = 700] loss: 2.505, acc: 0.499
******** [step = 750] loss: 2.508, acc: 0.499
******** [step = 800] loss: 2.510, acc: 0.499
******** [step = 850] loss: 2.512, acc: 0.498
EPOCH = 13 loss: 2.512, acc: 0.498, val_loss: 2.649, val_acc: 0.503

================================================================================2025-08_12 20:23:29
******** [step = 50] loss: 2.443, acc: 0.506
******** [step = 100] loss: 2.442, acc: 0.505
******** [step = 150] loss: 2.433, acc: 0.507
******** [step = 200] loss: 2.437, acc: 0.507
******** [step = 250] loss: 2.444, acc: 0.505
******** [step = 300] loss: 2.448, acc: 0.504
******** [step = 350] loss: 2.454, acc: 0.504
******** [step = 400] loss: 2.458, acc: 0.504
******** [step = 450] loss: 2.465, acc: 0.504
******** [step = 500] loss: 2.467, acc: 0.503
******** [step = 550] loss: 2.471, acc: 0.503
******** [step = 600] loss: 2.474, acc: 0.503
******** [step = 650] loss: 2.478, acc: 0.503
******** [step = 700] loss: 2.479, acc: 0.503
******** [step = 750] loss: 2.483, acc: 0.503
******** [step = 800] loss: 2.484, acc: 0.503
******** [step = 850] loss: 2.487, acc: 0.503
EPOCH = 14 loss: 2.487, acc: 0.503, val_loss: 2.637, val_acc: 0.507

================================================================================2025-08_12 20:25:30
******** [step = 50] loss: 2.389, acc: 0.512
******** [step = 100] loss: 2.391, acc: 0.513
******** [step = 150] loss: 2.388, acc: 0.514
******** [step = 200] loss: 2.404, acc: 0.512
******** [step = 250] loss: 2.414, acc: 0.511
******** [step = 300] loss: 2.420, acc: 0.510
******** [step = 350] loss: 2.425, acc: 0.510
******** [step = 400] loss: 2.430, acc: 0.509
******** [step = 450] loss: 2.434, acc: 0.509
******** [step = 500] loss: 2.438, acc: 0.508
******** [step = 550] loss: 2.443, acc: 0.508
******** [step = 600] loss: 2.446, acc: 0.508
******** [step = 650] loss: 2.449, acc: 0.508
******** [step = 700] loss: 2.451, acc: 0.507
******** [step = 750] loss: 2.452, acc: 0.508
******** [step = 800] loss: 2.456, acc: 0.507
******** [step = 850] loss: 2.460, acc: 0.507
EPOCH = 15 loss: 2.460, acc: 0.507, val_loss: 2.633, val_acc: 0.508

================================================================================2025-08_12 20:27:31
******** [step = 50] loss: 2.390, acc: 0.511
******** [step = 100] loss: 2.375, acc: 0.513
******** [step = 150] loss: 2.386, acc: 0.513
******** [step = 200] loss: 2.390, acc: 0.513
******** [step = 250] loss: 2.394, acc: 0.514
******** [step = 300] loss: 2.397, acc: 0.513
******** [step = 350] loss: 2.401, acc: 0.513
******** [step = 400] loss: 2.401, acc: 0.513
******** [step = 450] loss: 2.409, acc: 0.512
******** [step = 500] loss: 2.411, acc: 0.512
******** [step = 550] loss: 2.414, acc: 0.512
******** [step = 600] loss: 2.419, acc: 0.512
******** [step = 650] loss: 2.421, acc: 0.511
******** [step = 700] loss: 2.424, acc: 0.511
******** [step = 750] loss: 2.426, acc: 0.511
******** [step = 800] loss: 2.429, acc: 0.511
******** [step = 850] loss: 2.431, acc: 0.511
EPOCH = 16 loss: 2.431, acc: 0.511, val_loss: 2.620, val_acc: 0.510

================================================================================2025-08_12 20:29:32
******** [step = 50] loss: 2.345, acc: 0.517
******** [step = 100] loss: 2.350, acc: 0.517
******** [step = 150] loss: 2.364, acc: 0.517
******** [step = 200] loss: 2.367, acc: 0.517
******** [step = 250] loss: 2.372, acc: 0.517
******** [step = 300] loss: 2.374, acc: 0.516
******** [step = 350] loss: 2.378, acc: 0.516
******** [step = 400] loss: 2.382, acc: 0.515
******** [step = 450] loss: 2.385, acc: 0.516
******** [step = 500] loss: 2.390, acc: 0.515
******** [step = 550] loss: 2.392, acc: 0.515
******** [step = 600] loss: 2.394, acc: 0.515
******** [step = 650] loss: 2.399, acc: 0.514
******** [step = 700] loss: 2.402, acc: 0.514
******** [step = 750] loss: 2.407, acc: 0.514
******** [step = 800] loss: 2.408, acc: 0.514
******** [step = 850] loss: 2.410, acc: 0.513
EPOCH = 17 loss: 2.410, acc: 0.513, val_loss: 2.598, val_acc: 0.515

================================================================================2025-08_12 20:31:34
******** [step = 50] loss: 2.327, acc: 0.522
******** [step = 100] loss: 2.325, acc: 0.523
******** [step = 150] loss: 2.326, acc: 0.523
******** [step = 200] loss: 2.334, acc: 0.522
******** [step = 250] loss: 2.341, acc: 0.521
******** [step = 300] loss: 2.346, acc: 0.521
******** [step = 350] loss: 2.348, acc: 0.521
******** [step = 400] loss: 2.348, acc: 0.521
******** [step = 450] loss: 2.352, acc: 0.521
******** [step = 500] loss: 2.357, acc: 0.520
******** [step = 550] loss: 2.360, acc: 0.520
******** [step = 600] loss: 2.364, acc: 0.520
******** [step = 650] loss: 2.370, acc: 0.519
******** [step = 700] loss: 2.374, acc: 0.519
******** [step = 750] loss: 2.378, acc: 0.519
******** [step = 800] loss: 2.382, acc: 0.518
******** [step = 850] loss: 2.386, acc: 0.518
EPOCH = 18 loss: 2.386, acc: 0.518, val_loss: 2.593, val_acc: 0.517

================================================================================2025-08_12 20:33:35
******** [step = 50] loss: 2.302, acc: 0.526
******** [step = 100] loss: 2.298, acc: 0.527
******** [step = 150] loss: 2.310, acc: 0.526
******** [step = 200] loss: 2.319, acc: 0.525
******** [step = 250] loss: 2.321, acc: 0.525
******** [step = 300] loss: 2.326, acc: 0.525
******** [step = 350] loss: 2.329, acc: 0.525
******** [step = 400] loss: 2.335, acc: 0.524
******** [step = 450] loss: 2.335, acc: 0.524
******** [step = 500] loss: 2.338, acc: 0.524
******** [step = 550] loss: 2.341, acc: 0.523
******** [step = 600] loss: 2.347, acc: 0.523
******** [step = 650] loss: 2.352, acc: 0.522
******** [step = 700] loss: 2.357, acc: 0.521
******** [step = 750] loss: 2.359, acc: 0.521
******** [step = 800] loss: 2.362, acc: 0.521
******** [step = 850] loss: 2.363, acc: 0.521
EPOCH = 19 loss: 2.363, acc: 0.521, val_loss: 2.601, val_acc: 0.513

================================================================================2025-08_12 20:35:36
******** [step = 50] loss: 2.299, acc: 0.526
******** [step = 100] loss: 2.285, acc: 0.527
******** [step = 150] loss: 2.291, acc: 0.527
******** [step = 200] loss: 2.296, acc: 0.527
******** [step = 250] loss: 2.296, acc: 0.527
******** [step = 300] loss: 2.301, acc: 0.527
******** [step = 350] loss: 2.305, acc: 0.527
******** [step = 400] loss: 2.308, acc: 0.527
******** [step = 450] loss: 2.315, acc: 0.526
******** [step = 500] loss: 2.321, acc: 0.526
******** [step = 550] loss: 2.324, acc: 0.525
******** [step = 600] loss: 2.327, acc: 0.525
******** [step = 650] loss: 2.331, acc: 0.525
******** [step = 700] loss: 2.335, acc: 0.524
******** [step = 750] loss: 2.338, acc: 0.524
******** [step = 800] loss: 2.341, acc: 0.524
******** [step = 850] loss: 2.345, acc: 0.524
EPOCH = 20 loss: 2.345, acc: 0.524, val_loss: 2.582, val_acc: 0.522

================================================================================2025-08_12 20:37:37
******** [step = 50] loss: 2.263, acc: 0.533
******** [step = 100] loss: 2.258, acc: 0.532
******** [step = 150] loss: 2.258, acc: 0.532
******** [step = 200] loss: 2.270, acc: 0.532
******** [step = 250] loss: 2.276, acc: 0.532
******** [step = 300] loss: 2.281, acc: 0.531
******** [step = 350] loss: 2.287, acc: 0.531
******** [step = 400] loss: 2.292, acc: 0.530
******** [step = 450] loss: 2.298, acc: 0.530
******** [step = 500] loss: 2.302, acc: 0.529
******** [step = 550] loss: 2.306, acc: 0.529
******** [step = 600] loss: 2.310, acc: 0.529
******** [step = 650] loss: 2.315, acc: 0.528
******** [step = 700] loss: 2.317, acc: 0.528
******** [step = 750] loss: 2.321, acc: 0.528
******** [step = 800] loss: 2.324, acc: 0.528
******** [step = 850] loss: 2.325, acc: 0.528
EPOCH = 21 loss: 2.325, acc: 0.528, val_loss: 2.566, val_acc: 0.523

================================================================================2025-08_12 20:39:38
******** [step = 50] loss: 2.234, acc: 0.534
******** [step = 100] loss: 2.232, acc: 0.536
******** [step = 150] loss: 2.245, acc: 0.535
******** [step = 200] loss: 2.250, acc: 0.534
******** [step = 250] loss: 2.258, acc: 0.533
******** [step = 300] loss: 2.264, acc: 0.533
******** [step = 350] loss: 2.269, acc: 0.532
******** [step = 400] loss: 2.272, acc: 0.532
******** [step = 450] loss: 2.276, acc: 0.532
******** [step = 500] loss: 2.279, acc: 0.531
******** [step = 550] loss: 2.286, acc: 0.531
******** [step = 600] loss: 2.291, acc: 0.530
******** [step = 650] loss: 2.294, acc: 0.530
******** [step = 700] loss: 2.298, acc: 0.530
******** [step = 750] loss: 2.301, acc: 0.530
******** [step = 800] loss: 2.304, acc: 0.530
******** [step = 850] loss: 2.309, acc: 0.529
EPOCH = 22 loss: 2.309, acc: 0.529, val_loss: 2.564, val_acc: 0.523

================================================================================2025-08_12 20:41:40
******** [step = 50] loss: 2.222, acc: 0.537
******** [step = 100] loss: 2.233, acc: 0.536
******** [step = 150] loss: 2.231, acc: 0.537
******** [step = 200] loss: 2.236, acc: 0.537
******** [step = 250] loss: 2.246, acc: 0.536
******** [step = 300] loss: 2.248, acc: 0.536
******** [step = 350] loss: 2.255, acc: 0.535
******** [step = 400] loss: 2.259, acc: 0.535
******** [step = 450] loss: 2.265, acc: 0.534
******** [step = 500] loss: 2.269, acc: 0.534
******** [step = 550] loss: 2.273, acc: 0.533
******** [step = 600] loss: 2.276, acc: 0.533
******** [step = 650] loss: 2.280, acc: 0.533
******** [step = 700] loss: 2.283, acc: 0.533
******** [step = 750] loss: 2.286, acc: 0.533
******** [step = 800] loss: 2.289, acc: 0.532
******** [step = 850] loss: 2.291, acc: 0.532
EPOCH = 23 loss: 2.291, acc: 0.532, val_loss: 2.557, val_acc: 0.524

================================================================================2025-08_12 20:43:41
******** [step = 50] loss: 2.205, acc: 0.538
******** [step = 100] loss: 2.208, acc: 0.539
******** [step = 150] loss: 2.224, acc: 0.537
******** [step = 200] loss: 2.227, acc: 0.538
******** [step = 250] loss: 2.234, acc: 0.537
******** [step = 300] loss: 2.238, acc: 0.537
******** [step = 350] loss: 2.245, acc: 0.536
******** [step = 400] loss: 2.250, acc: 0.536
******** [step = 450] loss: 2.256, acc: 0.535
******** [step = 500] loss: 2.258, acc: 0.535
******** [step = 550] loss: 2.260, acc: 0.536
******** [step = 600] loss: 2.261, acc: 0.535
******** [step = 650] loss: 2.263, acc: 0.535
******** [step = 700] loss: 2.266, acc: 0.535
******** [step = 750] loss: 2.268, acc: 0.535
******** [step = 800] loss: 2.273, acc: 0.535
******** [step = 850] loss: 2.274, acc: 0.535
EPOCH = 24 loss: 2.274, acc: 0.535, val_loss: 2.553, val_acc: 0.526

================================================================================2025-08_12 20:45:44
******** [step = 50] loss: 2.210, acc: 0.540
******** [step = 100] loss: 2.197, acc: 0.542
******** [step = 150] loss: 2.203, acc: 0.542
******** [step = 200] loss: 2.204, acc: 0.542
******** [step = 250] loss: 2.217, acc: 0.541
******** [step = 300] loss: 2.222, acc: 0.541
******** [step = 350] loss: 2.226, acc: 0.540
******** [step = 400] loss: 2.230, acc: 0.540
******** [step = 450] loss: 2.234, acc: 0.539
******** [step = 500] loss: 2.235, acc: 0.539
******** [step = 550] loss: 2.238, acc: 0.539
******** [step = 600] loss: 2.240, acc: 0.539
******** [step = 650] loss: 2.243, acc: 0.539
******** [step = 700] loss: 2.246, acc: 0.538
******** [step = 750] loss: 2.250, acc: 0.538
******** [step = 800] loss: 2.253, acc: 0.538
******** [step = 850] loss: 2.257, acc: 0.538
EPOCH = 25 loss: 2.257, acc: 0.538, val_loss: 2.545, val_acc: 0.528

================================================================================2025-08_12 20:47:45
******** [step = 50] loss: 2.187, acc: 0.546
******** [step = 100] loss: 2.191, acc: 0.545
******** [step = 150] loss: 2.181, acc: 0.546
******** [step = 200] loss: 2.188, acc: 0.545
******** [step = 250] loss: 2.194, acc: 0.545
******** [step = 300] loss: 2.199, acc: 0.544
******** [step = 350] loss: 2.205, acc: 0.543
******** [step = 400] loss: 2.207, acc: 0.543
******** [step = 450] loss: 2.213, acc: 0.542
******** [step = 500] loss: 2.218, acc: 0.542
******** [step = 550] loss: 2.223, acc: 0.541
******** [step = 600] loss: 2.227, acc: 0.541
******** [step = 650] loss: 2.232, acc: 0.540
******** [step = 700] loss: 2.234, acc: 0.540
******** [step = 750] loss: 2.238, acc: 0.540
******** [step = 800] loss: 2.241, acc: 0.540
******** [step = 850] loss: 2.244, acc: 0.540
EPOCH = 26 loss: 2.244, acc: 0.540, val_loss: 2.539, val_acc: 0.530

================================================================================2025-08_12 20:49:46
******** [step = 50] loss: 2.172, acc: 0.545
******** [step = 100] loss: 2.183, acc: 0.545
******** [step = 150] loss: 2.184, acc: 0.546
******** [step = 200] loss: 2.181, acc: 0.546
******** [step = 250] loss: 2.183, acc: 0.546
******** [step = 300] loss: 2.188, acc: 0.546
******** [step = 350] loss: 2.193, acc: 0.546
******** [step = 400] loss: 2.198, acc: 0.545
******** [step = 450] loss: 2.201, acc: 0.545
******** [step = 500] loss: 2.207, acc: 0.544
******** [step = 550] loss: 2.210, acc: 0.544
******** [step = 600] loss: 2.214, acc: 0.543
******** [step = 650] loss: 2.219, acc: 0.543
******** [step = 700] loss: 2.222, acc: 0.543
******** [step = 750] loss: 2.225, acc: 0.542
******** [step = 800] loss: 2.229, acc: 0.542
******** [step = 850] loss: 2.230, acc: 0.542
EPOCH = 27 loss: 2.230, acc: 0.542, val_loss: 2.534, val_acc: 0.530

================================================================================2025-08_12 20:51:47
******** [step = 50] loss: 2.143, acc: 0.549
******** [step = 100] loss: 2.153, acc: 0.549
******** [step = 150] loss: 2.166, acc: 0.548
******** [step = 200] loss: 2.168, acc: 0.549
******** [step = 250] loss: 2.172, acc: 0.548
******** [step = 300] loss: 2.179, acc: 0.547
******** [step = 350] loss: 2.183, acc: 0.547
******** [step = 400] loss: 2.185, acc: 0.547
******** [step = 450] loss: 2.189, acc: 0.546
******** [step = 500] loss: 2.194, acc: 0.546
******** [step = 550] loss: 2.201, acc: 0.545
******** [step = 600] loss: 2.205, acc: 0.545
******** [step = 650] loss: 2.209, acc: 0.544
******** [step = 700] loss: 2.213, acc: 0.544
******** [step = 750] loss: 2.215, acc: 0.544
******** [step = 800] loss: 2.217, acc: 0.543
******** [step = 850] loss: 2.220, acc: 0.543
EPOCH = 28 loss: 2.220, acc: 0.543, val_loss: 2.528, val_acc: 0.533

================================================================================2025-08_12 20:53:48
******** [step = 50] loss: 2.138, acc: 0.553
******** [step = 100] loss: 2.152, acc: 0.552
******** [step = 150] loss: 2.156, acc: 0.552
******** [step = 200] loss: 2.158, acc: 0.551
******** [step = 250] loss: 2.162, acc: 0.550
******** [step = 300] loss: 2.168, acc: 0.549
******** [step = 350] loss: 2.176, acc: 0.548
******** [step = 400] loss: 2.183, acc: 0.547
******** [step = 450] loss: 2.188, acc: 0.547
******** [step = 500] loss: 2.191, acc: 0.547
******** [step = 550] loss: 2.196, acc: 0.546
******** [step = 600] loss: 2.200, acc: 0.545
******** [step = 650] loss: 2.205, acc: 0.545
******** [step = 700] loss: 2.206, acc: 0.545
******** [step = 750] loss: 2.210, acc: 0.545
******** [step = 800] loss: 2.212, acc: 0.545
******** [step = 850] loss: 2.216, acc: 0.544
EPOCH = 29 loss: 2.216, acc: 0.544, val_loss: 2.534, val_acc: 0.531

================================================================================2025-08_12 20:55:49
******** [step = 50] loss: 2.130, acc: 0.554
******** [step = 100] loss: 2.130, acc: 0.554
******** [step = 150] loss: 2.138, acc: 0.554
******** [step = 200] loss: 2.146, acc: 0.553
******** [step = 250] loss: 2.153, acc: 0.552
******** [step = 300] loss: 2.158, acc: 0.551
******** [step = 350] loss: 2.162, acc: 0.550
******** [step = 400] loss: 2.165, acc: 0.550
******** [step = 450] loss: 2.170, acc: 0.550
******** [step = 500] loss: 2.173, acc: 0.549
******** [step = 550] loss: 2.179, acc: 0.549
******** [step = 600] loss: 2.184, acc: 0.548
******** [step = 650] loss: 2.186, acc: 0.548
******** [step = 700] loss: 2.189, acc: 0.548
******** [step = 750] loss: 2.193, acc: 0.548
******** [step = 800] loss: 2.196, acc: 0.547
******** [step = 850] loss: 2.198, acc: 0.547
EPOCH = 30 loss: 2.198, acc: 0.547, val_loss: 2.529, val_acc: 0.533

================================================================================2025-08_12 20:57:50
******** [step = 50] loss: 2.109, acc: 0.556
******** [step = 100] loss: 2.124, acc: 0.555
******** [step = 150] loss: 2.131, acc: 0.555
******** [step = 200] loss: 2.141, acc: 0.553
******** [step = 250] loss: 2.150, acc: 0.553
******** [step = 300] loss: 2.151, acc: 0.552
******** [step = 350] loss: 2.151, acc: 0.552
******** [step = 400] loss: 2.154, acc: 0.552
******** [step = 450] loss: 2.160, acc: 0.551
******** [step = 500] loss: 2.163, acc: 0.551
******** [step = 550] loss: 2.167, acc: 0.551
******** [step = 600] loss: 2.172, acc: 0.550
******** [step = 650] loss: 2.175, acc: 0.550
******** [step = 700] loss: 2.178, acc: 0.549
******** [step = 750] loss: 2.182, acc: 0.549
******** [step = 800] loss: 2.185, acc: 0.548
******** [step = 850] loss: 2.190, acc: 0.548
EPOCH = 31 loss: 2.190, acc: 0.548, val_loss: 2.519, val_acc: 0.534

================================================================================2025-08_12 20:59:51
******** [step = 50] loss: 2.118, acc: 0.554
******** [step = 100] loss: 2.123, acc: 0.554
******** [step = 150] loss: 2.123, acc: 0.554
******** [step = 200] loss: 2.132, acc: 0.553
******** [step = 250] loss: 2.136, acc: 0.553
******** [step = 300] loss: 2.138, acc: 0.553
******** [step = 350] loss: 2.140, acc: 0.553
******** [step = 400] loss: 2.145, acc: 0.552
******** [step = 450] loss: 2.149, acc: 0.552
******** [step = 500] loss: 2.151, acc: 0.552
******** [step = 550] loss: 2.156, acc: 0.551
******** [step = 600] loss: 2.162, acc: 0.551
******** [step = 650] loss: 2.164, acc: 0.551
******** [step = 700] loss: 2.168, acc: 0.551
******** [step = 750] loss: 2.171, acc: 0.550
******** [step = 800] loss: 2.175, acc: 0.550
******** [step = 850] loss: 2.177, acc: 0.550
EPOCH = 32 loss: 2.177, acc: 0.550, val_loss: 2.516, val_acc: 0.536

================================================================================2025-08_12 21:01:52
******** [step = 50] loss: 2.120, acc: 0.557
******** [step = 100] loss: 2.115, acc: 0.558
******** [step = 150] loss: 2.120, acc: 0.556
******** [step = 200] loss: 2.125, acc: 0.556
******** [step = 250] loss: 2.129, acc: 0.555
******** [step = 300] loss: 2.133, acc: 0.555
******** [step = 350] loss: 2.134, acc: 0.555
******** [step = 400] loss: 2.138, acc: 0.554
******** [step = 450] loss: 2.142, acc: 0.554
******** [step = 500] loss: 2.148, acc: 0.553
******** [step = 550] loss: 2.152, acc: 0.553
******** [step = 600] loss: 2.154, acc: 0.553
******** [step = 650] loss: 2.158, acc: 0.552
******** [step = 700] loss: 2.162, acc: 0.552
******** [step = 750] loss: 2.166, acc: 0.551
******** [step = 800] loss: 2.168, acc: 0.551
******** [step = 850] loss: 2.170, acc: 0.551
EPOCH = 33 loss: 2.170, acc: 0.551, val_loss: 2.528, val_acc: 0.532

================================================================================2025-08_12 21:03:53
******** [step = 50] loss: 2.103, acc: 0.559
******** [step = 100] loss: 2.110, acc: 0.558
******** [step = 150] loss: 2.114, acc: 0.557
******** [step = 200] loss: 2.116, acc: 0.557
******** [step = 250] loss: 2.124, acc: 0.556
******** [step = 300] loss: 2.127, acc: 0.555
******** [step = 350] loss: 2.133, acc: 0.555
******** [step = 400] loss: 2.134, acc: 0.555
******** [step = 450] loss: 2.142, acc: 0.554
******** [step = 500] loss: 2.142, acc: 0.554
******** [step = 550] loss: 2.145, acc: 0.554
******** [step = 600] loss: 2.148, acc: 0.553
******** [step = 650] loss: 2.151, acc: 0.553
******** [step = 700] loss: 2.155, acc: 0.553
******** [step = 750] loss: 2.159, acc: 0.552
******** [step = 800] loss: 2.161, acc: 0.552
******** [step = 850] loss: 2.166, acc: 0.551
EPOCH = 34 loss: 2.166, acc: 0.551, val_loss: 2.508, val_acc: 0.538

================================================================================2025-08_12 21:05:54
******** [step = 50] loss: 2.087, acc: 0.561
******** [step = 100] loss: 2.093, acc: 0.560
******** [step = 150] loss: 2.093, acc: 0.560
******** [step = 200] loss: 2.097, acc: 0.560
******** [step = 250] loss: 2.101, acc: 0.560
******** [step = 300] loss: 2.110, acc: 0.559
******** [step = 350] loss: 2.114, acc: 0.558
******** [step = 400] loss: 2.118, acc: 0.557
******** [step = 450] loss: 2.121, acc: 0.557
******** [step = 500] loss: 2.125, acc: 0.556
******** [step = 550] loss: 2.132, acc: 0.555
******** [step = 600] loss: 2.137, acc: 0.555
******** [step = 650] loss: 2.140, acc: 0.555
******** [step = 700] loss: 2.143, acc: 0.555
******** [step = 750] loss: 2.146, acc: 0.555
******** [step = 800] loss: 2.149, acc: 0.554
******** [step = 850] loss: 2.151, acc: 0.554
EPOCH = 35 loss: 2.151, acc: 0.554, val_loss: 2.508, val_acc: 0.537

================================================================================2025-08_12 21:07:55
******** [step = 50] loss: 2.079, acc: 0.559
******** [step = 100] loss: 2.082, acc: 0.559
******** [step = 150] loss: 2.091, acc: 0.559
******** [step = 200] loss: 2.098, acc: 0.558
******** [step = 250] loss: 2.098, acc: 0.558
******** [step = 300] loss: 2.104, acc: 0.558
******** [step = 350] loss: 2.109, acc: 0.557
******** [step = 400] loss: 2.110, acc: 0.557
******** [step = 450] loss: 2.113, acc: 0.557
******** [step = 500] loss: 2.118, acc: 0.556
******** [step = 550] loss: 2.124, acc: 0.556
******** [step = 600] loss: 2.127, acc: 0.556
******** [step = 650] loss: 2.130, acc: 0.555
******** [step = 700] loss: 2.133, acc: 0.555
******** [step = 750] loss: 2.135, acc: 0.555
******** [step = 800] loss: 2.139, acc: 0.555
******** [step = 850] loss: 2.143, acc: 0.555
EPOCH = 36 loss: 2.143, acc: 0.555, val_loss: 2.519, val_acc: 0.537

================================================================================2025-08_12 21:09:55
******** [step = 50] loss: 2.078, acc: 0.563
******** [step = 100] loss: 2.077, acc: 0.562
******** [step = 150] loss: 2.072, acc: 0.563
******** [step = 200] loss: 2.077, acc: 0.562
******** [step = 250] loss: 2.084, acc: 0.561
******** [step = 300] loss: 2.090, acc: 0.561
******** [step = 350] loss: 2.093, acc: 0.561
******** [step = 400] loss: 2.097, acc: 0.560
******** [step = 450] loss: 2.101, acc: 0.560
******** [step = 500] loss: 2.106, acc: 0.559
******** [step = 550] loss: 2.111, acc: 0.559
******** [step = 600] loss: 2.114, acc: 0.558
******** [step = 650] loss: 2.119, acc: 0.558
******** [step = 700] loss: 2.123, acc: 0.557
******** [step = 750] loss: 2.126, acc: 0.557
******** [step = 800] loss: 2.130, acc: 0.557
******** [step = 850] loss: 2.135, acc: 0.556
EPOCH = 37 loss: 2.135, acc: 0.556, val_loss: 2.502, val_acc: 0.537

================================================================================2025-08_12 21:11:55
******** [step = 50] loss: 2.031, acc: 0.568
******** [step = 100] loss: 2.045, acc: 0.567
******** [step = 150] loss: 2.056, acc: 0.564
******** [step = 200] loss: 2.063, acc: 0.564
******** [step = 250] loss: 2.069, acc: 0.563
******** [step = 300] loss: 2.077, acc: 0.562
******** [step = 350] loss: 2.083, acc: 0.562
******** [step = 400] loss: 2.090, acc: 0.561
******** [step = 450] loss: 2.097, acc: 0.561
******** [step = 500] loss: 2.100, acc: 0.560
******** [step = 550] loss: 2.105, acc: 0.560
******** [step = 600] loss: 2.107, acc: 0.559
******** [step = 650] loss: 2.111, acc: 0.559
******** [step = 700] loss: 2.116, acc: 0.559
******** [step = 750] loss: 2.120, acc: 0.558
******** [step = 800] loss: 2.123, acc: 0.558
******** [step = 850] loss: 2.128, acc: 0.558
EPOCH = 38 loss: 2.128, acc: 0.558, val_loss: 2.501, val_acc: 0.540

================================================================================2025-08_12 21:13:56
******** [step = 50] loss: 2.027, acc: 0.568
******** [step = 100] loss: 2.044, acc: 0.566
******** [step = 150] loss: 2.059, acc: 0.565
******** [step = 200] loss: 2.063, acc: 0.565
******** [step = 250] loss: 2.068, acc: 0.565
******** [step = 300] loss: 2.075, acc: 0.564
******** [step = 350] loss: 2.077, acc: 0.563
******** [step = 400] loss: 2.081, acc: 0.563
******** [step = 450] loss: 2.089, acc: 0.562
******** [step = 500] loss: 2.093, acc: 0.562
******** [step = 550] loss: 2.096, acc: 0.561
******** [step = 600] loss: 2.099, acc: 0.561
******** [step = 650] loss: 2.105, acc: 0.560
******** [step = 700] loss: 2.109, acc: 0.560
******** [step = 750] loss: 2.112, acc: 0.560
******** [step = 800] loss: 2.115, acc: 0.559
******** [step = 850] loss: 2.117, acc: 0.559
EPOCH = 39 loss: 2.117, acc: 0.559, val_loss: 2.496, val_acc: 0.542

================================================================================2025-08_12 21:16:00
******** [step = 50] loss: 2.055, acc: 0.565
******** [step = 100] loss: 2.041, acc: 0.567
******** [step = 150] loss: 2.047, acc: 0.567
******** [step = 200] loss: 2.055, acc: 0.566
******** [step = 250] loss: 2.057, acc: 0.566
******** [step = 300] loss: 2.064, acc: 0.565
******** [step = 350] loss: 2.070, acc: 0.564
******** [step = 400] loss: 2.075, acc: 0.564
******** [step = 450] loss: 2.080, acc: 0.563
******** [step = 500] loss: 2.083, acc: 0.563
******** [step = 550] loss: 2.086, acc: 0.562
******** [step = 600] loss: 2.092, acc: 0.562
******** [step = 650] loss: 2.095, acc: 0.562
******** [step = 700] loss: 2.098, acc: 0.562
******** [step = 750] loss: 2.101, acc: 0.561
******** [step = 800] loss: 2.104, acc: 0.561
******** [step = 850] loss: 2.109, acc: 0.560
EPOCH = 40 loss: 2.109, acc: 0.560, val_loss: 2.491, val_acc: 0.541

================================================================================2025-08_12 21:18:01
******** [step = 50] loss: 2.048, acc: 0.565
******** [step = 100] loss: 2.035, acc: 0.566
******** [step = 150] loss: 2.041, acc: 0.566
******** [step = 200] loss: 2.043, acc: 0.566
******** [step = 250] loss: 2.048, acc: 0.566
******** [step = 300] loss: 2.056, acc: 0.565
******** [step = 350] loss: 2.062, acc: 0.565
******** [step = 400] loss: 2.063, acc: 0.564
******** [step = 450] loss: 2.068, acc: 0.564
******** [step = 500] loss: 2.075, acc: 0.564
******** [step = 550] loss: 2.080, acc: 0.563
******** [step = 600] loss: 2.085, acc: 0.563
******** [step = 650] loss: 2.090, acc: 0.562
******** [step = 700] loss: 2.094, acc: 0.562
******** [step = 750] loss: 2.097, acc: 0.562
******** [step = 800] loss: 2.100, acc: 0.561
******** [step = 850] loss: 2.103, acc: 0.561
EPOCH = 41 loss: 2.103, acc: 0.561, val_loss: 2.492, val_acc: 0.541

================================================================================2025-08_12 21:20:01
******** [step = 50] loss: 2.029, acc: 0.570
******** [step = 100] loss: 2.035, acc: 0.568
******** [step = 150] loss: 2.046, acc: 0.566
******** [step = 200] loss: 2.048, acc: 0.567
******** [step = 250] loss: 2.047, acc: 0.567
******** [step = 300] loss: 2.054, acc: 0.567
******** [step = 350] loss: 2.058, acc: 0.566
******** [step = 400] loss: 2.063, acc: 0.566
******** [step = 450] loss: 2.067, acc: 0.565
******** [step = 500] loss: 2.069, acc: 0.565
******** [step = 550] loss: 2.075, acc: 0.565
******** [step = 600] loss: 2.078, acc: 0.564
******** [step = 650] loss: 2.084, acc: 0.564
******** [step = 700] loss: 2.086, acc: 0.564
******** [step = 750] loss: 2.090, acc: 0.563
******** [step = 800] loss: 2.094, acc: 0.563
******** [step = 850] loss: 2.097, acc: 0.563
EPOCH = 42 loss: 2.097, acc: 0.563, val_loss: 2.487, val_acc: 0.542

================================================================================2025-08_12 21:22:02
******** [step = 50] loss: 2.017, acc: 0.573
******** [step = 100] loss: 2.022, acc: 0.571
******** [step = 150] loss: 2.031, acc: 0.570
******** [step = 200] loss: 2.038, acc: 0.569
******** [step = 250] loss: 2.042, acc: 0.568
******** [step = 300] loss: 2.045, acc: 0.568
******** [step = 350] loss: 2.051, acc: 0.567
******** [step = 400] loss: 2.053, acc: 0.567
******** [step = 450] loss: 2.057, acc: 0.566
******** [step = 500] loss: 2.064, acc: 0.566
******** [step = 550] loss: 2.068, acc: 0.565
******** [step = 600] loss: 2.073, acc: 0.565
******** [step = 650] loss: 2.079, acc: 0.565
******** [step = 700] loss: 2.083, acc: 0.564
******** [step = 750] loss: 2.085, acc: 0.564
******** [step = 800] loss: 2.087, acc: 0.564
******** [step = 850] loss: 2.090, acc: 0.564
EPOCH = 43 loss: 2.090, acc: 0.564, val_loss: 2.482, val_acc: 0.544

================================================================================2025-08_12 21:24:02
******** [step = 50] loss: 2.005, acc: 0.573
******** [step = 100] loss: 2.017, acc: 0.572
******** [step = 150] loss: 2.021, acc: 0.572
******** [step = 200] loss: 2.025, acc: 0.571
******** [step = 250] loss: 2.029, acc: 0.571
******** [step = 300] loss: 2.036, acc: 0.570
******** [step = 350] loss: 2.040, acc: 0.570
******** [step = 400] loss: 2.045, acc: 0.569
******** [step = 450] loss: 2.052, acc: 0.568
******** [step = 500] loss: 2.055, acc: 0.568
******** [step = 550] loss: 2.060, acc: 0.567
******** [step = 600] loss: 2.065, acc: 0.567
******** [step = 650] loss: 2.070, acc: 0.566
******** [step = 700] loss: 2.074, acc: 0.566
******** [step = 750] loss: 2.076, acc: 0.566
******** [step = 800] loss: 2.080, acc: 0.565
******** [step = 850] loss: 2.082, acc: 0.565
EPOCH = 44 loss: 2.082, acc: 0.565, val_loss: 2.485, val_acc: 0.544

================================================================================2025-08_12 21:26:03
******** [step = 50] loss: 2.015, acc: 0.571
******** [step = 100] loss: 2.018, acc: 0.570
******** [step = 150] loss: 2.019, acc: 0.571
******** [step = 200] loss: 2.026, acc: 0.570
******** [step = 250] loss: 2.029, acc: 0.570
******** [step = 300] loss: 2.035, acc: 0.569
******** [step = 350] loss: 2.040, acc: 0.569
******** [step = 400] loss: 2.046, acc: 0.568
******** [step = 450] loss: 2.048, acc: 0.568
******** [step = 500] loss: 2.052, acc: 0.568
******** [step = 550] loss: 2.054, acc: 0.568
******** [step = 600] loss: 2.057, acc: 0.567
******** [step = 650] loss: 2.060, acc: 0.567
******** [step = 700] loss: 2.064, acc: 0.567
******** [step = 750] loss: 2.068, acc: 0.566
******** [step = 800] loss: 2.072, acc: 0.566
******** [step = 850] loss: 2.075, acc: 0.566
EPOCH = 45 loss: 2.075, acc: 0.566, val_loss: 2.483, val_acc: 0.545

================================================================================2025-08_12 21:28:04
******** [step = 50] loss: 2.011, acc: 0.571
******** [step = 100] loss: 2.007, acc: 0.572
******** [step = 150] loss: 2.008, acc: 0.573
******** [step = 200] loss: 2.013, acc: 0.573
******** [step = 250] loss: 2.022, acc: 0.572
******** [step = 300] loss: 2.030, acc: 0.571
******** [step = 350] loss: 2.030, acc: 0.571
******** [step = 400] loss: 2.035, acc: 0.571
******** [step = 450] loss: 2.041, acc: 0.570
******** [step = 500] loss: 2.044, acc: 0.570
******** [step = 550] loss: 2.048, acc: 0.569
******** [step = 600] loss: 2.051, acc: 0.569
******** [step = 650] loss: 2.054, acc: 0.569
******** [step = 700] loss: 2.059, acc: 0.568
******** [step = 750] loss: 2.063, acc: 0.568
******** [step = 800] loss: 2.067, acc: 0.567
******** [step = 850] loss: 2.072, acc: 0.567
EPOCH = 46 loss: 2.072, acc: 0.567, val_loss: 2.471, val_acc: 0.545

================================================================================2025-08_12 21:30:04
******** [step = 50] loss: 1.994, acc: 0.574
******** [step = 100] loss: 1.993, acc: 0.576
******** [step = 150] loss: 1.988, acc: 0.576
******** [step = 200] loss: 1.999, acc: 0.575
******** [step = 250] loss: 2.007, acc: 0.574
******** [step = 300] loss: 2.015, acc: 0.573
******** [step = 350] loss: 2.022, acc: 0.573
******** [step = 400] loss: 2.030, acc: 0.572
******** [step = 450] loss: 2.036, acc: 0.571
******** [step = 500] loss: 2.040, acc: 0.571
******** [step = 550] loss: 2.043, acc: 0.570
******** [step = 600] loss: 2.045, acc: 0.570
******** [step = 650] loss: 2.050, acc: 0.570
******** [step = 700] loss: 2.053, acc: 0.569
******** [step = 750] loss: 2.057, acc: 0.569
******** [step = 800] loss: 2.060, acc: 0.568
******** [step = 850] loss: 2.063, acc: 0.568
EPOCH = 47 loss: 2.063, acc: 0.568, val_loss: 2.471, val_acc: 0.546

================================================================================2025-08_12 21:32:05
******** [step = 50] loss: 1.967, acc: 0.580
******** [step = 100] loss: 1.979, acc: 0.579
******** [step = 150] loss: 1.987, acc: 0.578
******** [step = 200] loss: 1.993, acc: 0.577
******** [step = 250] loss: 2.003, acc: 0.575
******** [step = 300] loss: 2.006, acc: 0.575
******** [step = 350] loss: 2.015, acc: 0.573
******** [step = 400] loss: 2.019, acc: 0.573
******** [step = 450] loss: 2.025, acc: 0.572
******** [step = 500] loss: 2.028, acc: 0.572
******** [step = 550] loss: 2.034, acc: 0.571
******** [step = 600] loss: 2.039, acc: 0.571
******** [step = 650] loss: 2.041, acc: 0.571
******** [step = 700] loss: 2.044, acc: 0.570
******** [step = 750] loss: 2.047, acc: 0.570
******** [step = 800] loss: 2.051, acc: 0.570
******** [step = 850] loss: 2.055, acc: 0.570
EPOCH = 48 loss: 2.055, acc: 0.570, val_loss: 2.471, val_acc: 0.547

================================================================================2025-08_12 21:34:06
******** [step = 50] loss: 2.008, acc: 0.574
******** [step = 100] loss: 1.983, acc: 0.577
******** [step = 150] loss: 1.985, acc: 0.577
******** [step = 200] loss: 1.993, acc: 0.576
******** [step = 250] loss: 1.998, acc: 0.575
******** [step = 300] loss: 2.004, acc: 0.575
******** [step = 350] loss: 2.011, acc: 0.574
******** [step = 400] loss: 2.017, acc: 0.573
******** [step = 450] loss: 2.024, acc: 0.572
******** [step = 500] loss: 2.028, acc: 0.572
******** [step = 550] loss: 2.033, acc: 0.571
******** [step = 600] loss: 2.035, acc: 0.571
******** [step = 650] loss: 2.039, acc: 0.571
******** [step = 700] loss: 2.041, acc: 0.571
******** [step = 750] loss: 2.045, acc: 0.571
******** [step = 800] loss: 2.050, acc: 0.570
******** [step = 850] loss: 2.052, acc: 0.570
EPOCH = 49 loss: 2.052, acc: 0.570, val_loss: 2.464, val_acc: 0.547

================================================================================2025-08_12 21:36:07
******** [step = 50] loss: 1.989, acc: 0.576
******** [step = 100] loss: 1.988, acc: 0.578
******** [step = 150] loss: 1.991, acc: 0.576
******** [step = 200] loss: 1.991, acc: 0.577
******** [step = 250] loss: 1.999, acc: 0.576
******** [step = 300] loss: 2.003, acc: 0.575
******** [step = 350] loss: 2.008, acc: 0.574
******** [step = 400] loss: 2.012, acc: 0.574
******** [step = 450] loss: 2.016, acc: 0.574
******** [step = 500] loss: 2.019, acc: 0.573
******** [step = 550] loss: 2.025, acc: 0.573
******** [step = 600] loss: 2.029, acc: 0.572
******** [step = 650] loss: 2.034, acc: 0.572
******** [step = 700] loss: 2.038, acc: 0.571
******** [step = 750] loss: 2.042, acc: 0.571
******** [step = 800] loss: 2.044, acc: 0.571
******** [step = 850] loss: 2.047, acc: 0.571
EPOCH = 50 loss: 2.047, acc: 0.571, val_loss: 2.467, val_acc: 0.548

================================================================================2025-08_12 21:38:07
******** [step = 50] loss: 1.976, acc: 0.576
******** [step = 100] loss: 1.981, acc: 0.578
******** [step = 150] loss: 1.984, acc: 0.578
******** [step = 200] loss: 1.991, acc: 0.577
******** [step = 250] loss: 1.995, acc: 0.577
******** [step = 300] loss: 1.999, acc: 0.576
******** [step = 350] loss: 2.006, acc: 0.576
******** [step = 400] loss: 2.010, acc: 0.575
******** [step = 450] loss: 2.015, acc: 0.575
******** [step = 500] loss: 2.019, acc: 0.574
******** [step = 550] loss: 2.023, acc: 0.574
******** [step = 600] loss: 2.025, acc: 0.574
******** [step = 650] loss: 2.030, acc: 0.573
******** [step = 700] loss: 2.034, acc: 0.573
******** [step = 750] loss: 2.037, acc: 0.572
******** [step = 800] loss: 2.040, acc: 0.572
******** [step = 850] loss: 2.042, acc: 0.572
EPOCH = 51 loss: 2.042, acc: 0.572, val_loss: 2.467, val_acc: 0.548

================================================================================2025-08_12 21:40:08
******** [step = 50] loss: 1.961, acc: 0.580
******** [step = 100] loss: 1.969, acc: 0.580
******** [step = 150] loss: 1.976, acc: 0.578
******** [step = 200] loss: 1.979, acc: 0.578
******** [step = 250] loss: 1.988, acc: 0.577
******** [step = 300] loss: 1.990, acc: 0.577
******** [step = 350] loss: 1.993, acc: 0.576
******** [step = 400] loss: 1.998, acc: 0.576
******** [step = 450] loss: 2.003, acc: 0.575
******** [step = 500] loss: 2.009, acc: 0.575
******** [step = 550] loss: 2.014, acc: 0.574
******** [step = 600] loss: 2.019, acc: 0.573
******** [step = 650] loss: 2.023, acc: 0.573
******** [step = 700] loss: 2.026, acc: 0.573
******** [step = 750] loss: 2.030, acc: 0.573
******** [step = 800] loss: 2.034, acc: 0.572
******** [step = 850] loss: 2.036, acc: 0.572
EPOCH = 52 loss: 2.036, acc: 0.572, val_loss: 2.468, val_acc: 0.548

================================================================================2025-08_12 21:42:08
******** [step = 50] loss: 1.969, acc: 0.579
******** [step = 100] loss: 1.974, acc: 0.579
******** [step = 150] loss: 1.978, acc: 0.578
******** [step = 200] loss: 1.980, acc: 0.578
******** [step = 250] loss: 1.982, acc: 0.578
******** [step = 300] loss: 1.985, acc: 0.578
******** [step = 350] loss: 1.992, acc: 0.577
******** [step = 400] loss: 1.996, acc: 0.577
******** [step = 450] loss: 2.000, acc: 0.576
******** [step = 500] loss: 2.004, acc: 0.576
******** [step = 550] loss: 2.007, acc: 0.576
******** [step = 600] loss: 2.011, acc: 0.575
******** [step = 650] loss: 2.017, acc: 0.574
******** [step = 700] loss: 2.020, acc: 0.574
******** [step = 750] loss: 2.025, acc: 0.574
******** [step = 800] loss: 2.029, acc: 0.573
******** [step = 850] loss: 2.033, acc: 0.573
EPOCH = 53 loss: 2.033, acc: 0.573, val_loss: 2.461, val_acc: 0.549

================================================================================2025-08_12 21:44:09
******** [step = 50] loss: 1.941, acc: 0.587
******** [step = 100] loss: 1.950, acc: 0.583
******** [step = 150] loss: 1.957, acc: 0.581
******** [step = 200] loss: 1.969, acc: 0.579
******** [step = 250] loss: 1.976, acc: 0.578
******** [step = 300] loss: 1.982, acc: 0.578
******** [step = 350] loss: 1.988, acc: 0.577
******** [step = 400] loss: 1.991, acc: 0.577
******** [step = 450] loss: 1.996, acc: 0.577
******** [step = 500] loss: 2.000, acc: 0.576
******** [step = 550] loss: 2.005, acc: 0.576
******** [step = 600] loss: 2.009, acc: 0.575
******** [step = 650] loss: 2.014, acc: 0.575
******** [step = 700] loss: 2.017, acc: 0.575
******** [step = 750] loss: 2.022, acc: 0.574
******** [step = 800] loss: 2.026, acc: 0.574
******** [step = 850] loss: 2.027, acc: 0.574
EPOCH = 54 loss: 2.027, acc: 0.574, val_loss: 2.459, val_acc: 0.549

================================================================================2025-08_12 21:46:09
******** [step = 50] loss: 1.967, acc: 0.578
******** [step = 100] loss: 1.965, acc: 0.580
******** [step = 150] loss: 1.968, acc: 0.580
******** [step = 200] loss: 1.970, acc: 0.579
******** [step = 250] loss: 1.975, acc: 0.579
******** [step = 300] loss: 1.980, acc: 0.578
******** [step = 350] loss: 1.983, acc: 0.578
******** [step = 400] loss: 1.988, acc: 0.578
******** [step = 450] loss: 1.994, acc: 0.577
******** [step = 500] loss: 1.998, acc: 0.577
******** [step = 550] loss: 2.002, acc: 0.577
******** [step = 600] loss: 2.006, acc: 0.576
******** [step = 650] loss: 2.008, acc: 0.576
******** [step = 700] loss: 2.012, acc: 0.576
******** [step = 750] loss: 2.017, acc: 0.575
******** [step = 800] loss: 2.019, acc: 0.575
******** [step = 850] loss: 2.022, acc: 0.575
EPOCH = 55 loss: 2.022, acc: 0.575, val_loss: 2.457, val_acc: 0.549

================================================================================2025-08_12 21:48:10
******** [step = 50] loss: 1.936, acc: 0.584
******** [step = 100] loss: 1.937, acc: 0.585
******** [step = 150] loss: 1.951, acc: 0.583
******** [step = 200] loss: 1.961, acc: 0.582
******** [step = 250] loss: 1.969, acc: 0.581
******** [step = 300] loss: 1.974, acc: 0.580
******** [step = 350] loss: 1.980, acc: 0.579
******** [step = 400] loss: 1.984, acc: 0.579
******** [step = 450] loss: 1.989, acc: 0.578
******** [step = 500] loss: 1.992, acc: 0.578
******** [step = 550] loss: 1.998, acc: 0.577
******** [step = 600] loss: 2.002, acc: 0.577
******** [step = 650] loss: 2.006, acc: 0.576
******** [step = 700] loss: 2.008, acc: 0.576
******** [step = 750] loss: 2.013, acc: 0.576
******** [step = 800] loss: 2.016, acc: 0.576
******** [step = 850] loss: 2.018, acc: 0.575
EPOCH = 56 loss: 2.018, acc: 0.575, val_loss: 2.462, val_acc: 0.550

================================================================================2025-08_12 21:50:11
******** [step = 50] loss: 1.943, acc: 0.581
******** [step = 100] loss: 1.946, acc: 0.583
******** [step = 150] loss: 1.951, acc: 0.583
******** [step = 200] loss: 1.955, acc: 0.582
******** [step = 250] loss: 1.960, acc: 0.581
******** [step = 300] loss: 1.967, acc: 0.581
******** [step = 350] loss: 1.971, acc: 0.580
******** [step = 400] loss: 1.976, acc: 0.580
******** [step = 450] loss: 1.980, acc: 0.579
******** [step = 500] loss: 1.986, acc: 0.579
******** [step = 550] loss: 1.991, acc: 0.578
******** [step = 600] loss: 1.995, acc: 0.578
******** [step = 650] loss: 2.000, acc: 0.577
******** [step = 700] loss: 2.005, acc: 0.577
******** [step = 750] loss: 2.009, acc: 0.576
******** [step = 800] loss: 2.011, acc: 0.576
******** [step = 850] loss: 2.014, acc: 0.576
EPOCH = 57 loss: 2.014, acc: 0.576, val_loss: 2.472, val_acc: 0.548

================================================================================2025-08_12 21:52:11
******** [step = 50] loss: 1.948, acc: 0.580
******** [step = 100] loss: 1.947, acc: 0.582
******** [step = 150] loss: 1.949, acc: 0.582
******** [step = 200] loss: 1.959, acc: 0.581
******** [step = 250] loss: 1.963, acc: 0.580
******** [step = 300] loss: 1.968, acc: 0.579
******** [step = 350] loss: 1.973, acc: 0.579
******** [step = 400] loss: 1.976, acc: 0.579
******** [step = 450] loss: 1.980, acc: 0.579
******** [step = 500] loss: 1.985, acc: 0.579
******** [step = 550] loss: 1.987, acc: 0.579
******** [step = 600] loss: 1.990, acc: 0.578
******** [step = 650] loss: 1.994, acc: 0.578
******** [step = 700] loss: 2.000, acc: 0.577
******** [step = 750] loss: 2.002, acc: 0.577
******** [step = 800] loss: 2.007, acc: 0.577
******** [step = 850] loss: 2.011, acc: 0.576
EPOCH = 58 loss: 2.011, acc: 0.576, val_loss: 2.455, val_acc: 0.551

================================================================================2025-08_12 21:54:12
******** [step = 50] loss: 1.931, acc: 0.587
******** [step = 100] loss: 1.931, acc: 0.586
******** [step = 150] loss: 1.940, acc: 0.585
******** [step = 200] loss: 1.944, acc: 0.585
******** [step = 250] loss: 1.951, acc: 0.584
******** [step = 300] loss: 1.958, acc: 0.583
******** [step = 350] loss: 1.964, acc: 0.582
******** [step = 400] loss: 1.971, acc: 0.581
******** [step = 450] loss: 1.976, acc: 0.581
******** [step = 500] loss: 1.980, acc: 0.580
******** [step = 550] loss: 1.984, acc: 0.580
******** [step = 600] loss: 1.989, acc: 0.579
******** [step = 650] loss: 1.993, acc: 0.579
******** [step = 700] loss: 1.996, acc: 0.578
******** [step = 750] loss: 2.000, acc: 0.578
******** [step = 800] loss: 2.003, acc: 0.577
******** [step = 850] loss: 2.005, acc: 0.577
EPOCH = 59 loss: 2.005, acc: 0.577, val_loss: 2.454, val_acc: 0.551

================================================================================2025-08_12 21:56:12
******** [step = 50] loss: 1.931, acc: 0.587
******** [step = 100] loss: 1.937, acc: 0.586
******** [step = 150] loss: 1.939, acc: 0.585
******** [step = 200] loss: 1.945, acc: 0.585
******** [step = 250] loss: 1.951, acc: 0.584
******** [step = 300] loss: 1.956, acc: 0.583
******** [step = 350] loss: 1.960, acc: 0.583
******** [step = 400] loss: 1.966, acc: 0.582
******** [step = 450] loss: 1.968, acc: 0.582
******** [step = 500] loss: 1.973, acc: 0.581
******** [step = 550] loss: 1.977, acc: 0.581
******** [step = 600] loss: 1.983, acc: 0.580
******** [step = 650] loss: 1.987, acc: 0.579
******** [step = 700] loss: 1.989, acc: 0.579
******** [step = 750] loss: 1.992, acc: 0.579
******** [step = 800] loss: 1.996, acc: 0.579
******** [step = 850] loss: 2.001, acc: 0.578
EPOCH = 60 loss: 2.001, acc: 0.578, val_loss: 2.453, val_acc: 0.551

================================================================================2025-08_12 21:58:13
******** [step = 50] loss: 1.921, acc: 0.585
******** [step = 100] loss: 1.932, acc: 0.585
******** [step = 150] loss: 1.939, acc: 0.585
******** [step = 200] loss: 1.944, acc: 0.584
******** [step = 250] loss: 1.948, acc: 0.584
******** [step = 300] loss: 1.954, acc: 0.583
******** [step = 350] loss: 1.960, acc: 0.582
******** [step = 400] loss: 1.963, acc: 0.582
******** [step = 450] loss: 1.966, acc: 0.581
******** [step = 500] loss: 1.968, acc: 0.581
******** [step = 550] loss: 1.970, acc: 0.581
******** [step = 600] loss: 1.976, acc: 0.580
******** [step = 650] loss: 1.982, acc: 0.580
******** [step = 700] loss: 1.985, acc: 0.580
******** [step = 750] loss: 1.989, acc: 0.579
******** [step = 800] loss: 1.992, acc: 0.579
******** [step = 850] loss: 1.996, acc: 0.579
EPOCH = 61 loss: 1.996, acc: 0.579, val_loss: 2.454, val_acc: 0.552

================================================================================2025-08_12 22:00:13
******** [step = 50] loss: 1.907, acc: 0.588
******** [step = 100] loss: 1.918, acc: 0.587
******** [step = 150] loss: 1.927, acc: 0.586
******** [step = 200] loss: 1.935, acc: 0.585
******** [step = 250] loss: 1.942, acc: 0.584
******** [step = 300] loss: 1.949, acc: 0.583
******** [step = 350] loss: 1.954, acc: 0.583
******** [step = 400] loss: 1.958, acc: 0.582
******** [step = 450] loss: 1.964, acc: 0.581
******** [step = 500] loss: 1.970, acc: 0.581
******** [step = 550] loss: 1.975, acc: 0.580
******** [step = 600] loss: 1.980, acc: 0.580
******** [step = 650] loss: 1.984, acc: 0.580
******** [step = 700] loss: 1.987, acc: 0.579
******** [step = 750] loss: 1.989, acc: 0.579
******** [step = 800] loss: 1.991, acc: 0.579
******** [step = 850] loss: 1.993, acc: 0.579
EPOCH = 62 loss: 1.993, acc: 0.579, val_loss: 2.447, val_acc: 0.552

================================================================================2025-08_12 22:02:14
******** [step = 50] loss: 1.913, acc: 0.585
******** [step = 100] loss: 1.918, acc: 0.587
******** [step = 150] loss: 1.925, acc: 0.587
******** [step = 200] loss: 1.927, acc: 0.587
******** [step = 250] loss: 1.933, acc: 0.586
******** [step = 300] loss: 1.940, acc: 0.585
******** [step = 350] loss: 1.946, acc: 0.585
******** [step = 400] loss: 1.955, acc: 0.584
******** [step = 450] loss: 1.959, acc: 0.584
******** [step = 500] loss: 1.962, acc: 0.583
******** [step = 550] loss: 1.967, acc: 0.583
******** [step = 600] loss: 1.971, acc: 0.582
******** [step = 650] loss: 1.975, acc: 0.582
******** [step = 700] loss: 1.979, acc: 0.581
******** [step = 750] loss: 1.984, acc: 0.581
******** [step = 800] loss: 1.988, acc: 0.581
******** [step = 850] loss: 1.990, acc: 0.580
EPOCH = 63 loss: 1.990, acc: 0.580, val_loss: 2.450, val_acc: 0.553

================================================================================2025-08_12 22:04:15
******** [step = 50] loss: 1.934, acc: 0.585
******** [step = 100] loss: 1.924, acc: 0.587
******** [step = 150] loss: 1.932, acc: 0.587
******** [step = 200] loss: 1.935, acc: 0.586
******** [step = 250] loss: 1.939, acc: 0.586
******** [step = 300] loss: 1.944, acc: 0.585
******** [step = 350] loss: 1.948, acc: 0.584
******** [step = 400] loss: 1.952, acc: 0.584
******** [step = 450] loss: 1.954, acc: 0.584
******** [step = 500] loss: 1.959, acc: 0.583
******** [step = 550] loss: 1.963, acc: 0.583
******** [step = 600] loss: 1.969, acc: 0.582
******** [step = 650] loss: 1.973, acc: 0.582
******** [step = 700] loss: 1.976, acc: 0.581
******** [step = 750] loss: 1.978, acc: 0.581
******** [step = 800] loss: 1.983, acc: 0.581
******** [step = 850] loss: 1.986, acc: 0.581
EPOCH = 64 loss: 1.986, acc: 0.581, val_loss: 2.448, val_acc: 0.552

================================================================================2025-08_12 22:06:15
******** [step = 50] loss: 1.920, acc: 0.589
******** [step = 100] loss: 1.922, acc: 0.588
******** [step = 150] loss: 1.927, acc: 0.587
******** [step = 200] loss: 1.927, acc: 0.586
******** [step = 250] loss: 1.935, acc: 0.586
******** [step = 300] loss: 1.938, acc: 0.585
******** [step = 350] loss: 1.942, acc: 0.585
******** [step = 400] loss: 1.947, acc: 0.585
******** [step = 450] loss: 1.951, acc: 0.584
******** [step = 500] loss: 1.955, acc: 0.584
******** [step = 550] loss: 1.961, acc: 0.583
******** [step = 600] loss: 1.966, acc: 0.582
******** [step = 650] loss: 1.969, acc: 0.582
******** [step = 700] loss: 1.972, acc: 0.582
******** [step = 750] loss: 1.976, acc: 0.582
******** [step = 800] loss: 1.979, acc: 0.581
******** [step = 850] loss: 1.982, acc: 0.581
EPOCH = 65 loss: 1.982, acc: 0.581, val_loss: 2.445, val_acc: 0.553

================================================================================2025-08_12 22:08:15
******** [step = 50] loss: 1.908, acc: 0.592
******** [step = 100] loss: 1.916, acc: 0.590
******** [step = 150] loss: 1.913, acc: 0.590
******** [step = 200] loss: 1.914, acc: 0.589
******** [step = 250] loss: 1.922, acc: 0.588
******** [step = 300] loss: 1.928, acc: 0.586
******** [step = 350] loss: 1.932, acc: 0.586
******** [step = 400] loss: 1.935, acc: 0.586
******** [step = 450] loss: 1.941, acc: 0.585
******** [step = 500] loss: 1.946, acc: 0.585
******** [step = 550] loss: 1.952, acc: 0.584
******** [step = 600] loss: 1.956, acc: 0.584
******** [step = 650] loss: 1.961, acc: 0.583
******** [step = 700] loss: 1.965, acc: 0.583
******** [step = 750] loss: 1.970, acc: 0.583
******** [step = 800] loss: 1.973, acc: 0.582
******** [step = 850] loss: 1.981, acc: 0.582
EPOCH = 66 loss: 1.981, acc: 0.582, val_loss: 2.452, val_acc: 0.553

================================================================================2025-08_12 22:10:15
******** [step = 50] loss: 1.895, acc: 0.591
******** [step = 100] loss: 1.904, acc: 0.591
******** [step = 150] loss: 1.907, acc: 0.589
******** [step = 200] loss: 1.913, acc: 0.589
******** [step = 250] loss: 1.921, acc: 0.587
******** [step = 300] loss: 1.925, acc: 0.587
******** [step = 350] loss: 1.930, acc: 0.586
******** [step = 400] loss: 1.936, acc: 0.586
******** [step = 450] loss: 1.940, acc: 0.586
******** [step = 500] loss: 1.946, acc: 0.585
******** [step = 550] loss: 1.950, acc: 0.585
******** [step = 600] loss: 1.954, acc: 0.584
******** [step = 650] loss: 1.957, acc: 0.584
******** [step = 700] loss: 1.961, acc: 0.584
******** [step = 750] loss: 1.966, acc: 0.583
******** [step = 800] loss: 1.971, acc: 0.583
******** [step = 850] loss: 1.973, acc: 0.583
EPOCH = 67 loss: 1.973, acc: 0.583, val_loss: 2.451, val_acc: 0.554

================================================================================2025-08_12 22:12:16
******** [step = 50] loss: 1.912, acc: 0.592
******** [step = 100] loss: 1.913, acc: 0.591
******** [step = 150] loss: 1.913, acc: 0.591
******** [step = 200] loss: 1.919, acc: 0.590
******** [step = 250] loss: 1.920, acc: 0.589
******** [step = 300] loss: 1.928, acc: 0.588
******** [step = 350] loss: 1.933, acc: 0.587
******** [step = 400] loss: 1.939, acc: 0.587
******** [step = 450] loss: 1.940, acc: 0.586
******** [step = 500] loss: 1.946, acc: 0.586
******** [step = 550] loss: 1.949, acc: 0.586
******** [step = 600] loss: 1.954, acc: 0.585
******** [step = 650] loss: 1.957, acc: 0.585
******** [step = 700] loss: 1.961, acc: 0.584
******** [step = 750] loss: 1.966, acc: 0.584
******** [step = 800] loss: 1.969, acc: 0.584
******** [step = 850] loss: 1.971, acc: 0.584
EPOCH = 68 loss: 1.971, acc: 0.584, val_loss: 2.439, val_acc: 0.554

================================================================================2025-08_12 22:14:16
******** [step = 50] loss: 1.879, acc: 0.595
******** [step = 100] loss: 1.887, acc: 0.593
******** [step = 150] loss: 1.898, acc: 0.591
******** [step = 200] loss: 1.907, acc: 0.590
******** [step = 250] loss: 1.911, acc: 0.590
******** [step = 300] loss: 1.917, acc: 0.589
******** [step = 350] loss: 1.921, acc: 0.589
******** [step = 400] loss: 1.927, acc: 0.588
******** [step = 450] loss: 1.931, acc: 0.587
******** [step = 500] loss: 1.937, acc: 0.587
******** [step = 550] loss: 1.940, acc: 0.586
******** [step = 600] loss: 1.945, acc: 0.586
******** [step = 650] loss: 1.950, acc: 0.585
******** [step = 700] loss: 1.955, acc: 0.585
******** [step = 750] loss: 1.959, acc: 0.584
******** [step = 800] loss: 1.963, acc: 0.584
******** [step = 850] loss: 1.966, acc: 0.584
EPOCH = 69 loss: 1.966, acc: 0.584, val_loss: 2.444, val_acc: 0.555

================================================================================2025-08_12 22:16:17
******** [step = 50] loss: 1.894, acc: 0.592
******** [step = 100] loss: 1.907, acc: 0.590
******** [step = 150] loss: 1.908, acc: 0.590
******** [step = 200] loss: 1.914, acc: 0.589
******** [step = 250] loss: 1.919, acc: 0.589
******** [step = 300] loss: 1.925, acc: 0.588
******** [step = 350] loss: 1.927, acc: 0.588
******** [step = 400] loss: 1.933, acc: 0.587
******** [step = 450] loss: 1.936, acc: 0.587
******** [step = 500] loss: 1.939, acc: 0.587
******** [step = 550] loss: 1.942, acc: 0.587
******** [step = 600] loss: 1.945, acc: 0.586
******** [step = 650] loss: 1.948, acc: 0.586
******** [step = 700] loss: 1.953, acc: 0.585
******** [step = 750] loss: 1.956, acc: 0.585
******** [step = 800] loss: 1.960, acc: 0.585
******** [step = 850] loss: 1.963, acc: 0.585
EPOCH = 70 loss: 1.963, acc: 0.585, val_loss: 2.439, val_acc: 0.556

================================================================================2025-08_12 22:18:18
******** [step = 50] loss: 1.881, acc: 0.592
******** [step = 100] loss: 1.880, acc: 0.594
******** [step = 150] loss: 1.892, acc: 0.592
******** [step = 200] loss: 1.894, acc: 0.592
******** [step = 250] loss: 1.901, acc: 0.591
******** [step = 300] loss: 1.907, acc: 0.591
******** [step = 350] loss: 1.912, acc: 0.590
******** [step = 400] loss: 1.920, acc: 0.589
******** [step = 450] loss: 1.923, acc: 0.589
******** [step = 500] loss: 1.927, acc: 0.588
******** [step = 550] loss: 1.932, acc: 0.588
******** [step = 600] loss: 1.936, acc: 0.587
******** [step = 650] loss: 1.940, acc: 0.587
******** [step = 700] loss: 1.946, acc: 0.586
******** [step = 750] loss: 1.951, acc: 0.586
******** [step = 800] loss: 1.955, acc: 0.586
******** [step = 850] loss: 1.959, acc: 0.585
EPOCH = 71 loss: 1.959, acc: 0.585, val_loss: 2.432, val_acc: 0.556

================================================================================2025-08_12 22:20:18
******** [step = 50] loss: 1.883, acc: 0.593
******** [step = 100] loss: 1.881, acc: 0.594
******** [step = 150] loss: 1.886, acc: 0.594
******** [step = 200] loss: 1.891, acc: 0.593
******** [step = 250] loss: 1.897, acc: 0.592
******** [step = 300] loss: 1.904, acc: 0.591
******** [step = 350] loss: 1.910, acc: 0.590
******** [step = 400] loss: 1.917, acc: 0.590
******** [step = 450] loss: 1.923, acc: 0.589
******** [step = 500] loss: 1.929, acc: 0.588
******** [step = 550] loss: 1.935, acc: 0.588
******** [step = 600] loss: 1.937, acc: 0.588
******** [step = 650] loss: 1.941, acc: 0.587
******** [step = 700] loss: 1.946, acc: 0.587
******** [step = 750] loss: 1.948, acc: 0.587
******** [step = 800] loss: 1.952, acc: 0.586
******** [step = 850] loss: 1.955, acc: 0.586
EPOCH = 72 loss: 1.955, acc: 0.586, val_loss: 2.438, val_acc: 0.556

================================================================================2025-08_12 22:22:19
******** [step = 50] loss: 1.863, acc: 0.594
******** [step = 100] loss: 1.868, acc: 0.594
******** [step = 150] loss: 1.881, acc: 0.593
******** [step = 200] loss: 1.891, acc: 0.592
******** [step = 250] loss: 1.895, acc: 0.592
******** [step = 300] loss: 1.902, acc: 0.591
******** [step = 350] loss: 1.911, acc: 0.590
******** [step = 400] loss: 1.918, acc: 0.589
******** [step = 450] loss: 1.921, acc: 0.588
******** [step = 500] loss: 1.927, acc: 0.588
******** [step = 550] loss: 1.933, acc: 0.587
******** [step = 600] loss: 1.936, acc: 0.587
******** [step = 650] loss: 1.940, acc: 0.587
******** [step = 700] loss: 1.942, acc: 0.587
******** [step = 750] loss: 1.946, acc: 0.586
******** [step = 800] loss: 1.950, acc: 0.586
******** [step = 850] loss: 1.953, acc: 0.586
EPOCH = 73 loss: 1.953, acc: 0.586, val_loss: 2.432, val_acc: 0.556

================================================================================2025-08_12 22:24:19
******** [step = 50] loss: 1.881, acc: 0.593
******** [step = 100] loss: 1.880, acc: 0.592
******** [step = 150] loss: 1.882, acc: 0.592
******** [step = 200] loss: 1.894, acc: 0.591
******** [step = 250] loss: 1.901, acc: 0.590
******** [step = 300] loss: 1.908, acc: 0.590
******** [step = 350] loss: 1.910, acc: 0.590
******** [step = 400] loss: 1.916, acc: 0.589
******** [step = 450] loss: 1.919, acc: 0.589
******** [step = 500] loss: 1.925, acc: 0.588
******** [step = 550] loss: 1.928, acc: 0.588
******** [step = 600] loss: 1.932, acc: 0.588
******** [step = 650] loss: 1.936, acc: 0.588
******** [step = 700] loss: 1.941, acc: 0.587
******** [step = 750] loss: 1.945, acc: 0.587
******** [step = 800] loss: 1.949, acc: 0.586
******** [step = 850] loss: 1.949, acc: 0.587
EPOCH = 74 loss: 1.949, acc: 0.587, val_loss: 2.431, val_acc: 0.557

================================================================================2025-08_12 22:26:19
******** [step = 50] loss: 1.868, acc: 0.597
******** [step = 100] loss: 1.885, acc: 0.593
******** [step = 150] loss: 1.882, acc: 0.593
******** [step = 200] loss: 1.888, acc: 0.592
******** [step = 250] loss: 1.895, acc: 0.592
******** [step = 300] loss: 1.901, acc: 0.591
******** [step = 350] loss: 1.903, acc: 0.591
******** [step = 400] loss: 1.907, acc: 0.590
******** [step = 450] loss: 1.912, acc: 0.590
******** [step = 500] loss: 1.917, acc: 0.590
******** [step = 550] loss: 1.922, acc: 0.589
******** [step = 600] loss: 1.927, acc: 0.589
******** [step = 650] loss: 1.932, acc: 0.588
******** [step = 700] loss: 1.935, acc: 0.588
******** [step = 750] loss: 1.939, acc: 0.588
******** [step = 800] loss: 1.943, acc: 0.587
******** [step = 850] loss: 1.946, acc: 0.587
EPOCH = 75 loss: 1.946, acc: 0.587, val_loss: 2.431, val_acc: 0.556

================================================================================2025-08_12 22:28:20
******** [step = 50] loss: 1.862, acc: 0.597
******** [step = 100] loss: 1.869, acc: 0.595
******** [step = 150] loss: 1.881, acc: 0.593
******** [step = 200] loss: 1.886, acc: 0.593
******** [step = 250] loss: 1.891, acc: 0.592
******** [step = 300] loss: 1.898, acc: 0.591
******** [step = 350] loss: 1.904, acc: 0.591
******** [step = 400] loss: 1.909, acc: 0.590
******** [step = 450] loss: 1.914, acc: 0.590
******** [step = 500] loss: 1.918, acc: 0.590
******** [step = 550] loss: 1.923, acc: 0.589
******** [step = 600] loss: 1.926, acc: 0.589
******** [step = 650] loss: 1.929, acc: 0.589
******** [step = 700] loss: 1.932, acc: 0.589
******** [step = 750] loss: 1.936, acc: 0.588
******** [step = 800] loss: 1.940, acc: 0.588
******** [step = 850] loss: 1.944, acc: 0.588
EPOCH = 76 loss: 1.944, acc: 0.588, val_loss: 2.431, val_acc: 0.557

================================================================================2025-08_12 22:30:20
******** [step = 50] loss: 1.865, acc: 0.597
******** [step = 100] loss: 1.871, acc: 0.596
******** [step = 150] loss: 1.877, acc: 0.595
******** [step = 200] loss: 1.885, acc: 0.594
******** [step = 250] loss: 1.890, acc: 0.594
******** [step = 300] loss: 1.895, acc: 0.593
******** [step = 350] loss: 1.901, acc: 0.592
******** [step = 400] loss: 1.907, acc: 0.591
******** [step = 450] loss: 1.915, acc: 0.590
******** [step = 500] loss: 1.917, acc: 0.589
******** [step = 550] loss: 1.921, acc: 0.589
******** [step = 600] loss: 1.926, acc: 0.588
******** [step = 650] loss: 1.932, acc: 0.588
******** [step = 700] loss: 1.934, acc: 0.588
******** [step = 750] loss: 1.938, acc: 0.587
******** [step = 800] loss: 1.941, acc: 0.587
******** [step = 850] loss: 1.944, acc: 0.587
EPOCH = 77 loss: 1.944, acc: 0.587, val_loss: 2.436, val_acc: 0.556

================================================================================2025-08_12 22:32:21
******** [step = 50] loss: 1.871, acc: 0.597
******** [step = 100] loss: 1.866, acc: 0.598
******** [step = 150] loss: 1.875, acc: 0.596
******** [step = 200] loss: 1.877, acc: 0.595
******** [step = 250] loss: 1.887, acc: 0.593
******** [step = 300] loss: 1.895, acc: 0.592
******** [step = 350] loss: 1.900, acc: 0.591
******** [step = 400] loss: 1.904, acc: 0.591
******** [step = 450] loss: 1.909, acc: 0.591
******** [step = 500] loss: 1.915, acc: 0.590
******** [step = 550] loss: 1.919, acc: 0.590
******** [step = 600] loss: 1.923, acc: 0.589
******** [step = 650] loss: 1.925, acc: 0.589
******** [step = 700] loss: 1.929, acc: 0.589
******** [step = 750] loss: 1.933, acc: 0.588
******** [step = 800] loss: 1.937, acc: 0.588
******** [step = 850] loss: 1.940, acc: 0.588
EPOCH = 78 loss: 1.940, acc: 0.588, val_loss: 2.433, val_acc: 0.557

================================================================================2025-08_12 22:34:21
******** [step = 50] loss: 1.857, acc: 0.597
******** [step = 100] loss: 1.868, acc: 0.596
******** [step = 150] loss: 1.876, acc: 0.596
******** [step = 200] loss: 1.882, acc: 0.595
******** [step = 250] loss: 1.884, acc: 0.594
******** [step = 300] loss: 1.891, acc: 0.593
******** [step = 350] loss: 1.898, acc: 0.593
******** [step = 400] loss: 1.902, acc: 0.592
******** [step = 450] loss: 1.904, acc: 0.592
******** [step = 500] loss: 1.908, acc: 0.592
******** [step = 550] loss: 1.912, acc: 0.591
******** [step = 600] loss: 1.915, acc: 0.591
******** [step = 650] loss: 1.920, acc: 0.591
******** [step = 700] loss: 1.924, acc: 0.590
******** [step = 750] loss: 1.929, acc: 0.590
******** [step = 800] loss: 1.933, acc: 0.589
******** [step = 850] loss: 1.940, acc: 0.589
EPOCH = 79 loss: 1.940, acc: 0.589, val_loss: 2.427, val_acc: 0.557

================================================================================2025-08_12 22:36:21
******** [step = 50] loss: 1.850, acc: 0.598
******** [step = 100] loss: 1.851, acc: 0.599
******** [step = 150] loss: 1.858, acc: 0.598
******** [step = 200] loss: 1.864, acc: 0.597
******** [step = 250] loss: 1.872, acc: 0.596
******** [step = 300] loss: 1.878, acc: 0.595
******** [step = 350] loss: 1.885, acc: 0.594
******** [step = 400] loss: 1.891, acc: 0.594
******** [step = 450] loss: 1.896, acc: 0.593
******** [step = 500] loss: 1.899, acc: 0.593
******** [step = 550] loss: 1.902, acc: 0.592
******** [step = 600] loss: 1.907, acc: 0.592
******** [step = 650] loss: 1.912, acc: 0.591
******** [step = 700] loss: 1.917, acc: 0.591
******** [step = 750] loss: 1.922, acc: 0.591
******** [step = 800] loss: 1.927, acc: 0.590
******** [step = 850] loss: 1.932, acc: 0.589
EPOCH = 80 loss: 1.932, acc: 0.589, val_loss: 2.424, val_acc: 0.559

================================================================================2025-08_12 22:38:22
finishing training...
Training complete in 161m 16s
    epoch  ...   val_acc
0     1.0  ...  0.366745
1     2.0  ...  0.403249
2     3.0  ...  0.422480
3     4.0  ...  0.438210
4     5.0  ...  0.443645
..    ...  ...       ...
75   76.0  ...  0.557405
76   77.0  ...  0.556130
77   78.0  ...  0.557369
78   79.0  ...  0.557418
79   80.0  ...  0.558598

[80 rows x 5 columns]
== Done ==
Tue Aug 12 10:38:44 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-12-2025 22:38:44
Job ID:        6981877
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=10:50:12,vmem=0,walltime=02:42:33,mem=37032K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-009-35-0
---------------------------------------
