---------------------------------------
Begin Slurm Prolog: Aug-12-2025 11:18:47
Job ID:    6954108
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Tue Aug 12 11:18:47 AM EDT 2025
atl1-1-02-011-34-0.pace.gatech.edu
== GPU check ==
Tue Aug 12 11:18:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   45C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 102675.34it/s] 29%|██▊       | 38733/135842 [00:00<00:00, 156848.23it/s] 41%|████      | 55844/135842 [00:00<00:00, 121200.01it/s] 51%|█████     | 69378/135842 [00:00<00:00, 95198.18it/s]  66%|██████▌   | 89562/135842 [00:00<00:00, 121266.91it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 91597.12it/s] 92%|█████████▏| 125637/135842 [00:01<00:00, 110292.80it/s]100%|██████████| 135842/135842 [00:01<00:00, 112832.71it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47968.51it/s] 33%|███▎      | 35758/108673 [00:00<00:00, 86305.78it/s] 50%|████▉     | 54130/108673 [00:00<00:00, 114320.40it/s] 67%|██████▋   | 72590/108673 [00:00<00:00, 134780.88it/s] 82%|████████▏ | 89156/108673 [00:01<00:00, 72656.24it/s]  99%|█████████▉| 107599/108673 [00:01<00:00, 92204.79it/s]100%|██████████| 108673/108673 [00:01<00:00, 90421.69it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 67%|██████▋   | 18189/27169 [00:00<00:00, 181875.86it/s]100%|██████████| 27169/27169 [00:00<00:00, 183198.04it/s]
tensor([    3,    16,   856,     6,  3235,    29,  1470,  2514,    26,    28,
         1542,    25,   224,  2356,    33, 16305,    16,  1798,     7,    18,
          818,     4,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,   80,   15,  803,   10,    9,   22,  213,  157,   13,   47,   16,
         128,  130,  481,    7,  557,   13,   24, 1917,  577,    8,    2,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,   74,   22,   11,    6, 1114,  276,   10,  279,  189,  230,  288,
         884,   78,   25,   29,  133,    9,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_12 11:22:03
******** [step = 50] loss: 9.371, acc: 0.018
******** [step = 100] loss: 8.962, acc: 0.080
******** [step = 150] loss: 8.617, acc: 0.123
******** [step = 200] loss: 8.294, acc: 0.154
******** [step = 250] loss: 7.986, acc: 0.173
******** [step = 300] loss: 7.685, acc: 0.185
******** [step = 350] loss: 7.387, acc: 0.196
******** [step = 400] loss: 7.108, acc: 0.207
******** [step = 450] loss: 6.859, acc: 0.218
******** [step = 500] loss: 6.640, acc: 0.228
******** [step = 550] loss: 6.447, acc: 0.238
******** [step = 600] loss: 6.276, acc: 0.246
******** [step = 650] loss: 6.123, acc: 0.254
******** [step = 700] loss: 5.984, acc: 0.262
******** [step = 750] loss: 5.857, acc: 0.268
******** [step = 800] loss: 5.740, acc: 0.275
******** [step = 850] loss: 5.632, acc: 0.280
EPOCH = 1 loss: 5.632, acc: 0.280, val_loss: 3.811, val_acc: 0.390

================================================================================2025-08_12 11:24:08
******** [step = 50] loss: 3.891, acc: 0.372
******** [step = 100] loss: 3.854, acc: 0.374
******** [step = 150] loss: 3.829, acc: 0.376
******** [step = 200] loss: 3.799, acc: 0.379
******** [step = 250] loss: 3.774, acc: 0.381
******** [step = 300] loss: 3.753, acc: 0.382
******** [step = 350] loss: 3.735, acc: 0.384
******** [step = 400] loss: 3.718, acc: 0.385
******** [step = 450] loss: 3.702, acc: 0.386
******** [step = 500] loss: 3.684, acc: 0.388
******** [step = 550] loss: 3.668, acc: 0.389
******** [step = 600] loss: 3.655, acc: 0.390
******** [step = 650] loss: 3.640, acc: 0.392
******** [step = 700] loss: 3.627, acc: 0.393
******** [step = 750] loss: 3.615, acc: 0.394
******** [step = 800] loss: 3.603, acc: 0.395
******** [step = 850] loss: 3.592, acc: 0.396
EPOCH = 2 loss: 3.592, acc: 0.396, val_loss: 3.310, val_acc: 0.426

================================================================================2025-08_12 11:26:11
******** [step = 50] loss: 3.371, acc: 0.412
******** [step = 100] loss: 3.338, acc: 0.415
******** [step = 150] loss: 3.330, acc: 0.415
******** [step = 200] loss: 3.323, acc: 0.416
******** [step = 250] loss: 3.318, acc: 0.417
******** [step = 300] loss: 3.312, acc: 0.419
******** [step = 350] loss: 3.307, acc: 0.419
******** [step = 400] loss: 3.301, acc: 0.420
******** [step = 450] loss: 3.292, acc: 0.421
******** [step = 500] loss: 3.286, acc: 0.422
******** [step = 550] loss: 3.281, acc: 0.423
******** [step = 600] loss: 3.275, acc: 0.424
******** [step = 650] loss: 3.269, acc: 0.425
******** [step = 700] loss: 3.263, acc: 0.425
******** [step = 750] loss: 3.257, acc: 0.426
******** [step = 800] loss: 3.250, acc: 0.427
******** [step = 850] loss: 3.243, acc: 0.428
EPOCH = 3 loss: 3.243, acc: 0.428, val_loss: 3.076, val_acc: 0.452

================================================================================2025-08_12 11:28:14
******** [step = 50] loss: 3.125, acc: 0.438
******** [step = 100] loss: 3.109, acc: 0.441
******** [step = 150] loss: 3.089, acc: 0.443
******** [step = 200] loss: 3.083, acc: 0.444
******** [step = 250] loss: 3.081, acc: 0.445
******** [step = 300] loss: 3.076, acc: 0.445
******** [step = 350] loss: 3.068, acc: 0.446
******** [step = 400] loss: 3.068, acc: 0.446
******** [step = 450] loss: 3.063, acc: 0.447
******** [step = 500] loss: 3.061, acc: 0.447
******** [step = 550] loss: 3.058, acc: 0.447
******** [step = 600] loss: 3.054, acc: 0.448
******** [step = 650] loss: 3.051, acc: 0.448
******** [step = 700] loss: 3.047, acc: 0.449
******** [step = 750] loss: 3.045, acc: 0.449
******** [step = 800] loss: 3.042, acc: 0.449
******** [step = 850] loss: 3.040, acc: 0.450
EPOCH = 4 loss: 3.040, acc: 0.450, val_loss: 2.928, val_acc: 0.472

================================================================================2025-08_12 11:30:16
******** [step = 50] loss: 2.941, acc: 0.456
******** [step = 100] loss: 2.922, acc: 0.459
******** [step = 150] loss: 2.923, acc: 0.459
******** [step = 200] loss: 2.918, acc: 0.459
******** [step = 250] loss: 2.914, acc: 0.461
******** [step = 300] loss: 2.914, acc: 0.461
******** [step = 350] loss: 2.913, acc: 0.461
******** [step = 400] loss: 2.913, acc: 0.462
******** [step = 450] loss: 2.911, acc: 0.462
******** [step = 500] loss: 2.910, acc: 0.462
******** [step = 550] loss: 2.910, acc: 0.462
******** [step = 600] loss: 2.911, acc: 0.463
******** [step = 650] loss: 2.910, acc: 0.463
******** [step = 700] loss: 2.908, acc: 0.463
******** [step = 750] loss: 2.908, acc: 0.463
******** [step = 800] loss: 2.907, acc: 0.463
******** [step = 850] loss: 2.903, acc: 0.464
EPOCH = 5 loss: 2.903, acc: 0.464, val_loss: 2.839, val_acc: 0.484

================================================================================2025-08_12 11:32:18
******** [step = 50] loss: 2.801, acc: 0.471
******** [step = 100] loss: 2.792, acc: 0.473
******** [step = 150] loss: 2.797, acc: 0.473
******** [step = 200] loss: 2.788, acc: 0.474
******** [step = 250] loss: 2.786, acc: 0.475
******** [step = 300] loss: 2.788, acc: 0.475
******** [step = 350] loss: 2.790, acc: 0.475
******** [step = 400] loss: 2.785, acc: 0.476
******** [step = 450] loss: 2.784, acc: 0.476
******** [step = 500] loss: 2.783, acc: 0.477
******** [step = 550] loss: 2.781, acc: 0.477
******** [step = 600] loss: 2.779, acc: 0.477
******** [step = 650] loss: 2.778, acc: 0.478
******** [step = 700] loss: 2.777, acc: 0.478
******** [step = 750] loss: 2.778, acc: 0.478
******** [step = 800] loss: 2.778, acc: 0.478
******** [step = 850] loss: 2.777, acc: 0.478
EPOCH = 6 loss: 2.777, acc: 0.478, val_loss: 2.721, val_acc: 0.498

================================================================================2025-08_12 11:34:22
******** [step = 50] loss: 2.685, acc: 0.484
******** [step = 100] loss: 2.663, acc: 0.488
******** [step = 150] loss: 2.652, acc: 0.490
******** [step = 200] loss: 2.654, acc: 0.490
******** [step = 250] loss: 2.657, acc: 0.490
******** [step = 300] loss: 2.660, acc: 0.490
******** [step = 350] loss: 2.662, acc: 0.490
******** [step = 400] loss: 2.665, acc: 0.490
******** [step = 450] loss: 2.667, acc: 0.490
******** [step = 500] loss: 2.665, acc: 0.491
******** [step = 550] loss: 2.666, acc: 0.491
******** [step = 600] loss: 2.664, acc: 0.491
******** [step = 650] loss: 2.662, acc: 0.492
******** [step = 700] loss: 2.662, acc: 0.492
******** [step = 750] loss: 2.661, acc: 0.493
******** [step = 800] loss: 2.661, acc: 0.493
******** [step = 850] loss: 2.660, acc: 0.493
EPOCH = 7 loss: 2.660, acc: 0.493, val_loss: 2.640, val_acc: 0.506

================================================================================2025-08_12 11:36:25
******** [step = 50] loss: 2.584, acc: 0.498
******** [step = 100] loss: 2.555, acc: 0.501
******** [step = 150] loss: 2.548, acc: 0.503
******** [step = 200] loss: 2.545, acc: 0.504
******** [step = 250] loss: 2.547, acc: 0.505
******** [step = 300] loss: 2.553, acc: 0.504
******** [step = 350] loss: 2.554, acc: 0.504
******** [step = 400] loss: 2.553, acc: 0.504
******** [step = 450] loss: 2.557, acc: 0.504
******** [step = 500] loss: 2.559, acc: 0.504
******** [step = 550] loss: 2.559, acc: 0.505
******** [step = 600] loss: 2.561, acc: 0.505
******** [step = 650] loss: 2.563, acc: 0.505
******** [step = 700] loss: 2.565, acc: 0.505
******** [step = 750] loss: 2.564, acc: 0.505
******** [step = 800] loss: 2.564, acc: 0.505
******** [step = 850] loss: 2.563, acc: 0.505
EPOCH = 8 loss: 2.563, acc: 0.505, val_loss: 2.559, val_acc: 0.521

================================================================================2025-08_12 11:38:26
******** [step = 50] loss: 2.463, acc: 0.510
******** [step = 100] loss: 2.463, acc: 0.512
******** [step = 150] loss: 2.460, acc: 0.513
******** [step = 200] loss: 2.457, acc: 0.515
******** [step = 250] loss: 2.455, acc: 0.516
******** [step = 300] loss: 2.459, acc: 0.516
******** [step = 350] loss: 2.461, acc: 0.516
******** [step = 400] loss: 2.463, acc: 0.516
******** [step = 450] loss: 2.463, acc: 0.516
******** [step = 500] loss: 2.466, acc: 0.516
******** [step = 550] loss: 2.468, acc: 0.516
******** [step = 600] loss: 2.470, acc: 0.516
******** [step = 650] loss: 2.473, acc: 0.516
******** [step = 700] loss: 2.474, acc: 0.516
******** [step = 750] loss: 2.475, acc: 0.516
******** [step = 800] loss: 2.477, acc: 0.516
******** [step = 850] loss: 2.480, acc: 0.516
EPOCH = 9 loss: 2.480, acc: 0.516, val_loss: 2.511, val_acc: 0.529

================================================================================2025-08_12 11:40:28
******** [step = 50] loss: 2.402, acc: 0.520
******** [step = 100] loss: 2.387, acc: 0.524
******** [step = 150] loss: 2.384, acc: 0.525
******** [step = 200] loss: 2.386, acc: 0.525
******** [step = 250] loss: 2.385, acc: 0.526
******** [step = 300] loss: 2.388, acc: 0.526
******** [step = 350] loss: 2.385, acc: 0.527
******** [step = 400] loss: 2.390, acc: 0.526
******** [step = 450] loss: 2.396, acc: 0.526
******** [step = 500] loss: 2.398, acc: 0.526
******** [step = 550] loss: 2.399, acc: 0.527
******** [step = 600] loss: 2.402, acc: 0.527
******** [step = 650] loss: 2.404, acc: 0.527
******** [step = 700] loss: 2.404, acc: 0.527
******** [step = 750] loss: 2.406, acc: 0.527
******** [step = 800] loss: 2.410, acc: 0.526
******** [step = 850] loss: 2.413, acc: 0.526
EPOCH = 10 loss: 2.413, acc: 0.526, val_loss: 2.453, val_acc: 0.539

================================================================================2025-08_12 11:42:31
******** [step = 50] loss: 2.301, acc: 0.535
******** [step = 100] loss: 2.290, acc: 0.539
******** [step = 150] loss: 2.304, acc: 0.537
******** [step = 200] loss: 2.308, acc: 0.537
******** [step = 250] loss: 2.319, acc: 0.536
******** [step = 300] loss: 2.320, acc: 0.536
******** [step = 350] loss: 2.325, acc: 0.536
******** [step = 400] loss: 2.332, acc: 0.535
******** [step = 450] loss: 2.336, acc: 0.535
******** [step = 500] loss: 2.337, acc: 0.535
******** [step = 550] loss: 2.341, acc: 0.535
******** [step = 600] loss: 2.341, acc: 0.535
******** [step = 650] loss: 2.344, acc: 0.535
******** [step = 700] loss: 2.349, acc: 0.534
******** [step = 750] loss: 2.351, acc: 0.534
******** [step = 800] loss: 2.353, acc: 0.534
******** [step = 850] loss: 2.354, acc: 0.535
EPOCH = 11 loss: 2.354, acc: 0.535, val_loss: 2.416, val_acc: 0.548

================================================================================2025-08_12 11:44:32
******** [step = 50] loss: 2.265, acc: 0.539
******** [step = 100] loss: 2.261, acc: 0.542
******** [step = 150] loss: 2.262, acc: 0.543
******** [step = 200] loss: 2.262, acc: 0.544
******** [step = 250] loss: 2.264, acc: 0.544
******** [step = 300] loss: 2.268, acc: 0.544
******** [step = 350] loss: 2.271, acc: 0.544
******** [step = 400] loss: 2.274, acc: 0.544
******** [step = 450] loss: 2.281, acc: 0.543
******** [step = 500] loss: 2.285, acc: 0.543
******** [step = 550] loss: 2.287, acc: 0.543
******** [step = 600] loss: 2.290, acc: 0.543
******** [step = 650] loss: 2.292, acc: 0.543
******** [step = 700] loss: 2.295, acc: 0.543
******** [step = 750] loss: 2.296, acc: 0.543
******** [step = 800] loss: 2.299, acc: 0.542
******** [step = 850] loss: 2.299, acc: 0.543
EPOCH = 12 loss: 2.299, acc: 0.543, val_loss: 2.386, val_acc: 0.554

================================================================================2025-08_12 11:46:34
******** [step = 50] loss: 2.228, acc: 0.549
******** [step = 100] loss: 2.216, acc: 0.551
******** [step = 150] loss: 2.214, acc: 0.552
******** [step = 200] loss: 2.216, acc: 0.552
******** [step = 250] loss: 2.218, acc: 0.552
******** [step = 300] loss: 2.223, acc: 0.551
******** [step = 350] loss: 2.227, acc: 0.550
******** [step = 400] loss: 2.228, acc: 0.551
******** [step = 450] loss: 2.234, acc: 0.550
******** [step = 500] loss: 2.238, acc: 0.550
******** [step = 550] loss: 2.240, acc: 0.550
******** [step = 600] loss: 2.243, acc: 0.550
******** [step = 650] loss: 2.243, acc: 0.550
******** [step = 700] loss: 2.247, acc: 0.550
******** [step = 750] loss: 2.247, acc: 0.550
******** [step = 800] loss: 2.253, acc: 0.549
******** [step = 850] loss: 2.254, acc: 0.549
EPOCH = 13 loss: 2.254, acc: 0.549, val_loss: 2.363, val_acc: 0.556

================================================================================2025-08_12 11:48:37
******** [step = 50] loss: 2.169, acc: 0.557
******** [step = 100] loss: 2.172, acc: 0.556
******** [step = 150] loss: 2.177, acc: 0.556
******** [step = 200] loss: 2.180, acc: 0.557
******** [step = 250] loss: 2.184, acc: 0.556
******** [step = 300] loss: 2.184, acc: 0.557
******** [step = 350] loss: 2.186, acc: 0.557
******** [step = 400] loss: 2.187, acc: 0.557
******** [step = 450] loss: 2.194, acc: 0.557
******** [step = 500] loss: 2.196, acc: 0.556
******** [step = 550] loss: 2.201, acc: 0.556
******** [step = 600] loss: 2.206, acc: 0.555
******** [step = 650] loss: 2.207, acc: 0.556
******** [step = 700] loss: 2.211, acc: 0.555
******** [step = 750] loss: 2.210, acc: 0.556
******** [step = 800] loss: 2.210, acc: 0.556
******** [step = 850] loss: 2.215, acc: 0.556
EPOCH = 14 loss: 2.215, acc: 0.556, val_loss: 2.333, val_acc: 0.565

================================================================================2025-08_12 11:50:38
******** [step = 50] loss: 2.110, acc: 0.567
******** [step = 100] loss: 2.111, acc: 0.567
******** [step = 150] loss: 2.114, acc: 0.567
******** [step = 200] loss: 2.119, acc: 0.566
******** [step = 250] loss: 2.127, acc: 0.565
******** [step = 300] loss: 2.132, acc: 0.565
******** [step = 350] loss: 2.139, acc: 0.564
******** [step = 400] loss: 2.146, acc: 0.564
******** [step = 450] loss: 2.151, acc: 0.564
******** [step = 500] loss: 2.155, acc: 0.563
******** [step = 550] loss: 2.158, acc: 0.563
******** [step = 600] loss: 2.161, acc: 0.563
******** [step = 650] loss: 2.162, acc: 0.563
******** [step = 700] loss: 2.166, acc: 0.563
******** [step = 750] loss: 2.170, acc: 0.563
******** [step = 800] loss: 2.174, acc: 0.562
******** [step = 850] loss: 2.177, acc: 0.562
EPOCH = 15 loss: 2.177, acc: 0.562, val_loss: 2.328, val_acc: 0.566

================================================================================2025-08_12 11:52:40
******** [step = 50] loss: 2.080, acc: 0.573
******** [step = 100] loss: 2.087, acc: 0.572
******** [step = 150] loss: 2.090, acc: 0.571
******** [step = 200] loss: 2.095, acc: 0.570
******** [step = 250] loss: 2.101, acc: 0.570
******** [step = 300] loss: 2.105, acc: 0.569
******** [step = 350] loss: 2.111, acc: 0.569
******** [step = 400] loss: 2.114, acc: 0.569
******** [step = 450] loss: 2.118, acc: 0.569
******** [step = 500] loss: 2.122, acc: 0.568
******** [step = 550] loss: 2.127, acc: 0.568
******** [step = 600] loss: 2.131, acc: 0.568
******** [step = 650] loss: 2.133, acc: 0.568
******** [step = 700] loss: 2.135, acc: 0.568
******** [step = 750] loss: 2.138, acc: 0.568
******** [step = 800] loss: 2.139, acc: 0.568
******** [step = 850] loss: 2.140, acc: 0.568
EPOCH = 16 loss: 2.140, acc: 0.568, val_loss: 2.309, val_acc: 0.568

================================================================================2025-08_12 11:54:43
******** [step = 50] loss: 2.043, acc: 0.577
******** [step = 100] loss: 2.060, acc: 0.575
******** [step = 150] loss: 2.063, acc: 0.575
******** [step = 200] loss: 2.071, acc: 0.574
******** [step = 250] loss: 2.077, acc: 0.574
******** [step = 300] loss: 2.082, acc: 0.573
******** [step = 350] loss: 2.083, acc: 0.573
******** [step = 400] loss: 2.085, acc: 0.574
******** [step = 450] loss: 2.087, acc: 0.573
******** [step = 500] loss: 2.091, acc: 0.573
******** [step = 550] loss: 2.092, acc: 0.574
******** [step = 600] loss: 2.094, acc: 0.574
******** [step = 650] loss: 2.097, acc: 0.573
******** [step = 700] loss: 2.099, acc: 0.573
******** [step = 750] loss: 2.102, acc: 0.573
******** [step = 800] loss: 2.104, acc: 0.573
******** [step = 850] loss: 2.108, acc: 0.573
EPOCH = 17 loss: 2.108, acc: 0.573, val_loss: 2.281, val_acc: 0.574

================================================================================2025-08_12 11:56:45
******** [step = 50] loss: 2.023, acc: 0.579
******** [step = 100] loss: 2.017, acc: 0.582
******** [step = 150] loss: 2.016, acc: 0.582
******** [step = 200] loss: 2.023, acc: 0.581
******** [step = 250] loss: 2.036, acc: 0.579
******** [step = 300] loss: 2.043, acc: 0.579
******** [step = 350] loss: 2.050, acc: 0.579
******** [step = 400] loss: 2.053, acc: 0.578
******** [step = 450] loss: 2.057, acc: 0.578
******** [step = 500] loss: 2.059, acc: 0.578
******** [step = 550] loss: 2.063, acc: 0.578
******** [step = 600] loss: 2.068, acc: 0.577
******** [step = 650] loss: 2.072, acc: 0.577
******** [step = 700] loss: 2.076, acc: 0.577
******** [step = 750] loss: 2.077, acc: 0.577
******** [step = 800] loss: 2.080, acc: 0.576
******** [step = 850] loss: 2.080, acc: 0.577
EPOCH = 18 loss: 2.080, acc: 0.577, val_loss: 2.261, val_acc: 0.580

================================================================================2025-08_12 11:58:47
******** [step = 50] loss: 2.001, acc: 0.584
******** [step = 100] loss: 1.997, acc: 0.585
******** [step = 150] loss: 1.991, acc: 0.586
******** [step = 200] loss: 1.997, acc: 0.585
******** [step = 250] loss: 2.005, acc: 0.585
******** [step = 300] loss: 2.007, acc: 0.585
******** [step = 350] loss: 2.013, acc: 0.585
******** [step = 400] loss: 2.020, acc: 0.584
******** [step = 450] loss: 2.024, acc: 0.583
******** [step = 500] loss: 2.031, acc: 0.583
******** [step = 550] loss: 2.038, acc: 0.582
******** [step = 600] loss: 2.043, acc: 0.582
******** [step = 650] loss: 2.045, acc: 0.581
******** [step = 700] loss: 2.048, acc: 0.581
******** [step = 750] loss: 2.051, acc: 0.581
******** [step = 800] loss: 2.052, acc: 0.581
******** [step = 850] loss: 2.053, acc: 0.581
EPOCH = 19 loss: 2.053, acc: 0.581, val_loss: 2.245, val_acc: 0.582

================================================================================2025-08_12 12:00:49
******** [step = 50] loss: 1.960, acc: 0.594
******** [step = 100] loss: 1.956, acc: 0.593
******** [step = 150] loss: 1.956, acc: 0.593
******** [step = 200] loss: 1.961, acc: 0.592
******** [step = 250] loss: 1.970, acc: 0.591
******** [step = 300] loss: 1.977, acc: 0.590
******** [step = 350] loss: 1.983, acc: 0.590
******** [step = 400] loss: 1.988, acc: 0.589
******** [step = 450] loss: 1.994, acc: 0.589
******** [step = 500] loss: 1.997, acc: 0.589
******** [step = 550] loss: 2.001, acc: 0.588
******** [step = 600] loss: 2.006, acc: 0.588
******** [step = 650] loss: 2.010, acc: 0.587
******** [step = 700] loss: 2.013, acc: 0.587
******** [step = 750] loss: 2.015, acc: 0.587
******** [step = 800] loss: 2.019, acc: 0.587
******** [step = 850] loss: 2.025, acc: 0.586
EPOCH = 20 loss: 2.025, acc: 0.586, val_loss: 2.240, val_acc: 0.583

================================================================================2025-08_12 12:02:51
******** [step = 50] loss: 1.938, acc: 0.591
******** [step = 100] loss: 1.944, acc: 0.593
******** [step = 150] loss: 1.940, acc: 0.595
******** [step = 200] loss: 1.943, acc: 0.595
******** [step = 250] loss: 1.951, acc: 0.594
******** [step = 300] loss: 1.954, acc: 0.593
******** [step = 350] loss: 1.958, acc: 0.593
******** [step = 400] loss: 1.964, acc: 0.592
******** [step = 450] loss: 1.969, acc: 0.592
******** [step = 500] loss: 1.974, acc: 0.592
******** [step = 550] loss: 1.978, acc: 0.591
******** [step = 600] loss: 1.981, acc: 0.591
******** [step = 650] loss: 1.986, acc: 0.590
******** [step = 700] loss: 1.988, acc: 0.590
******** [step = 750] loss: 1.991, acc: 0.590
******** [step = 800] loss: 1.994, acc: 0.590
******** [step = 850] loss: 1.998, acc: 0.590
EPOCH = 21 loss: 1.998, acc: 0.590, val_loss: 2.217, val_acc: 0.586

================================================================================2025-08_12 12:04:52
******** [step = 50] loss: 1.922, acc: 0.598
******** [step = 100] loss: 1.906, acc: 0.600
******** [step = 150] loss: 1.913, acc: 0.599
******** [step = 200] loss: 1.919, acc: 0.598
******** [step = 250] loss: 1.919, acc: 0.598
******** [step = 300] loss: 1.925, acc: 0.597
******** [step = 350] loss: 1.931, acc: 0.597
******** [step = 400] loss: 1.938, acc: 0.596
******** [step = 450] loss: 1.942, acc: 0.596
******** [step = 500] loss: 1.943, acc: 0.597
******** [step = 550] loss: 1.948, acc: 0.596
******** [step = 600] loss: 1.953, acc: 0.596
******** [step = 650] loss: 1.958, acc: 0.595
******** [step = 700] loss: 1.963, acc: 0.595
******** [step = 750] loss: 1.967, acc: 0.595
******** [step = 800] loss: 1.970, acc: 0.594
******** [step = 850] loss: 1.972, acc: 0.594
EPOCH = 22 loss: 1.972, acc: 0.594, val_loss: 2.200, val_acc: 0.591

================================================================================2025-08_12 12:06:55
******** [step = 50] loss: 1.866, acc: 0.607
******** [step = 100] loss: 1.882, acc: 0.604
******** [step = 150] loss: 1.896, acc: 0.602
******** [step = 200] loss: 1.897, acc: 0.602
******** [step = 250] loss: 1.901, acc: 0.602
******** [step = 300] loss: 1.906, acc: 0.602
******** [step = 350] loss: 1.909, acc: 0.601
******** [step = 400] loss: 1.911, acc: 0.601
******** [step = 450] loss: 1.914, acc: 0.601
******** [step = 500] loss: 1.919, acc: 0.601
******** [step = 550] loss: 1.926, acc: 0.600
******** [step = 600] loss: 1.932, acc: 0.599
******** [step = 650] loss: 1.935, acc: 0.599
******** [step = 700] loss: 1.938, acc: 0.599
******** [step = 750] loss: 1.941, acc: 0.599
******** [step = 800] loss: 1.944, acc: 0.598
******** [step = 850] loss: 1.947, acc: 0.598
EPOCH = 23 loss: 1.947, acc: 0.598, val_loss: 2.182, val_acc: 0.594

================================================================================2025-08_12 12:08:57
******** [step = 50] loss: 1.863, acc: 0.606
******** [step = 100] loss: 1.872, acc: 0.604
******** [step = 150] loss: 1.872, acc: 0.604
******** [step = 200] loss: 1.877, acc: 0.605
******** [step = 250] loss: 1.883, acc: 0.604
******** [step = 300] loss: 1.888, acc: 0.603
******** [step = 350] loss: 1.890, acc: 0.603
******** [step = 400] loss: 1.894, acc: 0.603
******** [step = 450] loss: 1.895, acc: 0.603
******** [step = 500] loss: 1.901, acc: 0.603
******** [step = 550] loss: 1.907, acc: 0.602
******** [step = 600] loss: 1.911, acc: 0.602
******** [step = 650] loss: 1.915, acc: 0.601
******** [step = 700] loss: 1.919, acc: 0.601
******** [step = 750] loss: 1.923, acc: 0.601
******** [step = 800] loss: 1.925, acc: 0.601
******** [step = 850] loss: 1.928, acc: 0.601
EPOCH = 24 loss: 1.928, acc: 0.601, val_loss: 2.189, val_acc: 0.593

================================================================================2025-08_12 12:10:58
******** [step = 50] loss: 1.864, acc: 0.609
******** [step = 100] loss: 1.855, acc: 0.610
******** [step = 150] loss: 1.859, acc: 0.610
******** [step = 200] loss: 1.865, acc: 0.610
******** [step = 250] loss: 1.867, acc: 0.609
******** [step = 300] loss: 1.866, acc: 0.609
******** [step = 350] loss: 1.869, acc: 0.609
******** [step = 400] loss: 1.874, acc: 0.608
******** [step = 450] loss: 1.877, acc: 0.608
******** [step = 500] loss: 1.881, acc: 0.607
******** [step = 550] loss: 1.887, acc: 0.607
******** [step = 600] loss: 1.891, acc: 0.606
******** [step = 650] loss: 1.892, acc: 0.606
******** [step = 700] loss: 1.896, acc: 0.606
******** [step = 750] loss: 1.900, acc: 0.605
******** [step = 800] loss: 1.905, acc: 0.605
******** [step = 850] loss: 1.907, acc: 0.605
EPOCH = 25 loss: 1.907, acc: 0.605, val_loss: 2.166, val_acc: 0.599

================================================================================2025-08_12 12:13:00
******** [step = 50] loss: 1.859, acc: 0.610
******** [step = 100] loss: 1.845, acc: 0.613
******** [step = 150] loss: 1.842, acc: 0.613
******** [step = 200] loss: 1.840, acc: 0.614
******** [step = 250] loss: 1.844, acc: 0.613
******** [step = 300] loss: 1.850, acc: 0.612
******** [step = 350] loss: 1.852, acc: 0.611
******** [step = 400] loss: 1.855, acc: 0.611
******** [step = 450] loss: 1.858, acc: 0.611
******** [step = 500] loss: 1.862, acc: 0.610
******** [step = 550] loss: 1.865, acc: 0.610
******** [step = 600] loss: 1.868, acc: 0.610
******** [step = 650] loss: 1.871, acc: 0.609
******** [step = 700] loss: 1.875, acc: 0.609
******** [step = 750] loss: 1.879, acc: 0.609
******** [step = 800] loss: 1.883, acc: 0.608
******** [step = 850] loss: 1.890, acc: 0.608
EPOCH = 26 loss: 1.890, acc: 0.608, val_loss: 2.155, val_acc: 0.600

================================================================================2025-08_12 12:15:05
******** [step = 50] loss: 1.816, acc: 0.617
******** [step = 100] loss: 1.804, acc: 0.617
******** [step = 150] loss: 1.811, acc: 0.616
******** [step = 200] loss: 1.815, acc: 0.615
******** [step = 250] loss: 1.825, acc: 0.614
******** [step = 300] loss: 1.829, acc: 0.613
******** [step = 350] loss: 1.835, acc: 0.613
******** [step = 400] loss: 1.838, acc: 0.613
******** [step = 450] loss: 1.840, acc: 0.613
******** [step = 500] loss: 1.845, acc: 0.612
******** [step = 550] loss: 1.848, acc: 0.612
******** [step = 600] loss: 1.854, acc: 0.611
******** [step = 650] loss: 1.857, acc: 0.611
******** [step = 700] loss: 1.862, acc: 0.610
******** [step = 750] loss: 1.866, acc: 0.610
******** [step = 800] loss: 1.870, acc: 0.610
******** [step = 850] loss: 1.875, acc: 0.609
EPOCH = 27 loss: 1.875, acc: 0.609, val_loss: 2.148, val_acc: 0.601

================================================================================2025-08_12 12:17:07
******** [step = 50] loss: 1.831, acc: 0.615
******** [step = 100] loss: 1.802, acc: 0.617
******** [step = 150] loss: 1.797, acc: 0.618
******** [step = 200] loss: 1.804, acc: 0.617
******** [step = 250] loss: 1.811, acc: 0.616
******** [step = 300] loss: 1.814, acc: 0.616
******** [step = 350] loss: 1.818, acc: 0.616
******** [step = 400] loss: 1.824, acc: 0.615
******** [step = 450] loss: 1.829, acc: 0.615
******** [step = 500] loss: 1.831, acc: 0.615
******** [step = 550] loss: 1.836, acc: 0.615
******** [step = 600] loss: 1.841, acc: 0.614
******** [step = 650] loss: 1.843, acc: 0.614
******** [step = 700] loss: 1.845, acc: 0.614
******** [step = 750] loss: 1.849, acc: 0.614
******** [step = 800] loss: 1.852, acc: 0.613
******** [step = 850] loss: 1.858, acc: 0.613
EPOCH = 28 loss: 1.858, acc: 0.613, val_loss: 2.135, val_acc: 0.606

================================================================================2025-08_12 12:19:09
******** [step = 50] loss: 1.803, acc: 0.616
******** [step = 100] loss: 1.790, acc: 0.619
******** [step = 150] loss: 1.787, acc: 0.620
******** [step = 200] loss: 1.791, acc: 0.620
******** [step = 250] loss: 1.793, acc: 0.620
******** [step = 300] loss: 1.798, acc: 0.620
******** [step = 350] loss: 1.797, acc: 0.620
******** [step = 400] loss: 1.802, acc: 0.619
******** [step = 450] loss: 1.807, acc: 0.619
******** [step = 500] loss: 1.808, acc: 0.619
******** [step = 550] loss: 1.814, acc: 0.618
******** [step = 600] loss: 1.819, acc: 0.617
******** [step = 650] loss: 1.823, acc: 0.617
******** [step = 700] loss: 1.827, acc: 0.617
******** [step = 750] loss: 1.831, acc: 0.617
******** [step = 800] loss: 1.835, acc: 0.616
******** [step = 850] loss: 1.840, acc: 0.615
EPOCH = 29 loss: 1.840, acc: 0.615, val_loss: 2.131, val_acc: 0.606

================================================================================2025-08_12 12:21:11
******** [step = 50] loss: 1.768, acc: 0.623
******** [step = 100] loss: 1.763, acc: 0.624
******** [step = 150] loss: 1.763, acc: 0.624
******** [step = 200] loss: 1.772, acc: 0.623
******** [step = 250] loss: 1.777, acc: 0.622
******** [step = 300] loss: 1.782, acc: 0.622
******** [step = 350] loss: 1.784, acc: 0.622
******** [step = 400] loss: 1.789, acc: 0.621
******** [step = 450] loss: 1.793, acc: 0.621
******** [step = 500] loss: 1.797, acc: 0.620
******** [step = 550] loss: 1.801, acc: 0.620
******** [step = 600] loss: 1.805, acc: 0.620
******** [step = 650] loss: 1.809, acc: 0.619
******** [step = 700] loss: 1.812, acc: 0.619
******** [step = 750] loss: 1.815, acc: 0.619
******** [step = 800] loss: 1.818, acc: 0.619
******** [step = 850] loss: 1.821, acc: 0.618
EPOCH = 30 loss: 1.821, acc: 0.618, val_loss: 2.130, val_acc: 0.604

================================================================================2025-08_12 12:23:13
******** [step = 50] loss: 1.735, acc: 0.626
******** [step = 100] loss: 1.750, acc: 0.624
******** [step = 150] loss: 1.749, acc: 0.624
******** [step = 200] loss: 1.758, acc: 0.624
******** [step = 250] loss: 1.759, acc: 0.624
******** [step = 300] loss: 1.765, acc: 0.623
******** [step = 350] loss: 1.768, acc: 0.623
******** [step = 400] loss: 1.775, acc: 0.623
******** [step = 450] loss: 1.778, acc: 0.623
******** [step = 500] loss: 1.784, acc: 0.622
******** [step = 550] loss: 1.789, acc: 0.622
******** [step = 600] loss: 1.791, acc: 0.622
******** [step = 650] loss: 1.794, acc: 0.622
******** [step = 700] loss: 1.798, acc: 0.621
******** [step = 750] loss: 1.802, acc: 0.621
******** [step = 800] loss: 1.806, acc: 0.621
******** [step = 850] loss: 1.812, acc: 0.620
EPOCH = 31 loss: 1.812, acc: 0.620, val_loss: 2.112, val_acc: 0.609

================================================================================2025-08_12 12:25:14
******** [step = 50] loss: 1.743, acc: 0.625
******** [step = 100] loss: 1.726, acc: 0.628
******** [step = 150] loss: 1.733, acc: 0.629
******** [step = 200] loss: 1.738, acc: 0.628
******** [step = 250] loss: 1.745, acc: 0.628
******** [step = 300] loss: 1.749, acc: 0.627
******** [step = 350] loss: 1.756, acc: 0.626
******** [step = 400] loss: 1.759, acc: 0.626
******** [step = 450] loss: 1.764, acc: 0.626
******** [step = 500] loss: 1.767, acc: 0.626
******** [step = 550] loss: 1.773, acc: 0.625
******** [step = 600] loss: 1.778, acc: 0.625
******** [step = 650] loss: 1.781, acc: 0.624
******** [step = 700] loss: 1.784, acc: 0.624
******** [step = 750] loss: 1.789, acc: 0.623
******** [step = 800] loss: 1.792, acc: 0.623
******** [step = 850] loss: 1.798, acc: 0.623
EPOCH = 32 loss: 1.798, acc: 0.623, val_loss: 2.115, val_acc: 0.611

================================================================================2025-08_12 12:27:16
******** [step = 50] loss: 1.730, acc: 0.630
******** [step = 100] loss: 1.725, acc: 0.630
******** [step = 150] loss: 1.729, acc: 0.630
******** [step = 200] loss: 1.734, acc: 0.630
******** [step = 250] loss: 1.739, acc: 0.628
******** [step = 300] loss: 1.747, acc: 0.627
******** [step = 350] loss: 1.749, acc: 0.627
******** [step = 400] loss: 1.754, acc: 0.626
******** [step = 450] loss: 1.760, acc: 0.626
******** [step = 500] loss: 1.765, acc: 0.625
******** [step = 550] loss: 1.771, acc: 0.625
******** [step = 600] loss: 1.772, acc: 0.625
******** [step = 650] loss: 1.775, acc: 0.625
******** [step = 700] loss: 1.780, acc: 0.624
******** [step = 750] loss: 1.783, acc: 0.624
******** [step = 800] loss: 1.785, acc: 0.624
******** [step = 850] loss: 1.788, acc: 0.623
EPOCH = 33 loss: 1.788, acc: 0.623, val_loss: 2.106, val_acc: 0.613

================================================================================2025-08_12 12:29:20
******** [step = 50] loss: 1.715, acc: 0.631
******** [step = 100] loss: 1.704, acc: 0.633
******** [step = 150] loss: 1.709, acc: 0.632
******** [step = 200] loss: 1.717, acc: 0.632
******** [step = 250] loss: 1.719, acc: 0.633
******** [step = 300] loss: 1.720, acc: 0.632
******** [step = 350] loss: 1.728, acc: 0.632
******** [step = 400] loss: 1.730, acc: 0.631
******** [step = 450] loss: 1.734, acc: 0.631
******** [step = 500] loss: 1.739, acc: 0.630
******** [step = 550] loss: 1.742, acc: 0.630
******** [step = 600] loss: 1.746, acc: 0.629
******** [step = 650] loss: 1.750, acc: 0.629
******** [step = 700] loss: 1.755, acc: 0.629
******** [step = 750] loss: 1.758, acc: 0.628
******** [step = 800] loss: 1.764, acc: 0.627
******** [step = 850] loss: 1.767, acc: 0.627
EPOCH = 34 loss: 1.767, acc: 0.627, val_loss: 2.092, val_acc: 0.614

================================================================================2025-08_12 12:31:21
******** [step = 50] loss: 1.679, acc: 0.637
******** [step = 100] loss: 1.686, acc: 0.637
******** [step = 150] loss: 1.690, acc: 0.636
******** [step = 200] loss: 1.696, acc: 0.635
******** [step = 250] loss: 1.702, acc: 0.635
******** [step = 300] loss: 1.706, acc: 0.634
******** [step = 350] loss: 1.711, acc: 0.634
******** [step = 400] loss: 1.717, acc: 0.633
******** [step = 450] loss: 1.720, acc: 0.633
******** [step = 500] loss: 1.725, acc: 0.632
******** [step = 550] loss: 1.729, acc: 0.632
******** [step = 600] loss: 1.734, acc: 0.631
******** [step = 650] loss: 1.740, acc: 0.631
******** [step = 700] loss: 1.743, acc: 0.630
******** [step = 750] loss: 1.748, acc: 0.630
******** [step = 800] loss: 1.751, acc: 0.630
******** [step = 850] loss: 1.754, acc: 0.630
EPOCH = 35 loss: 1.754, acc: 0.630, val_loss: 2.082, val_acc: 0.616

================================================================================2025-08_12 12:33:23
******** [step = 50] loss: 1.656, acc: 0.639
******** [step = 100] loss: 1.672, acc: 0.638
******** [step = 150] loss: 1.680, acc: 0.637
******** [step = 200] loss: 1.684, acc: 0.638
******** [step = 250] loss: 1.689, acc: 0.637
******** [step = 300] loss: 1.697, acc: 0.636
******** [step = 350] loss: 1.705, acc: 0.635
******** [step = 400] loss: 1.709, acc: 0.634
******** [step = 450] loss: 1.714, acc: 0.634
******** [step = 500] loss: 1.720, acc: 0.633
******** [step = 550] loss: 1.724, acc: 0.633
******** [step = 600] loss: 1.730, acc: 0.632
******** [step = 650] loss: 1.733, acc: 0.632
******** [step = 700] loss: 1.737, acc: 0.631
******** [step = 750] loss: 1.741, acc: 0.631
******** [step = 800] loss: 1.743, acc: 0.631
******** [step = 850] loss: 1.745, acc: 0.631
EPOCH = 36 loss: 1.745, acc: 0.631, val_loss: 2.077, val_acc: 0.617

================================================================================2025-08_12 12:35:29
******** [step = 50] loss: 1.676, acc: 0.639
******** [step = 100] loss: 1.669, acc: 0.641
******** [step = 150] loss: 1.677, acc: 0.640
******** [step = 200] loss: 1.687, acc: 0.639
******** [step = 250] loss: 1.693, acc: 0.637
******** [step = 300] loss: 1.694, acc: 0.637
******** [step = 350] loss: 1.697, acc: 0.637
******** [step = 400] loss: 1.700, acc: 0.636
******** [step = 450] loss: 1.704, acc: 0.636
******** [step = 500] loss: 1.706, acc: 0.636
******** [step = 550] loss: 1.711, acc: 0.635
******** [step = 600] loss: 1.714, acc: 0.635
******** [step = 650] loss: 1.719, acc: 0.634
******** [step = 700] loss: 1.723, acc: 0.634
******** [step = 750] loss: 1.726, acc: 0.634
******** [step = 800] loss: 1.729, acc: 0.633
******** [step = 850] loss: 1.732, acc: 0.633
EPOCH = 37 loss: 1.732, acc: 0.633, val_loss: 2.078, val_acc: 0.618

================================================================================2025-08_12 12:37:37
******** [step = 50] loss: 1.656, acc: 0.642
******** [step = 100] loss: 1.662, acc: 0.642
******** [step = 150] loss: 1.662, acc: 0.642
******** [step = 200] loss: 1.665, acc: 0.641
******** [step = 250] loss: 1.669, acc: 0.641
******** [step = 300] loss: 1.676, acc: 0.640
******** [step = 350] loss: 1.679, acc: 0.639
******** [step = 400] loss: 1.684, acc: 0.639
******** [step = 450] loss: 1.689, acc: 0.638
******** [step = 500] loss: 1.695, acc: 0.638
******** [step = 550] loss: 1.699, acc: 0.637
******** [step = 600] loss: 1.703, acc: 0.637
******** [step = 650] loss: 1.707, acc: 0.636
******** [step = 700] loss: 1.710, acc: 0.636
******** [step = 750] loss: 1.714, acc: 0.635
******** [step = 800] loss: 1.719, acc: 0.634
******** [step = 850] loss: 1.722, acc: 0.634
EPOCH = 38 loss: 1.722, acc: 0.634, val_loss: 2.066, val_acc: 0.618

================================================================================2025-08_12 12:39:39
******** [step = 50] loss: 1.638, acc: 0.643
******** [step = 100] loss: 1.652, acc: 0.642
******** [step = 150] loss: 1.656, acc: 0.642
******** [step = 200] loss: 1.657, acc: 0.642
******** [step = 250] loss: 1.660, acc: 0.641
******** [step = 300] loss: 1.667, acc: 0.641
******** [step = 350] loss: 1.670, acc: 0.640
******** [step = 400] loss: 1.674, acc: 0.640
******** [step = 450] loss: 1.678, acc: 0.640
******** [step = 500] loss: 1.682, acc: 0.639
******** [step = 550] loss: 1.687, acc: 0.639
******** [step = 600] loss: 1.690, acc: 0.639
******** [step = 650] loss: 1.693, acc: 0.639
******** [step = 700] loss: 1.699, acc: 0.638
******** [step = 750] loss: 1.703, acc: 0.637
******** [step = 800] loss: 1.708, acc: 0.637
******** [step = 850] loss: 1.711, acc: 0.637
EPOCH = 39 loss: 1.711, acc: 0.637, val_loss: 2.068, val_acc: 0.620

================================================================================2025-08_12 12:41:41
******** [step = 50] loss: 1.639, acc: 0.643
******** [step = 100] loss: 1.637, acc: 0.645
******** [step = 150] loss: 1.641, acc: 0.645
******** [step = 200] loss: 1.649, acc: 0.644
******** [step = 250] loss: 1.652, acc: 0.643
******** [step = 300] loss: 1.658, acc: 0.642
******** [step = 350] loss: 1.661, acc: 0.642
******** [step = 400] loss: 1.664, acc: 0.641
******** [step = 450] loss: 1.669, acc: 0.641
******** [step = 500] loss: 1.674, acc: 0.641
******** [step = 550] loss: 1.678, acc: 0.640
******** [step = 600] loss: 1.682, acc: 0.640
******** [step = 650] loss: 1.685, acc: 0.640
******** [step = 700] loss: 1.691, acc: 0.639
******** [step = 750] loss: 1.694, acc: 0.639
******** [step = 800] loss: 1.697, acc: 0.639
******** [step = 850] loss: 1.700, acc: 0.638
EPOCH = 40 loss: 1.700, acc: 0.638, val_loss: 2.063, val_acc: 0.621

================================================================================2025-08_12 12:43:46
******** [step = 50] loss: 1.621, acc: 0.647
******** [step = 100] loss: 1.621, acc: 0.647
******** [step = 150] loss: 1.630, acc: 0.646
******** [step = 200] loss: 1.634, acc: 0.646
******** [step = 250] loss: 1.636, acc: 0.646
******** [step = 300] loss: 1.642, acc: 0.645
******** [step = 350] loss: 1.646, acc: 0.644
******** [step = 400] loss: 1.652, acc: 0.643
******** [step = 450] loss: 1.656, acc: 0.643
******** [step = 500] loss: 1.659, acc: 0.643
******** [step = 550] loss: 1.665, acc: 0.642
******** [step = 600] loss: 1.669, acc: 0.642
******** [step = 650] loss: 1.674, acc: 0.641
******** [step = 700] loss: 1.676, acc: 0.641
******** [step = 750] loss: 1.680, acc: 0.641
******** [step = 800] loss: 1.685, acc: 0.640
******** [step = 850] loss: 1.688, acc: 0.640
EPOCH = 41 loss: 1.688, acc: 0.640, val_loss: 2.048, val_acc: 0.623

================================================================================2025-08_12 12:45:48
******** [step = 50] loss: 1.615, acc: 0.648
******** [step = 100] loss: 1.624, acc: 0.647
******** [step = 150] loss: 1.626, acc: 0.648
******** [step = 200] loss: 1.630, acc: 0.647
******** [step = 250] loss: 1.633, acc: 0.647
******** [step = 300] loss: 1.634, acc: 0.646
******** [step = 350] loss: 1.639, acc: 0.646
******** [step = 400] loss: 1.642, acc: 0.646
******** [step = 450] loss: 1.646, acc: 0.645
******** [step = 500] loss: 1.650, acc: 0.645
******** [step = 550] loss: 1.653, acc: 0.645
******** [step = 600] loss: 1.660, acc: 0.644
******** [step = 650] loss: 1.663, acc: 0.643
******** [step = 700] loss: 1.667, acc: 0.643
******** [step = 750] loss: 1.671, acc: 0.643
******** [step = 800] loss: 1.676, acc: 0.642
******** [step = 850] loss: 1.679, acc: 0.642
EPOCH = 42 loss: 1.679, acc: 0.642, val_loss: 2.048, val_acc: 0.624

================================================================================2025-08_12 12:47:49
******** [step = 50] loss: 1.593, acc: 0.654
******** [step = 100] loss: 1.601, acc: 0.652
******** [step = 150] loss: 1.603, acc: 0.651
******** [step = 200] loss: 1.607, acc: 0.652
******** [step = 250] loss: 1.611, acc: 0.651
******** [step = 300] loss: 1.622, acc: 0.650
******** [step = 350] loss: 1.626, acc: 0.649
******** [step = 400] loss: 1.633, acc: 0.648
******** [step = 450] loss: 1.639, acc: 0.647
******** [step = 500] loss: 1.646, acc: 0.646
******** [step = 550] loss: 1.649, acc: 0.646
******** [step = 600] loss: 1.654, acc: 0.645
******** [step = 650] loss: 1.656, acc: 0.645
******** [step = 700] loss: 1.660, acc: 0.645
******** [step = 750] loss: 1.663, acc: 0.644
******** [step = 800] loss: 1.667, acc: 0.644
******** [step = 850] loss: 1.669, acc: 0.644
EPOCH = 43 loss: 1.669, acc: 0.644, val_loss: 2.039, val_acc: 0.625

================================================================================2025-08_12 12:49:50
******** [step = 50] loss: 1.602, acc: 0.651
******** [step = 100] loss: 1.599, acc: 0.653
******** [step = 150] loss: 1.598, acc: 0.653
******** [step = 200] loss: 1.605, acc: 0.652
******** [step = 250] loss: 1.612, acc: 0.651
******** [step = 300] loss: 1.618, acc: 0.649
******** [step = 350] loss: 1.624, acc: 0.649
******** [step = 400] loss: 1.628, acc: 0.648
******** [step = 450] loss: 1.632, acc: 0.648
******** [step = 500] loss: 1.638, acc: 0.647
******** [step = 550] loss: 1.643, acc: 0.646
******** [step = 600] loss: 1.646, acc: 0.646
******** [step = 650] loss: 1.649, acc: 0.646
******** [step = 700] loss: 1.652, acc: 0.645
******** [step = 750] loss: 1.656, acc: 0.645
******** [step = 800] loss: 1.658, acc: 0.645
******** [step = 850] loss: 1.659, acc: 0.645
EPOCH = 44 loss: 1.659, acc: 0.645, val_loss: 2.049, val_acc: 0.623

================================================================================2025-08_12 12:51:53
******** [step = 50] loss: 1.570, acc: 0.658
******** [step = 100] loss: 1.579, acc: 0.655
******** [step = 150] loss: 1.581, acc: 0.655
******** [step = 200] loss: 1.590, acc: 0.654
******** [step = 250] loss: 1.600, acc: 0.653
******** [step = 300] loss: 1.607, acc: 0.652
******** [step = 350] loss: 1.613, acc: 0.651
******** [step = 400] loss: 1.614, acc: 0.651
******** [step = 450] loss: 1.620, acc: 0.650
******** [step = 500] loss: 1.623, acc: 0.649
******** [step = 550] loss: 1.629, acc: 0.649
******** [step = 600] loss: 1.634, acc: 0.648
******** [step = 650] loss: 1.636, acc: 0.648
******** [step = 700] loss: 1.641, acc: 0.647
******** [step = 750] loss: 1.646, acc: 0.647
******** [step = 800] loss: 1.649, acc: 0.646
******** [step = 850] loss: 1.652, acc: 0.646
EPOCH = 45 loss: 1.652, acc: 0.646, val_loss: 2.030, val_acc: 0.626

================================================================================2025-08_12 12:53:54
******** [step = 50] loss: 1.560, acc: 0.657
******** [step = 100] loss: 1.573, acc: 0.655
******** [step = 150] loss: 1.581, acc: 0.654
******** [step = 200] loss: 1.585, acc: 0.653
******** [step = 250] loss: 1.591, acc: 0.653
******** [step = 300] loss: 1.596, acc: 0.652
******** [step = 350] loss: 1.600, acc: 0.651
******** [step = 400] loss: 1.607, acc: 0.651
******** [step = 450] loss: 1.612, acc: 0.650
******** [step = 500] loss: 1.617, acc: 0.650
******** [step = 550] loss: 1.621, acc: 0.649
******** [step = 600] loss: 1.626, acc: 0.649
******** [step = 650] loss: 1.628, acc: 0.649
******** [step = 700] loss: 1.633, acc: 0.648
******** [step = 750] loss: 1.636, acc: 0.648
******** [step = 800] loss: 1.639, acc: 0.648
******** [step = 850] loss: 1.642, acc: 0.648
EPOCH = 46 loss: 1.642, acc: 0.648, val_loss: 2.031, val_acc: 0.626

================================================================================2025-08_12 12:55:56
******** [step = 50] loss: 1.567, acc: 0.657
******** [step = 100] loss: 1.554, acc: 0.659
******** [step = 150] loss: 1.563, acc: 0.658
******** [step = 200] loss: 1.573, acc: 0.656
******** [step = 250] loss: 1.577, acc: 0.656
******** [step = 300] loss: 1.586, acc: 0.655
******** [step = 350] loss: 1.590, acc: 0.654
******** [step = 400] loss: 1.597, acc: 0.653
******** [step = 450] loss: 1.601, acc: 0.653
******** [step = 500] loss: 1.605, acc: 0.653
******** [step = 550] loss: 1.609, acc: 0.652
******** [step = 600] loss: 1.614, acc: 0.652
******** [step = 650] loss: 1.620, acc: 0.651
******** [step = 700] loss: 1.624, acc: 0.651
******** [step = 750] loss: 1.628, acc: 0.650
******** [step = 800] loss: 1.631, acc: 0.650
******** [step = 850] loss: 1.634, acc: 0.650
EPOCH = 47 loss: 1.634, acc: 0.650, val_loss: 2.018, val_acc: 0.629

================================================================================2025-08_12 12:57:58
******** [step = 50] loss: 1.559, acc: 0.655
******** [step = 100] loss: 1.551, acc: 0.658
******** [step = 150] loss: 1.561, acc: 0.657
******** [step = 200] loss: 1.567, acc: 0.656
******** [step = 250] loss: 1.579, acc: 0.655
******** [step = 300] loss: 1.585, acc: 0.654
******** [step = 350] loss: 1.590, acc: 0.654
******** [step = 400] loss: 1.595, acc: 0.653
******** [step = 450] loss: 1.599, acc: 0.653
******** [step = 500] loss: 1.605, acc: 0.652
******** [step = 550] loss: 1.607, acc: 0.652
******** [step = 600] loss: 1.612, acc: 0.652
******** [step = 650] loss: 1.616, acc: 0.652
******** [step = 700] loss: 1.618, acc: 0.651
******** [step = 750] loss: 1.621, acc: 0.651
******** [step = 800] loss: 1.623, acc: 0.651
******** [step = 850] loss: 1.628, acc: 0.650
EPOCH = 48 loss: 1.628, acc: 0.650, val_loss: 2.019, val_acc: 0.630

================================================================================2025-08_12 13:00:00
******** [step = 50] loss: 1.556, acc: 0.658
******** [step = 100] loss: 1.555, acc: 0.659
******** [step = 150] loss: 1.552, acc: 0.660
******** [step = 200] loss: 1.559, acc: 0.659
******** [step = 250] loss: 1.565, acc: 0.658
******** [step = 300] loss: 1.571, acc: 0.658
******** [step = 350] loss: 1.576, acc: 0.657
******** [step = 400] loss: 1.583, acc: 0.656
******** [step = 450] loss: 1.589, acc: 0.656
******** [step = 500] loss: 1.594, acc: 0.655
******** [step = 550] loss: 1.598, acc: 0.655
******** [step = 600] loss: 1.602, acc: 0.654
******** [step = 650] loss: 1.606, acc: 0.654
******** [step = 700] loss: 1.610, acc: 0.653
******** [step = 750] loss: 1.614, acc: 0.653
******** [step = 800] loss: 1.617, acc: 0.652
******** [step = 850] loss: 1.622, acc: 0.652
EPOCH = 49 loss: 1.622, acc: 0.652, val_loss: 2.011, val_acc: 0.632

================================================================================2025-08_12 13:02:02
******** [step = 50] loss: 1.529, acc: 0.662
******** [step = 100] loss: 1.537, acc: 0.662
******** [step = 150] loss: 1.547, acc: 0.660
******** [step = 200] loss: 1.554, acc: 0.659
******** [step = 250] loss: 1.561, acc: 0.658
******** [step = 300] loss: 1.565, acc: 0.658
******** [step = 350] loss: 1.569, acc: 0.658
******** [step = 400] loss: 1.573, acc: 0.657
******** [step = 450] loss: 1.577, acc: 0.657
******** [step = 500] loss: 1.583, acc: 0.656
******** [step = 550] loss: 1.587, acc: 0.656
******** [step = 600] loss: 1.591, acc: 0.655
******** [step = 650] loss: 1.595, acc: 0.655
******** [step = 700] loss: 1.599, acc: 0.655
******** [step = 750] loss: 1.603, acc: 0.654
******** [step = 800] loss: 1.607, acc: 0.654
******** [step = 850] loss: 1.608, acc: 0.654
EPOCH = 50 loss: 1.608, acc: 0.654, val_loss: 2.012, val_acc: 0.631

================================================================================2025-08_12 13:04:03
******** [step = 50] loss: 1.534, acc: 0.661
******** [step = 100] loss: 1.537, acc: 0.663
******** [step = 150] loss: 1.539, acc: 0.663
******** [step = 200] loss: 1.548, acc: 0.661
******** [step = 250] loss: 1.553, acc: 0.661
******** [step = 300] loss: 1.558, acc: 0.659
******** [step = 350] loss: 1.563, acc: 0.659
******** [step = 400] loss: 1.567, acc: 0.658
******** [step = 450] loss: 1.572, acc: 0.657
******** [step = 500] loss: 1.577, acc: 0.657
******** [step = 550] loss: 1.581, acc: 0.657
******** [step = 600] loss: 1.586, acc: 0.656
******** [step = 650] loss: 1.591, acc: 0.656
******** [step = 700] loss: 1.595, acc: 0.655
******** [step = 750] loss: 1.598, acc: 0.655
******** [step = 800] loss: 1.602, acc: 0.655
******** [step = 850] loss: 1.603, acc: 0.655
EPOCH = 51 loss: 1.603, acc: 0.655, val_loss: 2.008, val_acc: 0.631

================================================================================2025-08_12 13:06:06
******** [step = 50] loss: 1.531, acc: 0.662
******** [step = 100] loss: 1.535, acc: 0.663
******** [step = 150] loss: 1.526, acc: 0.663
******** [step = 200] loss: 1.538, acc: 0.662
******** [step = 250] loss: 1.544, acc: 0.661
******** [step = 300] loss: 1.546, acc: 0.661
******** [step = 350] loss: 1.551, acc: 0.660
******** [step = 400] loss: 1.559, acc: 0.659
******** [step = 450] loss: 1.566, acc: 0.658
******** [step = 500] loss: 1.571, acc: 0.658
******** [step = 550] loss: 1.575, acc: 0.657
******** [step = 600] loss: 1.579, acc: 0.657
******** [step = 650] loss: 1.584, acc: 0.656
******** [step = 700] loss: 1.587, acc: 0.656
******** [step = 750] loss: 1.591, acc: 0.656
******** [step = 800] loss: 1.595, acc: 0.655
******** [step = 850] loss: 1.597, acc: 0.655
EPOCH = 52 loss: 1.597, acc: 0.655, val_loss: 2.000, val_acc: 0.634

================================================================================2025-08_12 13:08:07
******** [step = 50] loss: 1.533, acc: 0.663
******** [step = 100] loss: 1.534, acc: 0.663
******** [step = 150] loss: 1.529, acc: 0.664
******** [step = 200] loss: 1.533, acc: 0.664
******** [step = 250] loss: 1.544, acc: 0.663
******** [step = 300] loss: 1.548, acc: 0.662
******** [step = 350] loss: 1.549, acc: 0.663
******** [step = 400] loss: 1.552, acc: 0.662
******** [step = 450] loss: 1.557, acc: 0.661
******** [step = 500] loss: 1.564, acc: 0.660
******** [step = 550] loss: 1.567, acc: 0.660
******** [step = 600] loss: 1.570, acc: 0.660
******** [step = 650] loss: 1.574, acc: 0.659
******** [step = 700] loss: 1.579, acc: 0.659
******** [step = 750] loss: 1.583, acc: 0.658
******** [step = 800] loss: 1.586, acc: 0.658
******** [step = 850] loss: 1.589, acc: 0.658
EPOCH = 53 loss: 1.589, acc: 0.658, val_loss: 2.003, val_acc: 0.632

================================================================================2025-08_12 13:10:09
******** [step = 50] loss: 1.500, acc: 0.668
******** [step = 100] loss: 1.496, acc: 0.669
******** [step = 150] loss: 1.504, acc: 0.667
******** [step = 200] loss: 1.514, acc: 0.666
******** [step = 250] loss: 1.518, acc: 0.666
******** [step = 300] loss: 1.528, acc: 0.664
******** [step = 350] loss: 1.535, acc: 0.664
******** [step = 400] loss: 1.545, acc: 0.663
******** [step = 450] loss: 1.552, acc: 0.662
******** [step = 500] loss: 1.555, acc: 0.661
******** [step = 550] loss: 1.560, acc: 0.661
******** [step = 600] loss: 1.564, acc: 0.660
******** [step = 650] loss: 1.568, acc: 0.660
******** [step = 700] loss: 1.571, acc: 0.660
******** [step = 750] loss: 1.575, acc: 0.659
******** [step = 800] loss: 1.577, acc: 0.659
******** [step = 850] loss: 1.583, acc: 0.659
EPOCH = 54 loss: 1.583, acc: 0.659, val_loss: 1.997, val_acc: 0.634

================================================================================2025-08_12 13:12:10
******** [step = 50] loss: 1.510, acc: 0.666
******** [step = 100] loss: 1.504, acc: 0.667
******** [step = 150] loss: 1.507, acc: 0.667
******** [step = 200] loss: 1.513, acc: 0.666
******** [step = 250] loss: 1.524, acc: 0.665
******** [step = 300] loss: 1.532, acc: 0.664
******** [step = 350] loss: 1.533, acc: 0.664
******** [step = 400] loss: 1.540, acc: 0.663
******** [step = 450] loss: 1.545, acc: 0.662
******** [step = 500] loss: 1.549, acc: 0.662
******** [step = 550] loss: 1.553, acc: 0.662
******** [step = 600] loss: 1.558, acc: 0.661
******** [step = 650] loss: 1.561, acc: 0.661
******** [step = 700] loss: 1.564, acc: 0.661
******** [step = 750] loss: 1.568, acc: 0.660
******** [step = 800] loss: 1.573, acc: 0.660
******** [step = 850] loss: 1.575, acc: 0.660
EPOCH = 55 loss: 1.575, acc: 0.660, val_loss: 1.993, val_acc: 0.635

================================================================================2025-08_12 13:14:12
******** [step = 50] loss: 1.486, acc: 0.670
******** [step = 100] loss: 1.491, acc: 0.670
******** [step = 150] loss: 1.501, acc: 0.669
******** [step = 200] loss: 1.510, acc: 0.667
******** [step = 250] loss: 1.521, acc: 0.666
******** [step = 300] loss: 1.526, acc: 0.665
******** [step = 350] loss: 1.531, acc: 0.665
******** [step = 400] loss: 1.533, acc: 0.665
******** [step = 450] loss: 1.539, acc: 0.664
******** [step = 500] loss: 1.544, acc: 0.663
******** [step = 550] loss: 1.547, acc: 0.663
******** [step = 600] loss: 1.549, acc: 0.663
******** [step = 650] loss: 1.553, acc: 0.662
******** [step = 700] loss: 1.557, acc: 0.662
******** [step = 750] loss: 1.561, acc: 0.661
******** [step = 800] loss: 1.566, acc: 0.661
******** [step = 850] loss: 1.572, acc: 0.660
EPOCH = 56 loss: 1.572, acc: 0.660, val_loss: 1.994, val_acc: 0.636

================================================================================2025-08_12 13:16:14
******** [step = 50] loss: 1.511, acc: 0.666
******** [step = 100] loss: 1.501, acc: 0.669
******** [step = 150] loss: 1.507, acc: 0.668
******** [step = 200] loss: 1.508, acc: 0.668
******** [step = 250] loss: 1.512, acc: 0.668
******** [step = 300] loss: 1.518, acc: 0.666
******** [step = 350] loss: 1.520, acc: 0.666
******** [step = 400] loss: 1.526, acc: 0.665
******** [step = 450] loss: 1.531, acc: 0.664
******** [step = 500] loss: 1.538, acc: 0.664
******** [step = 550] loss: 1.542, acc: 0.663
******** [step = 600] loss: 1.546, acc: 0.663
******** [step = 650] loss: 1.549, acc: 0.663
******** [step = 700] loss: 1.554, acc: 0.662
******** [step = 750] loss: 1.557, acc: 0.662
******** [step = 800] loss: 1.560, acc: 0.662
******** [step = 850] loss: 1.562, acc: 0.662
EPOCH = 57 loss: 1.562, acc: 0.662, val_loss: 1.985, val_acc: 0.638

================================================================================2025-08_12 13:18:16
******** [step = 50] loss: 1.498, acc: 0.667
******** [step = 100] loss: 1.493, acc: 0.669
******** [step = 150] loss: 1.496, acc: 0.669
******** [step = 200] loss: 1.499, acc: 0.669
******** [step = 250] loss: 1.507, acc: 0.668
******** [step = 300] loss: 1.511, acc: 0.667
******** [step = 350] loss: 1.516, acc: 0.666
******** [step = 400] loss: 1.523, acc: 0.666
******** [step = 450] loss: 1.526, acc: 0.665
******** [step = 500] loss: 1.532, acc: 0.665
******** [step = 550] loss: 1.536, acc: 0.664
******** [step = 600] loss: 1.542, acc: 0.664
******** [step = 650] loss: 1.547, acc: 0.663
******** [step = 700] loss: 1.550, acc: 0.663
******** [step = 750] loss: 1.554, acc: 0.663
******** [step = 800] loss: 1.557, acc: 0.662
******** [step = 850] loss: 1.559, acc: 0.662
EPOCH = 58 loss: 1.559, acc: 0.662, val_loss: 1.987, val_acc: 0.637

================================================================================2025-08_12 13:20:17
******** [step = 50] loss: 1.483, acc: 0.673
******** [step = 100] loss: 1.487, acc: 0.671
******** [step = 150] loss: 1.485, acc: 0.671
******** [step = 200] loss: 1.490, acc: 0.670
******** [step = 250] loss: 1.496, acc: 0.670
******** [step = 300] loss: 1.499, acc: 0.670
******** [step = 350] loss: 1.506, acc: 0.669
******** [step = 400] loss: 1.513, acc: 0.668
******** [step = 450] loss: 1.518, acc: 0.667
******** [step = 500] loss: 1.523, acc: 0.667
******** [step = 550] loss: 1.527, acc: 0.666
******** [step = 600] loss: 1.531, acc: 0.666
******** [step = 650] loss: 1.536, acc: 0.665
******** [step = 700] loss: 1.540, acc: 0.665
******** [step = 750] loss: 1.545, acc: 0.664
******** [step = 800] loss: 1.548, acc: 0.664
******** [step = 850] loss: 1.552, acc: 0.663
EPOCH = 59 loss: 1.552, acc: 0.663, val_loss: 1.987, val_acc: 0.636

================================================================================2025-08_12 13:22:20
******** [step = 50] loss: 1.459, acc: 0.676
******** [step = 100] loss: 1.476, acc: 0.673
******** [step = 150] loss: 1.483, acc: 0.672
******** [step = 200] loss: 1.487, acc: 0.671
******** [step = 250] loss: 1.494, acc: 0.670
******** [step = 300] loss: 1.500, acc: 0.669
******** [step = 350] loss: 1.508, acc: 0.668
******** [step = 400] loss: 1.511, acc: 0.668
******** [step = 450] loss: 1.515, acc: 0.668
******** [step = 500] loss: 1.521, acc: 0.667
******** [step = 550] loss: 1.525, acc: 0.667
******** [step = 600] loss: 1.531, acc: 0.666
******** [step = 650] loss: 1.533, acc: 0.666
******** [step = 700] loss: 1.537, acc: 0.665
******** [step = 750] loss: 1.540, acc: 0.665
******** [step = 800] loss: 1.544, acc: 0.665
******** [step = 850] loss: 1.547, acc: 0.664
EPOCH = 60 loss: 1.547, acc: 0.664, val_loss: 1.978, val_acc: 0.639

================================================================================2025-08_12 13:24:22
******** [step = 50] loss: 1.443, acc: 0.678
******** [step = 100] loss: 1.463, acc: 0.675
******** [step = 150] loss: 1.471, acc: 0.675
******** [step = 200] loss: 1.482, acc: 0.672
******** [step = 250] loss: 1.488, acc: 0.671
******** [step = 300] loss: 1.494, acc: 0.671
******** [step = 350] loss: 1.497, acc: 0.670
******** [step = 400] loss: 1.501, acc: 0.670
******** [step = 450] loss: 1.505, acc: 0.670
******** [step = 500] loss: 1.510, acc: 0.669
******** [step = 550] loss: 1.513, acc: 0.669
******** [step = 600] loss: 1.517, acc: 0.668
******** [step = 650] loss: 1.522, acc: 0.668
******** [step = 700] loss: 1.526, acc: 0.667
******** [step = 750] loss: 1.530, acc: 0.667
******** [step = 800] loss: 1.534, acc: 0.666
******** [step = 850] loss: 1.538, acc: 0.666
EPOCH = 61 loss: 1.538, acc: 0.666, val_loss: 1.977, val_acc: 0.640

================================================================================2025-08_12 13:26:24
******** [step = 50] loss: 1.430, acc: 0.678
******** [step = 100] loss: 1.446, acc: 0.677
******** [step = 150] loss: 1.461, acc: 0.675
******** [step = 200] loss: 1.470, acc: 0.673
******** [step = 250] loss: 1.471, acc: 0.673
******** [step = 300] loss: 1.476, acc: 0.673
******** [step = 350] loss: 1.483, acc: 0.672
******** [step = 400] loss: 1.489, acc: 0.671
******** [step = 450] loss: 1.496, acc: 0.671
******** [step = 500] loss: 1.502, acc: 0.670
******** [step = 550] loss: 1.506, acc: 0.669
******** [step = 600] loss: 1.511, acc: 0.669
******** [step = 650] loss: 1.515, acc: 0.669
******** [step = 700] loss: 1.519, acc: 0.668
******** [step = 750] loss: 1.524, acc: 0.668
******** [step = 800] loss: 1.529, acc: 0.667
******** [step = 850] loss: 1.534, acc: 0.667
EPOCH = 62 loss: 1.534, acc: 0.667, val_loss: 1.969, val_acc: 0.641

================================================================================2025-08_12 13:28:25
******** [step = 50] loss: 1.436, acc: 0.680
******** [step = 100] loss: 1.460, acc: 0.677
******** [step = 150] loss: 1.470, acc: 0.675
******** [step = 200] loss: 1.477, acc: 0.674
******** [step = 250] loss: 1.483, acc: 0.673
******** [step = 300] loss: 1.486, acc: 0.672
******** [step = 350] loss: 1.490, acc: 0.671
******** [step = 400] loss: 1.494, acc: 0.671
******** [step = 450] loss: 1.500, acc: 0.670
******** [step = 500] loss: 1.504, acc: 0.669
******** [step = 550] loss: 1.507, acc: 0.669
******** [step = 600] loss: 1.511, acc: 0.669
******** [step = 650] loss: 1.514, acc: 0.669
******** [step = 700] loss: 1.519, acc: 0.669
******** [step = 750] loss: 1.521, acc: 0.668
******** [step = 800] loss: 1.525, acc: 0.668
******** [step = 850] loss: 1.528, acc: 0.667
EPOCH = 63 loss: 1.528, acc: 0.667, val_loss: 1.967, val_acc: 0.641

================================================================================2025-08_12 13:30:26
******** [step = 50] loss: 1.456, acc: 0.673
******** [step = 100] loss: 1.455, acc: 0.675
******** [step = 150] loss: 1.455, acc: 0.675
******** [step = 200] loss: 1.457, acc: 0.675
******** [step = 250] loss: 1.465, acc: 0.674
******** [step = 300] loss: 1.473, acc: 0.673
******** [step = 350] loss: 1.477, acc: 0.673
******** [step = 400] loss: 1.481, acc: 0.672
******** [step = 450] loss: 1.486, acc: 0.672
******** [step = 500] loss: 1.491, acc: 0.672
******** [step = 550] loss: 1.497, acc: 0.671
******** [step = 600] loss: 1.500, acc: 0.671
******** [step = 650] loss: 1.505, acc: 0.671
******** [step = 700] loss: 1.510, acc: 0.670
******** [step = 750] loss: 1.512, acc: 0.670
******** [step = 800] loss: 1.515, acc: 0.670
******** [step = 850] loss: 1.519, acc: 0.669
EPOCH = 64 loss: 1.519, acc: 0.669, val_loss: 1.969, val_acc: 0.640

================================================================================2025-08_12 13:32:29
******** [step = 50] loss: 1.438, acc: 0.679
******** [step = 100] loss: 1.440, acc: 0.678
******** [step = 150] loss: 1.447, acc: 0.677
******** [step = 200] loss: 1.450, acc: 0.676
******** [step = 250] loss: 1.459, acc: 0.675
******** [step = 300] loss: 1.466, acc: 0.674
******** [step = 350] loss: 1.470, acc: 0.674
******** [step = 400] loss: 1.476, acc: 0.673
******** [step = 450] loss: 1.480, acc: 0.673
******** [step = 500] loss: 1.486, acc: 0.672
******** [step = 550] loss: 1.491, acc: 0.671
******** [step = 600] loss: 1.496, acc: 0.671
******** [step = 650] loss: 1.499, acc: 0.671
******** [step = 700] loss: 1.503, acc: 0.670
******** [step = 750] loss: 1.509, acc: 0.670
******** [step = 800] loss: 1.513, acc: 0.670
******** [step = 850] loss: 1.516, acc: 0.669
EPOCH = 65 loss: 1.516, acc: 0.669, val_loss: 1.958, val_acc: 0.644

================================================================================2025-08_12 13:34:30
******** [step = 50] loss: 1.448, acc: 0.678
******** [step = 100] loss: 1.435, acc: 0.681
******** [step = 150] loss: 1.442, acc: 0.678
******** [step = 200] loss: 1.445, acc: 0.678
******** [step = 250] loss: 1.455, acc: 0.676
******** [step = 300] loss: 1.459, acc: 0.676
******** [step = 350] loss: 1.467, acc: 0.675
******** [step = 400] loss: 1.473, acc: 0.675
******** [step = 450] loss: 1.475, acc: 0.674
******** [step = 500] loss: 1.482, acc: 0.674
******** [step = 550] loss: 1.488, acc: 0.673
******** [step = 600] loss: 1.492, acc: 0.672
******** [step = 650] loss: 1.496, acc: 0.672
******** [step = 700] loss: 1.500, acc: 0.671
******** [step = 750] loss: 1.503, acc: 0.671
******** [step = 800] loss: 1.507, acc: 0.671
******** [step = 850] loss: 1.511, acc: 0.670
EPOCH = 66 loss: 1.511, acc: 0.670, val_loss: 1.959, val_acc: 0.643

================================================================================2025-08_12 13:36:31
******** [step = 50] loss: 1.426, acc: 0.682
******** [step = 100] loss: 1.439, acc: 0.680
******** [step = 150] loss: 1.446, acc: 0.679
******** [step = 200] loss: 1.450, acc: 0.679
******** [step = 250] loss: 1.457, acc: 0.678
******** [step = 300] loss: 1.455, acc: 0.678
******** [step = 350] loss: 1.459, acc: 0.677
******** [step = 400] loss: 1.465, acc: 0.677
******** [step = 450] loss: 1.469, acc: 0.676
******** [step = 500] loss: 1.475, acc: 0.675
******** [step = 550] loss: 1.480, acc: 0.675
******** [step = 600] loss: 1.484, acc: 0.674
******** [step = 650] loss: 1.490, acc: 0.674
******** [step = 700] loss: 1.493, acc: 0.673
******** [step = 750] loss: 1.498, acc: 0.673
******** [step = 800] loss: 1.501, acc: 0.672
******** [step = 850] loss: 1.511, acc: 0.671
EPOCH = 67 loss: 1.511, acc: 0.671, val_loss: 1.960, val_acc: 0.643

================================================================================2025-08_12 13:38:32
******** [step = 50] loss: 1.413, acc: 0.681
******** [step = 100] loss: 1.421, acc: 0.681
******** [step = 150] loss: 1.427, acc: 0.680
******** [step = 200] loss: 1.433, acc: 0.679
******** [step = 250] loss: 1.444, acc: 0.678
******** [step = 300] loss: 1.451, acc: 0.677
******** [step = 350] loss: 1.457, acc: 0.677
******** [step = 400] loss: 1.462, acc: 0.676
******** [step = 450] loss: 1.468, acc: 0.676
******** [step = 500] loss: 1.473, acc: 0.675
******** [step = 550] loss: 1.476, acc: 0.675
******** [step = 600] loss: 1.481, acc: 0.674
******** [step = 650] loss: 1.486, acc: 0.674
******** [step = 700] loss: 1.489, acc: 0.674
******** [step = 750] loss: 1.492, acc: 0.673
******** [step = 800] loss: 1.495, acc: 0.673
******** [step = 850] loss: 1.499, acc: 0.673
EPOCH = 68 loss: 1.499, acc: 0.673, val_loss: 1.961, val_acc: 0.642

================================================================================2025-08_12 13:40:33
******** [step = 50] loss: 1.431, acc: 0.680
******** [step = 100] loss: 1.440, acc: 0.678
******** [step = 150] loss: 1.442, acc: 0.678
******** [step = 200] loss: 1.444, acc: 0.678
******** [step = 250] loss: 1.448, acc: 0.677
******** [step = 300] loss: 1.455, acc: 0.676
******** [step = 350] loss: 1.458, acc: 0.676
******** [step = 400] loss: 1.460, acc: 0.676
******** [step = 450] loss: 1.463, acc: 0.676
******** [step = 500] loss: 1.470, acc: 0.675
******** [step = 550] loss: 1.475, acc: 0.675
******** [step = 600] loss: 1.480, acc: 0.674
******** [step = 650] loss: 1.483, acc: 0.674
******** [step = 700] loss: 1.484, acc: 0.674
******** [step = 750] loss: 1.489, acc: 0.673
******** [step = 800] loss: 1.493, acc: 0.673
******** [step = 850] loss: 1.494, acc: 0.673
EPOCH = 69 loss: 1.494, acc: 0.673, val_loss: 1.958, val_acc: 0.645

================================================================================2025-08_12 13:42:35
******** [step = 50] loss: 1.411, acc: 0.685
******** [step = 100] loss: 1.422, acc: 0.681
******** [step = 150] loss: 1.435, acc: 0.680
******** [step = 200] loss: 1.436, acc: 0.679
******** [step = 250] loss: 1.438, acc: 0.679
******** [step = 300] loss: 1.445, acc: 0.678
******** [step = 350] loss: 1.451, acc: 0.678
******** [step = 400] loss: 1.453, acc: 0.677
******** [step = 450] loss: 1.458, acc: 0.677
******** [step = 500] loss: 1.464, acc: 0.676
******** [step = 550] loss: 1.467, acc: 0.676
******** [step = 600] loss: 1.470, acc: 0.676
******** [step = 650] loss: 1.474, acc: 0.676
******** [step = 700] loss: 1.478, acc: 0.675
******** [step = 750] loss: 1.483, acc: 0.674
******** [step = 800] loss: 1.487, acc: 0.674
******** [step = 850] loss: 1.491, acc: 0.673
EPOCH = 70 loss: 1.491, acc: 0.673, val_loss: 1.950, val_acc: 0.645

================================================================================2025-08_12 13:44:38
******** [step = 50] loss: 1.413, acc: 0.683
******** [step = 100] loss: 1.418, acc: 0.683
******** [step = 150] loss: 1.422, acc: 0.684
******** [step = 200] loss: 1.432, acc: 0.682
******** [step = 250] loss: 1.437, acc: 0.681
******** [step = 300] loss: 1.443, acc: 0.680
******** [step = 350] loss: 1.448, acc: 0.679
******** [step = 400] loss: 1.454, acc: 0.678
******** [step = 450] loss: 1.458, acc: 0.678
******** [step = 500] loss: 1.461, acc: 0.678
******** [step = 550] loss: 1.463, acc: 0.677
******** [step = 600] loss: 1.468, acc: 0.677
******** [step = 650] loss: 1.473, acc: 0.676
******** [step = 700] loss: 1.476, acc: 0.676
******** [step = 750] loss: 1.479, acc: 0.675
******** [step = 800] loss: 1.483, acc: 0.675
******** [step = 850] loss: 1.487, acc: 0.675
EPOCH = 71 loss: 1.487, acc: 0.675, val_loss: 1.952, val_acc: 0.645

================================================================================2025-08_12 13:46:39
******** [step = 50] loss: 1.413, acc: 0.682
******** [step = 100] loss: 1.407, acc: 0.685
******** [step = 150] loss: 1.413, acc: 0.684
******** [step = 200] loss: 1.419, acc: 0.683
******** [step = 250] loss: 1.427, acc: 0.682
******** [step = 300] loss: 1.433, acc: 0.681
******** [step = 350] loss: 1.439, acc: 0.680
******** [step = 400] loss: 1.444, acc: 0.679
******** [step = 450] loss: 1.447, acc: 0.679
******** [step = 500] loss: 1.450, acc: 0.679
******** [step = 550] loss: 1.455, acc: 0.678
******** [step = 600] loss: 1.460, acc: 0.678
******** [step = 650] loss: 1.464, acc: 0.677
******** [step = 700] loss: 1.468, acc: 0.677
******** [step = 750] loss: 1.473, acc: 0.676
******** [step = 800] loss: 1.477, acc: 0.676
******** [step = 850] loss: 1.481, acc: 0.676
EPOCH = 72 loss: 1.481, acc: 0.676, val_loss: 1.944, val_acc: 0.647

================================================================================2025-08_12 13:48:40
******** [step = 50] loss: 1.394, acc: 0.686
******** [step = 100] loss: 1.396, acc: 0.686
******** [step = 150] loss: 1.406, acc: 0.685
******** [step = 200] loss: 1.413, acc: 0.684
******** [step = 250] loss: 1.422, acc: 0.683
******** [step = 300] loss: 1.426, acc: 0.682
******** [step = 350] loss: 1.429, acc: 0.682
******** [step = 400] loss: 1.434, acc: 0.681
******** [step = 450] loss: 1.442, acc: 0.681
******** [step = 500] loss: 1.447, acc: 0.680
******** [step = 550] loss: 1.452, acc: 0.679
******** [step = 600] loss: 1.457, acc: 0.679
******** [step = 650] loss: 1.461, acc: 0.679
******** [step = 700] loss: 1.464, acc: 0.678
******** [step = 750] loss: 1.468, acc: 0.678
******** [step = 800] loss: 1.471, acc: 0.677
******** [step = 850] loss: 1.475, acc: 0.677
EPOCH = 73 loss: 1.475, acc: 0.677, val_loss: 1.946, val_acc: 0.648

================================================================================2025-08_12 13:50:41
******** [step = 50] loss: 1.376, acc: 0.689
******** [step = 100] loss: 1.400, acc: 0.685
******** [step = 150] loss: 1.403, acc: 0.685
******** [step = 200] loss: 1.409, acc: 0.684
******** [step = 250] loss: 1.412, acc: 0.684
******** [step = 300] loss: 1.417, acc: 0.684
******** [step = 350] loss: 1.426, acc: 0.682
******** [step = 400] loss: 1.429, acc: 0.682
******** [step = 450] loss: 1.435, acc: 0.681
******** [step = 500] loss: 1.441, acc: 0.680
******** [step = 550] loss: 1.445, acc: 0.680
******** [step = 600] loss: 1.450, acc: 0.679
******** [step = 650] loss: 1.454, acc: 0.679
******** [step = 700] loss: 1.459, acc: 0.679
******** [step = 750] loss: 1.463, acc: 0.678
******** [step = 800] loss: 1.467, acc: 0.678
******** [step = 850] loss: 1.470, acc: 0.678
EPOCH = 74 loss: 1.470, acc: 0.678, val_loss: 1.940, val_acc: 0.648

================================================================================2025-08_12 13:52:42
******** [step = 50] loss: 1.360, acc: 0.694
******** [step = 100] loss: 1.373, acc: 0.691
******** [step = 150] loss: 1.383, acc: 0.690
******** [step = 200] loss: 1.394, acc: 0.688
******** [step = 250] loss: 1.404, acc: 0.686
******** [step = 300] loss: 1.410, acc: 0.685
******** [step = 350] loss: 1.418, acc: 0.684
******** [step = 400] loss: 1.425, acc: 0.683
******** [step = 450] loss: 1.430, acc: 0.682
******** [step = 500] loss: 1.434, acc: 0.682
******** [step = 550] loss: 1.440, acc: 0.681
******** [step = 600] loss: 1.445, acc: 0.680
******** [step = 650] loss: 1.449, acc: 0.680
******** [step = 700] loss: 1.454, acc: 0.679
******** [step = 750] loss: 1.459, acc: 0.679
******** [step = 800] loss: 1.463, acc: 0.678
******** [step = 850] loss: 1.465, acc: 0.678
EPOCH = 75 loss: 1.465, acc: 0.678, val_loss: 1.941, val_acc: 0.647

================================================================================2025-08_12 13:54:44
******** [step = 50] loss: 1.392, acc: 0.686
******** [step = 100] loss: 1.389, acc: 0.687
******** [step = 150] loss: 1.397, acc: 0.686
******** [step = 200] loss: 1.400, acc: 0.686
******** [step = 250] loss: 1.404, acc: 0.685
******** [step = 300] loss: 1.411, acc: 0.684
******** [step = 350] loss: 1.414, acc: 0.684
******** [step = 400] loss: 1.422, acc: 0.684
******** [step = 450] loss: 1.428, acc: 0.683
******** [step = 500] loss: 1.432, acc: 0.682
******** [step = 550] loss: 1.437, acc: 0.682
******** [step = 600] loss: 1.442, acc: 0.681
******** [step = 650] loss: 1.445, acc: 0.681
******** [step = 700] loss: 1.449, acc: 0.680
******** [step = 750] loss: 1.451, acc: 0.680
******** [step = 800] loss: 1.455, acc: 0.680
******** [step = 850] loss: 1.460, acc: 0.679
EPOCH = 76 loss: 1.460, acc: 0.679, val_loss: 1.938, val_acc: 0.648

================================================================================2025-08_12 13:56:45
******** [step = 50] loss: 1.379, acc: 0.689
******** [step = 100] loss: 1.384, acc: 0.688
******** [step = 150] loss: 1.392, acc: 0.688
******** [step = 200] loss: 1.397, acc: 0.688
******** [step = 250] loss: 1.402, acc: 0.687
******** [step = 300] loss: 1.408, acc: 0.686
******** [step = 350] loss: 1.413, acc: 0.685
******** [step = 400] loss: 1.419, acc: 0.684
******** [step = 450] loss: 1.424, acc: 0.684
******** [step = 500] loss: 1.428, acc: 0.684
******** [step = 550] loss: 1.430, acc: 0.683
******** [step = 600] loss: 1.437, acc: 0.683
******** [step = 650] loss: 1.440, acc: 0.682
******** [step = 700] loss: 1.446, acc: 0.682
******** [step = 750] loss: 1.450, acc: 0.681
******** [step = 800] loss: 1.455, acc: 0.681
******** [step = 850] loss: 1.457, acc: 0.680
EPOCH = 77 loss: 1.457, acc: 0.680, val_loss: 1.937, val_acc: 0.648

================================================================================2025-08_12 13:58:48
******** [step = 50] loss: 1.377, acc: 0.690
******** [step = 100] loss: 1.383, acc: 0.688
******** [step = 150] loss: 1.390, acc: 0.687
******** [step = 200] loss: 1.397, acc: 0.686
******** [step = 250] loss: 1.402, acc: 0.685
******** [step = 300] loss: 1.404, acc: 0.686
******** [step = 350] loss: 1.412, acc: 0.685
******** [step = 400] loss: 1.417, acc: 0.684
******** [step = 450] loss: 1.423, acc: 0.684
******** [step = 500] loss: 1.427, acc: 0.683
******** [step = 550] loss: 1.429, acc: 0.683
******** [step = 600] loss: 1.433, acc: 0.683
******** [step = 650] loss: 1.436, acc: 0.682
******** [step = 700] loss: 1.440, acc: 0.682
******** [step = 750] loss: 1.444, acc: 0.681
******** [step = 800] loss: 1.449, acc: 0.681
******** [step = 850] loss: 1.452, acc: 0.681
EPOCH = 78 loss: 1.452, acc: 0.681, val_loss: 1.935, val_acc: 0.648

================================================================================2025-08_12 14:00:49
******** [step = 50] loss: 1.380, acc: 0.688
******** [step = 100] loss: 1.377, acc: 0.689
******** [step = 150] loss: 1.382, acc: 0.689
******** [step = 200] loss: 1.389, acc: 0.688
******** [step = 250] loss: 1.389, acc: 0.688
******** [step = 300] loss: 1.396, acc: 0.687
******** [step = 350] loss: 1.402, acc: 0.686
******** [step = 400] loss: 1.408, acc: 0.686
******** [step = 450] loss: 1.412, acc: 0.685
******** [step = 500] loss: 1.417, acc: 0.685
******** [step = 550] loss: 1.422, acc: 0.684
******** [step = 600] loss: 1.428, acc: 0.683
******** [step = 650] loss: 1.434, acc: 0.683
******** [step = 700] loss: 1.436, acc: 0.683
******** [step = 750] loss: 1.441, acc: 0.682
******** [step = 800] loss: 1.445, acc: 0.682
******** [step = 850] loss: 1.448, acc: 0.682
EPOCH = 79 loss: 1.448, acc: 0.682, val_loss: 1.934, val_acc: 0.649

================================================================================2025-08_12 14:02:50
******** [step = 50] loss: 1.394, acc: 0.688
******** [step = 100] loss: 1.382, acc: 0.689
******** [step = 150] loss: 1.377, acc: 0.690
******** [step = 200] loss: 1.383, acc: 0.689
******** [step = 250] loss: 1.390, acc: 0.688
******** [step = 300] loss: 1.397, acc: 0.688
******** [step = 350] loss: 1.399, acc: 0.687
******** [step = 400] loss: 1.405, acc: 0.687
******** [step = 450] loss: 1.410, acc: 0.686
******** [step = 500] loss: 1.415, acc: 0.686
******** [step = 550] loss: 1.422, acc: 0.685
******** [step = 600] loss: 1.425, acc: 0.684
******** [step = 650] loss: 1.427, acc: 0.684
******** [step = 700] loss: 1.431, acc: 0.684
******** [step = 750] loss: 1.436, acc: 0.684
******** [step = 800] loss: 1.440, acc: 0.683
******** [step = 850] loss: 1.443, acc: 0.683
EPOCH = 80 loss: 1.443, acc: 0.683, val_loss: 1.926, val_acc: 0.651

================================================================================2025-08_12 14:04:52
finishing training...
Training complete in 162m 49s
    epoch  ...   val_acc
0     1.0  ...  0.390124
1     2.0  ...  0.425689
2     3.0  ...  0.451930
3     4.0  ...  0.471634
4     5.0  ...  0.483662
..    ...  ...       ...
75   76.0  ...  0.648383
76   77.0  ...  0.647903
77   78.0  ...  0.648374
78   79.0  ...  0.649423
79   80.0  ...  0.651096

[80 rows x 5 columns]
== Done ==
Tue Aug 12 02:05:17 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-12-2025 14:05:17
Job ID:        6954108
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=11:06:00,vmem=0,walltime=02:46:30,mem=34960K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-011-34-0
---------------------------------------
