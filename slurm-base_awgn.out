---------------------------------------
Begin Slurm Prolog: Aug-11-2025 16:10:30
Job ID:    6901215
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Mon Aug 11 04:10:30 PM EDT 2025
atl1-1-02-008-35-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Mon Aug 11 16:10:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   42C    P0             30W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 107082.79it/s] 29%|██▊       | 38716/135842 [00:00<00:00, 160294.48it/s] 41%|████      | 55999/135842 [00:00<00:00, 122605.43it/s] 51%|█████▏    | 69629/135842 [00:00<00:00, 95815.82it/s]  66%|██████▌   | 89774/135842 [00:00<00:00, 121674.25it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 91507.70it/s] 92%|█████████▏| 125580/135842 [00:01<00:00, 110128.28it/s]100%|██████████| 135842/135842 [00:01<00:00, 113393.77it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47945.98it/s] 33%|███▎      | 35832/108673 [00:00<00:00, 86486.96it/s] 50%|████▉     | 54187/108673 [00:00<00:00, 114393.84it/s] 67%|██████▋   | 72682/108673 [00:00<00:00, 134933.24it/s] 82%|████████▏ | 89271/108673 [00:01<00:00, 72475.52it/s]  99%|█████████▉| 107657/108673 [00:01<00:00, 91894.67it/s]100%|██████████| 108673/108673 [00:01<00:00, 90246.26it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 67%|██████▋   | 18096/27169 [00:00<00:00, 180945.50it/s]100%|██████████| 27169/27169 [00:00<00:00, 182397.53it/s]
tensor([  3,  27,  71, 204, 142,  26, 121, 156, 641, 123, 156,  53,  57,  88,
         43,  13,  20,  13,  27,  71,   4,   2,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,   10,   15,  348,    9,  256,    6,   20,   13,   10,  632,   22,
         325,  153,  939,   10,  144, 5083,  255,   97,   82,  253,  114,    4,
           2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([  3,   6,  22,  11,  43, 432,   6,  63,  62,  10, 373,  26, 731,   6,
         29, 135,  85, 104, 703,   7, 154,   4,   2,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1]) torch.int64
*************************** start training...

================================================================================2025-08_11 16:13:11
******** [step = 50] loss: 9.279, acc: 0.018
******** [step = 100] loss: 8.873, acc: 0.104
******** [step = 150] loss: 8.536, acc: 0.143
******** [step = 200] loss: 8.210, acc: 0.169
******** [step = 250] loss: 7.904, acc: 0.185
******** [step = 300] loss: 7.600, acc: 0.196
******** [step = 350] loss: 7.304, acc: 0.206
******** [step = 400] loss: 7.027, acc: 0.217
******** [step = 450] loss: 6.781, acc: 0.228
******** [step = 500] loss: 6.562, acc: 0.239
******** [step = 550] loss: 6.371, acc: 0.248
******** [step = 600] loss: 6.200, acc: 0.257
******** [step = 650] loss: 6.047, acc: 0.265
******** [step = 700] loss: 5.908, acc: 0.272
******** [step = 750] loss: 5.784, acc: 0.279
******** [step = 800] loss: 5.670, acc: 0.285
******** [step = 850] loss: 5.563, acc: 0.291
EPOCH = 1 loss: 5.563, acc: 0.291, val_loss: 3.753, val_acc: 0.398

================================================================================2025-08_11 16:15:18
******** [step = 50] loss: 3.807, acc: 0.385
******** [step = 100] loss: 3.792, acc: 0.386
******** [step = 150] loss: 3.776, acc: 0.388
******** [step = 200] loss: 3.759, acc: 0.388
******** [step = 250] loss: 3.740, acc: 0.389
******** [step = 300] loss: 3.718, acc: 0.392
******** [step = 350] loss: 3.697, acc: 0.393
******** [step = 400] loss: 3.675, acc: 0.395
******** [step = 450] loss: 3.658, acc: 0.397
******** [step = 500] loss: 3.639, acc: 0.399
******** [step = 550] loss: 3.623, acc: 0.400
******** [step = 600] loss: 3.606, acc: 0.402
******** [step = 650] loss: 3.592, acc: 0.404
******** [step = 700] loss: 3.575, acc: 0.405
******** [step = 750] loss: 3.560, acc: 0.407
******** [step = 800] loss: 3.547, acc: 0.408
******** [step = 850] loss: 3.535, acc: 0.409
EPOCH = 2 loss: 3.535, acc: 0.409, val_loss: 3.231, val_acc: 0.446

================================================================================2025-08_11 16:17:23
******** [step = 50] loss: 3.262, acc: 0.433
******** [step = 100] loss: 3.248, acc: 0.436
******** [step = 150] loss: 3.229, acc: 0.438
******** [step = 200] loss: 3.226, acc: 0.438
******** [step = 250] loss: 3.218, acc: 0.439
******** [step = 300] loss: 3.210, acc: 0.440
******** [step = 350] loss: 3.206, acc: 0.440
******** [step = 400] loss: 3.197, acc: 0.441
******** [step = 450] loss: 3.192, acc: 0.442
******** [step = 500] loss: 3.183, acc: 0.443
******** [step = 550] loss: 3.177, acc: 0.444
******** [step = 600] loss: 3.170, acc: 0.445
******** [step = 650] loss: 3.161, acc: 0.446
******** [step = 700] loss: 3.155, acc: 0.447
******** [step = 750] loss: 3.148, acc: 0.448
******** [step = 800] loss: 3.140, acc: 0.449
******** [step = 850] loss: 3.137, acc: 0.450
EPOCH = 3 loss: 3.137, acc: 0.450, val_loss: 2.992, val_acc: 0.475

================================================================================2025-08_11 16:19:27
******** [step = 50] loss: 2.990, acc: 0.461
******** [step = 100] loss: 2.955, acc: 0.466
******** [step = 150] loss: 2.941, acc: 0.469
******** [step = 200] loss: 2.934, acc: 0.470
******** [step = 250] loss: 2.929, acc: 0.471
******** [step = 300] loss: 2.924, acc: 0.472
******** [step = 350] loss: 2.919, acc: 0.473
******** [step = 400] loss: 2.915, acc: 0.474
******** [step = 450] loss: 2.911, acc: 0.475
******** [step = 500] loss: 2.905, acc: 0.477
******** [step = 550] loss: 2.901, acc: 0.477
******** [step = 600] loss: 2.900, acc: 0.478
******** [step = 650] loss: 2.895, acc: 0.479
******** [step = 700] loss: 2.890, acc: 0.480
******** [step = 750] loss: 2.885, acc: 0.480
******** [step = 800] loss: 2.881, acc: 0.481
******** [step = 850] loss: 2.877, acc: 0.482
EPOCH = 4 loss: 2.877, acc: 0.482, val_loss: 2.713, val_acc: 0.511

================================================================================2025-08_11 16:21:31
******** [step = 50] loss: 2.726, acc: 0.497
******** [step = 100] loss: 2.714, acc: 0.498
******** [step = 150] loss: 2.706, acc: 0.500
******** [step = 200] loss: 2.709, acc: 0.500
******** [step = 250] loss: 2.704, acc: 0.501
******** [step = 300] loss: 2.697, acc: 0.502
******** [step = 350] loss: 2.696, acc: 0.503
******** [step = 400] loss: 2.695, acc: 0.503
******** [step = 450] loss: 2.692, acc: 0.504
******** [step = 500] loss: 2.690, acc: 0.505
******** [step = 550] loss: 2.688, acc: 0.505
******** [step = 600] loss: 2.684, acc: 0.506
******** [step = 650] loss: 2.682, acc: 0.507
******** [step = 700] loss: 2.681, acc: 0.507
******** [step = 750] loss: 2.678, acc: 0.508
******** [step = 800] loss: 2.675, acc: 0.509
******** [step = 850] loss: 2.669, acc: 0.510
EPOCH = 5 loss: 2.669, acc: 0.510, val_loss: 2.541, val_acc: 0.539

================================================================================2025-08_11 16:23:39
******** [step = 50] loss: 2.525, acc: 0.524
******** [step = 100] loss: 2.501, acc: 0.528
******** [step = 150] loss: 2.495, acc: 0.530
******** [step = 200] loss: 2.491, acc: 0.531
******** [step = 250] loss: 2.490, acc: 0.532
******** [step = 300] loss: 2.487, acc: 0.533
******** [step = 350] loss: 2.482, acc: 0.533
******** [step = 400] loss: 2.478, acc: 0.535
******** [step = 450] loss: 2.474, acc: 0.535
******** [step = 500] loss: 2.468, acc: 0.536
******** [step = 550] loss: 2.465, acc: 0.537
******** [step = 600] loss: 2.462, acc: 0.538
******** [step = 650] loss: 2.459, acc: 0.539
******** [step = 700] loss: 2.452, acc: 0.540
******** [step = 750] loss: 2.449, acc: 0.541
******** [step = 800] loss: 2.442, acc: 0.542
******** [step = 850] loss: 2.437, acc: 0.543
EPOCH = 6 loss: 2.437, acc: 0.543, val_loss: 2.280, val_acc: 0.578

================================================================================2025-08_11 16:25:44
******** [step = 50] loss: 2.287, acc: 0.557
******** [step = 100] loss: 2.266, acc: 0.561
******** [step = 150] loss: 2.261, acc: 0.563
******** [step = 200] loss: 2.250, acc: 0.566
******** [step = 250] loss: 2.248, acc: 0.566
******** [step = 300] loss: 2.246, acc: 0.568
******** [step = 350] loss: 2.242, acc: 0.569
******** [step = 400] loss: 2.242, acc: 0.569
******** [step = 450] loss: 2.239, acc: 0.570
******** [step = 500] loss: 2.236, acc: 0.571
******** [step = 550] loss: 2.233, acc: 0.571
******** [step = 600] loss: 2.230, acc: 0.572
******** [step = 650] loss: 2.227, acc: 0.573
******** [step = 700] loss: 2.226, acc: 0.573
******** [step = 750] loss: 2.222, acc: 0.574
******** [step = 800] loss: 2.221, acc: 0.575
******** [step = 850] loss: 2.218, acc: 0.575
EPOCH = 7 loss: 2.218, acc: 0.575, val_loss: 2.097, val_acc: 0.607

================================================================================2025-08_11 16:27:49
******** [step = 50] loss: 2.069, acc: 0.595
******** [step = 100] loss: 2.054, acc: 0.599
******** [step = 150] loss: 2.050, acc: 0.599
******** [step = 200] loss: 2.052, acc: 0.599
******** [step = 250] loss: 2.046, acc: 0.599
******** [step = 300] loss: 2.045, acc: 0.600
******** [step = 350] loss: 2.044, acc: 0.600
******** [step = 400] loss: 2.047, acc: 0.600
******** [step = 450] loss: 2.044, acc: 0.601
******** [step = 500] loss: 2.043, acc: 0.601
******** [step = 550] loss: 2.040, acc: 0.602
******** [step = 600] loss: 2.040, acc: 0.603
******** [step = 650] loss: 2.041, acc: 0.603
******** [step = 700] loss: 2.039, acc: 0.603
******** [step = 750] loss: 2.037, acc: 0.604
******** [step = 800] loss: 2.036, acc: 0.604
******** [step = 850] loss: 2.036, acc: 0.604
EPOCH = 8 loss: 2.036, acc: 0.604, val_loss: 1.950, val_acc: 0.633

================================================================================2025-08_11 16:29:52
******** [step = 50] loss: 1.926, acc: 0.617
******** [step = 100] loss: 1.889, acc: 0.622
******** [step = 150] loss: 1.882, acc: 0.625
******** [step = 200] loss: 1.883, acc: 0.626
******** [step = 250] loss: 1.884, acc: 0.625
******** [step = 300] loss: 1.886, acc: 0.626
******** [step = 350] loss: 1.888, acc: 0.626
******** [step = 400] loss: 1.890, acc: 0.626
******** [step = 450] loss: 1.891, acc: 0.626
******** [step = 500] loss: 1.888, acc: 0.627
******** [step = 550] loss: 1.887, acc: 0.627
******** [step = 600] loss: 1.885, acc: 0.628
******** [step = 650] loss: 1.885, acc: 0.628
******** [step = 700] loss: 1.883, acc: 0.628
******** [step = 750] loss: 1.882, acc: 0.629
******** [step = 800] loss: 1.882, acc: 0.629
******** [step = 850] loss: 1.882, acc: 0.629
EPOCH = 9 loss: 1.882, acc: 0.629, val_loss: 1.849, val_acc: 0.652

================================================================================2025-08_11 16:31:54
******** [step = 50] loss: 1.771, acc: 0.641
******** [step = 100] loss: 1.755, acc: 0.644
******** [step = 150] loss: 1.755, acc: 0.645
******** [step = 200] loss: 1.755, acc: 0.645
******** [step = 250] loss: 1.751, acc: 0.647
******** [step = 300] loss: 1.753, acc: 0.647
******** [step = 350] loss: 1.753, acc: 0.647
******** [step = 400] loss: 1.752, acc: 0.648
******** [step = 450] loss: 1.754, acc: 0.648
******** [step = 500] loss: 1.755, acc: 0.648
******** [step = 550] loss: 1.760, acc: 0.648
******** [step = 600] loss: 1.760, acc: 0.648
******** [step = 650] loss: 1.762, acc: 0.648
******** [step = 700] loss: 1.762, acc: 0.648
******** [step = 750] loss: 1.762, acc: 0.649
******** [step = 800] loss: 1.763, acc: 0.649
******** [step = 850] loss: 1.763, acc: 0.649
EPOCH = 10 loss: 1.763, acc: 0.649, val_loss: 1.757, val_acc: 0.668

================================================================================2025-08_11 16:33:56
******** [step = 50] loss: 1.621, acc: 0.667
******** [step = 100] loss: 1.636, acc: 0.665
******** [step = 150] loss: 1.634, acc: 0.665
******** [step = 200] loss: 1.629, acc: 0.665
******** [step = 250] loss: 1.633, acc: 0.665
******** [step = 300] loss: 1.642, acc: 0.665
******** [step = 350] loss: 1.650, acc: 0.665
******** [step = 400] loss: 1.650, acc: 0.666
******** [step = 450] loss: 1.652, acc: 0.666
******** [step = 500] loss: 1.656, acc: 0.665
******** [step = 550] loss: 1.657, acc: 0.665
******** [step = 600] loss: 1.662, acc: 0.665
******** [step = 650] loss: 1.662, acc: 0.666
******** [step = 700] loss: 1.664, acc: 0.665
******** [step = 750] loss: 1.664, acc: 0.666
******** [step = 800] loss: 1.665, acc: 0.666
******** [step = 850] loss: 1.669, acc: 0.666
EPOCH = 11 loss: 1.669, acc: 0.666, val_loss: 1.679, val_acc: 0.685

================================================================================2025-08_11 16:35:57
******** [step = 50] loss: 1.568, acc: 0.679
******** [step = 100] loss: 1.561, acc: 0.680
******** [step = 150] loss: 1.555, acc: 0.680
******** [step = 200] loss: 1.553, acc: 0.681
******** [step = 250] loss: 1.560, acc: 0.680
******** [step = 300] loss: 1.561, acc: 0.681
******** [step = 350] loss: 1.564, acc: 0.681
******** [step = 400] loss: 1.568, acc: 0.680
******** [step = 450] loss: 1.572, acc: 0.680
******** [step = 500] loss: 1.574, acc: 0.680
******** [step = 550] loss: 1.575, acc: 0.680
******** [step = 600] loss: 1.577, acc: 0.680
******** [step = 650] loss: 1.580, acc: 0.680
******** [step = 700] loss: 1.582, acc: 0.680
******** [step = 750] loss: 1.583, acc: 0.680
******** [step = 800] loss: 1.584, acc: 0.680
******** [step = 850] loss: 1.584, acc: 0.680
EPOCH = 12 loss: 1.584, acc: 0.680, val_loss: 1.625, val_acc: 0.695

================================================================================2025-08_11 16:38:00
******** [step = 50] loss: 1.487, acc: 0.693
******** [step = 100] loss: 1.479, acc: 0.695
******** [step = 150] loss: 1.480, acc: 0.695
******** [step = 200] loss: 1.485, acc: 0.694
******** [step = 250] loss: 1.491, acc: 0.694
******** [step = 300] loss: 1.495, acc: 0.693
******** [step = 350] loss: 1.495, acc: 0.693
******** [step = 400] loss: 1.499, acc: 0.693
******** [step = 450] loss: 1.501, acc: 0.693
******** [step = 500] loss: 1.502, acc: 0.693
******** [step = 550] loss: 1.505, acc: 0.693
******** [step = 600] loss: 1.507, acc: 0.693
******** [step = 650] loss: 1.508, acc: 0.693
******** [step = 700] loss: 1.511, acc: 0.693
******** [step = 750] loss: 1.512, acc: 0.693
******** [step = 800] loss: 1.513, acc: 0.693
******** [step = 850] loss: 1.514, acc: 0.693
EPOCH = 13 loss: 1.514, acc: 0.693, val_loss: 1.564, val_acc: 0.710

================================================================================2025-08_11 16:40:02
******** [step = 50] loss: 1.422, acc: 0.703
******** [step = 100] loss: 1.415, acc: 0.705
******** [step = 150] loss: 1.420, acc: 0.705
******** [step = 200] loss: 1.414, acc: 0.706
******** [step = 250] loss: 1.421, acc: 0.705
******** [step = 300] loss: 1.429, acc: 0.704
******** [step = 350] loss: 1.435, acc: 0.704
******** [step = 400] loss: 1.438, acc: 0.704
******** [step = 450] loss: 1.440, acc: 0.704
******** [step = 500] loss: 1.445, acc: 0.704
******** [step = 550] loss: 1.446, acc: 0.704
******** [step = 600] loss: 1.448, acc: 0.704
******** [step = 650] loss: 1.449, acc: 0.704
******** [step = 700] loss: 1.451, acc: 0.704
******** [step = 750] loss: 1.454, acc: 0.703
******** [step = 800] loss: 1.455, acc: 0.703
******** [step = 850] loss: 1.456, acc: 0.704
EPOCH = 14 loss: 1.456, acc: 0.704, val_loss: 1.536, val_acc: 0.715

================================================================================2025-08_11 16:42:04
******** [step = 50] loss: 1.355, acc: 0.714
******** [step = 100] loss: 1.358, acc: 0.715
******** [step = 150] loss: 1.360, acc: 0.715
******** [step = 200] loss: 1.370, acc: 0.714
******** [step = 250] loss: 1.378, acc: 0.713
******** [step = 300] loss: 1.381, acc: 0.713
******** [step = 350] loss: 1.381, acc: 0.713
******** [step = 400] loss: 1.382, acc: 0.713
******** [step = 450] loss: 1.385, acc: 0.713
******** [step = 500] loss: 1.389, acc: 0.713
******** [step = 550] loss: 1.391, acc: 0.713
******** [step = 600] loss: 1.392, acc: 0.713
******** [step = 650] loss: 1.394, acc: 0.713
******** [step = 700] loss: 1.396, acc: 0.713
******** [step = 750] loss: 1.398, acc: 0.713
******** [step = 800] loss: 1.399, acc: 0.713
******** [step = 850] loss: 1.399, acc: 0.713
EPOCH = 15 loss: 1.399, acc: 0.713, val_loss: 1.496, val_acc: 0.722

================================================================================2025-08_11 16:44:06
******** [step = 50] loss: 1.288, acc: 0.727
******** [step = 100] loss: 1.307, acc: 0.724
******** [step = 150] loss: 1.311, acc: 0.723
******** [step = 200] loss: 1.310, acc: 0.724
******** [step = 250] loss: 1.318, acc: 0.723
******** [step = 300] loss: 1.323, acc: 0.723
******** [step = 350] loss: 1.325, acc: 0.723
******** [step = 400] loss: 1.329, acc: 0.723
******** [step = 450] loss: 1.331, acc: 0.722
******** [step = 500] loss: 1.334, acc: 0.722
******** [step = 550] loss: 1.336, acc: 0.722
******** [step = 600] loss: 1.339, acc: 0.722
******** [step = 650] loss: 1.344, acc: 0.721
******** [step = 700] loss: 1.346, acc: 0.721
******** [step = 750] loss: 1.349, acc: 0.721
******** [step = 800] loss: 1.351, acc: 0.721
******** [step = 850] loss: 1.353, acc: 0.721
EPOCH = 16 loss: 1.353, acc: 0.721, val_loss: 1.468, val_acc: 0.728

================================================================================2025-08_11 16:46:11
******** [step = 50] loss: 1.259, acc: 0.734
******** [step = 100] loss: 1.252, acc: 0.735
******** [step = 150] loss: 1.256, acc: 0.734
******** [step = 200] loss: 1.265, acc: 0.733
******** [step = 250] loss: 1.275, acc: 0.732
******** [step = 300] loss: 1.279, acc: 0.731
******** [step = 350] loss: 1.281, acc: 0.731
******** [step = 400] loss: 1.285, acc: 0.731
******** [step = 450] loss: 1.290, acc: 0.730
******** [step = 500] loss: 1.291, acc: 0.730
******** [step = 550] loss: 1.293, acc: 0.730
******** [step = 600] loss: 1.294, acc: 0.730
******** [step = 650] loss: 1.299, acc: 0.729
******** [step = 700] loss: 1.302, acc: 0.729
******** [step = 750] loss: 1.304, acc: 0.729
******** [step = 800] loss: 1.307, acc: 0.729
******** [step = 850] loss: 1.311, acc: 0.728
EPOCH = 17 loss: 1.311, acc: 0.728, val_loss: 1.436, val_acc: 0.734

================================================================================2025-08_11 16:48:14
******** [step = 50] loss: 1.208, acc: 0.743
******** [step = 100] loss: 1.212, acc: 0.742
******** [step = 150] loss: 1.219, acc: 0.741
******** [step = 200] loss: 1.230, acc: 0.739
******** [step = 250] loss: 1.236, acc: 0.738
******** [step = 300] loss: 1.236, acc: 0.738
******** [step = 350] loss: 1.240, acc: 0.738
******** [step = 400] loss: 1.242, acc: 0.738
******** [step = 450] loss: 1.247, acc: 0.737
******** [step = 500] loss: 1.251, acc: 0.737
******** [step = 550] loss: 1.257, acc: 0.736
******** [step = 600] loss: 1.259, acc: 0.736
******** [step = 650] loss: 1.263, acc: 0.735
******** [step = 700] loss: 1.264, acc: 0.735
******** [step = 750] loss: 1.267, acc: 0.735
******** [step = 800] loss: 1.270, acc: 0.735
******** [step = 850] loss: 1.274, acc: 0.735
EPOCH = 18 loss: 1.274, acc: 0.735, val_loss: 1.416, val_acc: 0.738

================================================================================2025-08_11 16:50:16
******** [step = 50] loss: 1.192, acc: 0.742
******** [step = 100] loss: 1.184, acc: 0.745
******** [step = 150] loss: 1.180, acc: 0.745
******** [step = 200] loss: 1.187, acc: 0.745
******** [step = 250] loss: 1.190, acc: 0.745
******** [step = 300] loss: 1.196, acc: 0.744
******** [step = 350] loss: 1.203, acc: 0.743
******** [step = 400] loss: 1.208, acc: 0.742
******** [step = 450] loss: 1.211, acc: 0.742
******** [step = 500] loss: 1.213, acc: 0.742
******** [step = 550] loss: 1.216, acc: 0.742
******** [step = 600] loss: 1.220, acc: 0.742
******** [step = 650] loss: 1.224, acc: 0.742
******** [step = 700] loss: 1.227, acc: 0.741
******** [step = 750] loss: 1.232, acc: 0.741
******** [step = 800] loss: 1.235, acc: 0.741
******** [step = 850] loss: 1.236, acc: 0.741
EPOCH = 19 loss: 1.236, acc: 0.741, val_loss: 1.386, val_acc: 0.744

================================================================================2025-08_11 16:52:18
******** [step = 50] loss: 1.148, acc: 0.753
******** [step = 100] loss: 1.152, acc: 0.753
******** [step = 150] loss: 1.154, acc: 0.752
******** [step = 200] loss: 1.157, acc: 0.751
******** [step = 250] loss: 1.162, acc: 0.751
******** [step = 300] loss: 1.161, acc: 0.750
******** [step = 350] loss: 1.165, acc: 0.750
******** [step = 400] loss: 1.171, acc: 0.750
******** [step = 450] loss: 1.173, acc: 0.750
******** [step = 500] loss: 1.177, acc: 0.749
******** [step = 550] loss: 1.181, acc: 0.749
******** [step = 600] loss: 1.184, acc: 0.748
******** [step = 650] loss: 1.191, acc: 0.748
******** [step = 700] loss: 1.195, acc: 0.747
******** [step = 750] loss: 1.198, acc: 0.747
******** [step = 800] loss: 1.201, acc: 0.747
******** [step = 850] loss: 1.206, acc: 0.746
EPOCH = 20 loss: 1.206, acc: 0.746, val_loss: 1.374, val_acc: 0.746

================================================================================2025-08_11 16:54:19
******** [step = 50] loss: 1.126, acc: 0.756
******** [step = 100] loss: 1.115, acc: 0.758
******** [step = 150] loss: 1.124, acc: 0.757
******** [step = 200] loss: 1.122, acc: 0.758
******** [step = 250] loss: 1.130, acc: 0.757
******** [step = 300] loss: 1.134, acc: 0.756
******** [step = 350] loss: 1.138, acc: 0.755
******** [step = 400] loss: 1.143, acc: 0.755
******** [step = 450] loss: 1.147, acc: 0.754
******** [step = 500] loss: 1.153, acc: 0.754
******** [step = 550] loss: 1.155, acc: 0.754
******** [step = 600] loss: 1.159, acc: 0.753
******** [step = 650] loss: 1.164, acc: 0.753
******** [step = 700] loss: 1.168, acc: 0.752
******** [step = 750] loss: 1.171, acc: 0.752
******** [step = 800] loss: 1.174, acc: 0.752
******** [step = 850] loss: 1.178, acc: 0.751
EPOCH = 21 loss: 1.178, acc: 0.751, val_loss: 1.356, val_acc: 0.750

================================================================================2025-08_11 16:56:20
******** [step = 50] loss: 1.125, acc: 0.757
******** [step = 100] loss: 1.106, acc: 0.760
******** [step = 150] loss: 1.106, acc: 0.760
******** [step = 200] loss: 1.108, acc: 0.759
******** [step = 250] loss: 1.111, acc: 0.759
******** [step = 300] loss: 1.110, acc: 0.760
******** [step = 350] loss: 1.115, acc: 0.759
******** [step = 400] loss: 1.121, acc: 0.758
******** [step = 450] loss: 1.124, acc: 0.758
******** [step = 500] loss: 1.127, acc: 0.758
******** [step = 550] loss: 1.130, acc: 0.757
******** [step = 600] loss: 1.137, acc: 0.757
******** [step = 650] loss: 1.141, acc: 0.756
******** [step = 700] loss: 1.145, acc: 0.756
******** [step = 750] loss: 1.145, acc: 0.756
******** [step = 800] loss: 1.149, acc: 0.756
******** [step = 850] loss: 1.153, acc: 0.755
EPOCH = 22 loss: 1.153, acc: 0.755, val_loss: 1.341, val_acc: 0.755

================================================================================2025-08_11 16:58:21
******** [step = 50] loss: 1.076, acc: 0.763
******** [step = 100] loss: 1.074, acc: 0.766
******** [step = 150] loss: 1.071, acc: 0.766
******** [step = 200] loss: 1.069, acc: 0.768
******** [step = 250] loss: 1.074, acc: 0.766
******** [step = 300] loss: 1.084, acc: 0.765
******** [step = 350] loss: 1.088, acc: 0.765
******** [step = 400] loss: 1.093, acc: 0.764
******** [step = 450] loss: 1.097, acc: 0.763
******** [step = 500] loss: 1.101, acc: 0.763
******** [step = 550] loss: 1.106, acc: 0.762
******** [step = 600] loss: 1.110, acc: 0.762
******** [step = 650] loss: 1.114, acc: 0.762
******** [step = 700] loss: 1.118, acc: 0.761
******** [step = 750] loss: 1.121, acc: 0.761
******** [step = 800] loss: 1.124, acc: 0.761
******** [step = 850] loss: 1.127, acc: 0.760
EPOCH = 23 loss: 1.127, acc: 0.760, val_loss: 1.328, val_acc: 0.757

================================================================================2025-08_11 17:00:23
******** [step = 50] loss: 1.034, acc: 0.773
******** [step = 100] loss: 1.042, acc: 0.770
******** [step = 150] loss: 1.048, acc: 0.770
******** [step = 200] loss: 1.057, acc: 0.769
******** [step = 250] loss: 1.059, acc: 0.769
******** [step = 300] loss: 1.061, acc: 0.769
******** [step = 350] loss: 1.069, acc: 0.767
******** [step = 400] loss: 1.073, acc: 0.767
******** [step = 450] loss: 1.079, acc: 0.766
******** [step = 500] loss: 1.083, acc: 0.766
******** [step = 550] loss: 1.087, acc: 0.766
******** [step = 600] loss: 1.090, acc: 0.765
******** [step = 650] loss: 1.093, acc: 0.765
******** [step = 700] loss: 1.097, acc: 0.765
******** [step = 750] loss: 1.099, acc: 0.764
******** [step = 800] loss: 1.103, acc: 0.764
******** [step = 850] loss: 1.107, acc: 0.764
EPOCH = 24 loss: 1.107, acc: 0.764, val_loss: 1.313, val_acc: 0.759

================================================================================2025-08_11 17:02:25
******** [step = 50] loss: 1.019, acc: 0.775
******** [step = 100] loss: 1.016, acc: 0.775
******** [step = 150] loss: 1.016, acc: 0.775
******** [step = 200] loss: 1.025, acc: 0.774
******** [step = 250] loss: 1.031, acc: 0.774
******** [step = 300] loss: 1.040, acc: 0.773
******** [step = 350] loss: 1.043, acc: 0.773
******** [step = 400] loss: 1.049, acc: 0.772
******** [step = 450] loss: 1.055, acc: 0.771
******** [step = 500] loss: 1.059, acc: 0.771
******** [step = 550] loss: 1.061, acc: 0.770
******** [step = 600] loss: 1.066, acc: 0.770
******** [step = 650] loss: 1.068, acc: 0.770
******** [step = 700] loss: 1.072, acc: 0.769
******** [step = 750] loss: 1.076, acc: 0.769
******** [step = 800] loss: 1.080, acc: 0.768
******** [step = 850] loss: 1.086, acc: 0.768
EPOCH = 25 loss: 1.086, acc: 0.768, val_loss: 1.300, val_acc: 0.762

================================================================================2025-08_11 17:04:27
******** [step = 50] loss: 0.999, acc: 0.780
******** [step = 100] loss: 1.001, acc: 0.779
******** [step = 150] loss: 1.005, acc: 0.779
******** [step = 200] loss: 1.012, acc: 0.778
******** [step = 250] loss: 1.016, acc: 0.777
******** [step = 300] loss: 1.016, acc: 0.778
******** [step = 350] loss: 1.021, acc: 0.777
******** [step = 400] loss: 1.028, acc: 0.776
******** [step = 450] loss: 1.035, acc: 0.775
******** [step = 500] loss: 1.039, acc: 0.775
******** [step = 550] loss: 1.043, acc: 0.775
******** [step = 600] loss: 1.048, acc: 0.774
******** [step = 650] loss: 1.052, acc: 0.773
******** [step = 700] loss: 1.055, acc: 0.773
******** [step = 750] loss: 1.059, acc: 0.772
******** [step = 800] loss: 1.062, acc: 0.772
******** [step = 850] loss: 1.068, acc: 0.771
EPOCH = 26 loss: 1.068, acc: 0.771, val_loss: 1.299, val_acc: 0.762

================================================================================2025-08_11 17:06:29
******** [step = 50] loss: 0.993, acc: 0.779
******** [step = 100] loss: 1.002, acc: 0.779
******** [step = 150] loss: 1.001, acc: 0.779
******** [step = 200] loss: 1.000, acc: 0.780
******** [step = 250] loss: 1.005, acc: 0.779
******** [step = 300] loss: 1.013, acc: 0.778
******** [step = 350] loss: 1.015, acc: 0.777
******** [step = 400] loss: 1.017, acc: 0.778
******** [step = 450] loss: 1.020, acc: 0.778
******** [step = 500] loss: 1.023, acc: 0.777
******** [step = 550] loss: 1.026, acc: 0.777
******** [step = 600] loss: 1.029, acc: 0.776
******** [step = 650] loss: 1.033, acc: 0.776
******** [step = 700] loss: 1.036, acc: 0.776
******** [step = 750] loss: 1.040, acc: 0.775
******** [step = 800] loss: 1.044, acc: 0.775
******** [step = 850] loss: 1.047, acc: 0.774
EPOCH = 27 loss: 1.047, acc: 0.774, val_loss: 1.283, val_acc: 0.765

================================================================================2025-08_11 17:08:31
******** [step = 50] loss: 0.957, acc: 0.785
******** [step = 100] loss: 0.958, acc: 0.786
******** [step = 150] loss: 0.964, acc: 0.785
******** [step = 200] loss: 0.970, acc: 0.785
******** [step = 250] loss: 0.973, acc: 0.784
******** [step = 300] loss: 0.980, acc: 0.783
******** [step = 350] loss: 0.985, acc: 0.783
******** [step = 400] loss: 0.992, acc: 0.782
******** [step = 450] loss: 0.997, acc: 0.781
******** [step = 500] loss: 1.003, acc: 0.780
******** [step = 550] loss: 1.006, acc: 0.780
******** [step = 600] loss: 1.009, acc: 0.780
******** [step = 650] loss: 1.016, acc: 0.779
******** [step = 700] loss: 1.020, acc: 0.778
******** [step = 750] loss: 1.023, acc: 0.778
******** [step = 800] loss: 1.026, acc: 0.778
******** [step = 850] loss: 1.029, acc: 0.778
EPOCH = 28 loss: 1.029, acc: 0.778, val_loss: 1.272, val_acc: 0.768

================================================================================2025-08_11 17:10:33
******** [step = 50] loss: 0.947, acc: 0.787
******** [step = 100] loss: 0.948, acc: 0.788
******** [step = 150] loss: 0.954, acc: 0.787
******** [step = 200] loss: 0.958, acc: 0.787
******** [step = 250] loss: 0.959, acc: 0.787
******** [step = 300] loss: 0.965, acc: 0.786
******** [step = 350] loss: 0.971, acc: 0.785
******** [step = 400] loss: 0.977, acc: 0.785
******** [step = 450] loss: 0.983, acc: 0.784
******** [step = 500] loss: 0.987, acc: 0.783
******** [step = 550] loss: 0.992, acc: 0.783
******** [step = 600] loss: 0.996, acc: 0.782
******** [step = 650] loss: 1.001, acc: 0.782
******** [step = 700] loss: 1.004, acc: 0.782
******** [step = 750] loss: 1.007, acc: 0.781
******** [step = 800] loss: 1.011, acc: 0.781
******** [step = 850] loss: 1.016, acc: 0.780
EPOCH = 29 loss: 1.016, acc: 0.780, val_loss: 1.263, val_acc: 0.770

================================================================================2025-08_11 17:12:34
******** [step = 50] loss: 0.968, acc: 0.786
******** [step = 100] loss: 0.950, acc: 0.788
******** [step = 150] loss: 0.946, acc: 0.789
******** [step = 200] loss: 0.946, acc: 0.789
******** [step = 250] loss: 0.950, acc: 0.789
******** [step = 300] loss: 0.956, acc: 0.788
******** [step = 350] loss: 0.959, acc: 0.788
******** [step = 400] loss: 0.965, acc: 0.787
******** [step = 450] loss: 0.969, acc: 0.787
******** [step = 500] loss: 0.975, acc: 0.786
******** [step = 550] loss: 0.978, acc: 0.786
******** [step = 600] loss: 0.981, acc: 0.786
******** [step = 650] loss: 0.983, acc: 0.786
******** [step = 700] loss: 0.987, acc: 0.785
******** [step = 750] loss: 0.991, acc: 0.785
******** [step = 800] loss: 0.994, acc: 0.784
******** [step = 850] loss: 0.997, acc: 0.784
EPOCH = 30 loss: 0.997, acc: 0.784, val_loss: 1.259, val_acc: 0.771

================================================================================2025-08_11 17:14:35
******** [step = 50] loss: 0.898, acc: 0.796
******** [step = 100] loss: 0.916, acc: 0.795
******** [step = 150] loss: 0.931, acc: 0.792
******** [step = 200] loss: 0.932, acc: 0.792
******** [step = 250] loss: 0.936, acc: 0.791
******** [step = 300] loss: 0.939, acc: 0.791
******** [step = 350] loss: 0.943, acc: 0.791
******** [step = 400] loss: 0.950, acc: 0.790
******** [step = 450] loss: 0.953, acc: 0.790
******** [step = 500] loss: 0.957, acc: 0.789
******** [step = 550] loss: 0.960, acc: 0.789
******** [step = 600] loss: 0.967, acc: 0.788
******** [step = 650] loss: 0.971, acc: 0.788
******** [step = 700] loss: 0.972, acc: 0.787
******** [step = 750] loss: 0.976, acc: 0.787
******** [step = 800] loss: 0.981, acc: 0.786
******** [step = 850] loss: 0.983, acc: 0.786
EPOCH = 31 loss: 0.983, acc: 0.786, val_loss: 1.252, val_acc: 0.773

================================================================================2025-08_11 17:16:37
******** [step = 50] loss: 0.899, acc: 0.800
******** [step = 100] loss: 0.902, acc: 0.799
******** [step = 150] loss: 0.905, acc: 0.797
******** [step = 200] loss: 0.912, acc: 0.796
******** [step = 250] loss: 0.921, acc: 0.795
******** [step = 300] loss: 0.924, acc: 0.795
******** [step = 350] loss: 0.931, acc: 0.793
******** [step = 400] loss: 0.937, acc: 0.792
******** [step = 450] loss: 0.943, acc: 0.792
******** [step = 500] loss: 0.946, acc: 0.791
******** [step = 550] loss: 0.949, acc: 0.791
******** [step = 600] loss: 0.955, acc: 0.790
******** [step = 650] loss: 0.957, acc: 0.790
******** [step = 700] loss: 0.961, acc: 0.790
******** [step = 750] loss: 0.965, acc: 0.789
******** [step = 800] loss: 0.967, acc: 0.789
******** [step = 850] loss: 0.974, acc: 0.788
EPOCH = 32 loss: 0.974, acc: 0.788, val_loss: 1.244, val_acc: 0.774

================================================================================2025-08_11 17:18:39
******** [step = 50] loss: 0.906, acc: 0.795
******** [step = 100] loss: 0.902, acc: 0.797
******** [step = 150] loss: 0.904, acc: 0.797
******** [step = 200] loss: 0.905, acc: 0.797
******** [step = 250] loss: 0.909, acc: 0.797
******** [step = 300] loss: 0.914, acc: 0.796
******** [step = 350] loss: 0.919, acc: 0.795
******** [step = 400] loss: 0.925, acc: 0.794
******** [step = 450] loss: 0.931, acc: 0.794
******** [step = 500] loss: 0.935, acc: 0.793
******** [step = 550] loss: 0.937, acc: 0.793
******** [step = 600] loss: 0.939, acc: 0.793
******** [step = 650] loss: 0.943, acc: 0.792
******** [step = 700] loss: 0.948, acc: 0.792
******** [step = 750] loss: 0.951, acc: 0.792
******** [step = 800] loss: 0.954, acc: 0.792
******** [step = 850] loss: 0.957, acc: 0.791
EPOCH = 33 loss: 0.957, acc: 0.791, val_loss: 1.241, val_acc: 0.775

================================================================================2025-08_11 17:20:42
******** [step = 50] loss: 0.877, acc: 0.801
******** [step = 100] loss: 0.878, acc: 0.801
******** [step = 150] loss: 0.887, acc: 0.800
******** [step = 200] loss: 0.889, acc: 0.800
******** [step = 250] loss: 0.892, acc: 0.800
******** [step = 300] loss: 0.898, acc: 0.799
******** [step = 350] loss: 0.901, acc: 0.798
******** [step = 400] loss: 0.906, acc: 0.798
******** [step = 450] loss: 0.912, acc: 0.797
******** [step = 500] loss: 0.918, acc: 0.796
******** [step = 550] loss: 0.922, acc: 0.796
******** [step = 600] loss: 0.927, acc: 0.795
******** [step = 650] loss: 0.931, acc: 0.795
******** [step = 700] loss: 0.935, acc: 0.794
******** [step = 750] loss: 0.939, acc: 0.794
******** [step = 800] loss: 0.942, acc: 0.794
******** [step = 850] loss: 0.944, acc: 0.794
EPOCH = 34 loss: 0.944, acc: 0.794, val_loss: 1.233, val_acc: 0.776

================================================================================2025-08_11 17:22:45
******** [step = 50] loss: 0.855, acc: 0.806
******** [step = 100] loss: 0.865, acc: 0.806
******** [step = 150] loss: 0.874, acc: 0.804
******** [step = 200] loss: 0.879, acc: 0.803
******** [step = 250] loss: 0.885, acc: 0.802
******** [step = 300] loss: 0.889, acc: 0.801
******** [step = 350] loss: 0.894, acc: 0.800
******** [step = 400] loss: 0.895, acc: 0.800
******** [step = 450] loss: 0.900, acc: 0.800
******** [step = 500] loss: 0.906, acc: 0.799
******** [step = 550] loss: 0.911, acc: 0.799
******** [step = 600] loss: 0.915, acc: 0.798
******** [step = 650] loss: 0.919, acc: 0.798
******** [step = 700] loss: 0.923, acc: 0.797
******** [step = 750] loss: 0.927, acc: 0.797
******** [step = 800] loss: 0.930, acc: 0.796
******** [step = 850] loss: 0.934, acc: 0.796
EPOCH = 35 loss: 0.934, acc: 0.796, val_loss: 1.221, val_acc: 0.779

================================================================================2025-08_11 17:24:51
******** [step = 50] loss: 0.851, acc: 0.808
******** [step = 100] loss: 0.856, acc: 0.806
******** [step = 150] loss: 0.864, acc: 0.805
******** [step = 200] loss: 0.867, acc: 0.805
******** [step = 250] loss: 0.871, acc: 0.805
******** [step = 300] loss: 0.876, acc: 0.804
******** [step = 350] loss: 0.882, acc: 0.803
******** [step = 400] loss: 0.887, acc: 0.802
******** [step = 450] loss: 0.891, acc: 0.802
******** [step = 500] loss: 0.895, acc: 0.801
******** [step = 550] loss: 0.900, acc: 0.801
******** [step = 600] loss: 0.904, acc: 0.800
******** [step = 650] loss: 0.906, acc: 0.800
******** [step = 700] loss: 0.911, acc: 0.799
******** [step = 750] loss: 0.915, acc: 0.799
******** [step = 800] loss: 0.919, acc: 0.798
******** [step = 850] loss: 0.923, acc: 0.798
EPOCH = 36 loss: 0.923, acc: 0.798, val_loss: 1.220, val_acc: 0.780

================================================================================2025-08_11 17:26:55
******** [step = 50] loss: 0.850, acc: 0.807
******** [step = 100] loss: 0.850, acc: 0.807
******** [step = 150] loss: 0.855, acc: 0.806
******** [step = 200] loss: 0.856, acc: 0.807
******** [step = 250] loss: 0.862, acc: 0.806
******** [step = 300] loss: 0.869, acc: 0.805
******** [step = 350] loss: 0.877, acc: 0.804
******** [step = 400] loss: 0.880, acc: 0.804
******** [step = 450] loss: 0.883, acc: 0.804
******** [step = 500] loss: 0.886, acc: 0.803
******** [step = 550] loss: 0.890, acc: 0.803
******** [step = 600] loss: 0.895, acc: 0.802
******** [step = 650] loss: 0.899, acc: 0.801
******** [step = 700] loss: 0.902, acc: 0.801
******** [step = 750] loss: 0.905, acc: 0.801
******** [step = 800] loss: 0.908, acc: 0.800
******** [step = 850] loss: 0.913, acc: 0.800
EPOCH = 37 loss: 0.913, acc: 0.800, val_loss: 1.216, val_acc: 0.781

================================================================================2025-08_11 17:28:58
******** [step = 50] loss: 0.844, acc: 0.809
******** [step = 100] loss: 0.844, acc: 0.810
******** [step = 150] loss: 0.844, acc: 0.810
******** [step = 200] loss: 0.847, acc: 0.809
******** [step = 250] loss: 0.852, acc: 0.809
******** [step = 300] loss: 0.857, acc: 0.808
******** [step = 350] loss: 0.862, acc: 0.807
******** [step = 400] loss: 0.869, acc: 0.806
******** [step = 450] loss: 0.874, acc: 0.805
******** [step = 500] loss: 0.877, acc: 0.805
******** [step = 550] loss: 0.882, acc: 0.804
******** [step = 600] loss: 0.886, acc: 0.804
******** [step = 650] loss: 0.889, acc: 0.803
******** [step = 700] loss: 0.892, acc: 0.803
******** [step = 750] loss: 0.895, acc: 0.803
******** [step = 800] loss: 0.898, acc: 0.802
******** [step = 850] loss: 0.900, acc: 0.802
EPOCH = 38 loss: 0.900, acc: 0.802, val_loss: 1.212, val_acc: 0.782

================================================================================2025-08_11 17:31:00
******** [step = 50] loss: 0.811, acc: 0.816
******** [step = 100] loss: 0.820, acc: 0.814
******** [step = 150] loss: 0.827, acc: 0.812
******** [step = 200] loss: 0.837, acc: 0.811
******** [step = 250] loss: 0.842, acc: 0.810
******** [step = 300] loss: 0.849, acc: 0.809
******** [step = 350] loss: 0.854, acc: 0.808
******** [step = 400] loss: 0.858, acc: 0.808
******** [step = 450] loss: 0.860, acc: 0.808
******** [step = 500] loss: 0.865, acc: 0.807
******** [step = 550] loss: 0.871, acc: 0.806
******** [step = 600] loss: 0.873, acc: 0.806
******** [step = 650] loss: 0.878, acc: 0.805
******** [step = 700] loss: 0.882, acc: 0.805
******** [step = 750] loss: 0.886, acc: 0.804
******** [step = 800] loss: 0.890, acc: 0.804
******** [step = 850] loss: 0.893, acc: 0.803
EPOCH = 39 loss: 0.893, acc: 0.803, val_loss: 1.203, val_acc: 0.782

================================================================================2025-08_11 17:33:04
******** [step = 50] loss: 0.810, acc: 0.815
******** [step = 100] loss: 0.814, acc: 0.815
******** [step = 150] loss: 0.815, acc: 0.815
******** [step = 200] loss: 0.821, acc: 0.814
******** [step = 250] loss: 0.827, acc: 0.813
******** [step = 300] loss: 0.837, acc: 0.811
******** [step = 350] loss: 0.842, acc: 0.810
******** [step = 400] loss: 0.847, acc: 0.809
******** [step = 450] loss: 0.851, acc: 0.809
******** [step = 500] loss: 0.854, acc: 0.808
******** [step = 550] loss: 0.860, acc: 0.808
******** [step = 600] loss: 0.864, acc: 0.807
******** [step = 650] loss: 0.867, acc: 0.807
******** [step = 700] loss: 0.873, acc: 0.806
******** [step = 750] loss: 0.876, acc: 0.806
******** [step = 800] loss: 0.879, acc: 0.806
******** [step = 850] loss: 0.883, acc: 0.805
EPOCH = 40 loss: 0.883, acc: 0.805, val_loss: 1.203, val_acc: 0.784

================================================================================2025-08_11 17:35:07
******** [step = 50] loss: 0.787, acc: 0.818
******** [step = 100] loss: 0.800, acc: 0.816
******** [step = 150] loss: 0.811, acc: 0.815
******** [step = 200] loss: 0.818, acc: 0.814
******** [step = 250] loss: 0.825, acc: 0.813
******** [step = 300] loss: 0.830, acc: 0.812
******** [step = 350] loss: 0.836, acc: 0.811
******** [step = 400] loss: 0.841, acc: 0.811
******** [step = 450] loss: 0.845, acc: 0.810
******** [step = 500] loss: 0.848, acc: 0.810
******** [step = 550] loss: 0.851, acc: 0.810
******** [step = 600] loss: 0.856, acc: 0.809
******** [step = 650] loss: 0.859, acc: 0.809
******** [step = 700] loss: 0.864, acc: 0.808
******** [step = 750] loss: 0.867, acc: 0.808
******** [step = 800] loss: 0.871, acc: 0.807
******** [step = 850] loss: 0.873, acc: 0.807
EPOCH = 41 loss: 0.873, acc: 0.807, val_loss: 1.202, val_acc: 0.785

================================================================================2025-08_11 17:37:10
******** [step = 50] loss: 0.792, acc: 0.821
******** [step = 100] loss: 0.800, acc: 0.818
******** [step = 150] loss: 0.803, acc: 0.818
******** [step = 200] loss: 0.804, acc: 0.818
******** [step = 250] loss: 0.810, acc: 0.816
******** [step = 300] loss: 0.816, acc: 0.815
******** [step = 350] loss: 0.822, acc: 0.814
******** [step = 400] loss: 0.829, acc: 0.813
******** [step = 450] loss: 0.834, acc: 0.813
******** [step = 500] loss: 0.840, acc: 0.812
******** [step = 550] loss: 0.845, acc: 0.811
******** [step = 600] loss: 0.850, acc: 0.811
******** [step = 650] loss: 0.854, acc: 0.810
******** [step = 700] loss: 0.857, acc: 0.809
******** [step = 750] loss: 0.860, acc: 0.809
******** [step = 800] loss: 0.864, acc: 0.809
******** [step = 850] loss: 0.868, acc: 0.808
EPOCH = 42 loss: 0.868, acc: 0.808, val_loss: 1.195, val_acc: 0.786

================================================================================2025-08_11 17:39:13
******** [step = 50] loss: 0.814, acc: 0.814
******** [step = 100] loss: 0.805, acc: 0.816
******** [step = 150] loss: 0.809, acc: 0.816
******** [step = 200] loss: 0.810, acc: 0.816
******** [step = 250] loss: 0.813, acc: 0.816
******** [step = 300] loss: 0.816, acc: 0.815
******** [step = 350] loss: 0.821, acc: 0.815
******** [step = 400] loss: 0.824, acc: 0.815
******** [step = 450] loss: 0.829, acc: 0.814
******** [step = 500] loss: 0.834, acc: 0.813
******** [step = 550] loss: 0.837, acc: 0.813
******** [step = 600] loss: 0.842, acc: 0.812
******** [step = 650] loss: 0.845, acc: 0.812
******** [step = 700] loss: 0.848, acc: 0.811
******** [step = 750] loss: 0.852, acc: 0.811
******** [step = 800] loss: 0.856, acc: 0.811
******** [step = 850] loss: 0.858, acc: 0.811
EPOCH = 43 loss: 0.858, acc: 0.811, val_loss: 1.192, val_acc: 0.786

================================================================================2025-08_11 17:41:15
******** [step = 50] loss: 0.783, acc: 0.822
******** [step = 100] loss: 0.788, acc: 0.822
******** [step = 150] loss: 0.796, acc: 0.820
******** [step = 200] loss: 0.802, acc: 0.819
******** [step = 250] loss: 0.802, acc: 0.819
******** [step = 300] loss: 0.802, acc: 0.819
******** [step = 350] loss: 0.805, acc: 0.818
******** [step = 400] loss: 0.811, acc: 0.817
******** [step = 450] loss: 0.816, acc: 0.817
******** [step = 500] loss: 0.821, acc: 0.816
******** [step = 550] loss: 0.825, acc: 0.816
******** [step = 600] loss: 0.832, acc: 0.815
******** [step = 650] loss: 0.836, acc: 0.814
******** [step = 700] loss: 0.840, acc: 0.814
******** [step = 750] loss: 0.843, acc: 0.813
******** [step = 800] loss: 0.847, acc: 0.813
******** [step = 850] loss: 0.850, acc: 0.812
EPOCH = 44 loss: 0.850, acc: 0.812, val_loss: 1.182, val_acc: 0.789

================================================================================2025-08_11 17:43:17
******** [step = 50] loss: 0.779, acc: 0.822
******** [step = 100] loss: 0.784, acc: 0.820
******** [step = 150] loss: 0.788, acc: 0.820
******** [step = 200] loss: 0.789, acc: 0.819
******** [step = 250] loss: 0.791, acc: 0.819
******** [step = 300] loss: 0.801, acc: 0.818
******** [step = 350] loss: 0.804, acc: 0.817
******** [step = 400] loss: 0.809, acc: 0.817
******** [step = 450] loss: 0.814, acc: 0.816
******** [step = 500] loss: 0.817, acc: 0.816
******** [step = 550] loss: 0.822, acc: 0.815
******** [step = 600] loss: 0.826, acc: 0.815
******** [step = 650] loss: 0.828, acc: 0.815
******** [step = 700] loss: 0.832, acc: 0.814
******** [step = 750] loss: 0.836, acc: 0.814
******** [step = 800] loss: 0.840, acc: 0.813
******** [step = 850] loss: 0.843, acc: 0.813
EPOCH = 45 loss: 0.843, acc: 0.813, val_loss: 1.182, val_acc: 0.789

================================================================================2025-08_11 17:45:22
******** [step = 50] loss: 0.760, acc: 0.827
******** [step = 100] loss: 0.764, acc: 0.826
******** [step = 150] loss: 0.779, acc: 0.823
******** [step = 200] loss: 0.789, acc: 0.821
******** [step = 250] loss: 0.792, acc: 0.820
******** [step = 300] loss: 0.794, acc: 0.820
******** [step = 350] loss: 0.801, acc: 0.819
******** [step = 400] loss: 0.802, acc: 0.819
******** [step = 450] loss: 0.807, acc: 0.818
******** [step = 500] loss: 0.811, acc: 0.818
******** [step = 550] loss: 0.815, acc: 0.817
******** [step = 600] loss: 0.820, acc: 0.816
******** [step = 650] loss: 0.824, acc: 0.816
******** [step = 700] loss: 0.826, acc: 0.816
******** [step = 750] loss: 0.829, acc: 0.815
******** [step = 800] loss: 0.832, acc: 0.815
******** [step = 850] loss: 0.837, acc: 0.814
EPOCH = 46 loss: 0.837, acc: 0.814, val_loss: 1.180, val_acc: 0.789

================================================================================2025-08_11 17:47:26
******** [step = 50] loss: 0.745, acc: 0.828
******** [step = 100] loss: 0.753, acc: 0.827
******** [step = 150] loss: 0.755, acc: 0.827
******** [step = 200] loss: 0.763, acc: 0.826
******** [step = 250] loss: 0.767, acc: 0.825
******** [step = 300] loss: 0.775, acc: 0.824
******** [step = 350] loss: 0.781, acc: 0.823
******** [step = 400] loss: 0.789, acc: 0.821
******** [step = 450] loss: 0.797, acc: 0.820
******** [step = 500] loss: 0.801, acc: 0.820
******** [step = 550] loss: 0.804, acc: 0.819
******** [step = 600] loss: 0.809, acc: 0.819
******** [step = 650] loss: 0.813, acc: 0.818
******** [step = 700] loss: 0.816, acc: 0.818
******** [step = 750] loss: 0.822, acc: 0.817
******** [step = 800] loss: 0.825, acc: 0.817
******** [step = 850] loss: 0.829, acc: 0.816
EPOCH = 47 loss: 0.829, acc: 0.816, val_loss: 1.177, val_acc: 0.790

================================================================================2025-08_11 17:49:29
******** [step = 50] loss: 0.756, acc: 0.827
******** [step = 100] loss: 0.756, acc: 0.827
******** [step = 150] loss: 0.761, acc: 0.826
******** [step = 200] loss: 0.765, acc: 0.825
******** [step = 250] loss: 0.771, acc: 0.824
******** [step = 300] loss: 0.773, acc: 0.824
******** [step = 350] loss: 0.777, acc: 0.824
******** [step = 400] loss: 0.782, acc: 0.823
******** [step = 450] loss: 0.787, acc: 0.822
******** [step = 500] loss: 0.789, acc: 0.822
******** [step = 550] loss: 0.794, acc: 0.821
******** [step = 600] loss: 0.799, acc: 0.821
******** [step = 650] loss: 0.804, acc: 0.820
******** [step = 700] loss: 0.808, acc: 0.820
******** [step = 750] loss: 0.813, acc: 0.819
******** [step = 800] loss: 0.817, acc: 0.818
******** [step = 850] loss: 0.820, acc: 0.818
EPOCH = 48 loss: 0.820, acc: 0.818, val_loss: 1.174, val_acc: 0.791

================================================================================2025-08_11 17:51:31
******** [step = 50] loss: 0.744, acc: 0.828
******** [step = 100] loss: 0.753, acc: 0.827
******** [step = 150] loss: 0.755, acc: 0.826
******** [step = 200] loss: 0.761, acc: 0.826
******** [step = 250] loss: 0.770, acc: 0.825
******** [step = 300] loss: 0.773, acc: 0.824
******** [step = 350] loss: 0.778, acc: 0.823
******** [step = 400] loss: 0.783, acc: 0.823
******** [step = 450] loss: 0.786, acc: 0.822
******** [step = 500] loss: 0.792, acc: 0.821
******** [step = 550] loss: 0.794, acc: 0.821
******** [step = 600] loss: 0.798, acc: 0.820
******** [step = 650] loss: 0.801, acc: 0.820
******** [step = 700] loss: 0.805, acc: 0.820
******** [step = 750] loss: 0.807, acc: 0.820
******** [step = 800] loss: 0.810, acc: 0.819
******** [step = 850] loss: 0.813, acc: 0.819
EPOCH = 49 loss: 0.813, acc: 0.819, val_loss: 1.172, val_acc: 0.791

================================================================================2025-08_11 17:53:32
******** [step = 50] loss: 0.711, acc: 0.836
******** [step = 100] loss: 0.730, acc: 0.832
******** [step = 150] loss: 0.738, acc: 0.830
******** [step = 200] loss: 0.752, acc: 0.827
******** [step = 250] loss: 0.759, acc: 0.827
******** [step = 300] loss: 0.763, acc: 0.826
******** [step = 350] loss: 0.772, acc: 0.825
******** [step = 400] loss: 0.776, acc: 0.824
******** [step = 450] loss: 0.780, acc: 0.824
******** [step = 500] loss: 0.785, acc: 0.823
******** [step = 550] loss: 0.789, acc: 0.822
******** [step = 600] loss: 0.794, acc: 0.822
******** [step = 650] loss: 0.797, acc: 0.821
******** [step = 700] loss: 0.799, acc: 0.821
******** [step = 750] loss: 0.803, acc: 0.821
******** [step = 800] loss: 0.806, acc: 0.820
******** [step = 850] loss: 0.809, acc: 0.820
EPOCH = 50 loss: 0.809, acc: 0.820, val_loss: 1.170, val_acc: 0.791

================================================================================2025-08_11 17:55:33
******** [step = 50] loss: 0.744, acc: 0.832
******** [step = 100] loss: 0.736, acc: 0.831
******** [step = 150] loss: 0.740, acc: 0.830
******** [step = 200] loss: 0.741, acc: 0.830
******** [step = 250] loss: 0.743, acc: 0.829
******** [step = 300] loss: 0.749, acc: 0.829
******** [step = 350] loss: 0.758, acc: 0.828
******** [step = 400] loss: 0.765, acc: 0.827
******** [step = 450] loss: 0.768, acc: 0.826
******** [step = 500] loss: 0.774, acc: 0.825
******** [step = 550] loss: 0.778, acc: 0.825
******** [step = 600] loss: 0.784, acc: 0.824
******** [step = 650] loss: 0.788, acc: 0.824
******** [step = 700] loss: 0.792, acc: 0.823
******** [step = 750] loss: 0.796, acc: 0.822
******** [step = 800] loss: 0.800, acc: 0.822
******** [step = 850] loss: 0.804, acc: 0.821
EPOCH = 51 loss: 0.804, acc: 0.821, val_loss: 1.169, val_acc: 0.793

================================================================================2025-08_11 17:57:36
******** [step = 50] loss: 0.724, acc: 0.831
******** [step = 100] loss: 0.731, acc: 0.830
******** [step = 150] loss: 0.736, acc: 0.830
******** [step = 200] loss: 0.744, acc: 0.829
******** [step = 250] loss: 0.748, acc: 0.828
******** [step = 300] loss: 0.752, acc: 0.828
******** [step = 350] loss: 0.756, acc: 0.827
******** [step = 400] loss: 0.763, acc: 0.827
******** [step = 450] loss: 0.768, acc: 0.826
******** [step = 500] loss: 0.772, acc: 0.825
******** [step = 550] loss: 0.776, acc: 0.825
******** [step = 600] loss: 0.780, acc: 0.824
******** [step = 650] loss: 0.784, acc: 0.824
******** [step = 700] loss: 0.787, acc: 0.823
******** [step = 750] loss: 0.791, acc: 0.823
******** [step = 800] loss: 0.794, acc: 0.823
******** [step = 850] loss: 0.798, acc: 0.822
EPOCH = 52 loss: 0.798, acc: 0.822, val_loss: 1.168, val_acc: 0.792

================================================================================2025-08_11 17:59:37
******** [step = 50] loss: 0.719, acc: 0.835
******** [step = 100] loss: 0.722, acc: 0.835
******** [step = 150] loss: 0.732, acc: 0.833
******** [step = 200] loss: 0.737, acc: 0.831
******** [step = 250] loss: 0.744, acc: 0.830
******** [step = 300] loss: 0.747, acc: 0.829
******** [step = 350] loss: 0.750, acc: 0.829
******** [step = 400] loss: 0.754, acc: 0.829
******** [step = 450] loss: 0.758, acc: 0.828
******** [step = 500] loss: 0.763, acc: 0.828
******** [step = 550] loss: 0.769, acc: 0.827
******** [step = 600] loss: 0.774, acc: 0.826
******** [step = 650] loss: 0.778, acc: 0.825
******** [step = 700] loss: 0.781, acc: 0.825
******** [step = 750] loss: 0.785, acc: 0.824
******** [step = 800] loss: 0.788, acc: 0.824
******** [step = 850] loss: 0.790, acc: 0.824
EPOCH = 53 loss: 0.790, acc: 0.824, val_loss: 1.167, val_acc: 0.794

================================================================================2025-08_11 18:01:38
******** [step = 50] loss: 0.727, acc: 0.833
******** [step = 100] loss: 0.724, acc: 0.835
******** [step = 150] loss: 0.726, acc: 0.834
******** [step = 200] loss: 0.730, acc: 0.833
******** [step = 250] loss: 0.735, acc: 0.832
******** [step = 300] loss: 0.740, acc: 0.831
******** [step = 350] loss: 0.743, acc: 0.831
******** [step = 400] loss: 0.746, acc: 0.830
******** [step = 450] loss: 0.751, acc: 0.829
******** [step = 500] loss: 0.755, acc: 0.829
******** [step = 550] loss: 0.761, acc: 0.828
******** [step = 600] loss: 0.764, acc: 0.827
******** [step = 650] loss: 0.768, acc: 0.827
******** [step = 700] loss: 0.772, acc: 0.826
******** [step = 750] loss: 0.776, acc: 0.826
******** [step = 800] loss: 0.780, acc: 0.826
******** [step = 850] loss: 0.787, acc: 0.825
EPOCH = 54 loss: 0.787, acc: 0.825, val_loss: 1.160, val_acc: 0.794

================================================================================2025-08_11 18:03:41
******** [step = 50] loss: 0.717, acc: 0.837
******** [step = 100] loss: 0.714, acc: 0.835
******** [step = 150] loss: 0.722, acc: 0.834
******** [step = 200] loss: 0.724, acc: 0.834
******** [step = 250] loss: 0.729, acc: 0.833
******** [step = 300] loss: 0.733, acc: 0.832
******** [step = 350] loss: 0.739, acc: 0.831
******** [step = 400] loss: 0.743, acc: 0.830
******** [step = 450] loss: 0.747, acc: 0.830
******** [step = 500] loss: 0.751, acc: 0.829
******** [step = 550] loss: 0.757, acc: 0.828
******** [step = 600] loss: 0.761, acc: 0.828
******** [step = 650] loss: 0.765, acc: 0.827
******** [step = 700] loss: 0.768, acc: 0.827
******** [step = 750] loss: 0.772, acc: 0.826
******** [step = 800] loss: 0.775, acc: 0.826
******** [step = 850] loss: 0.779, acc: 0.826
EPOCH = 55 loss: 0.779, acc: 0.826, val_loss: 1.158, val_acc: 0.795

================================================================================2025-08_11 18:05:42
******** [step = 50] loss: 0.720, acc: 0.837
******** [step = 100] loss: 0.728, acc: 0.834
******** [step = 150] loss: 0.720, acc: 0.835
******** [step = 200] loss: 0.722, acc: 0.834
******** [step = 250] loss: 0.727, acc: 0.833
******** [step = 300] loss: 0.734, acc: 0.832
******** [step = 350] loss: 0.737, acc: 0.831
******** [step = 400] loss: 0.742, acc: 0.831
******** [step = 450] loss: 0.745, acc: 0.830
******** [step = 500] loss: 0.750, acc: 0.830
******** [step = 550] loss: 0.754, acc: 0.829
******** [step = 600] loss: 0.758, acc: 0.828
******** [step = 650] loss: 0.761, acc: 0.828
******** [step = 700] loss: 0.764, acc: 0.828
******** [step = 750] loss: 0.767, acc: 0.828
******** [step = 800] loss: 0.770, acc: 0.827
******** [step = 850] loss: 0.773, acc: 0.827
EPOCH = 56 loss: 0.773, acc: 0.827, val_loss: 1.157, val_acc: 0.796

================================================================================2025-08_11 18:07:44
******** [step = 50] loss: 0.693, acc: 0.838
******** [step = 100] loss: 0.701, acc: 0.837
******** [step = 150] loss: 0.710, acc: 0.836
******** [step = 200] loss: 0.714, acc: 0.835
******** [step = 250] loss: 0.723, acc: 0.833
******** [step = 300] loss: 0.728, acc: 0.833
******** [step = 350] loss: 0.733, acc: 0.832
******** [step = 400] loss: 0.736, acc: 0.832
******** [step = 450] loss: 0.741, acc: 0.831
******** [step = 500] loss: 0.744, acc: 0.831
******** [step = 550] loss: 0.748, acc: 0.830
******** [step = 600] loss: 0.753, acc: 0.830
******** [step = 650] loss: 0.757, acc: 0.829
******** [step = 700] loss: 0.761, acc: 0.829
******** [step = 750] loss: 0.765, acc: 0.828
******** [step = 800] loss: 0.768, acc: 0.828
******** [step = 850] loss: 0.770, acc: 0.828
EPOCH = 57 loss: 0.770, acc: 0.828, val_loss: 1.156, val_acc: 0.796

================================================================================2025-08_11 18:09:46
******** [step = 50] loss: 0.688, acc: 0.840
******** [step = 100] loss: 0.700, acc: 0.839
******** [step = 150] loss: 0.703, acc: 0.838
******** [step = 200] loss: 0.709, acc: 0.837
******** [step = 250] loss: 0.716, acc: 0.835
******** [step = 300] loss: 0.719, acc: 0.835
******** [step = 350] loss: 0.721, acc: 0.834
******** [step = 400] loss: 0.727, acc: 0.833
******** [step = 450] loss: 0.732, acc: 0.832
******** [step = 500] loss: 0.735, acc: 0.832
******** [step = 550] loss: 0.740, acc: 0.832
******** [step = 600] loss: 0.743, acc: 0.831
******** [step = 650] loss: 0.746, acc: 0.831
******** [step = 700] loss: 0.752, acc: 0.830
******** [step = 750] loss: 0.756, acc: 0.830
******** [step = 800] loss: 0.761, acc: 0.829
******** [step = 850] loss: 0.764, acc: 0.829
EPOCH = 58 loss: 0.764, acc: 0.829, val_loss: 1.151, val_acc: 0.796

================================================================================2025-08_11 18:11:47
******** [step = 50] loss: 0.697, acc: 0.839
******** [step = 100] loss: 0.706, acc: 0.838
******** [step = 150] loss: 0.703, acc: 0.838
******** [step = 200] loss: 0.708, acc: 0.837
******** [step = 250] loss: 0.713, acc: 0.836
******** [step = 300] loss: 0.717, acc: 0.835
******** [step = 350] loss: 0.719, acc: 0.835
******** [step = 400] loss: 0.726, acc: 0.834
******** [step = 450] loss: 0.728, acc: 0.834
******** [step = 500] loss: 0.733, acc: 0.833
******** [step = 550] loss: 0.737, acc: 0.832
******** [step = 600] loss: 0.741, acc: 0.832
******** [step = 650] loss: 0.746, acc: 0.831
******** [step = 700] loss: 0.750, acc: 0.831
******** [step = 750] loss: 0.753, acc: 0.830
******** [step = 800] loss: 0.757, acc: 0.830
******** [step = 850] loss: 0.759, acc: 0.830
EPOCH = 59 loss: 0.759, acc: 0.830, val_loss: 1.148, val_acc: 0.797

================================================================================2025-08_11 18:13:49
******** [step = 50] loss: 0.690, acc: 0.839
******** [step = 100] loss: 0.693, acc: 0.840
******** [step = 150] loss: 0.692, acc: 0.840
******** [step = 200] loss: 0.695, acc: 0.839
******** [step = 250] loss: 0.703, acc: 0.837
******** [step = 300] loss: 0.709, acc: 0.837
******** [step = 350] loss: 0.715, acc: 0.836
******** [step = 400] loss: 0.719, acc: 0.835
******** [step = 450] loss: 0.721, acc: 0.835
******** [step = 500] loss: 0.725, acc: 0.834
******** [step = 550] loss: 0.730, acc: 0.834
******** [step = 600] loss: 0.735, acc: 0.833
******** [step = 650] loss: 0.740, acc: 0.833
******** [step = 700] loss: 0.744, acc: 0.832
******** [step = 750] loss: 0.747, acc: 0.832
******** [step = 800] loss: 0.751, acc: 0.831
******** [step = 850] loss: 0.755, acc: 0.831
EPOCH = 60 loss: 0.755, acc: 0.831, val_loss: 1.155, val_acc: 0.797

================================================================================2025-08_11 18:15:52
******** [step = 50] loss: 0.667, acc: 0.844
******** [step = 100] loss: 0.677, acc: 0.843
******** [step = 150] loss: 0.683, acc: 0.842
******** [step = 200] loss: 0.691, acc: 0.840
******** [step = 250] loss: 0.697, acc: 0.839
******** [step = 300] loss: 0.700, acc: 0.838
******** [step = 350] loss: 0.706, acc: 0.837
******** [step = 400] loss: 0.713, acc: 0.836
******** [step = 450] loss: 0.717, acc: 0.836
******** [step = 500] loss: 0.720, acc: 0.835
******** [step = 550] loss: 0.725, acc: 0.835
******** [step = 600] loss: 0.729, acc: 0.834
******** [step = 650] loss: 0.733, acc: 0.834
******** [step = 700] loss: 0.738, acc: 0.833
******** [step = 750] loss: 0.742, acc: 0.833
******** [step = 800] loss: 0.746, acc: 0.832
******** [step = 850] loss: 0.751, acc: 0.831
EPOCH = 61 loss: 0.751, acc: 0.831, val_loss: 1.148, val_acc: 0.797

================================================================================2025-08_11 18:17:52
******** [step = 50] loss: 0.688, acc: 0.841
******** [step = 100] loss: 0.687, acc: 0.841
******** [step = 150] loss: 0.691, acc: 0.840
******** [step = 200] loss: 0.693, acc: 0.840
******** [step = 250] loss: 0.697, acc: 0.839
******** [step = 300] loss: 0.704, acc: 0.838
******** [step = 350] loss: 0.710, acc: 0.837
******** [step = 400] loss: 0.712, acc: 0.836
******** [step = 450] loss: 0.717, acc: 0.836
******** [step = 500] loss: 0.721, acc: 0.835
******** [step = 550] loss: 0.724, acc: 0.835
******** [step = 600] loss: 0.728, acc: 0.834
******** [step = 650] loss: 0.731, acc: 0.834
******** [step = 700] loss: 0.735, acc: 0.834
******** [step = 750] loss: 0.739, acc: 0.833
******** [step = 800] loss: 0.743, acc: 0.833
******** [step = 850] loss: 0.745, acc: 0.833
EPOCH = 62 loss: 0.745, acc: 0.833, val_loss: 1.149, val_acc: 0.798

================================================================================2025-08_11 18:19:54
******** [step = 50] loss: 0.669, acc: 0.843
******** [step = 100] loss: 0.680, acc: 0.842
******** [step = 150] loss: 0.683, acc: 0.842
******** [step = 200] loss: 0.689, acc: 0.840
******** [step = 250] loss: 0.692, acc: 0.840
******** [step = 300] loss: 0.695, acc: 0.839
******** [step = 350] loss: 0.700, acc: 0.839
******** [step = 400] loss: 0.705, acc: 0.838
******** [step = 450] loss: 0.709, acc: 0.837
******** [step = 500] loss: 0.714, acc: 0.837
******** [step = 550] loss: 0.718, acc: 0.836
******** [step = 600] loss: 0.721, acc: 0.836
******** [step = 650] loss: 0.726, acc: 0.835
******** [step = 700] loss: 0.730, acc: 0.835
******** [step = 750] loss: 0.733, acc: 0.835
******** [step = 800] loss: 0.737, acc: 0.834
******** [step = 850] loss: 0.741, acc: 0.834
EPOCH = 63 loss: 0.741, acc: 0.834, val_loss: 1.150, val_acc: 0.798

================================================================================2025-08_11 18:21:57
******** [step = 50] loss: 0.670, acc: 0.844
******** [step = 100] loss: 0.669, acc: 0.844
******** [step = 150] loss: 0.666, acc: 0.844
******** [step = 200] loss: 0.675, acc: 0.843
******** [step = 250] loss: 0.684, acc: 0.841
******** [step = 300] loss: 0.690, acc: 0.840
******** [step = 350] loss: 0.695, acc: 0.839
******** [step = 400] loss: 0.701, acc: 0.838
******** [step = 450] loss: 0.706, acc: 0.838
******** [step = 500] loss: 0.711, acc: 0.837
******** [step = 550] loss: 0.716, acc: 0.837
******** [step = 600] loss: 0.719, acc: 0.836
******** [step = 650] loss: 0.723, acc: 0.836
******** [step = 700] loss: 0.727, acc: 0.836
******** [step = 750] loss: 0.730, acc: 0.835
******** [step = 800] loss: 0.734, acc: 0.835
******** [step = 850] loss: 0.738, acc: 0.834
EPOCH = 64 loss: 0.738, acc: 0.834, val_loss: 1.144, val_acc: 0.799

================================================================================2025-08_11 18:23:59
******** [step = 50] loss: 0.645, acc: 0.849
******** [step = 100] loss: 0.659, acc: 0.846
******** [step = 150] loss: 0.668, acc: 0.845
******** [step = 200] loss: 0.673, acc: 0.844
******** [step = 250] loss: 0.680, acc: 0.843
******** [step = 300] loss: 0.686, acc: 0.842
******** [step = 350] loss: 0.693, acc: 0.841
******** [step = 400] loss: 0.696, acc: 0.840
******** [step = 450] loss: 0.702, acc: 0.839
******** [step = 500] loss: 0.706, acc: 0.838
******** [step = 550] loss: 0.711, acc: 0.838
******** [step = 600] loss: 0.714, acc: 0.838
******** [step = 650] loss: 0.718, acc: 0.837
******** [step = 700] loss: 0.722, acc: 0.837
******** [step = 750] loss: 0.726, acc: 0.836
******** [step = 800] loss: 0.729, acc: 0.836
******** [step = 850] loss: 0.732, acc: 0.835
EPOCH = 65 loss: 0.732, acc: 0.835, val_loss: 1.146, val_acc: 0.799

================================================================================2025-08_11 18:26:01
******** [step = 50] loss: 0.666, acc: 0.845
******** [step = 100] loss: 0.663, acc: 0.845
******** [step = 150] loss: 0.669, acc: 0.845
******** [step = 200] loss: 0.673, acc: 0.844
******** [step = 250] loss: 0.675, acc: 0.843
******** [step = 300] loss: 0.682, acc: 0.842
******** [step = 350] loss: 0.685, acc: 0.842
******** [step = 400] loss: 0.692, acc: 0.841
******** [step = 450] loss: 0.696, acc: 0.841
******** [step = 500] loss: 0.700, acc: 0.840
******** [step = 550] loss: 0.705, acc: 0.839
******** [step = 600] loss: 0.710, acc: 0.839
******** [step = 650] loss: 0.714, acc: 0.838
******** [step = 700] loss: 0.718, acc: 0.838
******** [step = 750] loss: 0.721, acc: 0.837
******** [step = 800] loss: 0.724, acc: 0.837
******** [step = 850] loss: 0.730, acc: 0.836
EPOCH = 66 loss: 0.730, acc: 0.836, val_loss: 1.143, val_acc: 0.799

================================================================================2025-08_11 18:28:02
******** [step = 50] loss: 0.666, acc: 0.845
******** [step = 100] loss: 0.661, acc: 0.845
******** [step = 150] loss: 0.664, acc: 0.845
******** [step = 200] loss: 0.672, acc: 0.844
******** [step = 250] loss: 0.677, acc: 0.843
******** [step = 300] loss: 0.680, acc: 0.843
******** [step = 350] loss: 0.686, acc: 0.842
******** [step = 400] loss: 0.689, acc: 0.842
******** [step = 450] loss: 0.691, acc: 0.842
******** [step = 500] loss: 0.696, acc: 0.841
******** [step = 550] loss: 0.699, acc: 0.840
******** [step = 600] loss: 0.703, acc: 0.840
******** [step = 650] loss: 0.708, acc: 0.839
******** [step = 700] loss: 0.712, acc: 0.839
******** [step = 750] loss: 0.715, acc: 0.838
******** [step = 800] loss: 0.719, acc: 0.838
******** [step = 850] loss: 0.727, acc: 0.837
EPOCH = 67 loss: 0.727, acc: 0.837, val_loss: 1.140, val_acc: 0.801

================================================================================2025-08_11 18:30:04
******** [step = 50] loss: 0.653, acc: 0.847
******** [step = 100] loss: 0.655, acc: 0.846
******** [step = 150] loss: 0.661, acc: 0.845
******** [step = 200] loss: 0.666, acc: 0.845
******** [step = 250] loss: 0.670, acc: 0.844
******** [step = 300] loss: 0.674, acc: 0.844
******** [step = 350] loss: 0.681, acc: 0.842
******** [step = 400] loss: 0.687, acc: 0.841
******** [step = 450] loss: 0.691, acc: 0.841
******** [step = 500] loss: 0.696, acc: 0.840
******** [step = 550] loss: 0.700, acc: 0.839
******** [step = 600] loss: 0.703, acc: 0.839
******** [step = 650] loss: 0.705, acc: 0.839
******** [step = 700] loss: 0.710, acc: 0.838
******** [step = 750] loss: 0.713, acc: 0.838
******** [step = 800] loss: 0.716, acc: 0.838
******** [step = 850] loss: 0.720, acc: 0.838
EPOCH = 68 loss: 0.720, acc: 0.838, val_loss: 1.142, val_acc: 0.801

================================================================================2025-08_11 18:32:06
******** [step = 50] loss: 0.656, acc: 0.845
******** [step = 100] loss: 0.658, acc: 0.846
******** [step = 150] loss: 0.664, acc: 0.845
******** [step = 200] loss: 0.664, acc: 0.845
******** [step = 250] loss: 0.670, acc: 0.844
******** [step = 300] loss: 0.674, acc: 0.843
******** [step = 350] loss: 0.678, acc: 0.843
******** [step = 400] loss: 0.683, acc: 0.842
******** [step = 450] loss: 0.687, acc: 0.842
******** [step = 500] loss: 0.690, acc: 0.841
******** [step = 550] loss: 0.696, acc: 0.840
******** [step = 600] loss: 0.699, acc: 0.840
******** [step = 650] loss: 0.702, acc: 0.840
******** [step = 700] loss: 0.706, acc: 0.839
******** [step = 750] loss: 0.708, acc: 0.839
******** [step = 800] loss: 0.712, acc: 0.839
******** [step = 850] loss: 0.714, acc: 0.839
EPOCH = 69 loss: 0.714, acc: 0.839, val_loss: 1.141, val_acc: 0.801

================================================================================2025-08_11 18:34:08
******** [step = 50] loss: 0.640, acc: 0.849
******** [step = 100] loss: 0.650, acc: 0.848
******** [step = 150] loss: 0.648, acc: 0.849
******** [step = 200] loss: 0.654, acc: 0.848
******** [step = 250] loss: 0.659, acc: 0.847
******** [step = 300] loss: 0.664, acc: 0.846
******** [step = 350] loss: 0.666, acc: 0.846
******** [step = 400] loss: 0.673, acc: 0.845
******** [step = 450] loss: 0.679, acc: 0.843
******** [step = 500] loss: 0.682, acc: 0.843
******** [step = 550] loss: 0.686, acc: 0.842
******** [step = 600] loss: 0.691, acc: 0.842
******** [step = 650] loss: 0.696, acc: 0.841
******** [step = 700] loss: 0.700, acc: 0.841
******** [step = 750] loss: 0.703, acc: 0.840
******** [step = 800] loss: 0.708, acc: 0.840
******** [step = 850] loss: 0.710, acc: 0.840
EPOCH = 70 loss: 0.710, acc: 0.840, val_loss: 1.138, val_acc: 0.802

================================================================================2025-08_11 18:36:09
******** [step = 50] loss: 0.641, acc: 0.850
******** [step = 100] loss: 0.641, acc: 0.851
******** [step = 150] loss: 0.646, acc: 0.849
******** [step = 200] loss: 0.653, acc: 0.848
******** [step = 250] loss: 0.658, acc: 0.847
******** [step = 300] loss: 0.662, acc: 0.847
******** [step = 350] loss: 0.667, acc: 0.846
******** [step = 400] loss: 0.671, acc: 0.845
******** [step = 450] loss: 0.674, acc: 0.845
******** [step = 500] loss: 0.680, acc: 0.844
******** [step = 550] loss: 0.685, acc: 0.843
******** [step = 600] loss: 0.690, acc: 0.843
******** [step = 650] loss: 0.694, acc: 0.842
******** [step = 700] loss: 0.696, acc: 0.842
******** [step = 750] loss: 0.700, acc: 0.841
******** [step = 800] loss: 0.704, acc: 0.841
******** [step = 850] loss: 0.708, acc: 0.840
EPOCH = 71 loss: 0.708, acc: 0.840, val_loss: 1.136, val_acc: 0.801

================================================================================2025-08_11 18:38:11
******** [step = 50] loss: 0.642, acc: 0.850
******** [step = 100] loss: 0.642, acc: 0.850
******** [step = 150] loss: 0.642, acc: 0.849
******** [step = 200] loss: 0.648, acc: 0.848
******** [step = 250] loss: 0.654, acc: 0.848
******** [step = 300] loss: 0.658, acc: 0.847
******** [step = 350] loss: 0.662, acc: 0.846
******** [step = 400] loss: 0.668, acc: 0.846
******** [step = 450] loss: 0.672, acc: 0.845
******** [step = 500] loss: 0.677, acc: 0.845
******** [step = 550] loss: 0.683, acc: 0.844
******** [step = 600] loss: 0.687, acc: 0.843
******** [step = 650] loss: 0.689, acc: 0.843
******** [step = 700] loss: 0.693, acc: 0.842
******** [step = 750] loss: 0.699, acc: 0.841
******** [step = 800] loss: 0.702, acc: 0.841
******** [step = 850] loss: 0.704, acc: 0.841
EPOCH = 72 loss: 0.704, acc: 0.841, val_loss: 1.139, val_acc: 0.802

================================================================================2025-08_11 18:40:11
******** [step = 50] loss: 0.646, acc: 0.851
******** [step = 100] loss: 0.641, acc: 0.851
******** [step = 150] loss: 0.639, acc: 0.851
******** [step = 200] loss: 0.644, acc: 0.850
******** [step = 250] loss: 0.650, acc: 0.849
******** [step = 300] loss: 0.655, acc: 0.848
******** [step = 350] loss: 0.659, acc: 0.847
******** [step = 400] loss: 0.664, acc: 0.847
******** [step = 450] loss: 0.670, acc: 0.846
******** [step = 500] loss: 0.675, acc: 0.845
******** [step = 550] loss: 0.679, acc: 0.845
******** [step = 600] loss: 0.684, acc: 0.844
******** [step = 650] loss: 0.687, acc: 0.844
******** [step = 700] loss: 0.691, acc: 0.843
******** [step = 750] loss: 0.693, acc: 0.843
******** [step = 800] loss: 0.697, acc: 0.842
******** [step = 850] loss: 0.702, acc: 0.842
EPOCH = 73 loss: 0.702, acc: 0.842, val_loss: 1.136, val_acc: 0.802

================================================================================2025-08_11 18:42:12
******** [step = 50] loss: 0.621, acc: 0.852
******** [step = 100] loss: 0.631, acc: 0.851
******** [step = 150] loss: 0.635, acc: 0.851
******** [step = 200] loss: 0.638, acc: 0.850
******** [step = 250] loss: 0.645, acc: 0.849
******** [step = 300] loss: 0.648, acc: 0.849
******** [step = 350] loss: 0.651, acc: 0.849
******** [step = 400] loss: 0.658, acc: 0.848
******** [step = 450] loss: 0.664, acc: 0.847
******** [step = 500] loss: 0.669, acc: 0.846
******** [step = 550] loss: 0.673, acc: 0.846
******** [step = 600] loss: 0.676, acc: 0.845
******** [step = 650] loss: 0.681, acc: 0.845
******** [step = 700] loss: 0.685, acc: 0.844
******** [step = 750] loss: 0.689, acc: 0.843
******** [step = 800] loss: 0.693, acc: 0.843
******** [step = 850] loss: 0.696, acc: 0.843
EPOCH = 74 loss: 0.696, acc: 0.843, val_loss: 1.136, val_acc: 0.803

================================================================================2025-08_11 18:44:15
******** [step = 50] loss: 0.640, acc: 0.850
******** [step = 100] loss: 0.636, acc: 0.850
******** [step = 150] loss: 0.641, acc: 0.849
******** [step = 200] loss: 0.646, acc: 0.849
******** [step = 250] loss: 0.648, acc: 0.849
******** [step = 300] loss: 0.648, acc: 0.849
******** [step = 350] loss: 0.655, acc: 0.848
******** [step = 400] loss: 0.660, acc: 0.847
******** [step = 450] loss: 0.665, acc: 0.847
******** [step = 500] loss: 0.670, acc: 0.846
******** [step = 550] loss: 0.673, acc: 0.846
******** [step = 600] loss: 0.675, acc: 0.845
******** [step = 650] loss: 0.679, acc: 0.845
******** [step = 700] loss: 0.683, acc: 0.844
******** [step = 750] loss: 0.688, acc: 0.844
******** [step = 800] loss: 0.691, acc: 0.843
******** [step = 850] loss: 0.694, acc: 0.843
EPOCH = 75 loss: 0.694, acc: 0.843, val_loss: 1.133, val_acc: 0.803

================================================================================2025-08_11 18:46:16
******** [step = 50] loss: 0.629, acc: 0.855
******** [step = 100] loss: 0.627, acc: 0.855
******** [step = 150] loss: 0.632, acc: 0.853
******** [step = 200] loss: 0.639, acc: 0.851
******** [step = 250] loss: 0.643, acc: 0.850
******** [step = 300] loss: 0.648, acc: 0.849
******** [step = 350] loss: 0.651, acc: 0.849
******** [step = 400] loss: 0.654, acc: 0.849
******** [step = 450] loss: 0.657, acc: 0.848
******** [step = 500] loss: 0.662, acc: 0.847
******** [step = 550] loss: 0.666, acc: 0.846
******** [step = 600] loss: 0.671, acc: 0.846
******** [step = 650] loss: 0.675, acc: 0.845
******** [step = 700] loss: 0.679, acc: 0.845
******** [step = 750] loss: 0.682, acc: 0.845
******** [step = 800] loss: 0.685, acc: 0.844
******** [step = 850] loss: 0.689, acc: 0.844
EPOCH = 76 loss: 0.689, acc: 0.844, val_loss: 1.131, val_acc: 0.803

================================================================================2025-08_11 18:48:17
******** [step = 50] loss: 0.621, acc: 0.853
******** [step = 100] loss: 0.624, acc: 0.853
******** [step = 150] loss: 0.628, acc: 0.852
******** [step = 200] loss: 0.628, acc: 0.852
******** [step = 250] loss: 0.634, acc: 0.851
******** [step = 300] loss: 0.637, acc: 0.851
******** [step = 350] loss: 0.642, acc: 0.850
******** [step = 400] loss: 0.647, acc: 0.849
******** [step = 450] loss: 0.651, acc: 0.849
******** [step = 500] loss: 0.656, acc: 0.848
******** [step = 550] loss: 0.662, acc: 0.847
******** [step = 600] loss: 0.667, acc: 0.847
******** [step = 650] loss: 0.671, acc: 0.846
******** [step = 700] loss: 0.675, acc: 0.846
******** [step = 750] loss: 0.679, acc: 0.845
******** [step = 800] loss: 0.683, acc: 0.845
******** [step = 850] loss: 0.687, acc: 0.844
EPOCH = 77 loss: 0.687, acc: 0.844, val_loss: 1.134, val_acc: 0.803

================================================================================2025-08_11 18:50:19
******** [step = 50] loss: 0.630, acc: 0.854
******** [step = 100] loss: 0.618, acc: 0.856
******** [step = 150] loss: 0.625, acc: 0.854
******** [step = 200] loss: 0.629, acc: 0.853
******** [step = 250] loss: 0.636, acc: 0.852
******** [step = 300] loss: 0.639, acc: 0.851
******** [step = 350] loss: 0.643, acc: 0.851
******** [step = 400] loss: 0.646, acc: 0.850
******** [step = 450] loss: 0.652, acc: 0.849
******** [step = 500] loss: 0.656, acc: 0.848
******** [step = 550] loss: 0.661, acc: 0.848
******** [step = 600] loss: 0.665, acc: 0.847
******** [step = 650] loss: 0.669, acc: 0.847
******** [step = 700] loss: 0.673, acc: 0.846
******** [step = 750] loss: 0.676, acc: 0.846
******** [step = 800] loss: 0.679, acc: 0.846
******** [step = 850] loss: 0.683, acc: 0.845
EPOCH = 78 loss: 0.683, acc: 0.845, val_loss: 1.132, val_acc: 0.804

================================================================================2025-08_11 18:52:21
******** [step = 50] loss: 0.616, acc: 0.854
******** [step = 100] loss: 0.616, acc: 0.854
******** [step = 150] loss: 0.616, acc: 0.854
******** [step = 200] loss: 0.621, acc: 0.854
******** [step = 250] loss: 0.627, acc: 0.852
******** [step = 300] loss: 0.633, acc: 0.852
******** [step = 350] loss: 0.637, acc: 0.851
******** [step = 400] loss: 0.644, acc: 0.850
******** [step = 450] loss: 0.651, acc: 0.849
******** [step = 500] loss: 0.657, acc: 0.848
******** [step = 550] loss: 0.659, acc: 0.848
******** [step = 600] loss: 0.663, acc: 0.848
******** [step = 650] loss: 0.667, acc: 0.847
******** [step = 700] loss: 0.669, acc: 0.847
******** [step = 750] loss: 0.673, acc: 0.847
******** [step = 800] loss: 0.677, acc: 0.846
******** [step = 850] loss: 0.680, acc: 0.846
EPOCH = 79 loss: 0.680, acc: 0.846, val_loss: 1.130, val_acc: 0.804

================================================================================2025-08_11 18:54:24
******** [step = 50] loss: 0.611, acc: 0.857
******** [step = 100] loss: 0.617, acc: 0.855
******** [step = 150] loss: 0.613, acc: 0.856
******** [step = 200] loss: 0.620, acc: 0.855
******** [step = 250] loss: 0.623, acc: 0.854
******** [step = 300] loss: 0.627, acc: 0.854
******** [step = 350] loss: 0.632, acc: 0.853
******** [step = 400] loss: 0.637, acc: 0.852
******** [step = 450] loss: 0.642, acc: 0.851
******** [step = 500] loss: 0.646, acc: 0.850
******** [step = 550] loss: 0.652, acc: 0.850
******** [step = 600] loss: 0.657, acc: 0.849
******** [step = 650] loss: 0.661, acc: 0.848
******** [step = 700] loss: 0.665, acc: 0.848
******** [step = 750] loss: 0.669, acc: 0.847
******** [step = 800] loss: 0.673, acc: 0.847
******** [step = 850] loss: 0.676, acc: 0.846
EPOCH = 80 loss: 0.676, acc: 0.846, val_loss: 1.130, val_acc: 0.804

================================================================================2025-08_11 18:56:26
finishing training...
Training complete in 163m 16s
    epoch  ...   val_acc
0     1.0  ...  0.398359
1     2.0  ...  0.445936
2     3.0  ...  0.474939
3     4.0  ...  0.510989
4     5.0  ...  0.539151
..    ...  ...       ...
75   76.0  ...  0.803207
76   77.0  ...  0.802698
77   78.0  ...  0.804131
78   79.0  ...  0.803811
79   80.0  ...  0.804336

[80 rows x 5 columns]
== Done ==
Mon Aug 11 06:57:00 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-11-2025 18:57:00
Job ID:        6901215
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=11:06:04,vmem=0,walltime=02:46:31,mem=37756K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-008-35-0
---------------------------------------
