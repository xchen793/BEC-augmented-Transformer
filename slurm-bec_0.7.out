---------------------------------------
Begin Slurm Prolog: Aug-10-2025 01:43:47
Job ID:    6778817
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 01:43:47 AM EDT 2025
atl1-1-02-007-31-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 01:43:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   44C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 108520.85it/s] 29%|██▊       | 38725/135842 [00:00<00:00, 161457.76it/s] 41%|████▏     | 56072/135842 [00:00<00:00, 122638.56it/s] 51%|█████▏    | 69719/135842 [00:00<00:00, 95657.94it/s]  66%|██████▌   | 89953/135842 [00:00<00:00, 121709.46it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 91244.51it/s] 92%|█████████▏| 125593/135842 [00:01<00:00, 109928.17it/s]100%|██████████| 135842/135842 [00:01<00:00, 113280.86it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47919.93it/s] 33%|███▎      | 35790/108673 [00:00<00:00, 86334.24it/s] 50%|████▉     | 54122/108673 [00:00<00:00, 114211.06it/s] 67%|██████▋   | 72534/108673 [00:00<00:00, 134558.72it/s] 82%|████████▏ | 89076/108673 [00:01<00:00, 72259.58it/s]  99%|█████████▉| 107357/108673 [00:01<00:00, 91527.99it/s]100%|██████████| 108673/108673 [00:01<00:00, 90104.25it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 67%|██████▋   | 18079/27169 [00:00<00:00, 180776.38it/s]100%|██████████| 27169/27169 [00:00<00:00, 182281.12it/s]
tensor([  3,  20,  21,  11,   9,  20,  35,  12,   6,  81, 123,  14, 391, 303,
         12,  18,   6,  81,   4,   2,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,    14,  1736,    21,     6,   158,     7,  1014,     7, 11368,
           16,   988,    54,   291,     7,    63, 18273,     6,    92,  1092,
            7,    15,     9,  3782,    29,  3253,     4,     2,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([    3,     8,  1979,    73,    97,  9875,     8,  1018,  4605,    54,
        11050,    40,    46,   516,    36,     7,  1320,  2840,     4,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
*************************** start training...

================================================================================2025-08_10 01:44:18
******** [step = 50] loss: 9.354, acc: 0.011
******** [step = 100] loss: 8.972, acc: 0.068
******** [step = 150] loss: 8.621, acc: 0.116
******** [step = 200] loss: 8.291, acc: 0.149
******** [step = 250] loss: 7.984, acc: 0.169
******** [step = 300] loss: 7.684, acc: 0.183
******** [step = 350] loss: 7.389, acc: 0.195
******** [step = 400] loss: 7.115, acc: 0.206
******** [step = 450] loss: 6.868, acc: 0.215
******** [step = 500] loss: 6.651, acc: 0.225
******** [step = 550] loss: 6.459, acc: 0.234
******** [step = 600] loss: 6.287, acc: 0.242
******** [step = 650] loss: 6.133, acc: 0.250
******** [step = 700] loss: 5.996, acc: 0.256
******** [step = 750] loss: 5.870, acc: 0.263
******** [step = 800] loss: 5.757, acc: 0.268
******** [step = 850] loss: 5.653, acc: 0.274
EPOCH = 1 loss: 5.653, acc: 0.274, val_loss: 3.859, val_acc: 0.375

================================================================================2025-08_10 01:45:38
******** [step = 50] loss: 3.929, acc: 0.358
******** [step = 100] loss: 3.883, acc: 0.363
******** [step = 150] loss: 3.861, acc: 0.363
******** [step = 200] loss: 3.842, acc: 0.364
******** [step = 250] loss: 3.827, acc: 0.365
******** [step = 300] loss: 3.805, acc: 0.367
******** [step = 350] loss: 3.792, acc: 0.368
******** [step = 400] loss: 3.775, acc: 0.369
******** [step = 450] loss: 3.759, acc: 0.370
******** [step = 500] loss: 3.747, acc: 0.371
******** [step = 550] loss: 3.733, acc: 0.372
******** [step = 600] loss: 3.718, acc: 0.373
******** [step = 650] loss: 3.707, acc: 0.374
******** [step = 700] loss: 3.695, acc: 0.375
******** [step = 750] loss: 3.683, acc: 0.376
******** [step = 800] loss: 3.671, acc: 0.376
******** [step = 850] loss: 3.659, acc: 0.377
EPOCH = 2 loss: 3.659, acc: 0.377, val_loss: 3.398, val_acc: 0.403

================================================================================2025-08_10 01:46:58
******** [step = 50] loss: 3.444, acc: 0.392
******** [step = 100] loss: 3.409, acc: 0.396
******** [step = 150] loss: 3.405, acc: 0.395
******** [step = 200] loss: 3.400, acc: 0.395
******** [step = 250] loss: 3.396, acc: 0.396
******** [step = 300] loss: 3.385, acc: 0.398
******** [step = 350] loss: 3.380, acc: 0.399
******** [step = 400] loss: 3.372, acc: 0.400
******** [step = 450] loss: 3.362, acc: 0.401
******** [step = 500] loss: 3.358, acc: 0.402
******** [step = 550] loss: 3.351, acc: 0.402
******** [step = 600] loss: 3.346, acc: 0.403
******** [step = 650] loss: 3.339, acc: 0.404
******** [step = 700] loss: 3.335, acc: 0.404
******** [step = 750] loss: 3.329, acc: 0.405
******** [step = 800] loss: 3.325, acc: 0.406
******** [step = 850] loss: 3.319, acc: 0.406
EPOCH = 3 loss: 3.319, acc: 0.406, val_loss: 3.195, val_acc: 0.426

================================================================================2025-08_10 01:48:18
******** [step = 50] loss: 3.182, acc: 0.418
******** [step = 100] loss: 3.159, acc: 0.420
******** [step = 150] loss: 3.156, acc: 0.420
******** [step = 200] loss: 3.155, acc: 0.421
******** [step = 250] loss: 3.149, acc: 0.422
******** [step = 300] loss: 3.147, acc: 0.422
******** [step = 350] loss: 3.146, acc: 0.423
******** [step = 400] loss: 3.146, acc: 0.423
******** [step = 450] loss: 3.144, acc: 0.423
******** [step = 500] loss: 3.142, acc: 0.423
******** [step = 550] loss: 3.142, acc: 0.424
******** [step = 600] loss: 3.140, acc: 0.424
******** [step = 650] loss: 3.138, acc: 0.424
******** [step = 700] loss: 3.135, acc: 0.425
******** [step = 750] loss: 3.132, acc: 0.425
******** [step = 800] loss: 3.130, acc: 0.426
******** [step = 850] loss: 3.128, acc: 0.427
EPOCH = 4 loss: 3.128, acc: 0.427, val_loss: 3.032, val_acc: 0.445

================================================================================2025-08_10 01:49:37
******** [step = 50] loss: 3.021, acc: 0.433
******** [step = 100] loss: 2.999, acc: 0.437
******** [step = 150] loss: 2.997, acc: 0.437
******** [step = 200] loss: 3.002, acc: 0.438
******** [step = 250] loss: 3.000, acc: 0.438
******** [step = 300] loss: 2.998, acc: 0.439
******** [step = 350] loss: 2.997, acc: 0.439
******** [step = 400] loss: 2.996, acc: 0.439
******** [step = 450] loss: 2.998, acc: 0.439
******** [step = 500] loss: 2.998, acc: 0.440
******** [step = 550] loss: 3.000, acc: 0.439
******** [step = 600] loss: 3.001, acc: 0.440
******** [step = 650] loss: 3.001, acc: 0.440
******** [step = 700] loss: 3.002, acc: 0.440
******** [step = 750] loss: 3.004, acc: 0.440
******** [step = 800] loss: 3.004, acc: 0.440
******** [step = 850] loss: 3.004, acc: 0.440
EPOCH = 5 loss: 3.004, acc: 0.440, val_loss: 2.985, val_acc: 0.453

================================================================================2025-08_10 01:50:57
******** [step = 50] loss: 2.912, acc: 0.447
******** [step = 100] loss: 2.889, acc: 0.449
******** [step = 150] loss: 2.890, acc: 0.449
******** [step = 200] loss: 2.895, acc: 0.448
******** [step = 250] loss: 2.894, acc: 0.449
******** [step = 300] loss: 2.895, acc: 0.449
******** [step = 350] loss: 2.899, acc: 0.449
******** [step = 400] loss: 2.900, acc: 0.449
******** [step = 450] loss: 2.899, acc: 0.450
******** [step = 500] loss: 2.899, acc: 0.450
******** [step = 550] loss: 2.897, acc: 0.451
******** [step = 600] loss: 2.903, acc: 0.450
******** [step = 650] loss: 2.907, acc: 0.449
******** [step = 700] loss: 2.908, acc: 0.449
******** [step = 750] loss: 2.906, acc: 0.450
******** [step = 800] loss: 2.906, acc: 0.450
******** [step = 850] loss: 2.904, acc: 0.451
EPOCH = 6 loss: 2.904, acc: 0.451, val_loss: 2.874, val_acc: 0.462

================================================================================2025-08_10 01:52:17
******** [step = 50] loss: 2.764, acc: 0.461
******** [step = 100] loss: 2.764, acc: 0.466
******** [step = 150] loss: 2.769, acc: 0.465
******** [step = 200] loss: 2.778, acc: 0.464
******** [step = 250] loss: 2.782, acc: 0.464
******** [step = 300] loss: 2.782, acc: 0.465
******** [step = 350] loss: 2.785, acc: 0.465
******** [step = 400] loss: 2.788, acc: 0.465
******** [step = 450] loss: 2.789, acc: 0.464
******** [step = 500] loss: 2.788, acc: 0.465
******** [step = 550] loss: 2.791, acc: 0.464
******** [step = 600] loss: 2.792, acc: 0.465
******** [step = 650] loss: 2.791, acc: 0.465
******** [step = 700] loss: 2.789, acc: 0.465
******** [step = 750] loss: 2.789, acc: 0.465
******** [step = 800] loss: 2.789, acc: 0.465
******** [step = 850] loss: 2.788, acc: 0.466
EPOCH = 7 loss: 2.788, acc: 0.466, val_loss: 2.784, val_acc: 0.477

================================================================================2025-08_10 01:53:37
******** [step = 50] loss: 2.663, acc: 0.478
******** [step = 100] loss: 2.656, acc: 0.479
******** [step = 150] loss: 2.662, acc: 0.479
******** [step = 200] loss: 2.672, acc: 0.478
******** [step = 250] loss: 2.670, acc: 0.478
******** [step = 300] loss: 2.671, acc: 0.478
******** [step = 350] loss: 2.675, acc: 0.478
******** [step = 400] loss: 2.678, acc: 0.478
******** [step = 450] loss: 2.690, acc: 0.476
******** [step = 500] loss: 2.699, acc: 0.475
******** [step = 550] loss: 2.700, acc: 0.475
******** [step = 600] loss: 2.700, acc: 0.475
******** [step = 650] loss: 2.701, acc: 0.476
******** [step = 700] loss: 2.701, acc: 0.476
******** [step = 750] loss: 2.702, acc: 0.476
******** [step = 800] loss: 2.704, acc: 0.476
******** [step = 850] loss: 2.702, acc: 0.477
EPOCH = 8 loss: 2.702, acc: 0.477, val_loss: 2.700, val_acc: 0.495

================================================================================2025-08_10 01:54:57
******** [step = 50] loss: 2.603, acc: 0.486
******** [step = 100] loss: 2.585, acc: 0.490
******** [step = 150] loss: 2.589, acc: 0.490
******** [step = 200] loss: 2.588, acc: 0.490
******** [step = 250] loss: 2.589, acc: 0.490
******** [step = 300] loss: 2.593, acc: 0.490
******** [step = 350] loss: 2.603, acc: 0.489
******** [step = 400] loss: 2.605, acc: 0.489
******** [step = 450] loss: 2.607, acc: 0.489
******** [step = 500] loss: 2.613, acc: 0.488
******** [step = 550] loss: 2.619, acc: 0.488
******** [step = 600] loss: 2.620, acc: 0.488
******** [step = 650] loss: 2.623, acc: 0.487
******** [step = 700] loss: 2.625, acc: 0.487
******** [step = 750] loss: 2.627, acc: 0.487
******** [step = 800] loss: 2.628, acc: 0.488
******** [step = 850] loss: 2.628, acc: 0.488
EPOCH = 9 loss: 2.628, acc: 0.488, val_loss: 2.699, val_acc: 0.491

================================================================================2025-08_10 01:56:17
******** [step = 50] loss: 2.553, acc: 0.493
******** [step = 100] loss: 2.562, acc: 0.493
******** [step = 150] loss: 2.569, acc: 0.492
******** [step = 200] loss: 2.570, acc: 0.492
******** [step = 250] loss: 2.574, acc: 0.492
******** [step = 300] loss: 2.575, acc: 0.491
******** [step = 350] loss: 2.577, acc: 0.492
******** [step = 400] loss: 2.580, acc: 0.491
******** [step = 450] loss: 2.588, acc: 0.490
******** [step = 500] loss: 2.592, acc: 0.490
******** [step = 550] loss: 2.594, acc: 0.489
******** [step = 600] loss: 2.598, acc: 0.489
******** [step = 650] loss: 2.601, acc: 0.489
******** [step = 700] loss: 2.603, acc: 0.489
******** [step = 750] loss: 2.604, acc: 0.489
******** [step = 800] loss: 2.605, acc: 0.490
******** [step = 850] loss: 2.605, acc: 0.490
EPOCH = 10 loss: 2.605, acc: 0.490, val_loss: 2.655, val_acc: 0.504

================================================================================2025-08_10 01:57:36
******** [step = 50] loss: 2.497, acc: 0.502
******** [step = 100] loss: 2.488, acc: 0.503
******** [step = 150] loss: 2.487, acc: 0.502
******** [step = 200] loss: 2.495, acc: 0.502
******** [step = 250] loss: 2.507, acc: 0.500
******** [step = 300] loss: 2.510, acc: 0.500
******** [step = 350] loss: 2.512, acc: 0.500
******** [step = 400] loss: 2.512, acc: 0.501
******** [step = 450] loss: 2.515, acc: 0.500
******** [step = 500] loss: 2.520, acc: 0.500
******** [step = 550] loss: 2.526, acc: 0.499
******** [step = 600] loss: 2.532, acc: 0.498
******** [step = 650] loss: 2.536, acc: 0.498
******** [step = 700] loss: 2.540, acc: 0.497
******** [step = 750] loss: 2.545, acc: 0.497
******** [step = 800] loss: 2.548, acc: 0.497
******** [step = 850] loss: 2.552, acc: 0.496
EPOCH = 11 loss: 2.552, acc: 0.496, val_loss: 2.653, val_acc: 0.502

================================================================================2025-08_10 01:58:55
******** [step = 50] loss: 2.456, acc: 0.507
******** [step = 100] loss: 2.447, acc: 0.508
******** [step = 150] loss: 2.456, acc: 0.505
******** [step = 200] loss: 2.465, acc: 0.505
******** [step = 250] loss: 2.469, acc: 0.505
******** [step = 300] loss: 2.480, acc: 0.503
******** [step = 350] loss: 2.488, acc: 0.502
******** [step = 400] loss: 2.497, acc: 0.501
******** [step = 450] loss: 2.505, acc: 0.500
******** [step = 500] loss: 2.511, acc: 0.500
******** [step = 550] loss: 2.513, acc: 0.500
******** [step = 600] loss: 2.515, acc: 0.499
******** [step = 650] loss: 2.517, acc: 0.499
******** [step = 700] loss: 2.520, acc: 0.499
******** [step = 750] loss: 2.522, acc: 0.499
******** [step = 800] loss: 2.524, acc: 0.499
******** [step = 850] loss: 2.524, acc: 0.499
EPOCH = 12 loss: 2.524, acc: 0.499, val_loss: 2.644, val_acc: 0.506

================================================================================2025-08_10 02:00:15
******** [step = 50] loss: 2.426, acc: 0.510
******** [step = 100] loss: 2.429, acc: 0.509
******** [step = 150] loss: 2.428, acc: 0.509
******** [step = 200] loss: 2.425, acc: 0.510
******** [step = 250] loss: 2.433, acc: 0.509
******** [step = 300] loss: 2.435, acc: 0.509
******** [step = 350] loss: 2.440, acc: 0.508
******** [step = 400] loss: 2.443, acc: 0.509
******** [step = 450] loss: 2.446, acc: 0.509
******** [step = 500] loss: 2.450, acc: 0.509
******** [step = 550] loss: 2.454, acc: 0.509
******** [step = 600] loss: 2.456, acc: 0.509
******** [step = 650] loss: 2.459, acc: 0.509
******** [step = 700] loss: 2.462, acc: 0.509
******** [step = 750] loss: 2.464, acc: 0.509
******** [step = 800] loss: 2.465, acc: 0.509
******** [step = 850] loss: 2.465, acc: 0.509
EPOCH = 13 loss: 2.465, acc: 0.509, val_loss: 2.604, val_acc: 0.513

================================================================================2025-08_10 02:01:35
******** [step = 50] loss: 2.384, acc: 0.515
******** [step = 100] loss: 2.375, acc: 0.517
******** [step = 150] loss: 2.378, acc: 0.518
******** [step = 200] loss: 2.384, acc: 0.517
******** [step = 250] loss: 2.386, acc: 0.517
******** [step = 300] loss: 2.392, acc: 0.516
******** [step = 350] loss: 2.393, acc: 0.516
******** [step = 400] loss: 2.396, acc: 0.517
******** [step = 450] loss: 2.398, acc: 0.517
******** [step = 500] loss: 2.401, acc: 0.517
******** [step = 550] loss: 2.403, acc: 0.516
******** [step = 600] loss: 2.407, acc: 0.516
******** [step = 650] loss: 2.410, acc: 0.516
******** [step = 700] loss: 2.412, acc: 0.516
******** [step = 750] loss: 2.413, acc: 0.516
******** [step = 800] loss: 2.416, acc: 0.516
******** [step = 850] loss: 2.417, acc: 0.516
EPOCH = 14 loss: 2.417, acc: 0.516, val_loss: 2.567, val_acc: 0.520

================================================================================2025-08_10 02:02:54
******** [step = 50] loss: 2.323, acc: 0.525
******** [step = 100] loss: 2.329, acc: 0.524
******** [step = 150] loss: 2.328, acc: 0.525
******** [step = 200] loss: 2.337, acc: 0.524
******** [step = 250] loss: 2.348, acc: 0.523
******** [step = 300] loss: 2.351, acc: 0.523
******** [step = 350] loss: 2.356, acc: 0.522
******** [step = 400] loss: 2.361, acc: 0.522
******** [step = 450] loss: 2.360, acc: 0.523
******** [step = 500] loss: 2.364, acc: 0.522
******** [step = 550] loss: 2.365, acc: 0.522
******** [step = 600] loss: 2.368, acc: 0.523
******** [step = 650] loss: 2.372, acc: 0.522
******** [step = 700] loss: 2.374, acc: 0.522
******** [step = 750] loss: 2.376, acc: 0.522
******** [step = 800] loss: 2.376, acc: 0.522
******** [step = 850] loss: 2.377, acc: 0.522
EPOCH = 15 loss: 2.377, acc: 0.522, val_loss: 2.590, val_acc: 0.518

================================================================================2025-08_10 02:04:14
******** [step = 50] loss: 2.309, acc: 0.530
******** [step = 100] loss: 2.297, acc: 0.530
******** [step = 150] loss: 2.297, acc: 0.530
******** [step = 200] loss: 2.300, acc: 0.530
******** [step = 250] loss: 2.308, acc: 0.530
******** [step = 300] loss: 2.310, acc: 0.529
******** [step = 350] loss: 2.316, acc: 0.529
******** [step = 400] loss: 2.319, acc: 0.529
******** [step = 450] loss: 2.320, acc: 0.529
******** [step = 500] loss: 2.325, acc: 0.528
******** [step = 550] loss: 2.330, acc: 0.528
******** [step = 600] loss: 2.333, acc: 0.528
******** [step = 650] loss: 2.336, acc: 0.528
******** [step = 700] loss: 2.338, acc: 0.528
******** [step = 750] loss: 2.343, acc: 0.527
******** [step = 800] loss: 2.344, acc: 0.527
******** [step = 850] loss: 2.344, acc: 0.527
EPOCH = 16 loss: 2.344, acc: 0.527, val_loss: 2.538, val_acc: 0.527

================================================================================2025-08_10 02:05:33
******** [step = 50] loss: 2.270, acc: 0.534
******** [step = 100] loss: 2.258, acc: 0.534
******** [step = 150] loss: 2.263, acc: 0.535
******** [step = 200] loss: 2.263, acc: 0.535
******** [step = 250] loss: 2.270, acc: 0.534
******** [step = 300] loss: 2.274, acc: 0.534
******** [step = 350] loss: 2.282, acc: 0.534
******** [step = 400] loss: 2.286, acc: 0.533
******** [step = 450] loss: 2.291, acc: 0.533
******** [step = 500] loss: 2.295, acc: 0.533
******** [step = 550] loss: 2.298, acc: 0.533
******** [step = 600] loss: 2.300, acc: 0.533
******** [step = 650] loss: 2.304, acc: 0.532
******** [step = 700] loss: 2.308, acc: 0.532
******** [step = 750] loss: 2.310, acc: 0.532
******** [step = 800] loss: 2.313, acc: 0.532
******** [step = 850] loss: 2.314, acc: 0.532
EPOCH = 17 loss: 2.314, acc: 0.532, val_loss: 2.506, val_acc: 0.533

================================================================================2025-08_10 02:06:52
******** [step = 50] loss: 2.224, acc: 0.537
******** [step = 100] loss: 2.222, acc: 0.541
******** [step = 150] loss: 2.228, acc: 0.540
******** [step = 200] loss: 2.234, acc: 0.540
******** [step = 250] loss: 2.239, acc: 0.539
******** [step = 300] loss: 2.244, acc: 0.539
******** [step = 350] loss: 2.250, acc: 0.538
******** [step = 400] loss: 2.252, acc: 0.538
******** [step = 450] loss: 2.255, acc: 0.538
******** [step = 500] loss: 2.260, acc: 0.538
******** [step = 550] loss: 2.265, acc: 0.537
******** [step = 600] loss: 2.269, acc: 0.537
******** [step = 650] loss: 2.271, acc: 0.537
******** [step = 700] loss: 2.276, acc: 0.536
******** [step = 750] loss: 2.280, acc: 0.536
******** [step = 800] loss: 2.282, acc: 0.536
******** [step = 850] loss: 2.287, acc: 0.536
EPOCH = 18 loss: 2.287, acc: 0.536, val_loss: 2.513, val_acc: 0.534

================================================================================2025-08_10 02:08:12
******** [step = 50] loss: 2.210, acc: 0.543
******** [step = 100] loss: 2.209, acc: 0.545
******** [step = 150] loss: 2.212, acc: 0.544
******** [step = 200] loss: 2.214, acc: 0.545
******** [step = 250] loss: 2.215, acc: 0.545
******** [step = 300] loss: 2.220, acc: 0.544
******** [step = 350] loss: 2.225, acc: 0.544
******** [step = 400] loss: 2.229, acc: 0.543
******** [step = 450] loss: 2.232, acc: 0.543
******** [step = 500] loss: 2.234, acc: 0.543
******** [step = 550] loss: 2.240, acc: 0.542
******** [step = 600] loss: 2.242, acc: 0.542
******** [step = 650] loss: 2.248, acc: 0.541
******** [step = 700] loss: 2.250, acc: 0.541
******** [step = 750] loss: 2.254, acc: 0.541
******** [step = 800] loss: 2.256, acc: 0.541
******** [step = 850] loss: 2.259, acc: 0.541
EPOCH = 19 loss: 2.259, acc: 0.541, val_loss: 2.493, val_acc: 0.538

================================================================================2025-08_10 02:09:32
******** [step = 50] loss: 2.178, acc: 0.550
******** [step = 100] loss: 2.173, acc: 0.549
******** [step = 150] loss: 2.172, acc: 0.549
******** [step = 200] loss: 2.180, acc: 0.549
******** [step = 250] loss: 2.184, acc: 0.548
******** [step = 300] loss: 2.190, acc: 0.547
******** [step = 350] loss: 2.194, acc: 0.547
******** [step = 400] loss: 2.203, acc: 0.546
******** [step = 450] loss: 2.209, acc: 0.546
******** [step = 500] loss: 2.211, acc: 0.546
******** [step = 550] loss: 2.215, acc: 0.545
******** [step = 600] loss: 2.219, acc: 0.545
******** [step = 650] loss: 2.222, acc: 0.545
******** [step = 700] loss: 2.224, acc: 0.545
******** [step = 750] loss: 2.228, acc: 0.545
******** [step = 800] loss: 2.231, acc: 0.545
******** [step = 850] loss: 2.237, acc: 0.544
EPOCH = 20 loss: 2.237, acc: 0.544, val_loss: 2.468, val_acc: 0.541

================================================================================2025-08_10 02:10:51
******** [step = 50] loss: 2.161, acc: 0.551
******** [step = 100] loss: 2.156, acc: 0.551
******** [step = 150] loss: 2.167, acc: 0.550
******** [step = 200] loss: 2.168, acc: 0.550
******** [step = 250] loss: 2.171, acc: 0.550
******** [step = 300] loss: 2.176, acc: 0.550
******** [step = 350] loss: 2.182, acc: 0.550
******** [step = 400] loss: 2.184, acc: 0.550
******** [step = 450] loss: 2.189, acc: 0.549
******** [step = 500] loss: 2.192, acc: 0.549
******** [step = 550] loss: 2.194, acc: 0.549
******** [step = 600] loss: 2.198, acc: 0.549
******** [step = 650] loss: 2.200, acc: 0.549
******** [step = 700] loss: 2.202, acc: 0.549
******** [step = 750] loss: 2.204, acc: 0.549
******** [step = 800] loss: 2.209, acc: 0.548
******** [step = 850] loss: 2.211, acc: 0.548
EPOCH = 21 loss: 2.211, acc: 0.548, val_loss: 2.460, val_acc: 0.545

================================================================================2025-08_10 02:12:11
******** [step = 50] loss: 2.134, acc: 0.556
******** [step = 100] loss: 2.126, acc: 0.558
******** [step = 150] loss: 2.143, acc: 0.556
******** [step = 200] loss: 2.141, acc: 0.557
******** [step = 250] loss: 2.147, acc: 0.556
******** [step = 300] loss: 2.150, acc: 0.555
******** [step = 350] loss: 2.156, acc: 0.555
******** [step = 400] loss: 2.159, acc: 0.555
******** [step = 450] loss: 2.161, acc: 0.554
******** [step = 500] loss: 2.166, acc: 0.554
******** [step = 550] loss: 2.169, acc: 0.553
******** [step = 600] loss: 2.173, acc: 0.553
******** [step = 650] loss: 2.177, acc: 0.553
******** [step = 700] loss: 2.181, acc: 0.553
******** [step = 750] loss: 2.185, acc: 0.552
******** [step = 800] loss: 2.186, acc: 0.552
******** [step = 850] loss: 2.188, acc: 0.552
EPOCH = 22 loss: 2.188, acc: 0.552, val_loss: 2.445, val_acc: 0.547

================================================================================2025-08_10 02:13:30
******** [step = 50] loss: 2.114, acc: 0.561
******** [step = 100] loss: 2.111, acc: 0.561
******** [step = 150] loss: 2.114, acc: 0.560
******** [step = 200] loss: 2.121, acc: 0.559
******** [step = 250] loss: 2.126, acc: 0.559
******** [step = 300] loss: 2.128, acc: 0.558
******** [step = 350] loss: 2.134, acc: 0.558
******** [step = 400] loss: 2.139, acc: 0.558
******** [step = 450] loss: 2.143, acc: 0.557
******** [step = 500] loss: 2.148, acc: 0.557
******** [step = 550] loss: 2.153, acc: 0.556
******** [step = 600] loss: 2.158, acc: 0.556
******** [step = 650] loss: 2.160, acc: 0.555
******** [step = 700] loss: 2.163, acc: 0.555
******** [step = 750] loss: 2.165, acc: 0.555
******** [step = 800] loss: 2.166, acc: 0.555
******** [step = 850] loss: 2.170, acc: 0.555
EPOCH = 23 loss: 2.170, acc: 0.555, val_loss: 2.441, val_acc: 0.549

================================================================================2025-08_10 02:14:50
******** [step = 50] loss: 2.096, acc: 0.561
******** [step = 100] loss: 2.096, acc: 0.563
******** [step = 150] loss: 2.095, acc: 0.563
******** [step = 200] loss: 2.102, acc: 0.562
******** [step = 250] loss: 2.107, acc: 0.562
******** [step = 300] loss: 2.112, acc: 0.561
******** [step = 350] loss: 2.118, acc: 0.561
******** [step = 400] loss: 2.121, acc: 0.560
******** [step = 450] loss: 2.124, acc: 0.560
******** [step = 500] loss: 2.128, acc: 0.560
******** [step = 550] loss: 2.131, acc: 0.559
******** [step = 600] loss: 2.136, acc: 0.559
******** [step = 650] loss: 2.140, acc: 0.559
******** [step = 700] loss: 2.145, acc: 0.558
******** [step = 750] loss: 2.148, acc: 0.558
******** [step = 800] loss: 2.149, acc: 0.558
******** [step = 850] loss: 2.151, acc: 0.558
EPOCH = 24 loss: 2.151, acc: 0.558, val_loss: 2.428, val_acc: 0.550

================================================================================2025-08_10 02:16:09
******** [step = 50] loss: 2.068, acc: 0.567
******** [step = 100] loss: 2.070, acc: 0.566
******** [step = 150] loss: 2.073, acc: 0.566
******** [step = 200] loss: 2.080, acc: 0.566
******** [step = 250] loss: 2.085, acc: 0.566
******** [step = 300] loss: 2.088, acc: 0.565
******** [step = 350] loss: 2.096, acc: 0.564
******** [step = 400] loss: 2.102, acc: 0.564
******** [step = 450] loss: 2.106, acc: 0.563
******** [step = 500] loss: 2.109, acc: 0.563
******** [step = 550] loss: 2.111, acc: 0.563
******** [step = 600] loss: 2.118, acc: 0.562
******** [step = 650] loss: 2.121, acc: 0.562
******** [step = 700] loss: 2.125, acc: 0.562
******** [step = 750] loss: 2.128, acc: 0.562
******** [step = 800] loss: 2.131, acc: 0.561
******** [step = 850] loss: 2.135, acc: 0.561
EPOCH = 25 loss: 2.135, acc: 0.561, val_loss: 2.424, val_acc: 0.552

================================================================================2025-08_10 02:17:29
******** [step = 50] loss: 2.045, acc: 0.570
******** [step = 100] loss: 2.037, acc: 0.571
******** [step = 150] loss: 2.054, acc: 0.569
******** [step = 200] loss: 2.059, acc: 0.568
******** [step = 250] loss: 2.064, acc: 0.568
******** [step = 300] loss: 2.068, acc: 0.568
******** [step = 350] loss: 2.075, acc: 0.567
******** [step = 400] loss: 2.078, acc: 0.567
******** [step = 450] loss: 2.083, acc: 0.567
******** [step = 500] loss: 2.088, acc: 0.566
******** [step = 550] loss: 2.094, acc: 0.566
******** [step = 600] loss: 2.098, acc: 0.566
******** [step = 650] loss: 2.103, acc: 0.565
******** [step = 700] loss: 2.106, acc: 0.565
******** [step = 750] loss: 2.110, acc: 0.564
******** [step = 800] loss: 2.114, acc: 0.564
******** [step = 850] loss: 2.117, acc: 0.564
EPOCH = 26 loss: 2.117, acc: 0.564, val_loss: 2.428, val_acc: 0.552

================================================================================2025-08_10 02:18:48
******** [step = 50] loss: 2.049, acc: 0.569
******** [step = 100] loss: 2.041, acc: 0.571
******** [step = 150] loss: 2.043, acc: 0.572
******** [step = 200] loss: 2.055, acc: 0.569
******** [step = 250] loss: 2.061, acc: 0.568
******** [step = 300] loss: 2.068, acc: 0.568
******** [step = 350] loss: 2.070, acc: 0.568
******** [step = 400] loss: 2.073, acc: 0.567
******** [step = 450] loss: 2.078, acc: 0.567
******** [step = 500] loss: 2.081, acc: 0.567
******** [step = 550] loss: 2.085, acc: 0.567
******** [step = 600] loss: 2.088, acc: 0.567
******** [step = 650] loss: 2.092, acc: 0.566
******** [step = 700] loss: 2.095, acc: 0.566
******** [step = 750] loss: 2.097, acc: 0.565
******** [step = 800] loss: 2.100, acc: 0.565
******** [step = 850] loss: 2.105, acc: 0.565
EPOCH = 27 loss: 2.105, acc: 0.565, val_loss: 2.412, val_acc: 0.557

================================================================================2025-08_10 02:20:07
******** [step = 50] loss: 2.009, acc: 0.572
******** [step = 100] loss: 2.009, acc: 0.575
******** [step = 150] loss: 2.022, acc: 0.574
******** [step = 200] loss: 2.032, acc: 0.574
******** [step = 250] loss: 2.041, acc: 0.572
******** [step = 300] loss: 2.044, acc: 0.572
******** [step = 350] loss: 2.049, acc: 0.571
******** [step = 400] loss: 2.052, acc: 0.571
******** [step = 450] loss: 2.057, acc: 0.571
******** [step = 500] loss: 2.062, acc: 0.570
******** [step = 550] loss: 2.067, acc: 0.570
******** [step = 600] loss: 2.070, acc: 0.569
******** [step = 650] loss: 2.074, acc: 0.569
******** [step = 700] loss: 2.077, acc: 0.569
******** [step = 750] loss: 2.081, acc: 0.568
******** [step = 800] loss: 2.085, acc: 0.568
******** [step = 850] loss: 2.092, acc: 0.567
EPOCH = 28 loss: 2.092, acc: 0.567, val_loss: 2.395, val_acc: 0.556

================================================================================2025-08_10 02:21:27
******** [step = 50] loss: 1.989, acc: 0.576
******** [step = 100] loss: 2.001, acc: 0.578
******** [step = 150] loss: 2.008, acc: 0.576
******** [step = 200] loss: 2.012, acc: 0.575
******** [step = 250] loss: 2.019, acc: 0.575
******** [step = 300] loss: 2.027, acc: 0.574
******** [step = 350] loss: 2.033, acc: 0.573
******** [step = 400] loss: 2.038, acc: 0.572
******** [step = 450] loss: 2.045, acc: 0.572
******** [step = 500] loss: 2.050, acc: 0.571
******** [step = 550] loss: 2.053, acc: 0.571
******** [step = 600] loss: 2.056, acc: 0.571
******** [step = 650] loss: 2.061, acc: 0.570
******** [step = 700] loss: 2.065, acc: 0.570
******** [step = 750] loss: 2.069, acc: 0.570
******** [step = 800] loss: 2.073, acc: 0.570
******** [step = 850] loss: 2.076, acc: 0.569
EPOCH = 29 loss: 2.076, acc: 0.569, val_loss: 2.434, val_acc: 0.553

================================================================================2025-08_10 02:22:46
******** [step = 50] loss: 1.986, acc: 0.579
******** [step = 100] loss: 1.991, acc: 0.579
******** [step = 150] loss: 2.000, acc: 0.577
******** [step = 200] loss: 2.006, acc: 0.576
******** [step = 250] loss: 2.011, acc: 0.576
******** [step = 300] loss: 2.012, acc: 0.576
******** [step = 350] loss: 2.016, acc: 0.576
******** [step = 400] loss: 2.021, acc: 0.576
******** [step = 450] loss: 2.027, acc: 0.575
******** [step = 500] loss: 2.032, acc: 0.574
******** [step = 550] loss: 2.037, acc: 0.574
******** [step = 600] loss: 2.043, acc: 0.573
******** [step = 650] loss: 2.048, acc: 0.573
******** [step = 700] loss: 2.052, acc: 0.573
******** [step = 750] loss: 2.056, acc: 0.572
******** [step = 800] loss: 2.060, acc: 0.572
******** [step = 850] loss: 2.064, acc: 0.572
EPOCH = 30 loss: 2.064, acc: 0.572, val_loss: 2.400, val_acc: 0.558

================================================================================2025-08_10 02:24:05
******** [step = 50] loss: 1.981, acc: 0.580
******** [step = 100] loss: 1.976, acc: 0.580
******** [step = 150] loss: 1.985, acc: 0.579
******** [step = 200] loss: 1.994, acc: 0.578
******** [step = 250] loss: 1.998, acc: 0.578
******** [step = 300] loss: 2.006, acc: 0.577
******** [step = 350] loss: 2.010, acc: 0.577
******** [step = 400] loss: 2.016, acc: 0.576
******** [step = 450] loss: 2.020, acc: 0.576
******** [step = 500] loss: 2.025, acc: 0.575
******** [step = 550] loss: 2.029, acc: 0.575
******** [step = 600] loss: 2.033, acc: 0.575
******** [step = 650] loss: 2.037, acc: 0.574
******** [step = 700] loss: 2.041, acc: 0.574
******** [step = 750] loss: 2.045, acc: 0.574
******** [step = 800] loss: 2.048, acc: 0.573
******** [step = 850] loss: 2.052, acc: 0.573
EPOCH = 31 loss: 2.052, acc: 0.573, val_loss: 2.383, val_acc: 0.561

================================================================================2025-08_10 02:25:25
******** [step = 50] loss: 1.979, acc: 0.582
******** [step = 100] loss: 1.970, acc: 0.583
******** [step = 150] loss: 1.975, acc: 0.582
******** [step = 200] loss: 1.979, acc: 0.581
******** [step = 250] loss: 1.988, acc: 0.580
******** [step = 300] loss: 1.996, acc: 0.580
******** [step = 350] loss: 1.998, acc: 0.579
******** [step = 400] loss: 2.004, acc: 0.579
******** [step = 450] loss: 2.010, acc: 0.578
******** [step = 500] loss: 2.014, acc: 0.578
******** [step = 550] loss: 2.018, acc: 0.577
******** [step = 600] loss: 2.022, acc: 0.577
******** [step = 650] loss: 2.024, acc: 0.577
******** [step = 700] loss: 2.027, acc: 0.576
******** [step = 750] loss: 2.031, acc: 0.576
******** [step = 800] loss: 2.033, acc: 0.576
******** [step = 850] loss: 2.037, acc: 0.576
EPOCH = 32 loss: 2.037, acc: 0.576, val_loss: 2.387, val_acc: 0.561

================================================================================2025-08_10 02:26:44
******** [step = 50] loss: 1.938, acc: 0.585
******** [step = 100] loss: 1.949, acc: 0.585
******** [step = 150] loss: 1.956, acc: 0.584
******** [step = 200] loss: 1.964, acc: 0.584
******** [step = 250] loss: 1.970, acc: 0.583
******** [step = 300] loss: 1.977, acc: 0.582
******** [step = 350] loss: 1.981, acc: 0.581
******** [step = 400] loss: 1.988, acc: 0.581
******** [step = 450] loss: 1.994, acc: 0.580
******** [step = 500] loss: 1.998, acc: 0.580
******** [step = 550] loss: 2.002, acc: 0.579
******** [step = 600] loss: 2.008, acc: 0.579
******** [step = 650] loss: 2.012, acc: 0.578
******** [step = 700] loss: 2.015, acc: 0.578
******** [step = 750] loss: 2.019, acc: 0.578
******** [step = 800] loss: 2.024, acc: 0.577
******** [step = 850] loss: 2.027, acc: 0.577
EPOCH = 33 loss: 2.027, acc: 0.577, val_loss: 2.384, val_acc: 0.562

================================================================================2025-08_10 02:28:04
******** [step = 50] loss: 1.937, acc: 0.584
******** [step = 100] loss: 1.944, acc: 0.585
******** [step = 150] loss: 1.956, acc: 0.584
******** [step = 200] loss: 1.959, acc: 0.585
******** [step = 250] loss: 1.964, acc: 0.584
******** [step = 300] loss: 1.970, acc: 0.584
******** [step = 350] loss: 1.973, acc: 0.583
******** [step = 400] loss: 1.978, acc: 0.583
******** [step = 450] loss: 1.982, acc: 0.583
******** [step = 500] loss: 1.987, acc: 0.582
******** [step = 550] loss: 1.990, acc: 0.582
******** [step = 600] loss: 1.995, acc: 0.582
******** [step = 650] loss: 2.000, acc: 0.581
******** [step = 700] loss: 2.004, acc: 0.581
******** [step = 750] loss: 2.008, acc: 0.580
******** [step = 800] loss: 2.012, acc: 0.580
******** [step = 850] loss: 2.014, acc: 0.580
EPOCH = 34 loss: 2.014, acc: 0.580, val_loss: 2.366, val_acc: 0.565

================================================================================2025-08_10 02:29:24
******** [step = 50] loss: 1.933, acc: 0.587
******** [step = 100] loss: 1.934, acc: 0.587
******** [step = 150] loss: 1.944, acc: 0.586
******** [step = 200] loss: 1.951, acc: 0.586
******** [step = 250] loss: 1.955, acc: 0.585
******** [step = 300] loss: 1.961, acc: 0.585
******** [step = 350] loss: 1.967, acc: 0.584
******** [step = 400] loss: 1.973, acc: 0.583
******** [step = 450] loss: 1.977, acc: 0.583
******** [step = 500] loss: 1.980, acc: 0.583
******** [step = 550] loss: 1.983, acc: 0.582
******** [step = 600] loss: 1.986, acc: 0.582
******** [step = 650] loss: 1.992, acc: 0.582
******** [step = 700] loss: 1.995, acc: 0.581
******** [step = 750] loss: 1.997, acc: 0.581
******** [step = 800] loss: 2.002, acc: 0.581
******** [step = 850] loss: 2.006, acc: 0.581
EPOCH = 35 loss: 2.006, acc: 0.581, val_loss: 2.389, val_acc: 0.561

================================================================================2025-08_10 02:30:43
******** [step = 50] loss: 1.933, acc: 0.587
******** [step = 100] loss: 1.926, acc: 0.589
******** [step = 150] loss: 1.936, acc: 0.588
******** [step = 200] loss: 1.941, acc: 0.587
******** [step = 250] loss: 1.948, acc: 0.586
******** [step = 300] loss: 1.955, acc: 0.585
******** [step = 350] loss: 1.958, acc: 0.585
******** [step = 400] loss: 1.967, acc: 0.584
******** [step = 450] loss: 1.970, acc: 0.584
******** [step = 500] loss: 1.975, acc: 0.583
******** [step = 550] loss: 1.977, acc: 0.583
******** [step = 600] loss: 1.981, acc: 0.583
******** [step = 650] loss: 1.984, acc: 0.583
******** [step = 700] loss: 1.986, acc: 0.583
******** [step = 750] loss: 1.989, acc: 0.582
******** [step = 800] loss: 1.993, acc: 0.582
******** [step = 850] loss: 1.997, acc: 0.582
EPOCH = 36 loss: 1.997, acc: 0.582, val_loss: 2.363, val_acc: 0.566

================================================================================2025-08_10 02:32:02
******** [step = 50] loss: 1.910, acc: 0.592
******** [step = 100] loss: 1.914, acc: 0.591
******** [step = 150] loss: 1.921, acc: 0.591
******** [step = 200] loss: 1.926, acc: 0.590
******** [step = 250] loss: 1.933, acc: 0.590
******** [step = 300] loss: 1.941, acc: 0.589
******** [step = 350] loss: 1.947, acc: 0.588
******** [step = 400] loss: 1.952, acc: 0.587
******** [step = 450] loss: 1.955, acc: 0.587
******** [step = 500] loss: 1.959, acc: 0.586
******** [step = 550] loss: 1.962, acc: 0.586
******** [step = 600] loss: 1.967, acc: 0.585
******** [step = 650] loss: 1.972, acc: 0.585
******** [step = 700] loss: 1.974, acc: 0.585
******** [step = 750] loss: 1.978, acc: 0.584
******** [step = 800] loss: 1.982, acc: 0.584
******** [step = 850] loss: 1.985, acc: 0.584
EPOCH = 37 loss: 1.985, acc: 0.584, val_loss: 2.364, val_acc: 0.567

================================================================================2025-08_10 02:33:21
******** [step = 50] loss: 1.909, acc: 0.591
******** [step = 100] loss: 1.910, acc: 0.591
******** [step = 150] loss: 1.917, acc: 0.590
******** [step = 200] loss: 1.923, acc: 0.590
******** [step = 250] loss: 1.925, acc: 0.589
******** [step = 300] loss: 1.932, acc: 0.589
******** [step = 350] loss: 1.936, acc: 0.588
******** [step = 400] loss: 1.941, acc: 0.588
******** [step = 450] loss: 1.945, acc: 0.587
******** [step = 500] loss: 1.950, acc: 0.587
******** [step = 550] loss: 1.956, acc: 0.587
******** [step = 600] loss: 1.960, acc: 0.586
******** [step = 650] loss: 1.963, acc: 0.586
******** [step = 700] loss: 1.968, acc: 0.586
******** [step = 750] loss: 1.972, acc: 0.585
******** [step = 800] loss: 1.975, acc: 0.585
******** [step = 850] loss: 1.978, acc: 0.585
EPOCH = 38 loss: 1.978, acc: 0.585, val_loss: 2.363, val_acc: 0.566

================================================================================2025-08_10 02:34:41
******** [step = 50] loss: 1.899, acc: 0.592
******** [step = 100] loss: 1.892, acc: 0.593
******** [step = 150] loss: 1.901, acc: 0.593
******** [step = 200] loss: 1.907, acc: 0.593
******** [step = 250] loss: 1.919, acc: 0.591
******** [step = 300] loss: 1.925, acc: 0.591
******** [step = 350] loss: 1.929, acc: 0.590
******** [step = 400] loss: 1.935, acc: 0.590
******** [step = 450] loss: 1.940, acc: 0.589
******** [step = 500] loss: 1.943, acc: 0.589
******** [step = 550] loss: 1.946, acc: 0.589
******** [step = 600] loss: 1.952, acc: 0.588
******** [step = 650] loss: 1.956, acc: 0.588
******** [step = 700] loss: 1.959, acc: 0.588
******** [step = 750] loss: 1.964, acc: 0.587
******** [step = 800] loss: 1.967, acc: 0.587
******** [step = 850] loss: 1.970, acc: 0.586
EPOCH = 39 loss: 1.970, acc: 0.586, val_loss: 2.377, val_acc: 0.566

================================================================================2025-08_10 02:36:03
******** [step = 50] loss: 1.890, acc: 0.594
******** [step = 100] loss: 1.895, acc: 0.594
******** [step = 150] loss: 1.894, acc: 0.594
******** [step = 200] loss: 1.903, acc: 0.593
******** [step = 250] loss: 1.910, acc: 0.592
******** [step = 300] loss: 1.913, acc: 0.592
******** [step = 350] loss: 1.918, acc: 0.591
******** [step = 400] loss: 1.923, acc: 0.591
******** [step = 450] loss: 1.926, acc: 0.591
******** [step = 500] loss: 1.932, acc: 0.590
******** [step = 550] loss: 1.938, acc: 0.590
******** [step = 600] loss: 1.944, acc: 0.589
******** [step = 650] loss: 1.946, acc: 0.589
******** [step = 700] loss: 1.950, acc: 0.589
******** [step = 750] loss: 1.955, acc: 0.588
******** [step = 800] loss: 1.957, acc: 0.588
******** [step = 850] loss: 1.960, acc: 0.588
EPOCH = 40 loss: 1.960, acc: 0.588, val_loss: 2.360, val_acc: 0.567

================================================================================2025-08_10 02:37:22
******** [step = 50] loss: 1.882, acc: 0.596
******** [step = 100] loss: 1.881, acc: 0.596
******** [step = 150] loss: 1.887, acc: 0.595
******** [step = 200] loss: 1.895, acc: 0.595
******** [step = 250] loss: 1.900, acc: 0.594
******** [step = 300] loss: 1.905, acc: 0.594
******** [step = 350] loss: 1.910, acc: 0.593
******** [step = 400] loss: 1.916, acc: 0.592
******** [step = 450] loss: 1.920, acc: 0.592
******** [step = 500] loss: 1.929, acc: 0.591
******** [step = 550] loss: 1.934, acc: 0.591
******** [step = 600] loss: 1.937, acc: 0.591
******** [step = 650] loss: 1.941, acc: 0.590
******** [step = 700] loss: 1.944, acc: 0.590
******** [step = 750] loss: 1.948, acc: 0.590
******** [step = 800] loss: 1.950, acc: 0.589
******** [step = 850] loss: 1.954, acc: 0.589
EPOCH = 41 loss: 1.954, acc: 0.589, val_loss: 2.334, val_acc: 0.571

================================================================================2025-08_10 02:38:42
******** [step = 50] loss: 1.878, acc: 0.597
******** [step = 100] loss: 1.872, acc: 0.597
******** [step = 150] loss: 1.875, acc: 0.597
******** [step = 200] loss: 1.881, acc: 0.596
******** [step = 250] loss: 1.891, acc: 0.595
******** [step = 300] loss: 1.897, acc: 0.594
******** [step = 350] loss: 1.903, acc: 0.594
******** [step = 400] loss: 1.907, acc: 0.594
******** [step = 450] loss: 1.910, acc: 0.593
******** [step = 500] loss: 1.916, acc: 0.593
******** [step = 550] loss: 1.919, acc: 0.593
******** [step = 600] loss: 1.924, acc: 0.592
******** [step = 650] loss: 1.930, acc: 0.592
******** [step = 700] loss: 1.934, acc: 0.591
******** [step = 750] loss: 1.938, acc: 0.591
******** [step = 800] loss: 1.940, acc: 0.591
******** [step = 850] loss: 1.945, acc: 0.590
EPOCH = 42 loss: 1.945, acc: 0.590, val_loss: 2.331, val_acc: 0.571

================================================================================2025-08_10 02:40:01
******** [step = 50] loss: 1.868, acc: 0.596
******** [step = 100] loss: 1.857, acc: 0.600
******** [step = 150] loss: 1.867, acc: 0.600
******** [step = 200] loss: 1.872, acc: 0.599
******** [step = 250] loss: 1.879, acc: 0.598
******** [step = 300] loss: 1.884, acc: 0.598
******** [step = 350] loss: 1.893, acc: 0.597
******** [step = 400] loss: 1.898, acc: 0.596
******** [step = 450] loss: 1.901, acc: 0.596
******** [step = 500] loss: 1.907, acc: 0.595
******** [step = 550] loss: 1.911, acc: 0.594
******** [step = 600] loss: 1.917, acc: 0.594
******** [step = 650] loss: 1.921, acc: 0.593
******** [step = 700] loss: 1.927, acc: 0.593
******** [step = 750] loss: 1.931, acc: 0.592
******** [step = 800] loss: 1.934, acc: 0.592
******** [step = 850] loss: 1.937, acc: 0.591
EPOCH = 43 loss: 1.937, acc: 0.591, val_loss: 2.328, val_acc: 0.571

================================================================================2025-08_10 02:41:20
******** [step = 50] loss: 1.858, acc: 0.598
******** [step = 100] loss: 1.865, acc: 0.599
******** [step = 150] loss: 1.863, acc: 0.600
******** [step = 200] loss: 1.873, acc: 0.599
******** [step = 250] loss: 1.877, acc: 0.598
******** [step = 300] loss: 1.884, acc: 0.597
******** [step = 350] loss: 1.888, acc: 0.597
******** [step = 400] loss: 1.892, acc: 0.597
******** [step = 450] loss: 1.895, acc: 0.597
******** [step = 500] loss: 1.899, acc: 0.596
******** [step = 550] loss: 1.903, acc: 0.596
******** [step = 600] loss: 1.909, acc: 0.595
******** [step = 650] loss: 1.913, acc: 0.595
******** [step = 700] loss: 1.917, acc: 0.594
******** [step = 750] loss: 1.921, acc: 0.594
******** [step = 800] loss: 1.925, acc: 0.594
******** [step = 850] loss: 1.930, acc: 0.593
EPOCH = 44 loss: 1.930, acc: 0.593, val_loss: 2.328, val_acc: 0.573

================================================================================2025-08_10 02:42:39
******** [step = 50] loss: 1.865, acc: 0.599
******** [step = 100] loss: 1.868, acc: 0.598
******** [step = 150] loss: 1.864, acc: 0.599
******** [step = 200] loss: 1.874, acc: 0.598
******** [step = 250] loss: 1.871, acc: 0.599
******** [step = 300] loss: 1.873, acc: 0.599
******** [step = 350] loss: 1.880, acc: 0.598
******** [step = 400] loss: 1.887, acc: 0.597
******** [step = 450] loss: 1.892, acc: 0.597
******** [step = 500] loss: 1.896, acc: 0.597
******** [step = 550] loss: 1.898, acc: 0.596
******** [step = 600] loss: 1.902, acc: 0.596
******** [step = 650] loss: 1.906, acc: 0.596
******** [step = 700] loss: 1.910, acc: 0.595
******** [step = 750] loss: 1.914, acc: 0.595
******** [step = 800] loss: 1.918, acc: 0.594
******** [step = 850] loss: 1.923, acc: 0.593
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 6778817 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T02:43:56 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 6778817.0 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T02:43:56 DUE TO TIME LIMIT ***
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 02:43:59
Job ID:        6778817
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=04:00:32,vmem=0,walltime=01:00:08,mem=34844K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-007-31-0
---------------------------------------
