---------------------------------------
Begin Slurm Prolog: Aug-10-2025 10:01:54
Job ID:    6779577
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 10:01:54 AM EDT 2025
atl1-1-02-011-33-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 10:02:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   46C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 108157.23it/s] 29%|██▊       | 38869/135842 [00:00<00:00, 161865.28it/s] 41%|████▏     | 56286/135842 [00:00<00:00, 123293.77it/s] 52%|█████▏    | 70003/135842 [00:00<00:00, 96192.00it/s]  66%|██████▋   | 90195/135842 [00:00<00:00, 122032.47it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 91253.01it/s] 92%|█████████▏| 125645/135842 [00:01<00:00, 110092.87it/s]100%|██████████| 135842/135842 [00:01<00:00, 113515.68it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 48097.66it/s] 33%|███▎      | 35898/108673 [00:00<00:00, 86869.22it/s] 50%|████▉     | 54278/108673 [00:00<00:00, 114784.38it/s] 67%|██████▋   | 72778/108673 [00:00<00:00, 135247.63it/s] 82%|████████▏ | 89391/108673 [00:01<00:00, 72861.93it/s]  99%|█████████▉| 107788/108673 [00:01<00:00, 92297.56it/s]100%|██████████| 108673/108673 [00:01<00:00, 90566.66it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17993/27169 [00:00<00:00, 179904.44it/s]100%|██████████| 27169/27169 [00:00<00:00, 181627.14it/s]
tensor([   3,  678,    7,   47,   20,   48,   11,   81,   73,   57,   11,  963,
         123,   15,    9,  678,   11,  129,   43, 1759,    4,    2,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,    47,    16,   128, 18367,    14,  1778,     7,  1371,   354,
            5,   411,    35,    12,    41,   132,   561,    22, 11501,   656,
           50,     4,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([    3,   323,    27, 10519,     8,  1884,  3123,    78,     5,   272,
           52,    27,   964,    10,  1884,  2361,   910,     4,     2,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
*************************** start training...

================================================================================2025-08_10 10:02:49
******** [step = 50] loss: 9.357, acc: 0.004
******** [step = 100] loss: 8.974, acc: 0.079
******** [step = 150] loss: 8.642, acc: 0.126
******** [step = 200] loss: 8.329, acc: 0.156
******** [step = 250] loss: 8.027, acc: 0.174
******** [step = 300] loss: 7.728, acc: 0.187
******** [step = 350] loss: 7.431, acc: 0.199
******** [step = 400] loss: 7.152, acc: 0.209
******** [step = 450] loss: 6.898, acc: 0.219
******** [step = 500] loss: 6.673, acc: 0.229
******** [step = 550] loss: 6.476, acc: 0.238
******** [step = 600] loss: 6.300, acc: 0.246
******** [step = 650] loss: 6.144, acc: 0.254
******** [step = 700] loss: 6.003, acc: 0.261
******** [step = 750] loss: 5.873, acc: 0.267
******** [step = 800] loss: 5.756, acc: 0.273
******** [step = 850] loss: 5.647, acc: 0.279
EPOCH = 1 loss: 5.647, acc: 0.279, val_loss: 3.844, val_acc: 0.378

================================================================================2025-08_10 10:04:11
******** [step = 50] loss: 3.873, acc: 0.368
******** [step = 100] loss: 3.831, acc: 0.374
******** [step = 150] loss: 3.800, acc: 0.377
******** [step = 200] loss: 3.779, acc: 0.379
******** [step = 250] loss: 3.757, acc: 0.381
******** [step = 300] loss: 3.738, acc: 0.383
******** [step = 350] loss: 3.719, acc: 0.385
******** [step = 400] loss: 3.700, acc: 0.387
******** [step = 450] loss: 3.680, acc: 0.389
******** [step = 500] loss: 3.663, acc: 0.391
******** [step = 550] loss: 3.646, acc: 0.393
******** [step = 600] loss: 3.632, acc: 0.394
******** [step = 650] loss: 3.616, acc: 0.396
******** [step = 700] loss: 3.603, acc: 0.397
******** [step = 750] loss: 3.588, acc: 0.398
******** [step = 800] loss: 3.575, acc: 0.400
******** [step = 850] loss: 3.563, acc: 0.401
EPOCH = 2 loss: 3.563, acc: 0.401, val_loss: 3.282, val_acc: 0.430

================================================================================2025-08_10 10:05:31
******** [step = 50] loss: 3.346, acc: 0.418
******** [step = 100] loss: 3.304, acc: 0.422
******** [step = 150] loss: 3.297, acc: 0.423
******** [step = 200] loss: 3.287, acc: 0.425
******** [step = 250] loss: 3.279, acc: 0.425
******** [step = 300] loss: 3.270, acc: 0.426
******** [step = 350] loss: 3.265, acc: 0.427
******** [step = 400] loss: 3.259, acc: 0.428
******** [step = 450] loss: 3.254, acc: 0.429
******** [step = 500] loss: 3.245, acc: 0.430
******** [step = 550] loss: 3.241, acc: 0.430
******** [step = 600] loss: 3.234, acc: 0.431
******** [step = 650] loss: 3.228, acc: 0.432
******** [step = 700] loss: 3.223, acc: 0.433
******** [step = 750] loss: 3.217, acc: 0.433
******** [step = 800] loss: 3.212, acc: 0.434
******** [step = 850] loss: 3.206, acc: 0.435
EPOCH = 3 loss: 3.206, acc: 0.435, val_loss: 3.066, val_acc: 0.450

================================================================================2025-08_10 10:06:50
******** [step = 50] loss: 3.046, acc: 0.448
******** [step = 100] loss: 3.045, acc: 0.450
******** [step = 150] loss: 3.048, acc: 0.450
******** [step = 200] loss: 3.048, acc: 0.450
******** [step = 250] loss: 3.040, acc: 0.451
******** [step = 300] loss: 3.037, acc: 0.451
******** [step = 350] loss: 3.034, acc: 0.452
******** [step = 400] loss: 3.031, acc: 0.452
******** [step = 450] loss: 3.027, acc: 0.452
******** [step = 500] loss: 3.021, acc: 0.453
******** [step = 550] loss: 3.018, acc: 0.453
******** [step = 600] loss: 3.017, acc: 0.454
******** [step = 650] loss: 3.016, acc: 0.454
******** [step = 700] loss: 3.013, acc: 0.454
******** [step = 750] loss: 3.011, acc: 0.454
******** [step = 800] loss: 3.006, acc: 0.455
******** [step = 850] loss: 3.008, acc: 0.455
EPOCH = 4 loss: 3.008, acc: 0.455, val_loss: 2.900, val_acc: 0.470

================================================================================2025-08_10 10:08:10
******** [step = 50] loss: 2.898, acc: 0.461
******** [step = 100] loss: 2.883, acc: 0.463
******** [step = 150] loss: 2.875, acc: 0.465
******** [step = 200] loss: 2.874, acc: 0.466
******** [step = 250] loss: 2.876, acc: 0.466
******** [step = 300] loss: 2.877, acc: 0.466
******** [step = 350] loss: 2.878, acc: 0.466
******** [step = 400] loss: 2.880, acc: 0.466
******** [step = 450] loss: 2.878, acc: 0.467
******** [step = 500] loss: 2.878, acc: 0.467
******** [step = 550] loss: 2.876, acc: 0.467
******** [step = 600] loss: 2.876, acc: 0.468
******** [step = 650] loss: 2.877, acc: 0.468
******** [step = 700] loss: 2.877, acc: 0.468
******** [step = 750] loss: 2.874, acc: 0.468
******** [step = 800] loss: 2.873, acc: 0.469
******** [step = 850] loss: 2.871, acc: 0.469
EPOCH = 5 loss: 2.871, acc: 0.469, val_loss: 2.881, val_acc: 0.474

================================================================================2025-08_10 10:09:29
******** [step = 50] loss: 2.812, acc: 0.467
******** [step = 100] loss: 2.777, acc: 0.472
******** [step = 150] loss: 2.758, acc: 0.475
******** [step = 200] loss: 2.752, acc: 0.476
******** [step = 250] loss: 2.749, acc: 0.477
******** [step = 300] loss: 2.748, acc: 0.478
******** [step = 350] loss: 2.747, acc: 0.479
******** [step = 400] loss: 2.748, acc: 0.480
******** [step = 450] loss: 2.747, acc: 0.480
******** [step = 500] loss: 2.745, acc: 0.481
******** [step = 550] loss: 2.744, acc: 0.481
******** [step = 600] loss: 2.744, acc: 0.482
******** [step = 650] loss: 2.743, acc: 0.482
******** [step = 700] loss: 2.741, acc: 0.483
******** [step = 750] loss: 2.741, acc: 0.483
******** [step = 800] loss: 2.740, acc: 0.484
******** [step = 850] loss: 2.738, acc: 0.484
EPOCH = 6 loss: 2.738, acc: 0.484, val_loss: 2.675, val_acc: 0.504

================================================================================2025-08_10 10:10:49
******** [step = 50] loss: 2.605, acc: 0.497
******** [step = 100] loss: 2.594, acc: 0.499
******** [step = 150] loss: 2.590, acc: 0.500
******** [step = 200] loss: 2.594, acc: 0.499
******** [step = 250] loss: 2.601, acc: 0.498
******** [step = 300] loss: 2.603, acc: 0.498
******** [step = 350] loss: 2.606, acc: 0.498
******** [step = 400] loss: 2.605, acc: 0.498
******** [step = 450] loss: 2.607, acc: 0.498
******** [step = 500] loss: 2.608, acc: 0.499
******** [step = 550] loss: 2.608, acc: 0.499
******** [step = 600] loss: 2.609, acc: 0.499
******** [step = 650] loss: 2.609, acc: 0.500
******** [step = 700] loss: 2.609, acc: 0.500
******** [step = 750] loss: 2.612, acc: 0.500
******** [step = 800] loss: 2.611, acc: 0.500
******** [step = 850] loss: 2.611, acc: 0.500
EPOCH = 7 loss: 2.611, acc: 0.500, val_loss: 2.586, val_acc: 0.516

================================================================================2025-08_10 10:12:08
******** [step = 50] loss: 2.526, acc: 0.506
******** [step = 100] loss: 2.493, acc: 0.510
******** [step = 150] loss: 2.489, acc: 0.511
******** [step = 200] loss: 2.491, acc: 0.511
******** [step = 250] loss: 2.497, acc: 0.511
******** [step = 300] loss: 2.501, acc: 0.511
******** [step = 350] loss: 2.504, acc: 0.511
******** [step = 400] loss: 2.509, acc: 0.511
******** [step = 450] loss: 2.509, acc: 0.512
******** [step = 500] loss: 2.509, acc: 0.512
******** [step = 550] loss: 2.510, acc: 0.512
******** [step = 600] loss: 2.511, acc: 0.512
******** [step = 650] loss: 2.513, acc: 0.512
******** [step = 700] loss: 2.515, acc: 0.512
******** [step = 750] loss: 2.515, acc: 0.512
******** [step = 800] loss: 2.515, acc: 0.512
******** [step = 850] loss: 2.517, acc: 0.512
EPOCH = 8 loss: 2.517, acc: 0.512, val_loss: 2.509, val_acc: 0.529

================================================================================2025-08_10 10:13:27
******** [step = 50] loss: 2.406, acc: 0.522
******** [step = 100] loss: 2.395, acc: 0.525
******** [step = 150] loss: 2.400, acc: 0.524
******** [step = 200] loss: 2.402, acc: 0.525
******** [step = 250] loss: 2.403, acc: 0.525
******** [step = 300] loss: 2.409, acc: 0.524
******** [step = 350] loss: 2.410, acc: 0.525
******** [step = 400] loss: 2.414, acc: 0.524
******** [step = 450] loss: 2.417, acc: 0.524
******** [step = 500] loss: 2.420, acc: 0.524
******** [step = 550] loss: 2.419, acc: 0.524
******** [step = 600] loss: 2.421, acc: 0.524
******** [step = 650] loss: 2.423, acc: 0.524
******** [step = 700] loss: 2.425, acc: 0.524
******** [step = 750] loss: 2.430, acc: 0.523
******** [step = 800] loss: 2.430, acc: 0.524
******** [step = 850] loss: 2.432, acc: 0.524
EPOCH = 9 loss: 2.432, acc: 0.524, val_loss: 2.472, val_acc: 0.536

================================================================================2025-08_10 10:14:47
******** [step = 50] loss: 2.363, acc: 0.529
******** [step = 100] loss: 2.345, acc: 0.532
******** [step = 150] loss: 2.341, acc: 0.533
******** [step = 200] loss: 2.337, acc: 0.534
******** [step = 250] loss: 2.336, acc: 0.534
******** [step = 300] loss: 2.336, acc: 0.534
******** [step = 350] loss: 2.344, acc: 0.533
******** [step = 400] loss: 2.350, acc: 0.533
******** [step = 450] loss: 2.351, acc: 0.533
******** [step = 500] loss: 2.354, acc: 0.533
******** [step = 550] loss: 2.357, acc: 0.532
******** [step = 600] loss: 2.359, acc: 0.532
******** [step = 650] loss: 2.363, acc: 0.532
******** [step = 700] loss: 2.364, acc: 0.532
******** [step = 750] loss: 2.366, acc: 0.532
******** [step = 800] loss: 2.368, acc: 0.532
******** [step = 850] loss: 2.369, acc: 0.532
EPOCH = 10 loss: 2.369, acc: 0.532, val_loss: 2.456, val_acc: 0.539

================================================================================2025-08_10 10:16:06
******** [step = 50] loss: 2.272, acc: 0.538
******** [step = 100] loss: 2.272, acc: 0.539
******** [step = 150] loss: 2.273, acc: 0.540
******** [step = 200] loss: 2.277, acc: 0.540
******** [step = 250] loss: 2.282, acc: 0.540
******** [step = 300] loss: 2.284, acc: 0.540
******** [step = 350] loss: 2.288, acc: 0.540
******** [step = 400] loss: 2.294, acc: 0.540
******** [step = 450] loss: 2.295, acc: 0.540
******** [step = 500] loss: 2.296, acc: 0.541
******** [step = 550] loss: 2.298, acc: 0.540
******** [step = 600] loss: 2.302, acc: 0.540
******** [step = 650] loss: 2.304, acc: 0.540
******** [step = 700] loss: 2.306, acc: 0.540
******** [step = 750] loss: 2.308, acc: 0.540
******** [step = 800] loss: 2.310, acc: 0.540
******** [step = 850] loss: 2.312, acc: 0.540
EPOCH = 11 loss: 2.312, acc: 0.540, val_loss: 2.408, val_acc: 0.546

================================================================================2025-08_10 10:17:26
******** [step = 50] loss: 2.233, acc: 0.545
******** [step = 100] loss: 2.224, acc: 0.549
******** [step = 150] loss: 2.227, acc: 0.549
******** [step = 200] loss: 2.233, acc: 0.548
******** [step = 250] loss: 2.234, acc: 0.548
******** [step = 300] loss: 2.236, acc: 0.549
******** [step = 350] loss: 2.239, acc: 0.549
******** [step = 400] loss: 2.241, acc: 0.549
******** [step = 450] loss: 2.246, acc: 0.548
******** [step = 500] loss: 2.248, acc: 0.548
******** [step = 550] loss: 2.251, acc: 0.548
******** [step = 600] loss: 2.254, acc: 0.548
******** [step = 650] loss: 2.258, acc: 0.548
******** [step = 700] loss: 2.260, acc: 0.547
******** [step = 750] loss: 2.263, acc: 0.547
******** [step = 800] loss: 2.263, acc: 0.547
******** [step = 850] loss: 2.266, acc: 0.547
EPOCH = 12 loss: 2.266, acc: 0.547, val_loss: 2.376, val_acc: 0.556

================================================================================2025-08_10 10:18:45
******** [step = 50] loss: 2.192, acc: 0.553
******** [step = 100] loss: 2.177, acc: 0.554
******** [step = 150] loss: 2.168, acc: 0.556
******** [step = 200] loss: 2.175, acc: 0.556
******** [step = 250] loss: 2.181, acc: 0.556
******** [step = 300] loss: 2.186, acc: 0.556
******** [step = 350] loss: 2.191, acc: 0.556
******** [step = 400] loss: 2.194, acc: 0.556
******** [step = 450] loss: 2.195, acc: 0.556
******** [step = 500] loss: 2.201, acc: 0.555
******** [step = 550] loss: 2.203, acc: 0.555
******** [step = 600] loss: 2.208, acc: 0.555
******** [step = 650] loss: 2.209, acc: 0.555
******** [step = 700] loss: 2.212, acc: 0.555
******** [step = 750] loss: 2.215, acc: 0.554
******** [step = 800] loss: 2.217, acc: 0.555
******** [step = 850] loss: 2.220, acc: 0.554
EPOCH = 13 loss: 2.220, acc: 0.554, val_loss: 2.364, val_acc: 0.559

================================================================================2025-08_10 10:20:04
******** [step = 50] loss: 2.166, acc: 0.556
******** [step = 100] loss: 2.145, acc: 0.560
******** [step = 150] loss: 2.140, acc: 0.561
******** [step = 200] loss: 2.143, acc: 0.561
******** [step = 250] loss: 2.145, acc: 0.561
******** [step = 300] loss: 2.146, acc: 0.562
******** [step = 350] loss: 2.151, acc: 0.562
******** [step = 400] loss: 2.153, acc: 0.562
******** [step = 450] loss: 2.156, acc: 0.562
******** [step = 500] loss: 2.162, acc: 0.562
******** [step = 550] loss: 2.167, acc: 0.561
******** [step = 600] loss: 2.170, acc: 0.561
******** [step = 650] loss: 2.174, acc: 0.561
******** [step = 700] loss: 2.178, acc: 0.560
******** [step = 750] loss: 2.180, acc: 0.560
******** [step = 800] loss: 2.181, acc: 0.560
******** [step = 850] loss: 2.185, acc: 0.560
EPOCH = 14 loss: 2.185, acc: 0.560, val_loss: 2.343, val_acc: 0.562

================================================================================2025-08_10 10:21:24
******** [step = 50] loss: 2.089, acc: 0.568
******** [step = 100] loss: 2.082, acc: 0.570
******** [step = 150] loss: 2.086, acc: 0.570
******** [step = 200] loss: 2.096, acc: 0.569
******** [step = 250] loss: 2.106, acc: 0.569
******** [step = 300] loss: 2.109, acc: 0.568
******** [step = 350] loss: 2.113, acc: 0.568
******** [step = 400] loss: 2.119, acc: 0.567
******** [step = 450] loss: 2.125, acc: 0.567
******** [step = 500] loss: 2.131, acc: 0.566
******** [step = 550] loss: 2.134, acc: 0.566
******** [step = 600] loss: 2.135, acc: 0.567
******** [step = 650] loss: 2.138, acc: 0.566
******** [step = 700] loss: 2.140, acc: 0.566
******** [step = 750] loss: 2.142, acc: 0.566
******** [step = 800] loss: 2.144, acc: 0.566
******** [step = 850] loss: 2.146, acc: 0.566
EPOCH = 15 loss: 2.146, acc: 0.566, val_loss: 2.327, val_acc: 0.566

================================================================================2025-08_10 10:22:43
******** [step = 50] loss: 2.055, acc: 0.574
******** [step = 100] loss: 2.059, acc: 0.575
******** [step = 150] loss: 2.067, acc: 0.574
******** [step = 200] loss: 2.074, acc: 0.573
******** [step = 250] loss: 2.078, acc: 0.573
******** [step = 300] loss: 2.084, acc: 0.572
******** [step = 350] loss: 2.091, acc: 0.572
******** [step = 400] loss: 2.092, acc: 0.572
******** [step = 450] loss: 2.095, acc: 0.571
******** [step = 500] loss: 2.096, acc: 0.571
******** [step = 550] loss: 2.099, acc: 0.571
******** [step = 600] loss: 2.103, acc: 0.570
******** [step = 650] loss: 2.107, acc: 0.570
******** [step = 700] loss: 2.109, acc: 0.570
******** [step = 750] loss: 2.112, acc: 0.570
******** [step = 800] loss: 2.114, acc: 0.570
******** [step = 850] loss: 2.117, acc: 0.570
EPOCH = 16 loss: 2.117, acc: 0.570, val_loss: 2.312, val_acc: 0.569

================================================================================2025-08_10 10:24:02
******** [step = 50] loss: 2.012, acc: 0.580
******** [step = 100] loss: 2.009, acc: 0.582
******** [step = 150] loss: 2.022, acc: 0.581
******** [step = 200] loss: 2.032, acc: 0.579
******** [step = 250] loss: 2.036, acc: 0.579
******** [step = 300] loss: 2.043, acc: 0.578
******** [step = 350] loss: 2.049, acc: 0.578
******** [step = 400] loss: 2.052, acc: 0.578
******** [step = 450] loss: 2.057, acc: 0.577
******** [step = 500] loss: 2.062, acc: 0.577
******** [step = 550] loss: 2.066, acc: 0.576
******** [step = 600] loss: 2.070, acc: 0.576
******** [step = 650] loss: 2.072, acc: 0.576
******** [step = 700] loss: 2.079, acc: 0.575
******** [step = 750] loss: 2.082, acc: 0.575
******** [step = 800] loss: 2.083, acc: 0.575
******** [step = 850] loss: 2.087, acc: 0.575
EPOCH = 17 loss: 2.087, acc: 0.575, val_loss: 2.298, val_acc: 0.572

================================================================================2025-08_10 10:25:21
******** [step = 50] loss: 2.029, acc: 0.579
******** [step = 100] loss: 2.033, acc: 0.580
******** [step = 150] loss: 2.022, acc: 0.581
******** [step = 200] loss: 2.018, acc: 0.582
******** [step = 250] loss: 2.019, acc: 0.582
******** [step = 300] loss: 2.023, acc: 0.582
******** [step = 350] loss: 2.025, acc: 0.582
******** [step = 400] loss: 2.027, acc: 0.582
******** [step = 450] loss: 2.032, acc: 0.581
******** [step = 500] loss: 2.038, acc: 0.581
******** [step = 550] loss: 2.041, acc: 0.581
******** [step = 600] loss: 2.044, acc: 0.580
******** [step = 650] loss: 2.048, acc: 0.580
******** [step = 700] loss: 2.053, acc: 0.579
******** [step = 750] loss: 2.056, acc: 0.579
******** [step = 800] loss: 2.058, acc: 0.579
******** [step = 850] loss: 2.060, acc: 0.579
EPOCH = 18 loss: 2.060, acc: 0.579, val_loss: 2.268, val_acc: 0.578

================================================================================2025-08_10 10:26:41
******** [step = 50] loss: 1.980, acc: 0.589
******** [step = 100] loss: 1.971, acc: 0.589
******** [step = 150] loss: 1.981, acc: 0.588
******** [step = 200] loss: 1.980, acc: 0.588
******** [step = 250] loss: 1.989, acc: 0.587
******** [step = 300] loss: 1.995, acc: 0.586
******** [step = 350] loss: 1.997, acc: 0.586
******** [step = 400] loss: 2.007, acc: 0.585
******** [step = 450] loss: 2.010, acc: 0.585
******** [step = 500] loss: 2.015, acc: 0.585
******** [step = 550] loss: 2.018, acc: 0.584
******** [step = 600] loss: 2.021, acc: 0.584
******** [step = 650] loss: 2.024, acc: 0.584
******** [step = 700] loss: 2.028, acc: 0.583
******** [step = 750] loss: 2.030, acc: 0.583
******** [step = 800] loss: 2.033, acc: 0.583
******** [step = 850] loss: 2.033, acc: 0.583
EPOCH = 19 loss: 2.033, acc: 0.583, val_loss: 2.256, val_acc: 0.580

================================================================================2025-08_10 10:28:00
******** [step = 50] loss: 1.958, acc: 0.589
******** [step = 100] loss: 1.952, acc: 0.591
******** [step = 150] loss: 1.954, acc: 0.591
******** [step = 200] loss: 1.958, acc: 0.590
******** [step = 250] loss: 1.964, acc: 0.590
******** [step = 300] loss: 1.967, acc: 0.590
******** [step = 350] loss: 1.972, acc: 0.589
******** [step = 400] loss: 1.978, acc: 0.589
******** [step = 450] loss: 1.982, acc: 0.588
******** [step = 500] loss: 1.985, acc: 0.588
******** [step = 550] loss: 1.987, acc: 0.588
******** [step = 600] loss: 1.993, acc: 0.588
******** [step = 650] loss: 1.999, acc: 0.587
******** [step = 700] loss: 2.003, acc: 0.587
******** [step = 750] loss: 2.004, acc: 0.587
******** [step = 800] loss: 2.008, acc: 0.586
******** [step = 850] loss: 2.011, acc: 0.586
EPOCH = 20 loss: 2.011, acc: 0.586, val_loss: 2.256, val_acc: 0.580

================================================================================2025-08_10 10:29:19
******** [step = 50] loss: 1.927, acc: 0.596
******** [step = 100] loss: 1.922, acc: 0.596
******** [step = 150] loss: 1.926, acc: 0.595
******** [step = 200] loss: 1.930, acc: 0.594
******** [step = 250] loss: 1.939, acc: 0.593
******** [step = 300] loss: 1.945, acc: 0.593
******** [step = 350] loss: 1.950, acc: 0.593
******** [step = 400] loss: 1.953, acc: 0.593
******** [step = 450] loss: 1.958, acc: 0.592
******** [step = 500] loss: 1.965, acc: 0.592
******** [step = 550] loss: 1.971, acc: 0.591
******** [step = 600] loss: 1.975, acc: 0.591
******** [step = 650] loss: 1.976, acc: 0.591
******** [step = 700] loss: 1.980, acc: 0.590
******** [step = 750] loss: 1.981, acc: 0.590
******** [step = 800] loss: 1.984, acc: 0.590
******** [step = 850] loss: 1.986, acc: 0.590
EPOCH = 21 loss: 1.986, acc: 0.590, val_loss: 2.245, val_acc: 0.584

================================================================================2025-08_10 10:30:38
******** [step = 50] loss: 1.913, acc: 0.598
******** [step = 100] loss: 1.915, acc: 0.598
******** [step = 150] loss: 1.920, acc: 0.598
******** [step = 200] loss: 1.919, acc: 0.598
******** [step = 250] loss: 1.925, acc: 0.597
******** [step = 300] loss: 1.928, acc: 0.597
******** [step = 350] loss: 1.930, acc: 0.596
******** [step = 400] loss: 1.934, acc: 0.596
******** [step = 450] loss: 1.939, acc: 0.596
******** [step = 500] loss: 1.943, acc: 0.595
******** [step = 550] loss: 1.948, acc: 0.595
******** [step = 600] loss: 1.952, acc: 0.595
******** [step = 650] loss: 1.955, acc: 0.594
******** [step = 700] loss: 1.957, acc: 0.594
******** [step = 750] loss: 1.960, acc: 0.594
******** [step = 800] loss: 1.964, acc: 0.594
******** [step = 850] loss: 1.966, acc: 0.594
EPOCH = 22 loss: 1.966, acc: 0.594, val_loss: 2.229, val_acc: 0.586

================================================================================2025-08_10 10:31:58
******** [step = 50] loss: 1.898, acc: 0.602
******** [step = 100] loss: 1.891, acc: 0.602
******** [step = 150] loss: 1.901, acc: 0.600
******** [step = 200] loss: 1.913, acc: 0.598
******** [step = 250] loss: 1.917, acc: 0.598
******** [step = 300] loss: 1.921, acc: 0.597
******** [step = 350] loss: 1.923, acc: 0.597
******** [step = 400] loss: 1.925, acc: 0.597
******** [step = 450] loss: 1.928, acc: 0.597
******** [step = 500] loss: 1.933, acc: 0.597
******** [step = 550] loss: 1.936, acc: 0.596
******** [step = 600] loss: 1.938, acc: 0.596
******** [step = 650] loss: 1.939, acc: 0.596
******** [step = 700] loss: 1.942, acc: 0.596
******** [step = 750] loss: 1.944, acc: 0.596
******** [step = 800] loss: 1.947, acc: 0.596
******** [step = 850] loss: 1.948, acc: 0.596
EPOCH = 23 loss: 1.948, acc: 0.596, val_loss: 2.218, val_acc: 0.589

================================================================================2025-08_10 10:33:17
******** [step = 50] loss: 1.873, acc: 0.601
******** [step = 100] loss: 1.873, acc: 0.602
******** [step = 150] loss: 1.868, acc: 0.604
******** [step = 200] loss: 1.873, acc: 0.604
******** [step = 250] loss: 1.880, acc: 0.604
******** [step = 300] loss: 1.888, acc: 0.603
******** [step = 350] loss: 1.894, acc: 0.602
******** [step = 400] loss: 1.897, acc: 0.602
******** [step = 450] loss: 1.901, acc: 0.601
******** [step = 500] loss: 1.908, acc: 0.601
******** [step = 550] loss: 1.911, acc: 0.600
******** [step = 600] loss: 1.916, acc: 0.600
******** [step = 650] loss: 1.919, acc: 0.600
******** [step = 700] loss: 1.923, acc: 0.599
******** [step = 750] loss: 1.926, acc: 0.599
******** [step = 800] loss: 1.929, acc: 0.599
******** [step = 850] loss: 1.931, acc: 0.599
EPOCH = 24 loss: 1.931, acc: 0.599, val_loss: 2.209, val_acc: 0.591

================================================================================2025-08_10 10:34:37
******** [step = 50] loss: 1.855, acc: 0.605
******** [step = 100] loss: 1.855, acc: 0.606
******** [step = 150] loss: 1.867, acc: 0.604
******** [step = 200] loss: 1.866, acc: 0.605
******** [step = 250] loss: 1.878, acc: 0.604
******** [step = 300] loss: 1.880, acc: 0.604
******** [step = 350] loss: 1.886, acc: 0.603
******** [step = 400] loss: 1.890, acc: 0.603
******** [step = 450] loss: 1.891, acc: 0.603
******** [step = 500] loss: 1.893, acc: 0.603
******** [step = 550] loss: 1.898, acc: 0.602
******** [step = 600] loss: 1.901, acc: 0.602
******** [step = 650] loss: 1.904, acc: 0.602
******** [step = 700] loss: 1.908, acc: 0.602
******** [step = 750] loss: 1.910, acc: 0.602
******** [step = 800] loss: 1.913, acc: 0.601
******** [step = 850] loss: 1.914, acc: 0.601
EPOCH = 25 loss: 1.914, acc: 0.601, val_loss: 2.205, val_acc: 0.591

================================================================================2025-08_10 10:35:56
******** [step = 50] loss: 1.839, acc: 0.610
******** [step = 100] loss: 1.840, acc: 0.609
******** [step = 150] loss: 1.844, acc: 0.609
******** [step = 200] loss: 1.853, acc: 0.608
******** [step = 250] loss: 1.851, acc: 0.608
******** [step = 300] loss: 1.851, acc: 0.608
******** [step = 350] loss: 1.857, acc: 0.608
******** [step = 400] loss: 1.864, acc: 0.607
******** [step = 450] loss: 1.870, acc: 0.607
******** [step = 500] loss: 1.876, acc: 0.606
******** [step = 550] loss: 1.879, acc: 0.606
******** [step = 600] loss: 1.881, acc: 0.606
******** [step = 650] loss: 1.887, acc: 0.605
******** [step = 700] loss: 1.890, acc: 0.605
******** [step = 750] loss: 1.894, acc: 0.604
******** [step = 800] loss: 1.897, acc: 0.604
******** [step = 850] loss: 1.898, acc: 0.604
EPOCH = 26 loss: 1.898, acc: 0.604, val_loss: 2.199, val_acc: 0.594

================================================================================2025-08_10 10:37:15
******** [step = 50] loss: 1.826, acc: 0.611
******** [step = 100] loss: 1.821, acc: 0.613
******** [step = 150] loss: 1.819, acc: 0.613
******** [step = 200] loss: 1.820, acc: 0.613
******** [step = 250] loss: 1.828, acc: 0.612
******** [step = 300] loss: 1.834, acc: 0.612
******** [step = 350] loss: 1.840, acc: 0.611
******** [step = 400] loss: 1.845, acc: 0.611
******** [step = 450] loss: 1.850, acc: 0.610
******** [step = 500] loss: 1.854, acc: 0.610
******** [step = 550] loss: 1.857, acc: 0.610
******** [step = 600] loss: 1.863, acc: 0.609
******** [step = 650] loss: 1.867, acc: 0.608
******** [step = 700] loss: 1.872, acc: 0.608
******** [step = 750] loss: 1.876, acc: 0.607
******** [step = 800] loss: 1.879, acc: 0.607
******** [step = 850] loss: 1.880, acc: 0.607
EPOCH = 27 loss: 1.880, acc: 0.607, val_loss: 2.189, val_acc: 0.598

================================================================================2025-08_10 10:38:35
******** [step = 50] loss: 1.802, acc: 0.615
******** [step = 100] loss: 1.798, acc: 0.617
******** [step = 150] loss: 1.804, acc: 0.616
******** [step = 200] loss: 1.810, acc: 0.615
******** [step = 250] loss: 1.814, acc: 0.614
******** [step = 300] loss: 1.822, acc: 0.613
******** [step = 350] loss: 1.825, acc: 0.613
******** [step = 400] loss: 1.831, acc: 0.612
******** [step = 450] loss: 1.835, acc: 0.612
******** [step = 500] loss: 1.839, acc: 0.612
******** [step = 550] loss: 1.847, acc: 0.611
******** [step = 600] loss: 1.853, acc: 0.610
******** [step = 650] loss: 1.856, acc: 0.609
******** [step = 700] loss: 1.860, acc: 0.609
******** [step = 750] loss: 1.863, acc: 0.609
******** [step = 800] loss: 1.868, acc: 0.608
******** [step = 850] loss: 1.870, acc: 0.608
EPOCH = 28 loss: 1.870, acc: 0.608, val_loss: 2.185, val_acc: 0.597

================================================================================2025-08_10 10:39:54
******** [step = 50] loss: 1.775, acc: 0.617
******** [step = 100] loss: 1.775, acc: 0.619
******** [step = 150] loss: 1.785, acc: 0.618
******** [step = 200] loss: 1.790, acc: 0.617
******** [step = 250] loss: 1.801, acc: 0.616
******** [step = 300] loss: 1.809, acc: 0.615
******** [step = 350] loss: 1.813, acc: 0.615
******** [step = 400] loss: 1.817, acc: 0.615
******** [step = 450] loss: 1.822, acc: 0.614
******** [step = 500] loss: 1.827, acc: 0.613
******** [step = 550] loss: 1.832, acc: 0.613
******** [step = 600] loss: 1.837, acc: 0.612
******** [step = 650] loss: 1.841, acc: 0.612
******** [step = 700] loss: 1.845, acc: 0.612
******** [step = 750] loss: 1.848, acc: 0.611
******** [step = 800] loss: 1.852, acc: 0.611
******** [step = 850] loss: 1.854, acc: 0.611
EPOCH = 29 loss: 1.854, acc: 0.611, val_loss: 2.182, val_acc: 0.598

================================================================================2025-08_10 10:41:13
******** [step = 50] loss: 1.764, acc: 0.621
******** [step = 100] loss: 1.769, acc: 0.620
******** [step = 150] loss: 1.777, acc: 0.619
******** [step = 200] loss: 1.784, acc: 0.618
******** [step = 250] loss: 1.793, acc: 0.617
******** [step = 300] loss: 1.798, acc: 0.616
******** [step = 350] loss: 1.803, acc: 0.616
******** [step = 400] loss: 1.807, acc: 0.616
******** [step = 450] loss: 1.815, acc: 0.615
******** [step = 500] loss: 1.819, acc: 0.614
******** [step = 550] loss: 1.821, acc: 0.614
******** [step = 600] loss: 1.825, acc: 0.614
******** [step = 650] loss: 1.828, acc: 0.613
******** [step = 700] loss: 1.831, acc: 0.613
******** [step = 750] loss: 1.834, acc: 0.613
******** [step = 800] loss: 1.839, acc: 0.613
******** [step = 850] loss: 1.843, acc: 0.613
EPOCH = 30 loss: 1.843, acc: 0.613, val_loss: 2.173, val_acc: 0.600

================================================================================2025-08_10 10:42:32
******** [step = 50] loss: 1.733, acc: 0.627
******** [step = 100] loss: 1.744, acc: 0.626
******** [step = 150] loss: 1.747, acc: 0.624
******** [step = 200] loss: 1.758, acc: 0.623
******** [step = 250] loss: 1.765, acc: 0.622
******** [step = 300] loss: 1.778, acc: 0.621
******** [step = 350] loss: 1.785, acc: 0.620
******** [step = 400] loss: 1.790, acc: 0.619
******** [step = 450] loss: 1.796, acc: 0.618
******** [step = 500] loss: 1.801, acc: 0.618
******** [step = 550] loss: 1.807, acc: 0.617
******** [step = 600] loss: 1.811, acc: 0.617
******** [step = 650] loss: 1.816, acc: 0.616
******** [step = 700] loss: 1.820, acc: 0.616
******** [step = 750] loss: 1.824, acc: 0.615
******** [step = 800] loss: 1.827, acc: 0.615
******** [step = 850] loss: 1.831, acc: 0.615
EPOCH = 31 loss: 1.831, acc: 0.615, val_loss: 2.174, val_acc: 0.599

================================================================================2025-08_10 10:43:51
******** [step = 50] loss: 1.758, acc: 0.623
******** [step = 100] loss: 1.763, acc: 0.622
******** [step = 150] loss: 1.759, acc: 0.622
******** [step = 200] loss: 1.761, acc: 0.622
******** [step = 250] loss: 1.770, acc: 0.621
******** [step = 300] loss: 1.774, acc: 0.621
******** [step = 350] loss: 1.778, acc: 0.620
******** [step = 400] loss: 1.783, acc: 0.620
******** [step = 450] loss: 1.788, acc: 0.619
******** [step = 500] loss: 1.791, acc: 0.619
******** [step = 550] loss: 1.796, acc: 0.619
******** [step = 600] loss: 1.800, acc: 0.618
******** [step = 650] loss: 1.805, acc: 0.618
******** [step = 700] loss: 1.808, acc: 0.617
******** [step = 750] loss: 1.812, acc: 0.617
******** [step = 800] loss: 1.817, acc: 0.617
******** [step = 850] loss: 1.818, acc: 0.617
EPOCH = 32 loss: 1.818, acc: 0.617, val_loss: 2.173, val_acc: 0.596

================================================================================2025-08_10 10:45:10
******** [step = 50] loss: 1.724, acc: 0.627
******** [step = 100] loss: 1.730, acc: 0.627
******** [step = 150] loss: 1.740, acc: 0.626
******** [step = 200] loss: 1.742, acc: 0.625
******** [step = 250] loss: 1.750, acc: 0.624
******** [step = 300] loss: 1.756, acc: 0.623
******** [step = 350] loss: 1.763, acc: 0.622
******** [step = 400] loss: 1.767, acc: 0.622
******** [step = 450] loss: 1.774, acc: 0.621
******** [step = 500] loss: 1.779, acc: 0.621
******** [step = 550] loss: 1.784, acc: 0.620
******** [step = 600] loss: 1.789, acc: 0.620
******** [step = 650] loss: 1.793, acc: 0.619
******** [step = 700] loss: 1.798, acc: 0.619
******** [step = 750] loss: 1.802, acc: 0.619
******** [step = 800] loss: 1.806, acc: 0.618
******** [step = 850] loss: 1.809, acc: 0.618
EPOCH = 33 loss: 1.809, acc: 0.618, val_loss: 2.154, val_acc: 0.604

================================================================================2025-08_10 10:46:29
******** [step = 50] loss: 1.712, acc: 0.627
******** [step = 100] loss: 1.705, acc: 0.628
******** [step = 150] loss: 1.710, acc: 0.628
******** [step = 200] loss: 1.722, acc: 0.627
******** [step = 250] loss: 1.734, acc: 0.626
******** [step = 300] loss: 1.742, acc: 0.625
******** [step = 350] loss: 1.748, acc: 0.624
******** [step = 400] loss: 1.757, acc: 0.623
******** [step = 450] loss: 1.762, acc: 0.623
******** [step = 500] loss: 1.767, acc: 0.623
******** [step = 550] loss: 1.772, acc: 0.622
******** [step = 600] loss: 1.776, acc: 0.622
******** [step = 650] loss: 1.782, acc: 0.621
******** [step = 700] loss: 1.788, acc: 0.621
******** [step = 750] loss: 1.792, acc: 0.620
******** [step = 800] loss: 1.796, acc: 0.620
******** [step = 850] loss: 1.797, acc: 0.620
EPOCH = 34 loss: 1.797, acc: 0.620, val_loss: 2.163, val_acc: 0.601

================================================================================2025-08_10 10:47:49
******** [step = 50] loss: 1.719, acc: 0.631
******** [step = 100] loss: 1.711, acc: 0.630
******** [step = 150] loss: 1.722, acc: 0.629
******** [step = 200] loss: 1.730, acc: 0.627
******** [step = 250] loss: 1.735, acc: 0.627
******** [step = 300] loss: 1.745, acc: 0.625
******** [step = 350] loss: 1.752, acc: 0.624
******** [step = 400] loss: 1.757, acc: 0.624
******** [step = 450] loss: 1.758, acc: 0.624
******** [step = 500] loss: 1.764, acc: 0.623
******** [step = 550] loss: 1.768, acc: 0.623
******** [step = 600] loss: 1.770, acc: 0.623
******** [step = 650] loss: 1.774, acc: 0.622
******** [step = 700] loss: 1.777, acc: 0.622
******** [step = 750] loss: 1.781, acc: 0.622
******** [step = 800] loss: 1.785, acc: 0.622
******** [step = 850] loss: 1.788, acc: 0.621
EPOCH = 35 loss: 1.788, acc: 0.621, val_loss: 2.155, val_acc: 0.603

================================================================================2025-08_10 10:49:08
******** [step = 50] loss: 1.712, acc: 0.629
******** [step = 100] loss: 1.708, acc: 0.630
******** [step = 150] loss: 1.709, acc: 0.630
******** [step = 200] loss: 1.716, acc: 0.629
******** [step = 250] loss: 1.725, acc: 0.628
******** [step = 300] loss: 1.730, acc: 0.627
******** [step = 350] loss: 1.736, acc: 0.627
******** [step = 400] loss: 1.743, acc: 0.626
******** [step = 450] loss: 1.749, acc: 0.625
******** [step = 500] loss: 1.754, acc: 0.625
******** [step = 550] loss: 1.758, acc: 0.624
******** [step = 600] loss: 1.762, acc: 0.624
******** [step = 650] loss: 1.766, acc: 0.623
******** [step = 700] loss: 1.769, acc: 0.623
******** [step = 750] loss: 1.773, acc: 0.623
******** [step = 800] loss: 1.777, acc: 0.622
******** [step = 850] loss: 1.782, acc: 0.622
EPOCH = 36 loss: 1.782, acc: 0.622, val_loss: 2.146, val_acc: 0.605

================================================================================2025-08_10 10:50:27
******** [step = 50] loss: 1.695, acc: 0.633
******** [step = 100] loss: 1.690, acc: 0.633
******** [step = 150] loss: 1.702, acc: 0.631
******** [step = 200] loss: 1.712, acc: 0.630
******** [step = 250] loss: 1.714, acc: 0.630
******** [step = 300] loss: 1.721, acc: 0.630
******** [step = 350] loss: 1.728, acc: 0.629
******** [step = 400] loss: 1.734, acc: 0.628
******** [step = 450] loss: 1.740, acc: 0.627
******** [step = 500] loss: 1.744, acc: 0.627
******** [step = 550] loss: 1.749, acc: 0.626
******** [step = 600] loss: 1.752, acc: 0.626
******** [step = 650] loss: 1.756, acc: 0.626
******** [step = 700] loss: 1.759, acc: 0.626
******** [step = 750] loss: 1.762, acc: 0.625
******** [step = 800] loss: 1.766, acc: 0.625
******** [step = 850] loss: 1.774, acc: 0.624
EPOCH = 37 loss: 1.774, acc: 0.624, val_loss: 2.144, val_acc: 0.606

================================================================================2025-08_10 10:51:46
******** [step = 50] loss: 1.695, acc: 0.631
******** [step = 100] loss: 1.687, acc: 0.632
******** [step = 150] loss: 1.700, acc: 0.631
******** [step = 200] loss: 1.701, acc: 0.632
******** [step = 250] loss: 1.705, acc: 0.632
******** [step = 300] loss: 1.711, acc: 0.631
******** [step = 350] loss: 1.717, acc: 0.630
******** [step = 400] loss: 1.722, acc: 0.629
******** [step = 450] loss: 1.727, acc: 0.629
******** [step = 500] loss: 1.730, acc: 0.629
******** [step = 550] loss: 1.736, acc: 0.628
******** [step = 600] loss: 1.741, acc: 0.627
******** [step = 650] loss: 1.744, acc: 0.627
******** [step = 700] loss: 1.748, acc: 0.627
******** [step = 750] loss: 1.754, acc: 0.626
******** [step = 800] loss: 1.758, acc: 0.626
******** [step = 850] loss: 1.760, acc: 0.626
EPOCH = 38 loss: 1.760, acc: 0.626, val_loss: 2.156, val_acc: 0.605

================================================================================2025-08_10 10:53:05
******** [step = 50] loss: 1.701, acc: 0.630
******** [step = 100] loss: 1.687, acc: 0.633
******** [step = 150] loss: 1.693, acc: 0.632
******** [step = 200] loss: 1.696, acc: 0.632
******** [step = 250] loss: 1.702, acc: 0.632
******** [step = 300] loss: 1.706, acc: 0.631
******** [step = 350] loss: 1.710, acc: 0.631
******** [step = 400] loss: 1.716, acc: 0.630
******** [step = 450] loss: 1.718, acc: 0.630
******** [step = 500] loss: 1.723, acc: 0.630
******** [step = 550] loss: 1.729, acc: 0.629
******** [step = 600] loss: 1.733, acc: 0.629
******** [step = 650] loss: 1.737, acc: 0.628
******** [step = 700] loss: 1.743, acc: 0.628
******** [step = 750] loss: 1.747, acc: 0.627
******** [step = 800] loss: 1.751, acc: 0.627
******** [step = 850] loss: 1.756, acc: 0.626
EPOCH = 39 loss: 1.756, acc: 0.626, val_loss: 2.138, val_acc: 0.607

================================================================================2025-08_10 10:54:24
******** [step = 50] loss: 1.681, acc: 0.634
******** [step = 100] loss: 1.671, acc: 0.636
******** [step = 150] loss: 1.678, acc: 0.635
******** [step = 200] loss: 1.686, acc: 0.634
******** [step = 250] loss: 1.695, acc: 0.633
******** [step = 300] loss: 1.699, acc: 0.633
******** [step = 350] loss: 1.703, acc: 0.632
******** [step = 400] loss: 1.707, acc: 0.632
******** [step = 450] loss: 1.713, acc: 0.631
******** [step = 500] loss: 1.718, acc: 0.631
******** [step = 550] loss: 1.723, acc: 0.630
******** [step = 600] loss: 1.727, acc: 0.630
******** [step = 650] loss: 1.731, acc: 0.629
******** [step = 700] loss: 1.734, acc: 0.629
******** [step = 750] loss: 1.737, acc: 0.629
******** [step = 800] loss: 1.742, acc: 0.628
******** [step = 850] loss: 1.746, acc: 0.628
EPOCH = 40 loss: 1.746, acc: 0.628, val_loss: 2.136, val_acc: 0.607

================================================================================2025-08_10 10:55:46
******** [step = 50] loss: 1.667, acc: 0.636
******** [step = 100] loss: 1.674, acc: 0.635
******** [step = 150] loss: 1.676, acc: 0.636
******** [step = 200] loss: 1.678, acc: 0.636
******** [step = 250] loss: 1.685, acc: 0.634
******** [step = 300] loss: 1.696, acc: 0.633
******** [step = 350] loss: 1.700, acc: 0.633
******** [step = 400] loss: 1.702, acc: 0.633
******** [step = 450] loss: 1.706, acc: 0.632
******** [step = 500] loss: 1.711, acc: 0.632
******** [step = 550] loss: 1.715, acc: 0.631
******** [step = 600] loss: 1.720, acc: 0.631
******** [step = 650] loss: 1.724, acc: 0.631
******** [step = 700] loss: 1.728, acc: 0.630
******** [step = 750] loss: 1.732, acc: 0.630
******** [step = 800] loss: 1.734, acc: 0.630
******** [step = 850] loss: 1.737, acc: 0.629
EPOCH = 41 loss: 1.737, acc: 0.629, val_loss: 2.132, val_acc: 0.609

================================================================================2025-08_10 10:57:05
******** [step = 50] loss: 1.650, acc: 0.639
******** [step = 100] loss: 1.666, acc: 0.637
******** [step = 150] loss: 1.668, acc: 0.637
******** [step = 200] loss: 1.672, acc: 0.636
******** [step = 250] loss: 1.676, acc: 0.636
******** [step = 300] loss: 1.683, acc: 0.635
******** [step = 350] loss: 1.686, acc: 0.635
******** [step = 400] loss: 1.691, acc: 0.635
******** [step = 450] loss: 1.696, acc: 0.634
******** [step = 500] loss: 1.699, acc: 0.634
******** [step = 550] loss: 1.703, acc: 0.634
******** [step = 600] loss: 1.710, acc: 0.633
******** [step = 650] loss: 1.713, acc: 0.633
******** [step = 700] loss: 1.716, acc: 0.632
******** [step = 750] loss: 1.721, acc: 0.632
******** [step = 800] loss: 1.725, acc: 0.631
******** [step = 850] loss: 1.729, acc: 0.631
EPOCH = 42 loss: 1.729, acc: 0.631, val_loss: 2.128, val_acc: 0.609

================================================================================2025-08_10 10:58:24
******** [step = 50] loss: 1.631, acc: 0.642
******** [step = 100] loss: 1.641, acc: 0.641
******** [step = 150] loss: 1.651, acc: 0.640
******** [step = 200] loss: 1.665, acc: 0.639
******** [step = 250] loss: 1.672, acc: 0.637
******** [step = 300] loss: 1.672, acc: 0.637
******** [step = 350] loss: 1.677, acc: 0.637
******** [step = 400] loss: 1.680, acc: 0.637
******** [step = 450] loss: 1.685, acc: 0.636
******** [step = 500] loss: 1.690, acc: 0.635
******** [step = 550] loss: 1.698, acc: 0.635
******** [step = 600] loss: 1.705, acc: 0.634
******** [step = 650] loss: 1.708, acc: 0.633
******** [step = 700] loss: 1.711, acc: 0.633
******** [step = 750] loss: 1.714, acc: 0.633
******** [step = 800] loss: 1.717, acc: 0.633
******** [step = 850] loss: 1.722, acc: 0.632
EPOCH = 43 loss: 1.722, acc: 0.632, val_loss: 2.126, val_acc: 0.611

================================================================================2025-08_10 10:59:43
******** [step = 50] loss: 1.639, acc: 0.640
******** [step = 100] loss: 1.641, acc: 0.641
******** [step = 150] loss: 1.648, acc: 0.640
******** [step = 200] loss: 1.653, acc: 0.640
******** [step = 250] loss: 1.654, acc: 0.640
******** [step = 300] loss: 1.662, acc: 0.639
******** [step = 350] loss: 1.669, acc: 0.638
******** [step = 400] loss: 1.674, acc: 0.637
******** [step = 450] loss: 1.683, acc: 0.636
******** [step = 500] loss: 1.688, acc: 0.636
******** [step = 550] loss: 1.692, acc: 0.635
******** [step = 600] loss: 1.694, acc: 0.635
******** [step = 650] loss: 1.698, acc: 0.635
******** [step = 700] loss: 1.701, acc: 0.634
******** [step = 750] loss: 1.708, acc: 0.634
******** [step = 800] loss: 1.712, acc: 0.633
******** [step = 850] loss: 1.714, acc: 0.633
EPOCH = 44 loss: 1.714, acc: 0.633, val_loss: 2.133, val_acc: 0.608

================================================================================2025-08_10 11:01:02
******** [step = 50] loss: 1.657, acc: 0.639
******** [step = 100] loss: 1.647, acc: 0.641
******** [step = 150] loss: 1.657, acc: 0.640
******** [step = 200] loss: 1.660, acc: 0.640
******** [step = 250] loss: 1.666, acc: 0.639
******** [step = 300] loss: 1.670, acc: 0.639
******** [step = 350] loss: 1.674, acc: 0.638
******** [step = 400] loss: 1.677, acc: 0.638
******** [step = 450] loss: 1.679, acc: 0.637
******** [step = 500] loss: 1.683, acc: 0.637
******** [step = 550] loss: 1.688, acc: 0.637
******** [step = 600] loss: 1.691, acc: 0.636
******** [step = 650] loss: 1.695, acc: 0.636
******** [step = 700] loss: 1.698, acc: 0.635
******** [step = 750] loss: 1.700, acc: 0.635
******** [step = 800] loss: 1.702, acc: 0.635
******** [step = 850] loss: 1.707, acc: 0.635
EPOCH = 45 loss: 1.707, acc: 0.635, val_loss: 2.120, val_acc: 0.612

================================================================================2025-08_10 11:02:21
******** [step = 50] loss: 1.644, acc: 0.639
******** [step = 100] loss: 1.637, acc: 0.640
******** [step = 150] loss: 1.636, acc: 0.641
******** [step = 200] loss: 1.640, acc: 0.641
******** [step = 250] loss: 1.650, acc: 0.641
******** [step = 300] loss: 1.655, acc: 0.640
******** [step = 350] loss: 1.659, acc: 0.639
******** [step = 400] loss: 1.663, acc: 0.639
******** [step = 450] loss: 1.666, acc: 0.639
******** [step = 500] loss: 1.670, acc: 0.639
******** [step = 550] loss: 1.675, acc: 0.638
******** [step = 600] loss: 1.682, acc: 0.637
******** [step = 650] loss: 1.686, acc: 0.637
******** [step = 700] loss: 1.692, acc: 0.636
******** [step = 750] loss: 1.698, acc: 0.636
******** [step = 800] loss: 1.701, acc: 0.635
******** [step = 850] loss: 1.703, acc: 0.635
EPOCH = 46 loss: 1.703, acc: 0.635, val_loss: 2.132, val_acc: 0.610

================================================================================2025-08_10 11:03:41
******** [step = 50] loss: 1.628, acc: 0.643
******** [step = 100] loss: 1.639, acc: 0.640
******** [step = 150] loss: 1.637, acc: 0.641
******** [step = 200] loss: 1.642, acc: 0.641
******** [step = 250] loss: 1.651, acc: 0.640
******** [step = 300] loss: 1.654, acc: 0.640
******** [step = 350] loss: 1.658, acc: 0.640
******** [step = 400] loss: 1.661, acc: 0.640
******** [step = 450] loss: 1.666, acc: 0.639
******** [step = 500] loss: 1.669, acc: 0.639
******** [step = 550] loss: 1.675, acc: 0.638
******** [step = 600] loss: 1.679, acc: 0.637
******** [step = 650] loss: 1.682, acc: 0.637
******** [step = 700] loss: 1.686, acc: 0.637
******** [step = 750] loss: 1.691, acc: 0.636
******** [step = 800] loss: 1.694, acc: 0.636
******** [step = 850] loss: 1.696, acc: 0.636
EPOCH = 47 loss: 1.696, acc: 0.636, val_loss: 2.117, val_acc: 0.611

================================================================================2025-08_10 11:05:00
******** [step = 50] loss: 1.613, acc: 0.646
******** [step = 100] loss: 1.604, acc: 0.648
******** [step = 150] loss: 1.615, acc: 0.647
******** [step = 200] loss: 1.626, acc: 0.645
******** [step = 250] loss: 1.639, acc: 0.644
******** [step = 300] loss: 1.645, acc: 0.642
******** [step = 350] loss: 1.649, acc: 0.642
******** [step = 400] loss: 1.653, acc: 0.641
******** [step = 450] loss: 1.657, acc: 0.641
******** [step = 500] loss: 1.660, acc: 0.641
******** [step = 550] loss: 1.663, acc: 0.640
******** [step = 600] loss: 1.668, acc: 0.640
******** [step = 650] loss: 1.673, acc: 0.639
******** [step = 700] loss: 1.677, acc: 0.639
******** [step = 750] loss: 1.681, acc: 0.638
******** [step = 800] loss: 1.685, acc: 0.638
******** [step = 850] loss: 1.691, acc: 0.637
EPOCH = 48 loss: 1.691, acc: 0.637, val_loss: 2.116, val_acc: 0.614

================================================================================2025-08_10 11:06:19
******** [step = 50] loss: 1.597, acc: 0.648
******** [step = 100] loss: 1.605, acc: 0.647
******** [step = 150] loss: 1.620, acc: 0.645
******** [step = 200] loss: 1.629, acc: 0.644
******** [step = 250] loss: 1.629, acc: 0.644
******** [step = 300] loss: 1.636, acc: 0.643
******** [step = 350] loss: 1.639, acc: 0.643
******** [step = 400] loss: 1.645, acc: 0.642
******** [step = 450] loss: 1.647, acc: 0.642
******** [step = 500] loss: 1.652, acc: 0.641
******** [step = 550] loss: 1.655, acc: 0.641
******** [step = 600] loss: 1.661, acc: 0.641
******** [step = 650] loss: 1.664, acc: 0.640
******** [step = 700] loss: 1.670, acc: 0.640
******** [step = 750] loss: 1.673, acc: 0.640
******** [step = 800] loss: 1.679, acc: 0.639
******** [step = 850] loss: 1.685, acc: 0.638
EPOCH = 49 loss: 1.685, acc: 0.638, val_loss: 2.114, val_acc: 0.614

================================================================================2025-08_10 11:07:38
******** [step = 50] loss: 1.602, acc: 0.647
******** [step = 100] loss: 1.602, acc: 0.647
******** [step = 150] loss: 1.614, acc: 0.646
******** [step = 200] loss: 1.623, acc: 0.645
******** [step = 250] loss: 1.627, acc: 0.645
******** [step = 300] loss: 1.636, acc: 0.643
******** [step = 350] loss: 1.643, acc: 0.643
******** [step = 400] loss: 1.646, acc: 0.642
******** [step = 450] loss: 1.649, acc: 0.642
******** [step = 500] loss: 1.653, acc: 0.642
******** [step = 550] loss: 1.659, acc: 0.641
******** [step = 600] loss: 1.663, acc: 0.641
******** [step = 650] loss: 1.665, acc: 0.641
******** [step = 700] loss: 1.668, acc: 0.640
******** [step = 750] loss: 1.671, acc: 0.640
******** [step = 800] loss: 1.674, acc: 0.640
******** [step = 850] loss: 1.676, acc: 0.640
EPOCH = 50 loss: 1.676, acc: 0.640, val_loss: 2.114, val_acc: 0.615

================================================================================2025-08_10 11:08:57
******** [step = 50] loss: 1.589, acc: 0.647
******** [step = 100] loss: 1.593, acc: 0.648
******** [step = 150] loss: 1.602, acc: 0.647
******** [step = 200] loss: 1.611, acc: 0.646
******** [step = 250] loss: 1.620, acc: 0.645
******** [step = 300] loss: 1.625, acc: 0.644
******** [step = 350] loss: 1.628, acc: 0.644
******** [step = 400] loss: 1.634, acc: 0.644
******** [step = 450] loss: 1.638, acc: 0.644
******** [step = 500] loss: 1.644, acc: 0.643
******** [step = 550] loss: 1.648, acc: 0.642
******** [step = 600] loss: 1.653, acc: 0.642
******** [step = 650] loss: 1.655, acc: 0.642
******** [step = 700] loss: 1.659, acc: 0.641
******** [step = 750] loss: 1.663, acc: 0.641
******** [step = 800] loss: 1.668, acc: 0.640
******** [step = 850] loss: 1.673, acc: 0.640
EPOCH = 51 loss: 1.673, acc: 0.640, val_loss: 2.117, val_acc: 0.615

================================================================================2025-08_10 11:10:16
******** [step = 50] loss: 1.610, acc: 0.644
******** [step = 100] loss: 1.607, acc: 0.647
******** [step = 150] loss: 1.603, acc: 0.648
******** [step = 200] loss: 1.612, acc: 0.647
******** [step = 250] loss: 1.616, acc: 0.646
******** [step = 300] loss: 1.621, acc: 0.646
******** [step = 350] loss: 1.626, acc: 0.645
******** [step = 400] loss: 1.631, acc: 0.644
******** [step = 450] loss: 1.635, acc: 0.644
******** [step = 500] loss: 1.639, acc: 0.643
******** [step = 550] loss: 1.644, acc: 0.643
******** [step = 600] loss: 1.647, acc: 0.643
******** [step = 650] loss: 1.651, acc: 0.642
******** [step = 700] loss: 1.655, acc: 0.642
******** [step = 750] loss: 1.658, acc: 0.641
******** [step = 800] loss: 1.663, acc: 0.641
******** [step = 850] loss: 1.665, acc: 0.641
EPOCH = 52 loss: 1.665, acc: 0.641, val_loss: 2.114, val_acc: 0.615

================================================================================2025-08_10 11:11:35
******** [step = 50] loss: 1.595, acc: 0.647
******** [step = 100] loss: 1.588, acc: 0.650
******** [step = 150] loss: 1.593, acc: 0.649
******** [step = 200] loss: 1.596, acc: 0.649
******** [step = 250] loss: 1.599, acc: 0.649
******** [step = 300] loss: 1.604, acc: 0.648
******** [step = 350] loss: 1.613, acc: 0.647
******** [step = 400] loss: 1.619, acc: 0.647
******** [step = 450] loss: 1.625, acc: 0.646
******** [step = 500] loss: 1.631, acc: 0.645
******** [step = 550] loss: 1.635, acc: 0.645
******** [step = 600] loss: 1.640, acc: 0.644
******** [step = 650] loss: 1.645, acc: 0.644
******** [step = 700] loss: 1.649, acc: 0.643
******** [step = 750] loss: 1.652, acc: 0.643
******** [step = 800] loss: 1.656, acc: 0.642
******** [step = 850] loss: 1.659, acc: 0.642
EPOCH = 53 loss: 1.659, acc: 0.642, val_loss: 2.108, val_acc: 0.615

================================================================================2025-08_10 11:12:54
******** [step = 50] loss: 1.580, acc: 0.649
******** [step = 100] loss: 1.582, acc: 0.651
******** [step = 150] loss: 1.591, acc: 0.650
******** [step = 200] loss: 1.594, acc: 0.650
******** [step = 250] loss: 1.601, acc: 0.649
******** [step = 300] loss: 1.609, acc: 0.648
******** [step = 350] loss: 1.613, acc: 0.647
******** [step = 400] loss: 1.619, acc: 0.646
******** [step = 450] loss: 1.624, acc: 0.646
******** [step = 500] loss: 1.629, acc: 0.645
******** [step = 550] loss: 1.633, acc: 0.645
******** [step = 600] loss: 1.638, acc: 0.644
******** [step = 650] loss: 1.642, acc: 0.644
******** [step = 700] loss: 1.644, acc: 0.644
******** [step = 750] loss: 1.649, acc: 0.643
******** [step = 800] loss: 1.654, acc: 0.643
******** [step = 850] loss: 1.657, acc: 0.643
EPOCH = 54 loss: 1.657, acc: 0.643, val_loss: 2.107, val_acc: 0.616

================================================================================2025-08_10 11:14:13
******** [step = 50] loss: 1.583, acc: 0.648
******** [step = 100] loss: 1.588, acc: 0.649
******** [step = 150] loss: 1.592, acc: 0.649
******** [step = 200] loss: 1.594, acc: 0.649
******** [step = 250] loss: 1.600, acc: 0.648
******** [step = 300] loss: 1.605, acc: 0.647
******** [step = 350] loss: 1.609, acc: 0.647
******** [step = 400] loss: 1.617, acc: 0.646
******** [step = 450] loss: 1.620, acc: 0.646
******** [step = 500] loss: 1.623, acc: 0.645
******** [step = 550] loss: 1.629, acc: 0.645
******** [step = 600] loss: 1.634, acc: 0.644
******** [step = 650] loss: 1.638, acc: 0.644
******** [step = 700] loss: 1.640, acc: 0.644
******** [step = 750] loss: 1.644, acc: 0.644
******** [step = 800] loss: 1.648, acc: 0.643
******** [step = 850] loss: 1.653, acc: 0.643
EPOCH = 55 loss: 1.653, acc: 0.643, val_loss: 2.101, val_acc: 0.618

================================================================================2025-08_10 11:15:33
******** [step = 50] loss: 1.568, acc: 0.653
******** [step = 100] loss: 1.576, acc: 0.652
******** [step = 150] loss: 1.586, acc: 0.652
******** [step = 200] loss: 1.590, acc: 0.651
******** [step = 250] loss: 1.595, acc: 0.650
******** [step = 300] loss: 1.598, acc: 0.649
******** [step = 350] loss: 1.602, acc: 0.649
******** [step = 400] loss: 1.607, acc: 0.648
******** [step = 450] loss: 1.612, acc: 0.648
******** [step = 500] loss: 1.615, acc: 0.648
******** [step = 550] loss: 1.620, acc: 0.647
******** [step = 600] loss: 1.625, acc: 0.647
******** [step = 650] loss: 1.631, acc: 0.646
******** [step = 700] loss: 1.635, acc: 0.646
******** [step = 750] loss: 1.639, acc: 0.646
******** [step = 800] loss: 1.644, acc: 0.645
******** [step = 850] loss: 1.648, acc: 0.645
EPOCH = 56 loss: 1.648, acc: 0.645, val_loss: 2.112, val_acc: 0.616

================================================================================2025-08_10 11:16:52
******** [step = 50] loss: 1.562, acc: 0.656
******** [step = 100] loss: 1.565, acc: 0.655
******** [step = 150] loss: 1.574, acc: 0.654
******** [step = 200] loss: 1.577, acc: 0.653
******** [step = 250] loss: 1.581, acc: 0.652
******** [step = 300] loss: 1.587, acc: 0.651
******** [step = 350] loss: 1.594, acc: 0.651
******** [step = 400] loss: 1.602, acc: 0.650
******** [step = 450] loss: 1.608, acc: 0.649
******** [step = 500] loss: 1.613, acc: 0.649
******** [step = 550] loss: 1.617, acc: 0.648
******** [step = 600] loss: 1.621, acc: 0.648
******** [step = 650] loss: 1.625, acc: 0.647
******** [step = 700] loss: 1.630, acc: 0.647
******** [step = 750] loss: 1.635, acc: 0.646
******** [step = 800] loss: 1.638, acc: 0.646
******** [step = 850] loss: 1.642, acc: 0.646
EPOCH = 57 loss: 1.642, acc: 0.646, val_loss: 2.104, val_acc: 0.618

================================================================================2025-08_10 11:18:11
******** [step = 50] loss: 1.540, acc: 0.656
******** [step = 100] loss: 1.549, acc: 0.656
******** [step = 150] loss: 1.562, acc: 0.655
******** [step = 200] loss: 1.562, acc: 0.654
******** [step = 250] loss: 1.572, acc: 0.653
******** [step = 300] loss: 1.580, acc: 0.652
******** [step = 350] loss: 1.591, acc: 0.651
******** [step = 400] loss: 1.597, acc: 0.650
******** [step = 450] loss: 1.603, acc: 0.649
******** [step = 500] loss: 1.609, acc: 0.649
******** [step = 550] loss: 1.616, acc: 0.648
******** [step = 600] loss: 1.621, acc: 0.647
******** [step = 650] loss: 1.624, acc: 0.647
******** [step = 700] loss: 1.628, acc: 0.647
******** [step = 750] loss: 1.631, acc: 0.647
******** [step = 800] loss: 1.635, acc: 0.646
******** [step = 850] loss: 1.640, acc: 0.646
EPOCH = 58 loss: 1.640, acc: 0.646, val_loss: 2.104, val_acc: 0.617

================================================================================2025-08_10 11:19:30
******** [step = 50] loss: 1.568, acc: 0.654
******** [step = 100] loss: 1.569, acc: 0.653
******** [step = 150] loss: 1.570, acc: 0.653
******** [step = 200] loss: 1.577, acc: 0.652
******** [step = 250] loss: 1.583, acc: 0.652
******** [step = 300] loss: 1.588, acc: 0.651
******** [step = 350] loss: 1.589, acc: 0.651
******** [step = 400] loss: 1.595, acc: 0.650
******** [step = 450] loss: 1.601, acc: 0.650
******** [step = 500] loss: 1.606, acc: 0.649
******** [step = 550] loss: 1.610, acc: 0.649
******** [step = 600] loss: 1.613, acc: 0.649
******** [step = 650] loss: 1.617, acc: 0.648
******** [step = 700] loss: 1.622, acc: 0.648
******** [step = 750] loss: 1.627, acc: 0.647
******** [step = 800] loss: 1.630, acc: 0.647
******** [step = 850] loss: 1.634, acc: 0.647
EPOCH = 59 loss: 1.634, acc: 0.647, val_loss: 2.109, val_acc: 0.616

================================================================================2025-08_10 11:20:49
******** [step = 50] loss: 1.569, acc: 0.654
******** [step = 100] loss: 1.568, acc: 0.655
******** [step = 150] loss: 1.570, acc: 0.654
******** [step = 200] loss: 1.575, acc: 0.654
******** [step = 250] loss: 1.577, acc: 0.654
******** [step = 300] loss: 1.580, acc: 0.653
******** [step = 350] loss: 1.583, acc: 0.653
******** [step = 400] loss: 1.588, acc: 0.652
******** [step = 450] loss: 1.593, acc: 0.652
******** [step = 500] loss: 1.599, acc: 0.651
******** [step = 550] loss: 1.603, acc: 0.650
******** [step = 600] loss: 1.607, acc: 0.650
******** [step = 650] loss: 1.611, acc: 0.649
******** [step = 700] loss: 1.614, acc: 0.649
******** [step = 750] loss: 1.619, acc: 0.649
******** [step = 800] loss: 1.623, acc: 0.648
******** [step = 850] loss: 1.627, acc: 0.648
EPOCH = 60 loss: 1.627, acc: 0.648, val_loss: 2.097, val_acc: 0.618

================================================================================2025-08_10 11:22:08
******** [step = 50] loss: 1.539, acc: 0.659
******** [step = 100] loss: 1.547, acc: 0.657
******** [step = 150] loss: 1.553, acc: 0.656
******** [step = 200] loss: 1.561, acc: 0.655
******** [step = 250] loss: 1.568, acc: 0.654
******** [step = 300] loss: 1.570, acc: 0.654
******** [step = 350] loss: 1.577, acc: 0.654
******** [step = 400] loss: 1.581, acc: 0.653
******** [step = 450] loss: 1.587, acc: 0.652
******** [step = 500] loss: 1.592, acc: 0.652
******** [step = 550] loss: 1.596, acc: 0.651
******** [step = 600] loss: 1.602, acc: 0.651
******** [step = 650] loss: 1.606, acc: 0.650
******** [step = 700] loss: 1.611, acc: 0.650
******** [step = 750] loss: 1.616, acc: 0.649
******** [step = 800] loss: 1.620, acc: 0.649
******** [step = 850] loss: 1.622, acc: 0.649
EPOCH = 61 loss: 1.622, acc: 0.649, val_loss: 2.101, val_acc: 0.619

================================================================================2025-08_10 11:23:27
******** [step = 50] loss: 1.539, acc: 0.660
******** [step = 100] loss: 1.546, acc: 0.658
******** [step = 150] loss: 1.553, acc: 0.656
******** [step = 200] loss: 1.560, acc: 0.655
******** [step = 250] loss: 1.566, acc: 0.654
******** [step = 300] loss: 1.572, acc: 0.654
******** [step = 350] loss: 1.576, acc: 0.653
******** [step = 400] loss: 1.582, acc: 0.652
******** [step = 450] loss: 1.586, acc: 0.652
******** [step = 500] loss: 1.591, acc: 0.652
******** [step = 550] loss: 1.595, acc: 0.651
******** [step = 600] loss: 1.599, acc: 0.651
******** [step = 650] loss: 1.604, acc: 0.650
******** [step = 700] loss: 1.608, acc: 0.650
******** [step = 750] loss: 1.613, acc: 0.649
******** [step = 800] loss: 1.616, acc: 0.649
******** [step = 850] loss: 1.619, acc: 0.649
EPOCH = 62 loss: 1.619, acc: 0.649, val_loss: 2.095, val_acc: 0.620

================================================================================2025-08_10 11:24:46
******** [step = 50] loss: 1.544, acc: 0.658
******** [step = 100] loss: 1.546, acc: 0.657
******** [step = 150] loss: 1.551, acc: 0.657
******** [step = 200] loss: 1.556, acc: 0.656
******** [step = 250] loss: 1.561, acc: 0.656
******** [step = 300] loss: 1.566, acc: 0.655
******** [step = 350] loss: 1.571, acc: 0.654
******** [step = 400] loss: 1.575, acc: 0.654
******** [step = 450] loss: 1.579, acc: 0.653
******** [step = 500] loss: 1.584, acc: 0.653
******** [step = 550] loss: 1.591, acc: 0.652
******** [step = 600] loss: 1.597, acc: 0.652
******** [step = 650] loss: 1.604, acc: 0.651
******** [step = 700] loss: 1.608, acc: 0.650
******** [step = 750] loss: 1.611, acc: 0.650
******** [step = 800] loss: 1.615, acc: 0.650
******** [step = 850] loss: 1.615, acc: 0.650
EPOCH = 63 loss: 1.615, acc: 0.650, val_loss: 2.094, val_acc: 0.620

================================================================================2025-08_10 11:26:06
******** [step = 50] loss: 1.525, acc: 0.661
******** [step = 100] loss: 1.530, acc: 0.659
******** [step = 150] loss: 1.532, acc: 0.659
******** [step = 200] loss: 1.541, acc: 0.658
******** [step = 250] loss: 1.551, acc: 0.657
******** [step = 300] loss: 1.556, acc: 0.656
******** [step = 350] loss: 1.562, acc: 0.655
******** [step = 400] loss: 1.568, acc: 0.655
******** [step = 450] loss: 1.574, acc: 0.654
******** [step = 500] loss: 1.582, acc: 0.653
******** [step = 550] loss: 1.585, acc: 0.653
******** [step = 600] loss: 1.588, acc: 0.653
******** [step = 650] loss: 1.593, acc: 0.652
******** [step = 700] loss: 1.598, acc: 0.652
******** [step = 750] loss: 1.602, acc: 0.651
******** [step = 800] loss: 1.605, acc: 0.651
******** [step = 850] loss: 1.608, acc: 0.651
EPOCH = 64 loss: 1.608, acc: 0.651, val_loss: 2.089, val_acc: 0.620

================================================================================2025-08_10 11:27:24
******** [step = 50] loss: 1.516, acc: 0.663
******** [step = 100] loss: 1.535, acc: 0.661
******** [step = 150] loss: 1.546, acc: 0.658
******** [step = 200] loss: 1.552, acc: 0.658
******** [step = 250] loss: 1.557, acc: 0.657
******** [step = 300] loss: 1.560, acc: 0.656
******** [step = 350] loss: 1.564, acc: 0.655
******** [step = 400] loss: 1.570, acc: 0.655
******** [step = 450] loss: 1.575, acc: 0.654
******** [step = 500] loss: 1.578, acc: 0.654
******** [step = 550] loss: 1.584, acc: 0.653
******** [step = 600] loss: 1.589, acc: 0.653
******** [step = 650] loss: 1.594, acc: 0.652
******** [step = 700] loss: 1.598, acc: 0.652
******** [step = 750] loss: 1.601, acc: 0.652
******** [step = 800] loss: 1.605, acc: 0.651
******** [step = 850] loss: 1.607, acc: 0.651
EPOCH = 65 loss: 1.607, acc: 0.651, val_loss: 2.091, val_acc: 0.620

================================================================================2025-08_10 11:28:44
******** [step = 50] loss: 1.524, acc: 0.662
******** [step = 100] loss: 1.518, acc: 0.663
******** [step = 150] loss: 1.527, acc: 0.661
******** [step = 200] loss: 1.536, acc: 0.659
******** [step = 250] loss: 1.542, acc: 0.658
******** [step = 300] loss: 1.548, acc: 0.657
******** [step = 350] loss: 1.555, acc: 0.657
******** [step = 400] loss: 1.558, acc: 0.656
******** [step = 450] loss: 1.566, acc: 0.655
******** [step = 500] loss: 1.569, acc: 0.655
******** [step = 550] loss: 1.574, acc: 0.655
******** [step = 600] loss: 1.580, acc: 0.654
******** [step = 650] loss: 1.585, acc: 0.653
******** [step = 700] loss: 1.589, acc: 0.653
******** [step = 750] loss: 1.595, acc: 0.653
******** [step = 800] loss: 1.599, acc: 0.652
******** [step = 850] loss: 1.603, acc: 0.652
EPOCH = 66 loss: 1.603, acc: 0.652, val_loss: 2.090, val_acc: 0.620

================================================================================2025-08_10 11:30:03
******** [step = 50] loss: 1.522, acc: 0.662
******** [step = 100] loss: 1.532, acc: 0.660
******** [step = 150] loss: 1.539, acc: 0.659
******** [step = 200] loss: 1.545, acc: 0.658
******** [step = 250] loss: 1.551, acc: 0.657
******** [step = 300] loss: 1.554, acc: 0.657
******** [step = 350] loss: 1.558, acc: 0.657
******** [step = 400] loss: 1.562, acc: 0.657
******** [step = 450] loss: 1.567, acc: 0.656
******** [step = 500] loss: 1.570, acc: 0.656
******** [step = 550] loss: 1.573, acc: 0.656
******** [step = 600] loss: 1.578, acc: 0.655
******** [step = 650] loss: 1.583, acc: 0.655
******** [step = 700] loss: 1.587, acc: 0.654
******** [step = 750] loss: 1.591, acc: 0.654
******** [step = 800] loss: 1.595, acc: 0.653
******** [step = 850] loss: 1.599, acc: 0.653
EPOCH = 67 loss: 1.599, acc: 0.653, val_loss: 2.095, val_acc: 0.620

================================================================================2025-08_10 11:31:22
******** [step = 50] loss: 1.494, acc: 0.664
******** [step = 100] loss: 1.509, acc: 0.662
******** [step = 150] loss: 1.519, acc: 0.660
******** [step = 200] loss: 1.530, acc: 0.659
******** [step = 250] loss: 1.537, acc: 0.659
******** [step = 300] loss: 1.543, acc: 0.658
******** [step = 350] loss: 1.550, acc: 0.658
******** [step = 400] loss: 1.555, acc: 0.657
******** [step = 450] loss: 1.561, acc: 0.656
******** [step = 500] loss: 1.565, acc: 0.656
******** [step = 550] loss: 1.568, acc: 0.656
******** [step = 600] loss: 1.573, acc: 0.655
******** [step = 650] loss: 1.578, acc: 0.655
******** [step = 700] loss: 1.582, acc: 0.654
******** [step = 750] loss: 1.586, acc: 0.654
******** [step = 800] loss: 1.590, acc: 0.654
******** [step = 850] loss: 1.594, acc: 0.653
EPOCH = 68 loss: 1.594, acc: 0.653, val_loss: 2.100, val_acc: 0.620

================================================================================2025-08_10 11:32:41
******** [step = 50] loss: 1.520, acc: 0.664
******** [step = 100] loss: 1.523, acc: 0.663
******** [step = 150] loss: 1.524, acc: 0.663
******** [step = 200] loss: 1.533, acc: 0.661
******** [step = 250] loss: 1.541, acc: 0.660
******** [step = 300] loss: 1.545, acc: 0.659
******** [step = 350] loss: 1.552, acc: 0.658
******** [step = 400] loss: 1.558, acc: 0.657
******** [step = 450] loss: 1.562, acc: 0.657
******** [step = 500] loss: 1.566, acc: 0.656
******** [step = 550] loss: 1.568, acc: 0.656
******** [step = 600] loss: 1.574, acc: 0.655
******** [step = 650] loss: 1.576, acc: 0.655
******** [step = 700] loss: 1.580, acc: 0.655
******** [step = 750] loss: 1.584, acc: 0.654
******** [step = 800] loss: 1.588, acc: 0.654
******** [step = 850] loss: 1.595, acc: 0.654
EPOCH = 69 loss: 1.595, acc: 0.654, val_loss: 2.087, val_acc: 0.622

================================================================================2025-08_10 11:34:00
******** [step = 50] loss: 1.510, acc: 0.662
******** [step = 100] loss: 1.530, acc: 0.660
******** [step = 150] loss: 1.532, acc: 0.660
******** [step = 200] loss: 1.542, acc: 0.659
******** [step = 250] loss: 1.544, acc: 0.658
******** [step = 300] loss: 1.547, acc: 0.658
******** [step = 350] loss: 1.550, acc: 0.658
******** [step = 400] loss: 1.554, acc: 0.658
******** [step = 450] loss: 1.560, acc: 0.657
******** [step = 500] loss: 1.564, acc: 0.657
******** [step = 550] loss: 1.566, acc: 0.657
******** [step = 600] loss: 1.571, acc: 0.656
******** [step = 650] loss: 1.575, acc: 0.655
******** [step = 700] loss: 1.578, acc: 0.655
******** [step = 750] loss: 1.582, acc: 0.655
******** [step = 800] loss: 1.585, acc: 0.654
******** [step = 850] loss: 1.588, acc: 0.654
EPOCH = 70 loss: 1.588, acc: 0.654, val_loss: 2.087, val_acc: 0.622

================================================================================2025-08_10 11:35:20
******** [step = 50] loss: 1.499, acc: 0.663
******** [step = 100] loss: 1.501, acc: 0.664
******** [step = 150] loss: 1.512, acc: 0.663
******** [step = 200] loss: 1.524, acc: 0.661
******** [step = 250] loss: 1.529, acc: 0.661
******** [step = 300] loss: 1.534, acc: 0.660
******** [step = 350] loss: 1.539, acc: 0.660
******** [step = 400] loss: 1.546, acc: 0.659
******** [step = 450] loss: 1.550, acc: 0.659
******** [step = 500] loss: 1.557, acc: 0.658
******** [step = 550] loss: 1.562, acc: 0.657
******** [step = 600] loss: 1.567, acc: 0.657
******** [step = 650] loss: 1.571, acc: 0.657
******** [step = 700] loss: 1.574, acc: 0.656
******** [step = 750] loss: 1.579, acc: 0.656
******** [step = 800] loss: 1.582, acc: 0.655
******** [step = 850] loss: 1.585, acc: 0.655
EPOCH = 71 loss: 1.585, acc: 0.655, val_loss: 2.086, val_acc: 0.621

================================================================================2025-08_10 11:36:39
******** [step = 50] loss: 1.509, acc: 0.660
******** [step = 100] loss: 1.517, acc: 0.661
******** [step = 150] loss: 1.521, acc: 0.661
******** [step = 200] loss: 1.524, acc: 0.661
******** [step = 250] loss: 1.531, acc: 0.661
******** [step = 300] loss: 1.538, acc: 0.660
******** [step = 350] loss: 1.541, acc: 0.659
******** [step = 400] loss: 1.546, acc: 0.659
******** [step = 450] loss: 1.552, acc: 0.658
******** [step = 500] loss: 1.555, acc: 0.658
******** [step = 550] loss: 1.557, acc: 0.658
******** [step = 600] loss: 1.560, acc: 0.657
******** [step = 650] loss: 1.564, acc: 0.657
******** [step = 700] loss: 1.568, acc: 0.657
******** [step = 750] loss: 1.572, acc: 0.656
******** [step = 800] loss: 1.577, acc: 0.656
******** [step = 850] loss: 1.581, acc: 0.655
EPOCH = 72 loss: 1.581, acc: 0.655, val_loss: 2.090, val_acc: 0.623

================================================================================2025-08_10 11:37:58
******** [step = 50] loss: 1.515, acc: 0.663
******** [step = 100] loss: 1.508, acc: 0.664
******** [step = 150] loss: 1.510, acc: 0.664
******** [step = 200] loss: 1.516, acc: 0.663
******** [step = 250] loss: 1.519, acc: 0.663
******** [step = 300] loss: 1.524, acc: 0.662
******** [step = 350] loss: 1.528, acc: 0.661
******** [step = 400] loss: 1.534, acc: 0.661
******** [step = 450] loss: 1.541, acc: 0.660
******** [step = 500] loss: 1.548, acc: 0.659
******** [step = 550] loss: 1.553, acc: 0.659
******** [step = 600] loss: 1.558, acc: 0.658
******** [step = 650] loss: 1.563, acc: 0.658
******** [step = 700] loss: 1.567, acc: 0.658
******** [step = 750] loss: 1.571, acc: 0.657
******** [step = 800] loss: 1.574, acc: 0.657
******** [step = 850] loss: 1.579, acc: 0.657
EPOCH = 73 loss: 1.579, acc: 0.657, val_loss: 2.085, val_acc: 0.622

================================================================================2025-08_10 11:39:17
******** [step = 50] loss: 1.507, acc: 0.663
******** [step = 100] loss: 1.499, acc: 0.665
******** [step = 150] loss: 1.512, acc: 0.664
******** [step = 200] loss: 1.512, acc: 0.664
******** [step = 250] loss: 1.518, acc: 0.663
******** [step = 300] loss: 1.526, acc: 0.663
******** [step = 350] loss: 1.529, acc: 0.663
******** [step = 400] loss: 1.534, acc: 0.662
******** [step = 450] loss: 1.541, acc: 0.661
******** [step = 500] loss: 1.545, acc: 0.660
******** [step = 550] loss: 1.550, acc: 0.659
******** [step = 600] loss: 1.555, acc: 0.658
******** [step = 650] loss: 1.559, acc: 0.658
******** [step = 700] loss: 1.562, acc: 0.658
******** [step = 750] loss: 1.567, acc: 0.657
******** [step = 800] loss: 1.570, acc: 0.657
******** [step = 850] loss: 1.574, acc: 0.656
EPOCH = 74 loss: 1.574, acc: 0.656, val_loss: 2.085, val_acc: 0.623

================================================================================2025-08_10 11:40:36
******** [step = 50] loss: 1.499, acc: 0.664
******** [step = 100] loss: 1.505, acc: 0.663
******** [step = 150] loss: 1.515, acc: 0.662
******** [step = 200] loss: 1.513, acc: 0.663
******** [step = 250] loss: 1.518, acc: 0.662
******** [step = 300] loss: 1.521, acc: 0.662
******** [step = 350] loss: 1.525, acc: 0.662
******** [step = 400] loss: 1.532, acc: 0.661
******** [step = 450] loss: 1.537, acc: 0.660
******** [step = 500] loss: 1.541, acc: 0.660
******** [step = 550] loss: 1.546, acc: 0.660
******** [step = 600] loss: 1.548, acc: 0.659
******** [step = 650] loss: 1.552, acc: 0.659
******** [step = 700] loss: 1.557, acc: 0.658
******** [step = 750] loss: 1.562, acc: 0.658
******** [step = 800] loss: 1.567, acc: 0.658
******** [step = 850] loss: 1.571, acc: 0.657
EPOCH = 75 loss: 1.571, acc: 0.657, val_loss: 2.089, val_acc: 0.623

================================================================================2025-08_10 11:41:55
******** [step = 50] loss: 1.488, acc: 0.666
******** [step = 100] loss: 1.490, acc: 0.665
******** [step = 150] loss: 1.494, acc: 0.665
******** [step = 200] loss: 1.498, acc: 0.665
******** [step = 250] loss: 1.503, acc: 0.665
******** [step = 300] loss: 1.512, acc: 0.664
******** [step = 350] loss: 1.520, acc: 0.663
******** [step = 400] loss: 1.526, acc: 0.662
******** [step = 450] loss: 1.532, acc: 0.661
******** [step = 500] loss: 1.535, acc: 0.661
******** [step = 550] loss: 1.540, acc: 0.661
******** [step = 600] loss: 1.548, acc: 0.660
******** [step = 650] loss: 1.554, acc: 0.659
******** [step = 700] loss: 1.558, acc: 0.659
******** [step = 750] loss: 1.562, acc: 0.659
******** [step = 800] loss: 1.566, acc: 0.658
******** [step = 850] loss: 1.568, acc: 0.658
EPOCH = 76 loss: 1.568, acc: 0.658, val_loss: 2.087, val_acc: 0.622

================================================================================2025-08_10 11:43:14
******** [step = 50] loss: 1.488, acc: 0.667
******** [step = 100] loss: 1.495, acc: 0.667
******** [step = 150] loss: 1.495, acc: 0.667
******** [step = 200] loss: 1.507, acc: 0.665
******** [step = 250] loss: 1.511, acc: 0.664
******** [step = 300] loss: 1.518, acc: 0.664
******** [step = 350] loss: 1.524, acc: 0.663
******** [step = 400] loss: 1.529, acc: 0.662
******** [step = 450] loss: 1.536, acc: 0.661
******** [step = 500] loss: 1.541, acc: 0.661
******** [step = 550] loss: 1.545, acc: 0.660
******** [step = 600] loss: 1.549, acc: 0.660
******** [step = 650] loss: 1.554, acc: 0.659
******** [step = 700] loss: 1.556, acc: 0.659
******** [step = 750] loss: 1.559, acc: 0.659
******** [step = 800] loss: 1.565, acc: 0.658
******** [step = 850] loss: 1.572, acc: 0.658
EPOCH = 77 loss: 1.572, acc: 0.658, val_loss: 2.089, val_acc: 0.623

================================================================================2025-08_10 11:44:33
******** [step = 50] loss: 1.494, acc: 0.667
******** [step = 100] loss: 1.495, acc: 0.666
******** [step = 150] loss: 1.504, acc: 0.665
******** [step = 200] loss: 1.504, acc: 0.666
******** [step = 250] loss: 1.512, acc: 0.665
******** [step = 300] loss: 1.514, acc: 0.665
******** [step = 350] loss: 1.518, acc: 0.664
******** [step = 400] loss: 1.522, acc: 0.664
******** [step = 450] loss: 1.529, acc: 0.663
******** [step = 500] loss: 1.534, acc: 0.662
******** [step = 550] loss: 1.537, acc: 0.662
******** [step = 600] loss: 1.543, acc: 0.661
******** [step = 650] loss: 1.548, acc: 0.661
******** [step = 700] loss: 1.552, acc: 0.660
******** [step = 750] loss: 1.556, acc: 0.660
******** [step = 800] loss: 1.559, acc: 0.660
******** [step = 850] loss: 1.564, acc: 0.659
EPOCH = 78 loss: 1.564, acc: 0.659, val_loss: 2.079, val_acc: 0.625

================================================================================2025-08_10 11:45:52
******** [step = 50] loss: 1.501, acc: 0.666
******** [step = 100] loss: 1.493, acc: 0.666
******** [step = 150] loss: 1.497, acc: 0.666
******** [step = 200] loss: 1.505, acc: 0.665
******** [step = 250] loss: 1.506, acc: 0.665
******** [step = 300] loss: 1.512, acc: 0.665
******** [step = 350] loss: 1.516, acc: 0.665
******** [step = 400] loss: 1.521, acc: 0.664
******** [step = 450] loss: 1.525, acc: 0.663
******** [step = 500] loss: 1.530, acc: 0.662
******** [step = 550] loss: 1.535, acc: 0.662
******** [step = 600] loss: 1.539, acc: 0.661
******** [step = 650] loss: 1.544, acc: 0.661
******** [step = 700] loss: 1.549, acc: 0.661
******** [step = 750] loss: 1.553, acc: 0.660
******** [step = 800] loss: 1.557, acc: 0.660
******** [step = 850] loss: 1.562, acc: 0.660
EPOCH = 79 loss: 1.562, acc: 0.660, val_loss: 2.074, val_acc: 0.626

================================================================================2025-08_10 11:47:11
******** [step = 50] loss: 1.490, acc: 0.667
******** [step = 100] loss: 1.497, acc: 0.666
******** [step = 150] loss: 1.501, acc: 0.666
******** [step = 200] loss: 1.507, acc: 0.665
******** [step = 250] loss: 1.509, acc: 0.665
******** [step = 300] loss: 1.514, acc: 0.664
******** [step = 350] loss: 1.520, acc: 0.664
******** [step = 400] loss: 1.525, acc: 0.663
******** [step = 450] loss: 1.529, acc: 0.663
******** [step = 500] loss: 1.533, acc: 0.663
******** [step = 550] loss: 1.535, acc: 0.662
******** [step = 600] loss: 1.539, acc: 0.662
******** [step = 650] loss: 1.542, acc: 0.662
******** [step = 700] loss: 1.547, acc: 0.661
******** [step = 750] loss: 1.550, acc: 0.661
******** [step = 800] loss: 1.553, acc: 0.661
******** [step = 850] loss: 1.556, acc: 0.661
EPOCH = 80 loss: 1.556, acc: 0.661, val_loss: 2.080, val_acc: 0.625

================================================================================2025-08_10 11:48:30
finishing training...
Training complete in 105m 41s
    epoch  ...   val_acc
0     1.0  ...  0.378197
1     2.0  ...  0.430209
2     3.0  ...  0.450017
3     4.0  ...  0.470132
4     5.0  ...  0.474277
..    ...  ...       ...
75   76.0  ...  0.621864
76   77.0  ...  0.623162
77   78.0  ...  0.625103
78   79.0  ...  0.625830
79   80.0  ...  0.625287

[80 rows x 5 columns]
== Done ==
Sun Aug 10 11:48:51 AM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 11:48:52
Job ID:        6779577
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=07:07:48,vmem=0,walltime=01:46:57,mem=63440K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-011-33-0
---------------------------------------
