---------------------------------------
Begin Slurm Prolog: Aug-12-2025 14:19:52
Job ID:    6967080
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Tue Aug 12 02:19:52 PM EDT 2025
atl1-1-02-009-36-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Tue Aug 12 14:20:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   36C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 105553.25it/s] 28%|██▊       | 38266/135842 [00:00<00:00, 156960.98it/s] 41%|████      | 55221/135842 [00:00<00:00, 119634.63it/s] 51%|█████     | 69378/135842 [00:00<00:00, 94380.35it/s]  66%|██████▌   | 89362/135842 [00:00<00:00, 119867.54it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90607.43it/s] 92%|█████████▏| 125362/135842 [00:01<00:00, 108736.71it/s]100%|██████████| 135842/135842 [00:01<00:00, 111736.99it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47655.70it/s] 33%|███▎      | 35458/108673 [00:00<00:00, 85058.07it/s] 49%|████▉     | 53377/108673 [00:00<00:00, 112039.28it/s] 66%|██████▌   | 71557/108673 [00:00<00:00, 132303.44it/s] 81%|████████  | 87825/108673 [00:01<00:00, 71149.14it/s]  97%|█████████▋| 105912/108673 [00:01<00:00, 90274.07it/s]100%|██████████| 108673/108673 [00:01<00:00, 89383.22it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17826/27169 [00:00<00:00, 178247.00it/s]100%|██████████| 27169/27169 [00:00<00:00, 180300.91it/s]
tensor([   3,   35,   11,   20,   48,   49,   57,  537,   13,   27,  515,  273,
        2500,   15,  459,   20,   35,   28, 1263,    7,  317,   58,  190,    8,
           2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,   16,  678,   34,   11,  116,   22, 5397, 1114,    4,   12,   21,
         307,    9,   14,  718,  123,   12, 3122,   26, 3022,    4,    2,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([    3,  2325,    12,    39,    10,  6404, 12078,     4,    13,    29,
           11,   188,     8,   382,   111,    13,  4144,     6,   678,   910,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
*************************** start training...

================================================================================2025-08_12 14:21:22
******** [step = 50] loss: 9.438, acc: 0.002
******** [step = 100] loss: 9.043, acc: 0.080
******** [step = 150] loss: 8.677, acc: 0.127
******** [step = 200] loss: 8.326, acc: 0.157
******** [step = 250] loss: 8.003, acc: 0.175
******** [step = 300] loss: 7.694, acc: 0.189
******** [step = 350] loss: 7.393, acc: 0.201
******** [step = 400] loss: 7.116, acc: 0.210
******** [step = 450] loss: 6.865, acc: 0.219
******** [step = 500] loss: 6.644, acc: 0.229
******** [step = 550] loss: 6.449, acc: 0.238
******** [step = 600] loss: 6.276, acc: 0.247
******** [step = 650] loss: 6.121, acc: 0.255
******** [step = 700] loss: 5.980, acc: 0.262
******** [step = 750] loss: 5.851, acc: 0.269
******** [step = 800] loss: 5.735, acc: 0.275
******** [step = 850] loss: 5.629, acc: 0.280
EPOCH = 1 loss: 5.629, acc: 0.280, val_loss: 3.797, val_acc: 0.384

================================================================================2025-08_12 14:23:25
******** [step = 50] loss: 3.879, acc: 0.367
******** [step = 100] loss: 3.850, acc: 0.370
******** [step = 150] loss: 3.823, acc: 0.373
******** [step = 200] loss: 3.803, acc: 0.374
******** [step = 250] loss: 3.781, acc: 0.375
******** [step = 300] loss: 3.765, acc: 0.376
******** [step = 350] loss: 3.745, acc: 0.378
******** [step = 400] loss: 3.730, acc: 0.379
******** [step = 450] loss: 3.713, acc: 0.381
******** [step = 500] loss: 3.700, acc: 0.382
******** [step = 550] loss: 3.685, acc: 0.383
******** [step = 600] loss: 3.670, acc: 0.384
******** [step = 650] loss: 3.658, acc: 0.385
******** [step = 700] loss: 3.644, acc: 0.386
******** [step = 750] loss: 3.631, acc: 0.388
******** [step = 800] loss: 3.619, acc: 0.389
******** [step = 850] loss: 3.608, acc: 0.390
EPOCH = 2 loss: 3.608, acc: 0.390, val_loss: 3.383, val_acc: 0.416

================================================================================2025-08_12 14:25:27
******** [step = 50] loss: 3.453, acc: 0.400
******** [step = 100] loss: 3.388, acc: 0.407
******** [step = 150] loss: 3.381, acc: 0.408
******** [step = 200] loss: 3.361, acc: 0.410
******** [step = 250] loss: 3.353, acc: 0.411
******** [step = 300] loss: 3.344, acc: 0.412
******** [step = 350] loss: 3.336, acc: 0.413
******** [step = 400] loss: 3.330, acc: 0.413
******** [step = 450] loss: 3.324, acc: 0.414
******** [step = 500] loss: 3.317, acc: 0.415
******** [step = 550] loss: 3.311, acc: 0.415
******** [step = 600] loss: 3.306, acc: 0.416
******** [step = 650] loss: 3.299, acc: 0.417
******** [step = 700] loss: 3.294, acc: 0.418
******** [step = 750] loss: 3.290, acc: 0.418
******** [step = 800] loss: 3.284, acc: 0.419
******** [step = 850] loss: 3.281, acc: 0.419
EPOCH = 3 loss: 3.281, acc: 0.419, val_loss: 3.152, val_acc: 0.436

================================================================================2025-08_12 14:27:28
******** [step = 50] loss: 3.132, acc: 0.428
******** [step = 100] loss: 3.106, acc: 0.433
******** [step = 150] loss: 3.107, acc: 0.434
******** [step = 200] loss: 3.104, acc: 0.435
******** [step = 250] loss: 3.105, acc: 0.435
******** [step = 300] loss: 3.106, acc: 0.435
******** [step = 350] loss: 3.103, acc: 0.435
******** [step = 400] loss: 3.101, acc: 0.436
******** [step = 450] loss: 3.099, acc: 0.436
******** [step = 500] loss: 3.098, acc: 0.437
******** [step = 550] loss: 3.097, acc: 0.437
******** [step = 600] loss: 3.096, acc: 0.437
******** [step = 650] loss: 3.093, acc: 0.438
******** [step = 700] loss: 3.091, acc: 0.438
******** [step = 750] loss: 3.089, acc: 0.438
******** [step = 800] loss: 3.086, acc: 0.439
******** [step = 850] loss: 3.084, acc: 0.439
EPOCH = 4 loss: 3.084, acc: 0.439, val_loss: 2.993, val_acc: 0.454

================================================================================2025-08_12 14:29:29
******** [step = 50] loss: 3.001, acc: 0.445
******** [step = 100] loss: 2.981, acc: 0.447
******** [step = 150] loss: 2.975, acc: 0.449
******** [step = 200] loss: 2.966, acc: 0.450
******** [step = 250] loss: 2.964, acc: 0.450
******** [step = 300] loss: 2.963, acc: 0.450
******** [step = 350] loss: 2.961, acc: 0.451
******** [step = 400] loss: 2.963, acc: 0.451
******** [step = 450] loss: 2.962, acc: 0.451
******** [step = 500] loss: 2.961, acc: 0.451
******** [step = 550] loss: 2.962, acc: 0.452
******** [step = 600] loss: 2.960, acc: 0.452
******** [step = 650] loss: 2.961, acc: 0.452
******** [step = 700] loss: 2.961, acc: 0.452
******** [step = 750] loss: 2.959, acc: 0.453
******** [step = 800] loss: 2.957, acc: 0.453
******** [step = 850] loss: 2.960, acc: 0.453
EPOCH = 5 loss: 2.960, acc: 0.453, val_loss: 3.033, val_acc: 0.451

================================================================================2025-08_12 14:31:31
******** [step = 50] loss: 2.926, acc: 0.451
******** [step = 100] loss: 2.889, acc: 0.455
******** [step = 150] loss: 2.878, acc: 0.458
******** [step = 200] loss: 2.866, acc: 0.460
******** [step = 250] loss: 2.856, acc: 0.461
******** [step = 300] loss: 2.856, acc: 0.462
******** [step = 350] loss: 2.852, acc: 0.463
******** [step = 400] loss: 2.851, acc: 0.463
******** [step = 450] loss: 2.848, acc: 0.464
******** [step = 500] loss: 2.847, acc: 0.464
******** [step = 550] loss: 2.845, acc: 0.465
******** [step = 600] loss: 2.842, acc: 0.465
******** [step = 650] loss: 2.842, acc: 0.466
******** [step = 700] loss: 2.840, acc: 0.466
******** [step = 750] loss: 2.839, acc: 0.466
******** [step = 800] loss: 2.838, acc: 0.467
******** [step = 850] loss: 2.836, acc: 0.467
EPOCH = 6 loss: 2.836, acc: 0.467, val_loss: 2.757, val_acc: 0.488

================================================================================2025-08_12 14:33:32
******** [step = 50] loss: 2.703, acc: 0.478
******** [step = 100] loss: 2.697, acc: 0.480
******** [step = 150] loss: 2.702, acc: 0.480
******** [step = 200] loss: 2.707, acc: 0.480
******** [step = 250] loss: 2.711, acc: 0.480
******** [step = 300] loss: 2.708, acc: 0.481
******** [step = 350] loss: 2.707, acc: 0.481
******** [step = 400] loss: 2.708, acc: 0.481
******** [step = 450] loss: 2.711, acc: 0.481
******** [step = 500] loss: 2.713, acc: 0.481
******** [step = 550] loss: 2.715, acc: 0.481
******** [step = 600] loss: 2.716, acc: 0.481
******** [step = 650] loss: 2.718, acc: 0.481
******** [step = 700] loss: 2.719, acc: 0.482
******** [step = 750] loss: 2.717, acc: 0.482
******** [step = 800] loss: 2.719, acc: 0.482
******** [step = 850] loss: 2.717, acc: 0.482
EPOCH = 7 loss: 2.717, acc: 0.482, val_loss: 2.724, val_acc: 0.494

================================================================================2025-08_12 14:35:33
******** [step = 50] loss: 2.657, acc: 0.483
******** [step = 100] loss: 2.631, acc: 0.489
******** [step = 150] loss: 2.628, acc: 0.490
******** [step = 200] loss: 2.621, acc: 0.492
******** [step = 250] loss: 2.616, acc: 0.493
******** [step = 300] loss: 2.615, acc: 0.493
******** [step = 350] loss: 2.617, acc: 0.493
******** [step = 400] loss: 2.619, acc: 0.493
******** [step = 450] loss: 2.620, acc: 0.493
******** [step = 500] loss: 2.618, acc: 0.493
******** [step = 550] loss: 2.619, acc: 0.493
******** [step = 600] loss: 2.619, acc: 0.494
******** [step = 650] loss: 2.620, acc: 0.494
******** [step = 700] loss: 2.620, acc: 0.494
******** [step = 750] loss: 2.623, acc: 0.494
******** [step = 800] loss: 2.623, acc: 0.494
******** [step = 850] loss: 2.624, acc: 0.494
EPOCH = 8 loss: 2.624, acc: 0.494, val_loss: 2.626, val_acc: 0.508

================================================================================2025-08_12 14:37:34
******** [step = 50] loss: 2.538, acc: 0.500
******** [step = 100] loss: 2.562, acc: 0.497
******** [step = 150] loss: 2.550, acc: 0.499
******** [step = 200] loss: 2.550, acc: 0.500
******** [step = 250] loss: 2.547, acc: 0.500
******** [step = 300] loss: 2.549, acc: 0.500
******** [step = 350] loss: 2.551, acc: 0.501
******** [step = 400] loss: 2.555, acc: 0.500
******** [step = 450] loss: 2.556, acc: 0.501
******** [step = 500] loss: 2.555, acc: 0.501
******** [step = 550] loss: 2.554, acc: 0.501
******** [step = 600] loss: 2.554, acc: 0.502
******** [step = 650] loss: 2.554, acc: 0.502
******** [step = 700] loss: 2.554, acc: 0.502
******** [step = 750] loss: 2.555, acc: 0.502
******** [step = 800] loss: 2.554, acc: 0.502
******** [step = 850] loss: 2.553, acc: 0.503
EPOCH = 9 loss: 2.553, acc: 0.503, val_loss: 2.570, val_acc: 0.518

================================================================================2025-08_12 14:39:35
******** [step = 50] loss: 2.475, acc: 0.511
******** [step = 100] loss: 2.470, acc: 0.511
******** [step = 150] loss: 2.462, acc: 0.512
******** [step = 200] loss: 2.465, acc: 0.512
******** [step = 250] loss: 2.467, acc: 0.512
******** [step = 300] loss: 2.468, acc: 0.512
******** [step = 350] loss: 2.470, acc: 0.512
******** [step = 400] loss: 2.471, acc: 0.512
******** [step = 450] loss: 2.474, acc: 0.512
******** [step = 500] loss: 2.478, acc: 0.512
******** [step = 550] loss: 2.480, acc: 0.511
******** [step = 600] loss: 2.482, acc: 0.511
******** [step = 650] loss: 2.483, acc: 0.512
******** [step = 700] loss: 2.483, acc: 0.512
******** [step = 750] loss: 2.485, acc: 0.512
******** [step = 800] loss: 2.484, acc: 0.512
******** [step = 850] loss: 2.485, acc: 0.512
EPOCH = 10 loss: 2.485, acc: 0.512, val_loss: 2.555, val_acc: 0.522

================================================================================2025-08_12 14:41:36
******** [step = 50] loss: 2.408, acc: 0.520
******** [step = 100] loss: 2.393, acc: 0.522
******** [step = 150] loss: 2.390, acc: 0.522
******** [step = 200] loss: 2.395, acc: 0.522
******** [step = 250] loss: 2.395, acc: 0.523
******** [step = 300] loss: 2.404, acc: 0.521
******** [step = 350] loss: 2.412, acc: 0.520
******** [step = 400] loss: 2.414, acc: 0.520
******** [step = 450] loss: 2.417, acc: 0.520
******** [step = 500] loss: 2.420, acc: 0.520
******** [step = 550] loss: 2.422, acc: 0.520
******** [step = 600] loss: 2.424, acc: 0.520
******** [step = 650] loss: 2.426, acc: 0.520
******** [step = 700] loss: 2.429, acc: 0.520
******** [step = 750] loss: 2.432, acc: 0.519
******** [step = 800] loss: 2.433, acc: 0.519
******** [step = 850] loss: 2.433, acc: 0.520
EPOCH = 11 loss: 2.433, acc: 0.520, val_loss: 2.511, val_acc: 0.529

================================================================================2025-08_12 14:43:37
******** [step = 50] loss: 2.337, acc: 0.527
******** [step = 100] loss: 2.345, acc: 0.526
******** [step = 150] loss: 2.350, acc: 0.527
******** [step = 200] loss: 2.351, acc: 0.527
******** [step = 250] loss: 2.361, acc: 0.526
******** [step = 300] loss: 2.366, acc: 0.525
******** [step = 350] loss: 2.371, acc: 0.525
******** [step = 400] loss: 2.377, acc: 0.525
******** [step = 450] loss: 2.384, acc: 0.524
******** [step = 500] loss: 2.386, acc: 0.524
******** [step = 550] loss: 2.391, acc: 0.523
******** [step = 600] loss: 2.392, acc: 0.524
******** [step = 650] loss: 2.394, acc: 0.523
******** [step = 700] loss: 2.397, acc: 0.523
******** [step = 750] loss: 2.401, acc: 0.523
******** [step = 800] loss: 2.403, acc: 0.523
******** [step = 850] loss: 2.403, acc: 0.523
EPOCH = 12 loss: 2.403, acc: 0.523, val_loss: 2.517, val_acc: 0.529

================================================================================2025-08_12 14:45:38
******** [step = 50] loss: 2.319, acc: 0.529
******** [step = 100] loss: 2.320, acc: 0.529
******** [step = 150] loss: 2.322, acc: 0.529
******** [step = 200] loss: 2.327, acc: 0.529
******** [step = 250] loss: 2.329, acc: 0.530
******** [step = 300] loss: 2.330, acc: 0.530
******** [step = 350] loss: 2.332, acc: 0.530
******** [step = 400] loss: 2.334, acc: 0.530
******** [step = 450] loss: 2.339, acc: 0.530
******** [step = 500] loss: 2.339, acc: 0.530
******** [step = 550] loss: 2.340, acc: 0.530
******** [step = 600] loss: 2.345, acc: 0.530
******** [step = 650] loss: 2.346, acc: 0.530
******** [step = 700] loss: 2.348, acc: 0.530
******** [step = 750] loss: 2.350, acc: 0.530
******** [step = 800] loss: 2.355, acc: 0.530
******** [step = 850] loss: 2.356, acc: 0.530
EPOCH = 13 loss: 2.356, acc: 0.530, val_loss: 2.479, val_acc: 0.535

================================================================================2025-08_12 14:47:39
******** [step = 50] loss: 2.261, acc: 0.537
******** [step = 100] loss: 2.263, acc: 0.537
******** [step = 150] loss: 2.274, acc: 0.537
******** [step = 200] loss: 2.276, acc: 0.538
******** [step = 250] loss: 2.282, acc: 0.538
******** [step = 300] loss: 2.287, acc: 0.537
******** [step = 350] loss: 2.291, acc: 0.536
******** [step = 400] loss: 2.296, acc: 0.537
******** [step = 450] loss: 2.298, acc: 0.536
******** [step = 500] loss: 2.302, acc: 0.536
******** [step = 550] loss: 2.307, acc: 0.536
******** [step = 600] loss: 2.310, acc: 0.536
******** [step = 650] loss: 2.315, acc: 0.535
******** [step = 700] loss: 2.318, acc: 0.535
******** [step = 750] loss: 2.319, acc: 0.535
******** [step = 800] loss: 2.321, acc: 0.535
******** [step = 850] loss: 2.323, acc: 0.535
EPOCH = 14 loss: 2.323, acc: 0.535, val_loss: 2.455, val_acc: 0.540

================================================================================2025-08_12 14:49:40
******** [step = 50] loss: 2.223, acc: 0.543
******** [step = 100] loss: 2.222, acc: 0.545
******** [step = 150] loss: 2.224, acc: 0.546
******** [step = 200] loss: 2.233, acc: 0.544
******** [step = 250] loss: 2.243, acc: 0.543
******** [step = 300] loss: 2.255, acc: 0.541
******** [step = 350] loss: 2.266, acc: 0.540
******** [step = 400] loss: 2.271, acc: 0.540
******** [step = 450] loss: 2.276, acc: 0.539
******** [step = 500] loss: 2.280, acc: 0.539
******** [step = 550] loss: 2.283, acc: 0.539
******** [step = 600] loss: 2.286, acc: 0.539
******** [step = 650] loss: 2.289, acc: 0.539
******** [step = 700] loss: 2.291, acc: 0.539
******** [step = 750] loss: 2.294, acc: 0.539
******** [step = 800] loss: 2.295, acc: 0.539
******** [step = 850] loss: 2.298, acc: 0.538
EPOCH = 15 loss: 2.298, acc: 0.538, val_loss: 2.435, val_acc: 0.545

================================================================================2025-08_12 14:51:41
******** [step = 50] loss: 2.192, acc: 0.548
******** [step = 100] loss: 2.205, acc: 0.547
******** [step = 150] loss: 2.209, acc: 0.548
******** [step = 200] loss: 2.208, acc: 0.548
******** [step = 250] loss: 2.215, acc: 0.548
******** [step = 300] loss: 2.220, acc: 0.547
******** [step = 350] loss: 2.227, acc: 0.547
******** [step = 400] loss: 2.230, acc: 0.546
******** [step = 450] loss: 2.235, acc: 0.546
******** [step = 500] loss: 2.240, acc: 0.545
******** [step = 550] loss: 2.243, acc: 0.545
******** [step = 600] loss: 2.249, acc: 0.545
******** [step = 650] loss: 2.251, acc: 0.545
******** [step = 700] loss: 2.254, acc: 0.545
******** [step = 750] loss: 2.257, acc: 0.545
******** [step = 800] loss: 2.260, acc: 0.544
******** [step = 850] loss: 2.260, acc: 0.544
EPOCH = 16 loss: 2.260, acc: 0.544, val_loss: 2.425, val_acc: 0.546

================================================================================2025-08_12 14:53:43
******** [step = 50] loss: 2.166, acc: 0.554
******** [step = 100] loss: 2.174, acc: 0.554
******** [step = 150] loss: 2.175, acc: 0.553
******** [step = 200] loss: 2.183, acc: 0.552
******** [step = 250] loss: 2.187, acc: 0.552
******** [step = 300] loss: 2.194, acc: 0.552
******** [step = 350] loss: 2.200, acc: 0.551
******** [step = 400] loss: 2.204, acc: 0.551
******** [step = 450] loss: 2.207, acc: 0.551
******** [step = 500] loss: 2.209, acc: 0.551
******** [step = 550] loss: 2.212, acc: 0.551
******** [step = 600] loss: 2.214, acc: 0.550
******** [step = 650] loss: 2.219, acc: 0.550
******** [step = 700] loss: 2.221, acc: 0.550
******** [step = 750] loss: 2.225, acc: 0.549
******** [step = 800] loss: 2.228, acc: 0.549
******** [step = 850] loss: 2.229, acc: 0.549
EPOCH = 17 loss: 2.229, acc: 0.549, val_loss: 2.416, val_acc: 0.548

================================================================================2025-08_12 14:55:44
******** [step = 50] loss: 2.140, acc: 0.560
******** [step = 100] loss: 2.151, acc: 0.556
******** [step = 150] loss: 2.159, acc: 0.555
******** [step = 200] loss: 2.163, acc: 0.556
******** [step = 250] loss: 2.164, acc: 0.556
******** [step = 300] loss: 2.169, acc: 0.555
******** [step = 350] loss: 2.170, acc: 0.555
******** [step = 400] loss: 2.172, acc: 0.555
******** [step = 450] loss: 2.176, acc: 0.555
******** [step = 500] loss: 2.180, acc: 0.555
******** [step = 550] loss: 2.184, acc: 0.555
******** [step = 600] loss: 2.187, acc: 0.554
******** [step = 650] loss: 2.189, acc: 0.554
******** [step = 700] loss: 2.193, acc: 0.554
******** [step = 750] loss: 2.195, acc: 0.554
******** [step = 800] loss: 2.199, acc: 0.554
******** [step = 850] loss: 2.203, acc: 0.553
EPOCH = 18 loss: 2.203, acc: 0.553, val_loss: 2.424, val_acc: 0.548

================================================================================2025-08_12 14:57:45
******** [step = 50] loss: 2.165, acc: 0.554
******** [step = 100] loss: 2.163, acc: 0.554
******** [step = 150] loss: 2.164, acc: 0.555
******** [step = 200] loss: 2.159, acc: 0.556
******** [step = 250] loss: 2.166, acc: 0.555
******** [step = 300] loss: 2.166, acc: 0.555
******** [step = 350] loss: 2.165, acc: 0.555
******** [step = 400] loss: 2.169, acc: 0.555
******** [step = 450] loss: 2.169, acc: 0.555
******** [step = 500] loss: 2.171, acc: 0.555
******** [step = 550] loss: 2.174, acc: 0.555
******** [step = 600] loss: 2.178, acc: 0.555
******** [step = 650] loss: 2.181, acc: 0.555
******** [step = 700] loss: 2.183, acc: 0.555
******** [step = 750] loss: 2.186, acc: 0.555
******** [step = 800] loss: 2.189, acc: 0.555
******** [step = 850] loss: 2.193, acc: 0.554
EPOCH = 19 loss: 2.193, acc: 0.554, val_loss: 2.400, val_acc: 0.551

================================================================================2025-08_12 14:59:46
******** [step = 50] loss: 2.103, acc: 0.563
******** [step = 100] loss: 2.103, acc: 0.564
******** [step = 150] loss: 2.109, acc: 0.563
******** [step = 200] loss: 2.116, acc: 0.563
******** [step = 250] loss: 2.124, acc: 0.562
******** [step = 300] loss: 2.129, acc: 0.561
******** [step = 350] loss: 2.133, acc: 0.561
******** [step = 400] loss: 2.140, acc: 0.560
******** [step = 450] loss: 2.142, acc: 0.560
******** [step = 500] loss: 2.145, acc: 0.560
******** [step = 550] loss: 2.150, acc: 0.560
******** [step = 600] loss: 2.152, acc: 0.559
******** [step = 650] loss: 2.157, acc: 0.559
******** [step = 700] loss: 2.162, acc: 0.559
******** [step = 750] loss: 2.165, acc: 0.558
******** [step = 800] loss: 2.166, acc: 0.558
******** [step = 850] loss: 2.167, acc: 0.558
EPOCH = 20 loss: 2.167, acc: 0.558, val_loss: 2.382, val_acc: 0.557

================================================================================2025-08_12 15:01:46
******** [step = 50] loss: 2.089, acc: 0.563
******** [step = 100] loss: 2.083, acc: 0.566
******** [step = 150] loss: 2.089, acc: 0.566
******** [step = 200] loss: 2.100, acc: 0.564
******** [step = 250] loss: 2.099, acc: 0.564
******** [step = 300] loss: 2.104, acc: 0.564
******** [step = 350] loss: 2.109, acc: 0.563
******** [step = 400] loss: 2.114, acc: 0.563
******** [step = 450] loss: 2.120, acc: 0.563
******** [step = 500] loss: 2.123, acc: 0.562
******** [step = 550] loss: 2.127, acc: 0.562
******** [step = 600] loss: 2.130, acc: 0.562
******** [step = 650] loss: 2.132, acc: 0.562
******** [step = 700] loss: 2.137, acc: 0.562
******** [step = 750] loss: 2.139, acc: 0.562
******** [step = 800] loss: 2.142, acc: 0.561
******** [step = 850] loss: 2.145, acc: 0.561
EPOCH = 21 loss: 2.145, acc: 0.561, val_loss: 2.401, val_acc: 0.555

================================================================================2025-08_12 15:03:48
******** [step = 50] loss: 2.083, acc: 0.563
******** [step = 100] loss: 2.076, acc: 0.567
******** [step = 150] loss: 2.074, acc: 0.568
******** [step = 200] loss: 2.080, acc: 0.567
******** [step = 250] loss: 2.084, acc: 0.567
******** [step = 300] loss: 2.090, acc: 0.567
******** [step = 350] loss: 2.091, acc: 0.567
******** [step = 400] loss: 2.092, acc: 0.567
******** [step = 450] loss: 2.096, acc: 0.566
******** [step = 500] loss: 2.100, acc: 0.566
******** [step = 550] loss: 2.102, acc: 0.566
******** [step = 600] loss: 2.106, acc: 0.566
******** [step = 650] loss: 2.108, acc: 0.566
******** [step = 700] loss: 2.112, acc: 0.566
******** [step = 750] loss: 2.116, acc: 0.565
******** [step = 800] loss: 2.122, acc: 0.565
******** [step = 850] loss: 2.128, acc: 0.564
EPOCH = 22 loss: 2.128, acc: 0.564, val_loss: 2.372, val_acc: 0.558

================================================================================2025-08_12 15:05:48
******** [step = 50] loss: 2.052, acc: 0.573
******** [step = 100] loss: 2.046, acc: 0.573
******** [step = 150] loss: 2.047, acc: 0.574
******** [step = 200] loss: 2.053, acc: 0.573
******** [step = 250] loss: 2.056, acc: 0.572
******** [step = 300] loss: 2.061, acc: 0.571
******** [step = 350] loss: 2.064, acc: 0.571
******** [step = 400] loss: 2.069, acc: 0.571
******** [step = 450] loss: 2.074, acc: 0.570
******** [step = 500] loss: 2.080, acc: 0.569
******** [step = 550] loss: 2.084, acc: 0.569
******** [step = 600] loss: 2.088, acc: 0.569
******** [step = 650] loss: 2.092, acc: 0.568
******** [step = 700] loss: 2.096, acc: 0.568
******** [step = 750] loss: 2.099, acc: 0.568
******** [step = 800] loss: 2.102, acc: 0.567
******** [step = 850] loss: 2.105, acc: 0.567
EPOCH = 23 loss: 2.105, acc: 0.567, val_loss: 2.353, val_acc: 0.561

================================================================================2025-08_12 15:07:49
******** [step = 50] loss: 2.049, acc: 0.576
******** [step = 100] loss: 2.032, acc: 0.577
******** [step = 150] loss: 2.035, acc: 0.575
******** [step = 200] loss: 2.035, acc: 0.575
******** [step = 250] loss: 2.037, acc: 0.575
******** [step = 300] loss: 2.038, acc: 0.575
******** [step = 350] loss: 2.044, acc: 0.574
******** [step = 400] loss: 2.050, acc: 0.574
******** [step = 450] loss: 2.054, acc: 0.573
******** [step = 500] loss: 2.059, acc: 0.573
******** [step = 550] loss: 2.062, acc: 0.572
******** [step = 600] loss: 2.065, acc: 0.572
******** [step = 650] loss: 2.071, acc: 0.572
******** [step = 700] loss: 2.078, acc: 0.571
******** [step = 750] loss: 2.084, acc: 0.570
******** [step = 800] loss: 2.087, acc: 0.570
******** [step = 850] loss: 2.090, acc: 0.570
EPOCH = 24 loss: 2.090, acc: 0.570, val_loss: 2.353, val_acc: 0.562

================================================================================2025-08_12 15:09:50
******** [step = 50] loss: 2.015, acc: 0.577
******** [step = 100] loss: 2.016, acc: 0.578
******** [step = 150] loss: 2.020, acc: 0.578
******** [step = 200] loss: 2.022, acc: 0.577
******** [step = 250] loss: 2.030, acc: 0.576
******** [step = 300] loss: 2.032, acc: 0.576
******** [step = 350] loss: 2.038, acc: 0.575
******** [step = 400] loss: 2.045, acc: 0.574
******** [step = 450] loss: 2.052, acc: 0.573
******** [step = 500] loss: 2.054, acc: 0.573
******** [step = 550] loss: 2.058, acc: 0.573
******** [step = 600] loss: 2.061, acc: 0.573
******** [step = 650] loss: 2.061, acc: 0.573
******** [step = 700] loss: 2.064, acc: 0.573
******** [step = 750] loss: 2.068, acc: 0.572
******** [step = 800] loss: 2.071, acc: 0.572
******** [step = 850] loss: 2.073, acc: 0.572
EPOCH = 25 loss: 2.073, acc: 0.572, val_loss: 2.340, val_acc: 0.565

================================================================================2025-08_12 15:11:51
******** [step = 50] loss: 2.003, acc: 0.578
******** [step = 100] loss: 2.006, acc: 0.579
******** [step = 150] loss: 2.009, acc: 0.578
******** [step = 200] loss: 2.012, acc: 0.578
******** [step = 250] loss: 2.016, acc: 0.578
******** [step = 300] loss: 2.017, acc: 0.578
******** [step = 350] loss: 2.022, acc: 0.577
******** [step = 400] loss: 2.027, acc: 0.577
******** [step = 450] loss: 2.027, acc: 0.577
******** [step = 500] loss: 2.030, acc: 0.577
******** [step = 550] loss: 2.035, acc: 0.577
******** [step = 600] loss: 2.041, acc: 0.576
******** [step = 650] loss: 2.044, acc: 0.576
******** [step = 700] loss: 2.046, acc: 0.575
******** [step = 750] loss: 2.049, acc: 0.575
******** [step = 800] loss: 2.052, acc: 0.575
******** [step = 850] loss: 2.056, acc: 0.575
EPOCH = 26 loss: 2.056, acc: 0.575, val_loss: 2.336, val_acc: 0.567

================================================================================2025-08_12 15:13:52
******** [step = 50] loss: 1.971, acc: 0.584
******** [step = 100] loss: 1.970, acc: 0.586
******** [step = 150] loss: 1.977, acc: 0.585
******** [step = 200] loss: 1.981, acc: 0.584
******** [step = 250] loss: 1.985, acc: 0.583
******** [step = 300] loss: 1.991, acc: 0.583
******** [step = 350] loss: 1.994, acc: 0.582
******** [step = 400] loss: 2.003, acc: 0.581
******** [step = 450] loss: 2.010, acc: 0.580
******** [step = 500] loss: 2.013, acc: 0.580
******** [step = 550] loss: 2.017, acc: 0.580
******** [step = 600] loss: 2.020, acc: 0.580
******** [step = 650] loss: 2.023, acc: 0.580
******** [step = 700] loss: 2.027, acc: 0.579
******** [step = 750] loss: 2.030, acc: 0.579
******** [step = 800] loss: 2.034, acc: 0.579
******** [step = 850] loss: 2.035, acc: 0.579
EPOCH = 27 loss: 2.035, acc: 0.579, val_loss: 2.328, val_acc: 0.567

================================================================================2025-08_12 15:15:53
******** [step = 50] loss: 1.968, acc: 0.587
******** [step = 100] loss: 1.970, acc: 0.587
******** [step = 150] loss: 1.976, acc: 0.585
******** [step = 200] loss: 1.975, acc: 0.586
******** [step = 250] loss: 1.978, acc: 0.586
******** [step = 300] loss: 1.981, acc: 0.585
******** [step = 350] loss: 1.985, acc: 0.585
******** [step = 400] loss: 1.989, acc: 0.584
******** [step = 450] loss: 1.991, acc: 0.584
******** [step = 500] loss: 1.996, acc: 0.583
******** [step = 550] loss: 2.000, acc: 0.583
******** [step = 600] loss: 2.004, acc: 0.583
******** [step = 650] loss: 2.007, acc: 0.582
******** [step = 700] loss: 2.011, acc: 0.582
******** [step = 750] loss: 2.014, acc: 0.582
******** [step = 800] loss: 2.017, acc: 0.581
******** [step = 850] loss: 2.019, acc: 0.582
EPOCH = 28 loss: 2.019, acc: 0.582, val_loss: 2.314, val_acc: 0.571

================================================================================2025-08_12 15:17:53
******** [step = 50] loss: 1.937, acc: 0.591
******** [step = 100] loss: 1.945, acc: 0.591
******** [step = 150] loss: 1.942, acc: 0.590
******** [step = 200] loss: 1.943, acc: 0.589
******** [step = 250] loss: 1.949, acc: 0.589
******** [step = 300] loss: 1.957, acc: 0.588
******** [step = 350] loss: 1.967, acc: 0.587
******** [step = 400] loss: 1.971, acc: 0.586
******** [step = 450] loss: 1.975, acc: 0.586
******** [step = 500] loss: 1.978, acc: 0.586
******** [step = 550] loss: 1.982, acc: 0.586
******** [step = 600] loss: 1.985, acc: 0.586
******** [step = 650] loss: 1.990, acc: 0.585
******** [step = 700] loss: 1.994, acc: 0.585
******** [step = 750] loss: 1.999, acc: 0.584
******** [step = 800] loss: 2.003, acc: 0.584
******** [step = 850] loss: 2.004, acc: 0.584
EPOCH = 29 loss: 2.004, acc: 0.584, val_loss: 2.311, val_acc: 0.571

================================================================================2025-08_12 15:19:55
******** [step = 50] loss: 1.918, acc: 0.593
******** [step = 100] loss: 1.927, acc: 0.592
******** [step = 150] loss: 1.927, acc: 0.592
******** [step = 200] loss: 1.932, acc: 0.591
******** [step = 250] loss: 1.939, acc: 0.591
******** [step = 300] loss: 1.942, acc: 0.590
******** [step = 350] loss: 1.946, acc: 0.590
******** [step = 400] loss: 1.953, acc: 0.589
******** [step = 450] loss: 1.957, acc: 0.588
******** [step = 500] loss: 1.961, acc: 0.588
******** [step = 550] loss: 1.966, acc: 0.587
******** [step = 600] loss: 1.968, acc: 0.587
******** [step = 650] loss: 1.974, acc: 0.587
******** [step = 700] loss: 1.982, acc: 0.586
******** [step = 750] loss: 1.989, acc: 0.585
******** [step = 800] loss: 1.995, acc: 0.584
******** [step = 850] loss: 1.999, acc: 0.584
EPOCH = 30 loss: 1.999, acc: 0.584, val_loss: 2.303, val_acc: 0.573

================================================================================2025-08_12 15:21:55
******** [step = 50] loss: 1.919, acc: 0.589
******** [step = 100] loss: 1.926, acc: 0.590
******** [step = 150] loss: 1.928, acc: 0.590
******** [step = 200] loss: 1.932, acc: 0.590
******** [step = 250] loss: 1.938, acc: 0.590
******** [step = 300] loss: 1.946, acc: 0.589
******** [step = 350] loss: 1.953, acc: 0.588
******** [step = 400] loss: 1.959, acc: 0.587
******** [step = 450] loss: 1.963, acc: 0.587
******** [step = 500] loss: 1.967, acc: 0.587
******** [step = 550] loss: 1.969, acc: 0.587
******** [step = 600] loss: 1.972, acc: 0.587
******** [step = 650] loss: 1.977, acc: 0.587
******** [step = 700] loss: 1.979, acc: 0.587
******** [step = 750] loss: 1.983, acc: 0.586
******** [step = 800] loss: 1.984, acc: 0.586
******** [step = 850] loss: 1.989, acc: 0.586
EPOCH = 31 loss: 1.989, acc: 0.586, val_loss: 2.292, val_acc: 0.575

================================================================================2025-08_12 15:23:56
******** [step = 50] loss: 1.909, acc: 0.592
******** [step = 100] loss: 1.909, acc: 0.593
******** [step = 150] loss: 1.915, acc: 0.593
******** [step = 200] loss: 1.906, acc: 0.594
******** [step = 250] loss: 1.908, acc: 0.594
******** [step = 300] loss: 1.920, acc: 0.593
******** [step = 350] loss: 1.928, acc: 0.593
******** [step = 400] loss: 1.934, acc: 0.592
******** [step = 450] loss: 1.940, acc: 0.591
******** [step = 500] loss: 1.945, acc: 0.591
******** [step = 550] loss: 1.950, acc: 0.590
******** [step = 600] loss: 1.954, acc: 0.590
******** [step = 650] loss: 1.958, acc: 0.590
******** [step = 700] loss: 1.961, acc: 0.589
******** [step = 750] loss: 1.964, acc: 0.589
******** [step = 800] loss: 1.966, acc: 0.589
******** [step = 850] loss: 1.969, acc: 0.589
EPOCH = 32 loss: 1.969, acc: 0.589, val_loss: 2.309, val_acc: 0.572

================================================================================2025-08_12 15:25:57
******** [step = 50] loss: 1.900, acc: 0.595
******** [step = 100] loss: 1.902, acc: 0.594
******** [step = 150] loss: 1.907, acc: 0.594
******** [step = 200] loss: 1.911, acc: 0.594
******** [step = 250] loss: 1.916, acc: 0.594
******** [step = 300] loss: 1.918, acc: 0.594
******** [step = 350] loss: 1.923, acc: 0.593
******** [step = 400] loss: 1.928, acc: 0.593
******** [step = 450] loss: 1.929, acc: 0.593
******** [step = 500] loss: 1.932, acc: 0.593
******** [step = 550] loss: 1.936, acc: 0.592
******** [step = 600] loss: 1.940, acc: 0.592
******** [step = 650] loss: 1.945, acc: 0.592
******** [step = 700] loss: 1.949, acc: 0.591
******** [step = 750] loss: 1.954, acc: 0.590
******** [step = 800] loss: 1.959, acc: 0.590
******** [step = 850] loss: 1.961, acc: 0.590
EPOCH = 33 loss: 1.961, acc: 0.590, val_loss: 2.285, val_acc: 0.577

================================================================================2025-08_12 15:27:58
******** [step = 50] loss: 1.917, acc: 0.594
******** [step = 100] loss: 1.915, acc: 0.594
******** [step = 150] loss: 1.904, acc: 0.595
******** [step = 200] loss: 1.910, acc: 0.594
******** [step = 250] loss: 1.912, acc: 0.594
******** [step = 300] loss: 1.919, acc: 0.594
******** [step = 350] loss: 1.919, acc: 0.594
******** [step = 400] loss: 1.923, acc: 0.594
******** [step = 450] loss: 1.925, acc: 0.594
******** [step = 500] loss: 1.927, acc: 0.593
******** [step = 550] loss: 1.932, acc: 0.593
******** [step = 600] loss: 1.936, acc: 0.593
******** [step = 650] loss: 1.939, acc: 0.592
******** [step = 700] loss: 1.942, acc: 0.592
******** [step = 750] loss: 1.947, acc: 0.592
******** [step = 800] loss: 1.952, acc: 0.591
******** [step = 850] loss: 1.957, acc: 0.591
EPOCH = 34 loss: 1.957, acc: 0.591, val_loss: 2.287, val_acc: 0.576

================================================================================2025-08_12 15:30:00
******** [step = 50] loss: 1.894, acc: 0.596
******** [step = 100] loss: 1.891, acc: 0.598
******** [step = 150] loss: 1.887, acc: 0.598
******** [step = 200] loss: 1.893, acc: 0.597
******** [step = 250] loss: 1.899, acc: 0.596
******** [step = 300] loss: 1.907, acc: 0.595
******** [step = 350] loss: 1.912, acc: 0.595
******** [step = 400] loss: 1.916, acc: 0.595
******** [step = 450] loss: 1.921, acc: 0.594
******** [step = 500] loss: 1.924, acc: 0.594
******** [step = 550] loss: 1.928, acc: 0.594
******** [step = 600] loss: 1.930, acc: 0.594
******** [step = 650] loss: 1.932, acc: 0.594
******** [step = 700] loss: 1.935, acc: 0.593
******** [step = 750] loss: 1.939, acc: 0.593
******** [step = 800] loss: 1.941, acc: 0.593
******** [step = 850] loss: 1.943, acc: 0.593
EPOCH = 35 loss: 1.943, acc: 0.593, val_loss: 2.274, val_acc: 0.579

================================================================================2025-08_12 15:32:01
******** [step = 50] loss: 1.862, acc: 0.603
******** [step = 100] loss: 1.859, acc: 0.603
******** [step = 150] loss: 1.857, acc: 0.604
******** [step = 200] loss: 1.864, acc: 0.603
******** [step = 250] loss: 1.875, acc: 0.602
******** [step = 300] loss: 1.879, acc: 0.601
******** [step = 350] loss: 1.884, acc: 0.600
******** [step = 400] loss: 1.890, acc: 0.599
******** [step = 450] loss: 1.895, acc: 0.599
******** [step = 500] loss: 1.897, acc: 0.599
******** [step = 550] loss: 1.902, acc: 0.598
******** [step = 600] loss: 1.905, acc: 0.598
******** [step = 650] loss: 1.911, acc: 0.597
******** [step = 700] loss: 1.915, acc: 0.597
******** [step = 750] loss: 1.918, acc: 0.597
******** [step = 800] loss: 1.922, acc: 0.596
******** [step = 850] loss: 1.928, acc: 0.596
EPOCH = 36 loss: 1.928, acc: 0.596, val_loss: 2.314, val_acc: 0.574

================================================================================2025-08_12 15:34:01
******** [step = 50] loss: 1.883, acc: 0.599
******** [step = 100] loss: 1.872, acc: 0.600
******** [step = 150] loss: 1.873, acc: 0.601
******** [step = 200] loss: 1.873, acc: 0.602
******** [step = 250] loss: 1.877, acc: 0.601
******** [step = 300] loss: 1.881, acc: 0.601
******** [step = 350] loss: 1.886, acc: 0.600
******** [step = 400] loss: 1.887, acc: 0.600
******** [step = 450] loss: 1.891, acc: 0.600
******** [step = 500] loss: 1.893, acc: 0.600
******** [step = 550] loss: 1.894, acc: 0.600
******** [step = 600] loss: 1.898, acc: 0.599
******** [step = 650] loss: 1.901, acc: 0.599
******** [step = 700] loss: 1.905, acc: 0.599
******** [step = 750] loss: 1.909, acc: 0.599
******** [step = 800] loss: 1.913, acc: 0.598
******** [step = 850] loss: 1.917, acc: 0.598
EPOCH = 37 loss: 1.917, acc: 0.598, val_loss: 2.271, val_acc: 0.581

================================================================================2025-08_12 15:36:02
******** [step = 50] loss: 1.830, acc: 0.606
******** [step = 100] loss: 1.845, acc: 0.606
******** [step = 150] loss: 1.855, acc: 0.604
******** [step = 200] loss: 1.853, acc: 0.604
******** [step = 250] loss: 1.859, acc: 0.603
******** [step = 300] loss: 1.861, acc: 0.603
******** [step = 350] loss: 1.864, acc: 0.603
******** [step = 400] loss: 1.868, acc: 0.603
******** [step = 450] loss: 1.873, acc: 0.602
******** [step = 500] loss: 1.879, acc: 0.602
******** [step = 550] loss: 1.884, acc: 0.601
******** [step = 600] loss: 1.888, acc: 0.601
******** [step = 650] loss: 1.891, acc: 0.601
******** [step = 700] loss: 1.895, acc: 0.600
******** [step = 750] loss: 1.900, acc: 0.600
******** [step = 800] loss: 1.903, acc: 0.599
******** [step = 850] loss: 1.907, acc: 0.599
EPOCH = 38 loss: 1.907, acc: 0.599, val_loss: 2.260, val_acc: 0.583

================================================================================2025-08_12 15:38:03
******** [step = 50] loss: 1.843, acc: 0.606
******** [step = 100] loss: 1.842, acc: 0.606
******** [step = 150] loss: 1.837, acc: 0.607
******** [step = 200] loss: 1.845, acc: 0.607
******** [step = 250] loss: 1.851, acc: 0.606
******** [step = 300] loss: 1.853, acc: 0.606
******** [step = 350] loss: 1.858, acc: 0.606
******** [step = 400] loss: 1.865, acc: 0.605
******** [step = 450] loss: 1.866, acc: 0.605
******** [step = 500] loss: 1.870, acc: 0.604
******** [step = 550] loss: 1.874, acc: 0.604
******** [step = 600] loss: 1.879, acc: 0.603
******** [step = 650] loss: 1.883, acc: 0.603
******** [step = 700] loss: 1.887, acc: 0.602
******** [step = 750] loss: 1.891, acc: 0.602
******** [step = 800] loss: 1.893, acc: 0.601
******** [step = 850] loss: 1.897, acc: 0.601
EPOCH = 39 loss: 1.897, acc: 0.601, val_loss: 2.251, val_acc: 0.586

================================================================================2025-08_12 15:40:04
******** [step = 50] loss: 1.821, acc: 0.606
******** [step = 100] loss: 1.815, acc: 0.608
******** [step = 150] loss: 1.820, acc: 0.608
******** [step = 200] loss: 1.826, acc: 0.608
******** [step = 250] loss: 1.830, acc: 0.608
******** [step = 300] loss: 1.837, acc: 0.607
******** [step = 350] loss: 1.841, acc: 0.607
******** [step = 400] loss: 1.850, acc: 0.606
******** [step = 450] loss: 1.856, acc: 0.605
******** [step = 500] loss: 1.860, acc: 0.605
******** [step = 550] loss: 1.862, acc: 0.605
******** [step = 600] loss: 1.867, acc: 0.604
******** [step = 650] loss: 1.871, acc: 0.604
******** [step = 700] loss: 1.876, acc: 0.603
******** [step = 750] loss: 1.879, acc: 0.603
******** [step = 800] loss: 1.884, acc: 0.603
******** [step = 850] loss: 1.886, acc: 0.603
EPOCH = 40 loss: 1.886, acc: 0.603, val_loss: 2.249, val_acc: 0.585

================================================================================2025-08_12 15:42:04
******** [step = 50] loss: 1.806, acc: 0.609
******** [step = 100] loss: 1.804, acc: 0.611
******** [step = 150] loss: 1.813, acc: 0.611
******** [step = 200] loss: 1.820, acc: 0.610
******** [step = 250] loss: 1.825, acc: 0.610
******** [step = 300] loss: 1.831, acc: 0.609
******** [step = 350] loss: 1.840, acc: 0.608
******** [step = 400] loss: 1.845, acc: 0.608
******** [step = 450] loss: 1.851, acc: 0.607
******** [step = 500] loss: 1.854, acc: 0.607
******** [step = 550] loss: 1.857, acc: 0.606
******** [step = 600] loss: 1.862, acc: 0.606
******** [step = 650] loss: 1.866, acc: 0.605
******** [step = 700] loss: 1.870, acc: 0.605
******** [step = 750] loss: 1.874, acc: 0.605
******** [step = 800] loss: 1.876, acc: 0.605
******** [step = 850] loss: 1.879, acc: 0.604
EPOCH = 41 loss: 1.879, acc: 0.604, val_loss: 2.250, val_acc: 0.585

================================================================================2025-08_12 15:44:05
******** [step = 50] loss: 1.814, acc: 0.610
******** [step = 100] loss: 1.800, acc: 0.613
******** [step = 150] loss: 1.804, acc: 0.614
******** [step = 200] loss: 1.814, acc: 0.612
******** [step = 250] loss: 1.820, acc: 0.611
******** [step = 300] loss: 1.823, acc: 0.610
******** [step = 350] loss: 1.828, acc: 0.610
******** [step = 400] loss: 1.833, acc: 0.609
******** [step = 450] loss: 1.839, acc: 0.608
******** [step = 500] loss: 1.843, acc: 0.608
******** [step = 550] loss: 1.849, acc: 0.607
******** [step = 600] loss: 1.853, acc: 0.607
******** [step = 650] loss: 1.857, acc: 0.607
******** [step = 700] loss: 1.861, acc: 0.606
******** [step = 750] loss: 1.865, acc: 0.606
******** [step = 800] loss: 1.869, acc: 0.605
******** [step = 850] loss: 1.871, acc: 0.605
EPOCH = 42 loss: 1.871, acc: 0.605, val_loss: 2.241, val_acc: 0.589

================================================================================2025-08_12 15:46:08
******** [step = 50] loss: 1.804, acc: 0.612
******** [step = 100] loss: 1.798, acc: 0.613
******** [step = 150] loss: 1.801, acc: 0.613
******** [step = 200] loss: 1.807, acc: 0.613
******** [step = 250] loss: 1.810, acc: 0.613
******** [step = 300] loss: 1.814, acc: 0.612
******** [step = 350] loss: 1.821, acc: 0.611
******** [step = 400] loss: 1.828, acc: 0.610
******** [step = 450] loss: 1.831, acc: 0.610
******** [step = 500] loss: 1.835, acc: 0.609
******** [step = 550] loss: 1.840, acc: 0.609
******** [step = 600] loss: 1.845, acc: 0.608
******** [step = 650] loss: 1.850, acc: 0.607
******** [step = 700] loss: 1.854, acc: 0.607
******** [step = 750] loss: 1.858, acc: 0.607
******** [step = 800] loss: 1.863, acc: 0.606
******** [step = 850] loss: 1.866, acc: 0.606
EPOCH = 43 loss: 1.866, acc: 0.606, val_loss: 2.242, val_acc: 0.585

================================================================================2025-08_12 15:48:10
******** [step = 50] loss: 1.806, acc: 0.612
******** [step = 100] loss: 1.808, acc: 0.611
******** [step = 150] loss: 1.802, acc: 0.612
******** [step = 200] loss: 1.801, acc: 0.612
******** [step = 250] loss: 1.802, acc: 0.612
******** [step = 300] loss: 1.807, acc: 0.611
******** [step = 350] loss: 1.813, acc: 0.611
******** [step = 400] loss: 1.820, acc: 0.610
******** [step = 450] loss: 1.827, acc: 0.609
******** [step = 500] loss: 1.830, acc: 0.609
******** [step = 550] loss: 1.834, acc: 0.609
******** [step = 600] loss: 1.839, acc: 0.609
******** [step = 650] loss: 1.842, acc: 0.608
******** [step = 700] loss: 1.845, acc: 0.608
******** [step = 750] loss: 1.849, acc: 0.608
******** [step = 800] loss: 1.853, acc: 0.608
******** [step = 850] loss: 1.856, acc: 0.607
EPOCH = 44 loss: 1.856, acc: 0.607, val_loss: 2.240, val_acc: 0.588

================================================================================2025-08_12 15:50:10
******** [step = 50] loss: 1.758, acc: 0.619
******** [step = 100] loss: 1.774, acc: 0.618
******** [step = 150] loss: 1.784, acc: 0.616
******** [step = 200] loss: 1.791, acc: 0.616
******** [step = 250] loss: 1.796, acc: 0.615
******** [step = 300] loss: 1.803, acc: 0.613
******** [step = 350] loss: 1.807, acc: 0.613
******** [step = 400] loss: 1.815, acc: 0.612
******** [step = 450] loss: 1.818, acc: 0.612
******** [step = 500] loss: 1.820, acc: 0.612
******** [step = 550] loss: 1.825, acc: 0.611
******** [step = 600] loss: 1.830, acc: 0.611
******** [step = 650] loss: 1.834, acc: 0.611
******** [step = 700] loss: 1.839, acc: 0.610
******** [step = 750] loss: 1.842, acc: 0.610
******** [step = 800] loss: 1.845, acc: 0.610
******** [step = 850] loss: 1.848, acc: 0.609
EPOCH = 45 loss: 1.848, acc: 0.609, val_loss: 2.234, val_acc: 0.588

================================================================================2025-08_12 15:52:11
******** [step = 50] loss: 1.774, acc: 0.614
******** [step = 100] loss: 1.770, acc: 0.618
******** [step = 150] loss: 1.775, acc: 0.617
******** [step = 200] loss: 1.784, acc: 0.617
******** [step = 250] loss: 1.787, acc: 0.617
******** [step = 300] loss: 1.792, acc: 0.616
******** [step = 350] loss: 1.798, acc: 0.615
******** [step = 400] loss: 1.802, acc: 0.615
******** [step = 450] loss: 1.809, acc: 0.614
******** [step = 500] loss: 1.811, acc: 0.613
******** [step = 550] loss: 1.816, acc: 0.613
******** [step = 600] loss: 1.819, acc: 0.612
******** [step = 650] loss: 1.823, acc: 0.612
******** [step = 700] loss: 1.827, acc: 0.612
******** [step = 750] loss: 1.831, acc: 0.611
******** [step = 800] loss: 1.834, acc: 0.611
******** [step = 850] loss: 1.839, acc: 0.611
EPOCH = 46 loss: 1.839, acc: 0.611, val_loss: 2.246, val_acc: 0.589

================================================================================2025-08_12 15:54:11
******** [step = 50] loss: 1.754, acc: 0.619
******** [step = 100] loss: 1.762, acc: 0.620
******** [step = 150] loss: 1.769, acc: 0.619
******** [step = 200] loss: 1.777, acc: 0.618
******** [step = 250] loss: 1.784, acc: 0.617
******** [step = 300] loss: 1.791, acc: 0.616
******** [step = 350] loss: 1.790, acc: 0.616
******** [step = 400] loss: 1.794, acc: 0.616
******** [step = 450] loss: 1.801, acc: 0.615
******** [step = 500] loss: 1.806, acc: 0.614
******** [step = 550] loss: 1.810, acc: 0.614
******** [step = 600] loss: 1.812, acc: 0.614
******** [step = 650] loss: 1.818, acc: 0.613
******** [step = 700] loss: 1.822, acc: 0.613
******** [step = 750] loss: 1.827, acc: 0.612
******** [step = 800] loss: 1.831, acc: 0.612
******** [step = 850] loss: 1.836, acc: 0.612
EPOCH = 47 loss: 1.836, acc: 0.612, val_loss: 2.222, val_acc: 0.592

================================================================================2025-08_12 15:56:13
******** [step = 50] loss: 1.757, acc: 0.617
******** [step = 100] loss: 1.761, acc: 0.619
******** [step = 150] loss: 1.761, acc: 0.619
******** [step = 200] loss: 1.765, acc: 0.619
******** [step = 250] loss: 1.771, acc: 0.618
******** [step = 300] loss: 1.776, acc: 0.617
******** [step = 350] loss: 1.783, acc: 0.617
******** [step = 400] loss: 1.788, acc: 0.616
******** [step = 450] loss: 1.789, acc: 0.616
******** [step = 500] loss: 1.795, acc: 0.615
******** [step = 550] loss: 1.800, acc: 0.615
******** [step = 600] loss: 1.804, acc: 0.615
******** [step = 650] loss: 1.809, acc: 0.614
******** [step = 700] loss: 1.813, acc: 0.614
******** [step = 750] loss: 1.818, acc: 0.614
******** [step = 800] loss: 1.820, acc: 0.614
******** [step = 850] loss: 1.823, acc: 0.613
EPOCH = 48 loss: 1.823, acc: 0.613, val_loss: 2.220, val_acc: 0.592

================================================================================2025-08_12 15:58:14
******** [step = 50] loss: 1.743, acc: 0.621
******** [step = 100] loss: 1.746, acc: 0.621
******** [step = 150] loss: 1.755, acc: 0.621
******** [step = 200] loss: 1.760, acc: 0.620
******** [step = 250] loss: 1.763, acc: 0.620
******** [step = 300] loss: 1.770, acc: 0.619
******** [step = 350] loss: 1.773, acc: 0.619
******** [step = 400] loss: 1.778, acc: 0.618
******** [step = 450] loss: 1.782, acc: 0.618
******** [step = 500] loss: 1.787, acc: 0.617
******** [step = 550] loss: 1.790, acc: 0.617
******** [step = 600] loss: 1.796, acc: 0.616
******** [step = 650] loss: 1.801, acc: 0.616
******** [step = 700] loss: 1.805, acc: 0.616
******** [step = 750] loss: 1.809, acc: 0.615
******** [step = 800] loss: 1.813, acc: 0.615
******** [step = 850] loss: 1.818, acc: 0.614
EPOCH = 49 loss: 1.818, acc: 0.614, val_loss: 2.210, val_acc: 0.595

================================================================================2025-08_12 16:00:14
******** [step = 50] loss: 1.750, acc: 0.622
******** [step = 100] loss: 1.743, acc: 0.623
******** [step = 150] loss: 1.755, acc: 0.621
******** [step = 200] loss: 1.761, acc: 0.620
******** [step = 250] loss: 1.767, acc: 0.619
******** [step = 300] loss: 1.771, acc: 0.619
******** [step = 350] loss: 1.774, acc: 0.619
******** [step = 400] loss: 1.781, acc: 0.618
******** [step = 450] loss: 1.786, acc: 0.617
******** [step = 500] loss: 1.790, acc: 0.617
******** [step = 550] loss: 1.792, acc: 0.616
******** [step = 600] loss: 1.796, acc: 0.616
******** [step = 650] loss: 1.800, acc: 0.616
******** [step = 700] loss: 1.804, acc: 0.616
******** [step = 750] loss: 1.808, acc: 0.615
******** [step = 800] loss: 1.811, acc: 0.615
******** [step = 850] loss: 1.813, acc: 0.615
EPOCH = 50 loss: 1.813, acc: 0.615, val_loss: 2.214, val_acc: 0.594

================================================================================2025-08_12 16:02:15
******** [step = 50] loss: 1.748, acc: 0.622
******** [step = 100] loss: 1.741, acc: 0.622
******** [step = 150] loss: 1.741, acc: 0.622
******** [step = 200] loss: 1.750, acc: 0.622
******** [step = 250] loss: 1.754, acc: 0.622
******** [step = 300] loss: 1.759, acc: 0.621
******** [step = 350] loss: 1.763, acc: 0.620
******** [step = 400] loss: 1.768, acc: 0.620
******** [step = 450] loss: 1.774, acc: 0.619
******** [step = 500] loss: 1.779, acc: 0.618
******** [step = 550] loss: 1.783, acc: 0.618
******** [step = 600] loss: 1.787, acc: 0.618
******** [step = 650] loss: 1.791, acc: 0.617
******** [step = 700] loss: 1.796, acc: 0.617
******** [step = 750] loss: 1.799, acc: 0.617
******** [step = 800] loss: 1.803, acc: 0.616
******** [step = 850] loss: 1.809, acc: 0.616
EPOCH = 51 loss: 1.809, acc: 0.616, val_loss: 2.215, val_acc: 0.593

================================================================================2025-08_12 16:04:16
******** [step = 50] loss: 1.756, acc: 0.620
******** [step = 100] loss: 1.755, acc: 0.620
******** [step = 150] loss: 1.750, acc: 0.621
******** [step = 200] loss: 1.754, acc: 0.621
******** [step = 250] loss: 1.762, acc: 0.620
******** [step = 300] loss: 1.762, acc: 0.620
******** [step = 350] loss: 1.765, acc: 0.620
******** [step = 400] loss: 1.768, acc: 0.620
******** [step = 450] loss: 1.771, acc: 0.620
******** [step = 500] loss: 1.775, acc: 0.619
******** [step = 550] loss: 1.778, acc: 0.619
******** [step = 600] loss: 1.782, acc: 0.618
******** [step = 650] loss: 1.786, acc: 0.618
******** [step = 700] loss: 1.790, acc: 0.618
******** [step = 750] loss: 1.794, acc: 0.617
******** [step = 800] loss: 1.797, acc: 0.617
******** [step = 850] loss: 1.800, acc: 0.617
EPOCH = 52 loss: 1.800, acc: 0.617, val_loss: 2.206, val_acc: 0.596

================================================================================2025-08_12 16:06:16
******** [step = 50] loss: 1.729, acc: 0.623
******** [step = 100] loss: 1.727, acc: 0.623
******** [step = 150] loss: 1.731, acc: 0.624
******** [step = 200] loss: 1.738, acc: 0.623
******** [step = 250] loss: 1.737, acc: 0.624
******** [step = 300] loss: 1.743, acc: 0.623
******** [step = 350] loss: 1.747, acc: 0.623
******** [step = 400] loss: 1.751, acc: 0.623
******** [step = 450] loss: 1.757, acc: 0.622
******** [step = 500] loss: 1.763, acc: 0.621
******** [step = 550] loss: 1.768, acc: 0.621
******** [step = 600] loss: 1.773, acc: 0.620
******** [step = 650] loss: 1.778, acc: 0.619
******** [step = 700] loss: 1.783, acc: 0.619
******** [step = 750] loss: 1.788, acc: 0.618
******** [step = 800] loss: 1.791, acc: 0.618
******** [step = 850] loss: 1.798, acc: 0.617
EPOCH = 53 loss: 1.798, acc: 0.617, val_loss: 2.214, val_acc: 0.594

================================================================================2025-08_12 16:08:18
******** [step = 50] loss: 1.719, acc: 0.624
******** [step = 100] loss: 1.723, acc: 0.624
******** [step = 150] loss: 1.737, acc: 0.623
******** [step = 200] loss: 1.742, acc: 0.623
******** [step = 250] loss: 1.747, acc: 0.622
******** [step = 300] loss: 1.754, acc: 0.622
******** [step = 350] loss: 1.758, acc: 0.621
******** [step = 400] loss: 1.763, acc: 0.621
******** [step = 450] loss: 1.766, acc: 0.620
******** [step = 500] loss: 1.771, acc: 0.620
******** [step = 550] loss: 1.776, acc: 0.619
******** [step = 600] loss: 1.780, acc: 0.619
******** [step = 650] loss: 1.783, acc: 0.619
******** [step = 700] loss: 1.786, acc: 0.618
******** [step = 750] loss: 1.789, acc: 0.618
******** [step = 800] loss: 1.792, acc: 0.618
******** [step = 850] loss: 1.794, acc: 0.618
EPOCH = 54 loss: 1.794, acc: 0.618, val_loss: 2.205, val_acc: 0.595

================================================================================2025-08_12 16:10:18
******** [step = 50] loss: 1.708, acc: 0.629
******** [step = 100] loss: 1.715, acc: 0.627
******** [step = 150] loss: 1.724, acc: 0.626
******** [step = 200] loss: 1.728, acc: 0.626
******** [step = 250] loss: 1.731, acc: 0.626
******** [step = 300] loss: 1.737, acc: 0.625
******** [step = 350] loss: 1.742, acc: 0.624
******** [step = 400] loss: 1.746, acc: 0.624
******** [step = 450] loss: 1.752, acc: 0.623
******** [step = 500] loss: 1.759, acc: 0.623
******** [step = 550] loss: 1.763, acc: 0.622
******** [step = 600] loss: 1.768, acc: 0.621
******** [step = 650] loss: 1.772, acc: 0.621
******** [step = 700] loss: 1.776, acc: 0.621
******** [step = 750] loss: 1.778, acc: 0.621
******** [step = 800] loss: 1.782, acc: 0.620
******** [step = 850] loss: 1.785, acc: 0.620
EPOCH = 55 loss: 1.785, acc: 0.620, val_loss: 2.199, val_acc: 0.598

================================================================================2025-08_12 16:12:18
******** [step = 50] loss: 1.715, acc: 0.629
******** [step = 100] loss: 1.710, acc: 0.628
******** [step = 150] loss: 1.720, acc: 0.626
******** [step = 200] loss: 1.728, acc: 0.625
******** [step = 250] loss: 1.732, acc: 0.625
******** [step = 300] loss: 1.736, acc: 0.624
******** [step = 350] loss: 1.741, acc: 0.624
******** [step = 400] loss: 1.745, acc: 0.623
******** [step = 450] loss: 1.750, acc: 0.623
******** [step = 500] loss: 1.752, acc: 0.623
******** [step = 550] loss: 1.757, acc: 0.623
******** [step = 600] loss: 1.761, acc: 0.622
******** [step = 650] loss: 1.765, acc: 0.622
******** [step = 700] loss: 1.770, acc: 0.621
******** [step = 750] loss: 1.772, acc: 0.621
******** [step = 800] loss: 1.776, acc: 0.621
******** [step = 850] loss: 1.780, acc: 0.621
EPOCH = 56 loss: 1.780, acc: 0.621, val_loss: 2.197, val_acc: 0.599

================================================================================2025-08_12 16:14:19
******** [step = 50] loss: 1.724, acc: 0.626
******** [step = 100] loss: 1.711, acc: 0.628
******** [step = 150] loss: 1.708, acc: 0.628
******** [step = 200] loss: 1.712, acc: 0.628
******** [step = 250] loss: 1.717, acc: 0.628
******** [step = 300] loss: 1.723, acc: 0.628
******** [step = 350] loss: 1.728, acc: 0.627
******** [step = 400] loss: 1.736, acc: 0.626
******** [step = 450] loss: 1.742, acc: 0.625
******** [step = 500] loss: 1.747, acc: 0.624
******** [step = 550] loss: 1.749, acc: 0.624
******** [step = 600] loss: 1.753, acc: 0.624
******** [step = 650] loss: 1.758, acc: 0.623
******** [step = 700] loss: 1.762, acc: 0.623
******** [step = 750] loss: 1.765, acc: 0.623
******** [step = 800] loss: 1.769, acc: 0.622
******** [step = 850] loss: 1.771, acc: 0.622
EPOCH = 57 loss: 1.771, acc: 0.622, val_loss: 2.195, val_acc: 0.597

================================================================================2025-08_12 16:16:20
******** [step = 50] loss: 1.695, acc: 0.632
******** [step = 100] loss: 1.703, acc: 0.630
******** [step = 150] loss: 1.707, acc: 0.629
******** [step = 200] loss: 1.710, acc: 0.629
******** [step = 250] loss: 1.718, acc: 0.628
******** [step = 300] loss: 1.721, acc: 0.627
******** [step = 350] loss: 1.729, acc: 0.626
******** [step = 400] loss: 1.733, acc: 0.626
******** [step = 450] loss: 1.734, acc: 0.625
******** [step = 500] loss: 1.738, acc: 0.625
******** [step = 550] loss: 1.743, acc: 0.625
******** [step = 600] loss: 1.748, acc: 0.624
******** [step = 650] loss: 1.753, acc: 0.624
******** [step = 700] loss: 1.758, acc: 0.623
******** [step = 750] loss: 1.761, acc: 0.623
******** [step = 800] loss: 1.765, acc: 0.623
******** [step = 850] loss: 1.769, acc: 0.622
EPOCH = 58 loss: 1.769, acc: 0.622, val_loss: 2.196, val_acc: 0.598

================================================================================2025-08_12 16:18:20
******** [step = 50] loss: 1.704, acc: 0.627
******** [step = 100] loss: 1.699, acc: 0.630
******** [step = 150] loss: 1.715, acc: 0.627
******** [step = 200] loss: 1.720, acc: 0.627
******** [step = 250] loss: 1.724, acc: 0.626
******** [step = 300] loss: 1.728, acc: 0.626
******** [step = 350] loss: 1.732, acc: 0.625
******** [step = 400] loss: 1.736, acc: 0.625
******** [step = 450] loss: 1.739, acc: 0.625
******** [step = 500] loss: 1.744, acc: 0.624
******** [step = 550] loss: 1.744, acc: 0.625
******** [step = 600] loss: 1.748, acc: 0.624
******** [step = 650] loss: 1.753, acc: 0.624
******** [step = 700] loss: 1.756, acc: 0.623
******** [step = 750] loss: 1.759, acc: 0.623
******** [step = 800] loss: 1.763, acc: 0.623
******** [step = 850] loss: 1.765, acc: 0.623
EPOCH = 59 loss: 1.765, acc: 0.623, val_loss: 2.193, val_acc: 0.598

================================================================================2025-08_12 16:20:20
******** [step = 50] loss: 1.672, acc: 0.632
******** [step = 100] loss: 1.691, acc: 0.630
******** [step = 150] loss: 1.700, acc: 0.629
******** [step = 200] loss: 1.706, acc: 0.629
******** [step = 250] loss: 1.709, acc: 0.629
******** [step = 300] loss: 1.716, acc: 0.628
******** [step = 350] loss: 1.721, acc: 0.628
******** [step = 400] loss: 1.729, acc: 0.627
******** [step = 450] loss: 1.733, acc: 0.626
******** [step = 500] loss: 1.737, acc: 0.626
******** [step = 550] loss: 1.741, acc: 0.626
******** [step = 600] loss: 1.743, acc: 0.625
******** [step = 650] loss: 1.749, acc: 0.625
******** [step = 700] loss: 1.753, acc: 0.624
******** [step = 750] loss: 1.757, acc: 0.624
******** [step = 800] loss: 1.761, acc: 0.623
******** [step = 850] loss: 1.763, acc: 0.623
EPOCH = 60 loss: 1.763, acc: 0.623, val_loss: 2.194, val_acc: 0.598

================================================================================2025-08_12 16:22:20
******** [step = 50] loss: 1.700, acc: 0.627
******** [step = 100] loss: 1.692, acc: 0.629
******** [step = 150] loss: 1.692, acc: 0.630
******** [step = 200] loss: 1.699, acc: 0.629
******** [step = 250] loss: 1.699, acc: 0.630
******** [step = 300] loss: 1.706, acc: 0.629
******** [step = 350] loss: 1.714, acc: 0.628
******** [step = 400] loss: 1.717, acc: 0.628
******** [step = 450] loss: 1.723, acc: 0.628
******** [step = 500] loss: 1.728, acc: 0.627
******** [step = 550] loss: 1.733, acc: 0.627
******** [step = 600] loss: 1.740, acc: 0.626
******** [step = 650] loss: 1.744, acc: 0.625
******** [step = 700] loss: 1.749, acc: 0.625
******** [step = 750] loss: 1.753, acc: 0.624
******** [step = 800] loss: 1.756, acc: 0.624
******** [step = 850] loss: 1.760, acc: 0.623
EPOCH = 61 loss: 1.760, acc: 0.623, val_loss: 2.195, val_acc: 0.599

================================================================================2025-08_12 16:24:21
******** [step = 50] loss: 1.694, acc: 0.632
******** [step = 100] loss: 1.696, acc: 0.630
******** [step = 150] loss: 1.698, acc: 0.631
******** [step = 200] loss: 1.702, acc: 0.631
******** [step = 250] loss: 1.708, acc: 0.629
******** [step = 300] loss: 1.712, acc: 0.629
******** [step = 350] loss: 1.714, acc: 0.628
******** [step = 400] loss: 1.716, acc: 0.628
******** [step = 450] loss: 1.721, acc: 0.628
******** [step = 500] loss: 1.725, acc: 0.628
******** [step = 550] loss: 1.727, acc: 0.628
******** [step = 600] loss: 1.731, acc: 0.627
******** [step = 650] loss: 1.735, acc: 0.627
******** [step = 700] loss: 1.739, acc: 0.626
******** [step = 750] loss: 1.744, acc: 0.626
******** [step = 800] loss: 1.748, acc: 0.625
******** [step = 850] loss: 1.750, acc: 0.625
EPOCH = 62 loss: 1.750, acc: 0.625, val_loss: 2.183, val_acc: 0.602

================================================================================2025-08_12 16:26:21
******** [step = 50] loss: 1.685, acc: 0.633
******** [step = 100] loss: 1.675, acc: 0.634
******** [step = 150] loss: 1.685, acc: 0.633
******** [step = 200] loss: 1.684, acc: 0.633
******** [step = 250] loss: 1.690, acc: 0.633
******** [step = 300] loss: 1.698, acc: 0.632
******** [step = 350] loss: 1.703, acc: 0.632
******** [step = 400] loss: 1.708, acc: 0.631
******** [step = 450] loss: 1.710, acc: 0.631
******** [step = 500] loss: 1.715, acc: 0.630
******** [step = 550] loss: 1.719, acc: 0.630
******** [step = 600] loss: 1.725, acc: 0.629
******** [step = 650] loss: 1.727, acc: 0.629
******** [step = 700] loss: 1.732, acc: 0.628
******** [step = 750] loss: 1.735, acc: 0.628
******** [step = 800] loss: 1.738, acc: 0.628
******** [step = 850] loss: 1.741, acc: 0.627
EPOCH = 63 loss: 1.741, acc: 0.627, val_loss: 2.187, val_acc: 0.600

================================================================================2025-08_12 16:28:23
******** [step = 50] loss: 1.655, acc: 0.637
******** [step = 100] loss: 1.659, acc: 0.637
******** [step = 150] loss: 1.671, acc: 0.635
******** [step = 200] loss: 1.677, acc: 0.634
******** [step = 250] loss: 1.683, acc: 0.634
******** [step = 300] loss: 1.685, acc: 0.633
******** [step = 350] loss: 1.692, acc: 0.633
******** [step = 400] loss: 1.696, acc: 0.633
******** [step = 450] loss: 1.702, acc: 0.632
******** [step = 500] loss: 1.708, acc: 0.631
******** [step = 550] loss: 1.712, acc: 0.631
******** [step = 600] loss: 1.716, acc: 0.630
******** [step = 650] loss: 1.721, acc: 0.630
******** [step = 700] loss: 1.726, acc: 0.629
******** [step = 750] loss: 1.730, acc: 0.629
******** [step = 800] loss: 1.733, acc: 0.629
******** [step = 850] loss: 1.736, acc: 0.628
EPOCH = 64 loss: 1.736, acc: 0.628, val_loss: 2.177, val_acc: 0.603

================================================================================2025-08_12 16:30:24
******** [step = 50] loss: 1.657, acc: 0.639
******** [step = 100] loss: 1.670, acc: 0.636
******** [step = 150] loss: 1.672, acc: 0.636
******** [step = 200] loss: 1.679, acc: 0.635
******** [step = 250] loss: 1.685, acc: 0.634
******** [step = 300] loss: 1.688, acc: 0.634
******** [step = 350] loss: 1.695, acc: 0.633
******** [step = 400] loss: 1.699, acc: 0.633
******** [step = 450] loss: 1.701, acc: 0.632
******** [step = 500] loss: 1.706, acc: 0.632
******** [step = 550] loss: 1.711, acc: 0.631
******** [step = 600] loss: 1.713, acc: 0.631
******** [step = 650] loss: 1.717, acc: 0.630
******** [step = 700] loss: 1.722, acc: 0.630
******** [step = 750] loss: 1.725, acc: 0.630
******** [step = 800] loss: 1.728, acc: 0.629
******** [step = 850] loss: 1.731, acc: 0.629
EPOCH = 65 loss: 1.731, acc: 0.629, val_loss: 2.180, val_acc: 0.602

================================================================================2025-08_12 16:32:28
******** [step = 50] loss: 1.676, acc: 0.636
******** [step = 100] loss: 1.662, acc: 0.638
******** [step = 150] loss: 1.667, acc: 0.636
******** [step = 200] loss: 1.670, acc: 0.635
******** [step = 250] loss: 1.675, acc: 0.635
******** [step = 300] loss: 1.681, acc: 0.634
******** [step = 350] loss: 1.683, acc: 0.634
******** [step = 400] loss: 1.689, acc: 0.633
******** [step = 450] loss: 1.694, acc: 0.633
******** [step = 500] loss: 1.697, acc: 0.633
******** [step = 550] loss: 1.705, acc: 0.632
******** [step = 600] loss: 1.709, acc: 0.632
******** [step = 650] loss: 1.713, acc: 0.631
******** [step = 700] loss: 1.717, acc: 0.631
******** [step = 750] loss: 1.721, acc: 0.631
******** [step = 800] loss: 1.724, acc: 0.630
******** [step = 850] loss: 1.728, acc: 0.630
EPOCH = 66 loss: 1.728, acc: 0.630, val_loss: 2.180, val_acc: 0.603

================================================================================2025-08_12 16:34:29
******** [step = 50] loss: 1.647, acc: 0.637
******** [step = 100] loss: 1.651, acc: 0.637
******** [step = 150] loss: 1.654, acc: 0.638
******** [step = 200] loss: 1.662, acc: 0.637
******** [step = 250] loss: 1.671, acc: 0.636
******** [step = 300] loss: 1.676, acc: 0.635
******** [step = 350] loss: 1.678, acc: 0.635
******** [step = 400] loss: 1.684, acc: 0.634
******** [step = 450] loss: 1.690, acc: 0.633
******** [step = 500] loss: 1.695, acc: 0.633
******** [step = 550] loss: 1.701, acc: 0.633
******** [step = 600] loss: 1.701, acc: 0.633
******** [step = 650] loss: 1.706, acc: 0.632
******** [step = 700] loss: 1.709, acc: 0.632
******** [step = 750] loss: 1.712, acc: 0.632
******** [step = 800] loss: 1.717, acc: 0.632
******** [step = 850] loss: 1.722, acc: 0.631
EPOCH = 67 loss: 1.722, acc: 0.631, val_loss: 2.182, val_acc: 0.603

================================================================================2025-08_12 16:36:32
******** [step = 50] loss: 1.650, acc: 0.639
******** [step = 100] loss: 1.654, acc: 0.639
******** [step = 150] loss: 1.654, acc: 0.638
******** [step = 200] loss: 1.663, acc: 0.637
******** [step = 250] loss: 1.667, acc: 0.637
******** [step = 300] loss: 1.673, acc: 0.636
******** [step = 350] loss: 1.677, acc: 0.635
******** [step = 400] loss: 1.681, acc: 0.635
******** [step = 450] loss: 1.687, acc: 0.634
******** [step = 500] loss: 1.691, acc: 0.634
******** [step = 550] loss: 1.695, acc: 0.633
******** [step = 600] loss: 1.700, acc: 0.633
******** [step = 650] loss: 1.707, acc: 0.632
******** [step = 700] loss: 1.710, acc: 0.632
******** [step = 750] loss: 1.716, acc: 0.631
******** [step = 800] loss: 1.721, acc: 0.631
******** [step = 850] loss: 1.724, acc: 0.630
EPOCH = 68 loss: 1.724, acc: 0.630, val_loss: 2.181, val_acc: 0.603

================================================================================2025-08_12 16:38:34
******** [step = 50] loss: 1.633, acc: 0.640
******** [step = 100] loss: 1.649, acc: 0.638
******** [step = 150] loss: 1.653, acc: 0.638
******** [step = 200] loss: 1.656, acc: 0.638
******** [step = 250] loss: 1.661, acc: 0.638
******** [step = 300] loss: 1.666, acc: 0.637
******** [step = 350] loss: 1.672, acc: 0.636
******** [step = 400] loss: 1.678, acc: 0.635
******** [step = 450] loss: 1.685, acc: 0.635
******** [step = 500] loss: 1.690, acc: 0.634
******** [step = 550] loss: 1.694, acc: 0.634
******** [step = 600] loss: 1.699, acc: 0.633
******** [step = 650] loss: 1.703, acc: 0.633
******** [step = 700] loss: 1.708, acc: 0.632
******** [step = 750] loss: 1.712, acc: 0.632
******** [step = 800] loss: 1.717, acc: 0.631
******** [step = 850] loss: 1.723, acc: 0.631
EPOCH = 69 loss: 1.723, acc: 0.631, val_loss: 2.192, val_acc: 0.601

================================================================================2025-08_12 16:40:36
******** [step = 50] loss: 1.648, acc: 0.638
******** [step = 100] loss: 1.659, acc: 0.637
******** [step = 150] loss: 1.663, acc: 0.636
******** [step = 200] loss: 1.668, acc: 0.636
******** [step = 250] loss: 1.674, acc: 0.635
******** [step = 300] loss: 1.681, acc: 0.634
******** [step = 350] loss: 1.685, acc: 0.633
******** [step = 400] loss: 1.688, acc: 0.633
******** [step = 450] loss: 1.690, acc: 0.633
******** [step = 500] loss: 1.694, acc: 0.633
******** [step = 550] loss: 1.698, acc: 0.632
******** [step = 600] loss: 1.702, acc: 0.632
******** [step = 650] loss: 1.708, acc: 0.631
******** [step = 700] loss: 1.709, acc: 0.631
******** [step = 750] loss: 1.713, acc: 0.631
******** [step = 800] loss: 1.718, acc: 0.631
******** [step = 850] loss: 1.722, acc: 0.630
EPOCH = 70 loss: 1.722, acc: 0.630, val_loss: 2.175, val_acc: 0.603

================================================================================2025-08_12 16:42:38
******** [step = 50] loss: 1.637, acc: 0.641
******** [step = 100] loss: 1.638, acc: 0.641
******** [step = 150] loss: 1.640, acc: 0.641
******** [step = 200] loss: 1.648, acc: 0.640
******** [step = 250] loss: 1.654, acc: 0.639
******** [step = 300] loss: 1.661, acc: 0.638
******** [step = 350] loss: 1.667, acc: 0.637
******** [step = 400] loss: 1.672, acc: 0.637
******** [step = 450] loss: 1.677, acc: 0.637
******** [step = 500] loss: 1.683, acc: 0.636
******** [step = 550] loss: 1.686, acc: 0.635
******** [step = 600] loss: 1.690, acc: 0.635
******** [step = 650] loss: 1.695, acc: 0.634
******** [step = 700] loss: 1.700, acc: 0.634
******** [step = 750] loss: 1.705, acc: 0.633
******** [step = 800] loss: 1.708, acc: 0.633
******** [step = 850] loss: 1.713, acc: 0.632
EPOCH = 71 loss: 1.713, acc: 0.632, val_loss: 2.185, val_acc: 0.600

================================================================================2025-08_12 16:44:43
******** [step = 50] loss: 1.651, acc: 0.638
******** [step = 100] loss: 1.653, acc: 0.638
******** [step = 150] loss: 1.662, acc: 0.636
******** [step = 200] loss: 1.671, acc: 0.636
******** [step = 250] loss: 1.674, acc: 0.635
******** [step = 300] loss: 1.678, acc: 0.634
******** [step = 350] loss: 1.681, acc: 0.634
******** [step = 400] loss: 1.686, acc: 0.634
******** [step = 450] loss: 1.692, acc: 0.633
******** [step = 500] loss: 1.696, acc: 0.633
******** [step = 550] loss: 1.701, acc: 0.632
******** [step = 600] loss: 1.704, acc: 0.632
******** [step = 650] loss: 1.708, acc: 0.632
******** [step = 700] loss: 1.711, acc: 0.632
******** [step = 750] loss: 1.714, acc: 0.631
******** [step = 800] loss: 1.718, acc: 0.631
******** [step = 850] loss: 1.721, acc: 0.631
EPOCH = 72 loss: 1.721, acc: 0.631, val_loss: 2.186, val_acc: 0.603

================================================================================2025-08_12 16:46:45
******** [step = 50] loss: 1.645, acc: 0.636
******** [step = 100] loss: 1.656, acc: 0.636
******** [step = 150] loss: 1.657, acc: 0.637
******** [step = 200] loss: 1.660, acc: 0.637
******** [step = 250] loss: 1.663, acc: 0.637
******** [step = 300] loss: 1.667, acc: 0.637
******** [step = 350] loss: 1.670, acc: 0.637
******** [step = 400] loss: 1.674, acc: 0.636
******** [step = 450] loss: 1.679, acc: 0.636
******** [step = 500] loss: 1.684, acc: 0.635
******** [step = 550] loss: 1.689, acc: 0.634
******** [step = 600] loss: 1.693, acc: 0.634
******** [step = 650] loss: 1.696, acc: 0.634
******** [step = 700] loss: 1.701, acc: 0.633
******** [step = 750] loss: 1.703, acc: 0.633
******** [step = 800] loss: 1.707, acc: 0.633
******** [step = 850] loss: 1.710, acc: 0.633
EPOCH = 73 loss: 1.710, acc: 0.633, val_loss: 2.167, val_acc: 0.606

================================================================================2025-08_12 16:48:46
******** [step = 50] loss: 1.639, acc: 0.638
******** [step = 100] loss: 1.636, acc: 0.638
******** [step = 150] loss: 1.640, acc: 0.639
******** [step = 200] loss: 1.647, acc: 0.638
******** [step = 250] loss: 1.655, acc: 0.638
******** [step = 300] loss: 1.658, acc: 0.638
******** [step = 350] loss: 1.665, acc: 0.637
******** [step = 400] loss: 1.671, acc: 0.637
******** [step = 450] loss: 1.673, acc: 0.636
******** [step = 500] loss: 1.678, acc: 0.636
******** [step = 550] loss: 1.685, acc: 0.635
******** [step = 600] loss: 1.687, acc: 0.635
******** [step = 650] loss: 1.693, acc: 0.634
******** [step = 700] loss: 1.697, acc: 0.634
******** [step = 750] loss: 1.702, acc: 0.634
******** [step = 800] loss: 1.704, acc: 0.633
******** [step = 850] loss: 1.707, acc: 0.633
EPOCH = 74 loss: 1.707, acc: 0.633, val_loss: 2.174, val_acc: 0.603

================================================================================2025-08_12 16:50:49
******** [step = 50] loss: 1.626, acc: 0.642
******** [step = 100] loss: 1.622, acc: 0.641
******** [step = 150] loss: 1.635, acc: 0.640
******** [step = 200] loss: 1.637, acc: 0.640
******** [step = 250] loss: 1.645, acc: 0.639
******** [step = 300] loss: 1.650, acc: 0.639
******** [step = 350] loss: 1.657, acc: 0.638
******** [step = 400] loss: 1.662, acc: 0.637
******** [step = 450] loss: 1.668, acc: 0.637
******** [step = 500] loss: 1.673, acc: 0.637
******** [step = 550] loss: 1.678, acc: 0.636
******** [step = 600] loss: 1.682, acc: 0.636
******** [step = 650] loss: 1.686, acc: 0.635
******** [step = 700] loss: 1.691, acc: 0.635
******** [step = 750] loss: 1.696, acc: 0.634
******** [step = 800] loss: 1.700, acc: 0.634
******** [step = 850] loss: 1.700, acc: 0.634
EPOCH = 75 loss: 1.700, acc: 0.634, val_loss: 2.162, val_acc: 0.606

================================================================================2025-08_12 16:52:50
******** [step = 50] loss: 1.632, acc: 0.642
******** [step = 100] loss: 1.628, acc: 0.643
******** [step = 150] loss: 1.627, acc: 0.643
******** [step = 200] loss: 1.630, acc: 0.643
******** [step = 250] loss: 1.638, acc: 0.641
******** [step = 300] loss: 1.648, acc: 0.640
******** [step = 350] loss: 1.654, acc: 0.639
******** [step = 400] loss: 1.661, acc: 0.638
******** [step = 450] loss: 1.665, acc: 0.638
******** [step = 500] loss: 1.673, acc: 0.637
******** [step = 550] loss: 1.678, acc: 0.636
******** [step = 600] loss: 1.681, acc: 0.636
******** [step = 650] loss: 1.685, acc: 0.635
******** [step = 700] loss: 1.690, acc: 0.635
******** [step = 750] loss: 1.693, acc: 0.635
******** [step = 800] loss: 1.697, acc: 0.634
******** [step = 850] loss: 1.701, acc: 0.634
EPOCH = 76 loss: 1.701, acc: 0.634, val_loss: 2.172, val_acc: 0.606

================================================================================2025-08_12 16:54:52
******** [step = 50] loss: 1.617, acc: 0.641
******** [step = 100] loss: 1.625, acc: 0.640
******** [step = 150] loss: 1.634, acc: 0.639
******** [step = 200] loss: 1.638, acc: 0.639
******** [step = 250] loss: 1.644, acc: 0.638
******** [step = 300] loss: 1.648, acc: 0.638
******** [step = 350] loss: 1.655, acc: 0.638
******** [step = 400] loss: 1.659, acc: 0.638
******** [step = 450] loss: 1.661, acc: 0.638
******** [step = 500] loss: 1.666, acc: 0.638
******** [step = 550] loss: 1.672, acc: 0.637
******** [step = 600] loss: 1.676, acc: 0.637
******** [step = 650] loss: 1.680, acc: 0.636
******** [step = 700] loss: 1.683, acc: 0.636
******** [step = 750] loss: 1.688, acc: 0.636
******** [step = 800] loss: 1.689, acc: 0.636
******** [step = 850] loss: 1.691, acc: 0.636
EPOCH = 77 loss: 1.691, acc: 0.636, val_loss: 2.173, val_acc: 0.605

================================================================================2025-08_12 16:56:54
******** [step = 50] loss: 1.596, acc: 0.646
******** [step = 100] loss: 1.610, acc: 0.644
******** [step = 150] loss: 1.616, acc: 0.643
******** [step = 200] loss: 1.626, acc: 0.642
******** [step = 250] loss: 1.631, acc: 0.641
******** [step = 300] loss: 1.641, acc: 0.641
******** [step = 350] loss: 1.646, acc: 0.640
******** [step = 400] loss: 1.650, acc: 0.640
******** [step = 450] loss: 1.656, acc: 0.639
******** [step = 500] loss: 1.660, acc: 0.639
******** [step = 550] loss: 1.664, acc: 0.639
******** [step = 600] loss: 1.667, acc: 0.639
******** [step = 650] loss: 1.670, acc: 0.638
******** [step = 700] loss: 1.673, acc: 0.638
******** [step = 750] loss: 1.677, acc: 0.637
******** [step = 800] loss: 1.681, acc: 0.637
******** [step = 850] loss: 1.685, acc: 0.637
EPOCH = 78 loss: 1.685, acc: 0.637, val_loss: 2.157, val_acc: 0.608

================================================================================2025-08_12 16:58:55
******** [step = 50] loss: 1.614, acc: 0.646
******** [step = 100] loss: 1.610, acc: 0.645
******** [step = 150] loss: 1.618, acc: 0.645
******** [step = 200] loss: 1.623, acc: 0.644
******** [step = 250] loss: 1.629, acc: 0.643
******** [step = 300] loss: 1.635, acc: 0.643
******** [step = 350] loss: 1.642, acc: 0.641
******** [step = 400] loss: 1.648, acc: 0.640
******** [step = 450] loss: 1.653, acc: 0.640
******** [step = 500] loss: 1.658, acc: 0.639
******** [step = 550] loss: 1.661, acc: 0.639
******** [step = 600] loss: 1.665, acc: 0.639
******** [step = 650] loss: 1.667, acc: 0.639
******** [step = 700] loss: 1.672, acc: 0.639
******** [step = 750] loss: 1.675, acc: 0.638
******** [step = 800] loss: 1.678, acc: 0.638
******** [step = 850] loss: 1.680, acc: 0.638
EPOCH = 79 loss: 1.680, acc: 0.638, val_loss: 2.162, val_acc: 0.607

================================================================================2025-08_12 17:00:56
******** [step = 50] loss: 1.595, acc: 0.646
******** [step = 100] loss: 1.606, acc: 0.644
******** [step = 150] loss: 1.612, acc: 0.644
******** [step = 200] loss: 1.618, acc: 0.643
******** [step = 250] loss: 1.622, acc: 0.643
******** [step = 300] loss: 1.630, acc: 0.642
******** [step = 350] loss: 1.634, acc: 0.642
******** [step = 400] loss: 1.640, acc: 0.641
******** [step = 450] loss: 1.645, acc: 0.641
******** [step = 500] loss: 1.650, acc: 0.640
******** [step = 550] loss: 1.657, acc: 0.640
******** [step = 600] loss: 1.660, acc: 0.639
******** [step = 650] loss: 1.663, acc: 0.639
******** [step = 700] loss: 1.666, acc: 0.639
******** [step = 750] loss: 1.670, acc: 0.638
******** [step = 800] loss: 1.674, acc: 0.638
******** [step = 850] loss: 1.675, acc: 0.638
EPOCH = 80 loss: 1.675, acc: 0.638, val_loss: 2.163, val_acc: 0.608

================================================================================2025-08_12 17:03:00
finishing training...
Training complete in 161m 39s
    epoch  ...   val_acc
0     1.0  ...  0.384346
1     2.0  ...  0.415871
2     3.0  ...  0.436171
3     4.0  ...  0.454296
4     5.0  ...  0.450653
..    ...  ...       ...
75   76.0  ...  0.605577
76   77.0  ...  0.605280
77   78.0  ...  0.607843
78   79.0  ...  0.606831
79   80.0  ...  0.608196

[80 rows x 5 columns]
== Done ==
Tue Aug 12 05:03:27 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-12-2025 17:03:27
Job ID:        6967080
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=10:54:24,vmem=0,walltime=02:43:36,mem=36612K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-009-36-0
---------------------------------------
