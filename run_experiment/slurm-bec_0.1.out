---------------------------------------
Begin Slurm Prolog: Aug-10-2025 23:45:50
Job ID:    6787012
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 11:45:50 PM EDT 2025
atl1-1-03-006-31-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 23:45:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:D8:00.0 Off |                    0 |
| N/A   33C    P0             24W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 107312.64it/s] 28%|██▊       | 38672/135842 [00:00<00:00, 160265.71it/s] 41%|████      | 55938/135842 [00:00<00:00, 122069.79it/s] 51%|█████     | 69528/135842 [00:00<00:00, 95159.69it/s]  66%|██████▌   | 89677/135842 [00:00<00:00, 121088.48it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90956.49it/s] 92%|█████████▏| 125631/135842 [00:01<00:00, 109683.79it/s]100%|██████████| 135842/135842 [00:01<00:00, 112816.60it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47556.67it/s] 33%|███▎      | 35794/108673 [00:00<00:00, 85874.02it/s] 50%|████▉     | 54165/108673 [00:00<00:00, 113886.29it/s] 67%|██████▋   | 72605/108673 [00:00<00:00, 134354.25it/s] 82%|████████▏ | 89164/108673 [00:01<00:00, 71909.96it/s]  99%|█████████▉| 107421/108673 [00:01<00:00, 91102.00it/s]100%|██████████| 108673/108673 [00:01<00:00, 89669.16it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 67%|██████▋   | 18105/27169 [00:00<00:00, 181041.54it/s]100%|██████████| 27169/27169 [00:00<00:00, 182307.65it/s]
tensor([  3,  15,  42,  61,  17,   9,  81, 152,  13,  17,  15, 388,   9,  25,
        276, 163, 164,   8,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,    56,   159,    38, 14948,    45,    57,    35,    12,    21,
            6,    83,    77,  1302,    40,    22,   359,     7,    14,  1489,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([   3,   28,  171,   12,  561,   17,    8, 1005,   15,   14,   16,   99,
          97,   21,   10,  831,  369,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_10 23:48:28
******** [step = 50] loss: 9.171, acc: 0.038
******** [step = 100] loss: 8.752, acc: 0.114
******** [step = 150] loss: 8.427, acc: 0.147
******** [step = 200] loss: 8.108, acc: 0.172
******** [step = 250] loss: 7.794, acc: 0.189
******** [step = 300] loss: 7.489, acc: 0.203
******** [step = 350] loss: 7.190, acc: 0.216
******** [step = 400] loss: 6.912, acc: 0.229
******** [step = 450] loss: 6.667, acc: 0.241
******** [step = 500] loss: 6.450, acc: 0.252
******** [step = 550] loss: 6.257, acc: 0.263
******** [step = 600] loss: 6.085, acc: 0.274
******** [step = 650] loss: 5.929, acc: 0.283
******** [step = 700] loss: 5.789, acc: 0.292
******** [step = 750] loss: 5.658, acc: 0.300
******** [step = 800] loss: 5.538, acc: 0.308
******** [step = 850] loss: 5.427, acc: 0.315
EPOCH = 1 loss: 5.427, acc: 0.315, val_loss: 3.513, val_acc: 0.451

================================================================================2025-08_10 23:49:55
******** [step = 50] loss: 3.616, acc: 0.431
******** [step = 100] loss: 3.569, acc: 0.436
******** [step = 150] loss: 3.530, acc: 0.439
******** [step = 200] loss: 3.500, acc: 0.442
******** [step = 250] loss: 3.474, acc: 0.445
******** [step = 300] loss: 3.449, acc: 0.448
******** [step = 350] loss: 3.421, acc: 0.451
******** [step = 400] loss: 3.391, acc: 0.455
******** [step = 450] loss: 3.368, acc: 0.458
******** [step = 500] loss: 3.348, acc: 0.460
******** [step = 550] loss: 3.325, acc: 0.463
******** [step = 600] loss: 3.302, acc: 0.466
******** [step = 650] loss: 3.279, acc: 0.469
******** [step = 700] loss: 3.255, acc: 0.472
******** [step = 750] loss: 3.232, acc: 0.474
******** [step = 800] loss: 3.211, acc: 0.477
******** [step = 850] loss: 3.190, acc: 0.480
EPOCH = 2 loss: 3.190, acc: 0.480, val_loss: 2.726, val_acc: 0.544

================================================================================2025-08_10 23:51:21
******** [step = 50] loss: 2.812, acc: 0.516
******** [step = 100] loss: 2.776, acc: 0.524
******** [step = 150] loss: 2.756, acc: 0.527
******** [step = 200] loss: 2.741, acc: 0.530
******** [step = 250] loss: 2.727, acc: 0.533
******** [step = 300] loss: 2.710, acc: 0.535
******** [step = 350] loss: 2.699, acc: 0.536
******** [step = 400] loss: 2.687, acc: 0.538
******** [step = 450] loss: 2.675, acc: 0.540
******** [step = 500] loss: 2.664, acc: 0.542
******** [step = 550] loss: 2.653, acc: 0.543
******** [step = 600] loss: 2.638, acc: 0.545
******** [step = 650] loss: 2.628, acc: 0.547
******** [step = 700] loss: 2.616, acc: 0.548
******** [step = 750] loss: 2.602, acc: 0.550
******** [step = 800] loss: 2.590, acc: 0.552
******** [step = 850] loss: 2.577, acc: 0.554
EPOCH = 3 loss: 2.577, acc: 0.554, val_loss: 2.296, val_acc: 0.601

================================================================================2025-08_10 23:52:42
******** [step = 50] loss: 2.309, acc: 0.586
******** [step = 100] loss: 2.289, acc: 0.589
******** [step = 150] loss: 2.265, acc: 0.592
******** [step = 200] loss: 2.251, acc: 0.595
******** [step = 250] loss: 2.246, acc: 0.595
******** [step = 300] loss: 2.241, acc: 0.596
******** [step = 350] loss: 2.240, acc: 0.596
******** [step = 400] loss: 2.235, acc: 0.597
******** [step = 450] loss: 2.229, acc: 0.598
******** [step = 500] loss: 2.220, acc: 0.600
******** [step = 550] loss: 2.212, acc: 0.601
******** [step = 600] loss: 2.206, acc: 0.602
******** [step = 650] loss: 2.199, acc: 0.603
******** [step = 700] loss: 2.195, acc: 0.604
******** [step = 750] loss: 2.188, acc: 0.605
******** [step = 800] loss: 2.182, acc: 0.606
******** [step = 850] loss: 2.176, acc: 0.607
EPOCH = 4 loss: 2.176, acc: 0.607, val_loss: 1.964, val_acc: 0.644

================================================================================2025-08_10 23:54:12
******** [step = 50] loss: 2.035, acc: 0.619
******** [step = 100] loss: 1.984, acc: 0.628
******** [step = 150] loss: 1.965, acc: 0.631
******** [step = 200] loss: 1.964, acc: 0.632
******** [step = 250] loss: 1.959, acc: 0.633
******** [step = 300] loss: 1.955, acc: 0.634
******** [step = 350] loss: 1.955, acc: 0.634
******** [step = 400] loss: 1.951, acc: 0.635
******** [step = 450] loss: 1.949, acc: 0.635
******** [step = 500] loss: 1.946, acc: 0.636
******** [step = 550] loss: 1.943, acc: 0.637
******** [step = 600] loss: 1.938, acc: 0.637
******** [step = 650] loss: 1.934, acc: 0.638
******** [step = 700] loss: 1.935, acc: 0.638
******** [step = 750] loss: 1.934, acc: 0.639
******** [step = 800] loss: 1.930, acc: 0.639
******** [step = 850] loss: 1.929, acc: 0.640
EPOCH = 5 loss: 1.929, acc: 0.640, val_loss: 1.781, val_acc: 0.673

================================================================================2025-08_10 23:55:33
******** [step = 50] loss: 1.777, acc: 0.658
******** [step = 100] loss: 1.774, acc: 0.659
******** [step = 150] loss: 1.762, acc: 0.661
******** [step = 200] loss: 1.756, acc: 0.662
******** [step = 250] loss: 1.752, acc: 0.662
******** [step = 300] loss: 1.750, acc: 0.663
******** [step = 350] loss: 1.749, acc: 0.663
******** [step = 400] loss: 1.746, acc: 0.664
******** [step = 450] loss: 1.745, acc: 0.664
******** [step = 500] loss: 1.741, acc: 0.665
******** [step = 550] loss: 1.736, acc: 0.666
******** [step = 600] loss: 1.735, acc: 0.666
******** [step = 650] loss: 1.733, acc: 0.667
******** [step = 700] loss: 1.727, acc: 0.668
******** [step = 750] loss: 1.725, acc: 0.669
******** [step = 800] loss: 1.724, acc: 0.669
******** [step = 850] loss: 1.722, acc: 0.670
EPOCH = 6 loss: 1.722, acc: 0.670, val_loss: 1.599, val_acc: 0.702

================================================================================2025-08_10 23:56:58
******** [step = 50] loss: 1.539, acc: 0.690
******** [step = 100] loss: 1.549, acc: 0.690
******** [step = 150] loss: 1.554, acc: 0.690
******** [step = 200] loss: 1.556, acc: 0.690
******** [step = 250] loss: 1.555, acc: 0.691
******** [step = 300] loss: 1.557, acc: 0.691
******** [step = 350] loss: 1.555, acc: 0.692
******** [step = 400] loss: 1.561, acc: 0.692
******** [step = 450] loss: 1.560, acc: 0.692
******** [step = 500] loss: 1.559, acc: 0.692
******** [step = 550] loss: 1.558, acc: 0.693
******** [step = 600] loss: 1.556, acc: 0.693
******** [step = 650] loss: 1.556, acc: 0.693
******** [step = 700] loss: 1.556, acc: 0.694
******** [step = 750] loss: 1.557, acc: 0.694
******** [step = 800] loss: 1.557, acc: 0.694
******** [step = 850] loss: 1.557, acc: 0.694
EPOCH = 7 loss: 1.557, acc: 0.694, val_loss: 1.511, val_acc: 0.717

================================================================================2025-08_10 23:58:25
******** [step = 50] loss: 1.412, acc: 0.713
******** [step = 100] loss: 1.414, acc: 0.713
******** [step = 150] loss: 1.412, acc: 0.714
******** [step = 200] loss: 1.423, acc: 0.712
******** [step = 250] loss: 1.425, acc: 0.712
******** [step = 300] loss: 1.425, acc: 0.712
******** [step = 350] loss: 1.423, acc: 0.713
******** [step = 400] loss: 1.424, acc: 0.713
******** [step = 450] loss: 1.430, acc: 0.712
******** [step = 500] loss: 1.432, acc: 0.712
******** [step = 550] loss: 1.433, acc: 0.712
******** [step = 600] loss: 1.433, acc: 0.712
******** [step = 650] loss: 1.434, acc: 0.713
******** [step = 700] loss: 1.434, acc: 0.713
******** [step = 750] loss: 1.434, acc: 0.713
******** [step = 800] loss: 1.435, acc: 0.713
******** [step = 850] loss: 1.437, acc: 0.713
EPOCH = 8 loss: 1.437, acc: 0.713, val_loss: 1.429, val_acc: 0.733

================================================================================2025-08_10 23:59:48
******** [step = 50] loss: 1.348, acc: 0.720
******** [step = 100] loss: 1.323, acc: 0.726
******** [step = 150] loss: 1.326, acc: 0.726
******** [step = 200] loss: 1.326, acc: 0.727
******** [step = 250] loss: 1.329, acc: 0.727
******** [step = 300] loss: 1.337, acc: 0.727
******** [step = 350] loss: 1.338, acc: 0.727
******** [step = 400] loss: 1.342, acc: 0.726
******** [step = 450] loss: 1.343, acc: 0.727
******** [step = 500] loss: 1.344, acc: 0.727
******** [step = 550] loss: 1.346, acc: 0.727
******** [step = 600] loss: 1.344, acc: 0.727
******** [step = 650] loss: 1.346, acc: 0.727
******** [step = 700] loss: 1.346, acc: 0.727
******** [step = 750] loss: 1.348, acc: 0.727
******** [step = 800] loss: 1.349, acc: 0.727
******** [step = 850] loss: 1.351, acc: 0.728
EPOCH = 9 loss: 1.351, acc: 0.728, val_loss: 1.384, val_acc: 0.743

================================================================================2025-08_11 00:01:08
******** [step = 50] loss: 1.234, acc: 0.742
******** [step = 100] loss: 1.241, acc: 0.742
******** [step = 150] loss: 1.242, acc: 0.742
******** [step = 200] loss: 1.248, acc: 0.742
******** [step = 250] loss: 1.252, acc: 0.741
******** [step = 300] loss: 1.250, acc: 0.742
******** [step = 350] loss: 1.253, acc: 0.742
******** [step = 400] loss: 1.258, acc: 0.741
******** [step = 450] loss: 1.258, acc: 0.742
******** [step = 500] loss: 1.263, acc: 0.741
******** [step = 550] loss: 1.265, acc: 0.741
******** [step = 600] loss: 1.269, acc: 0.740
******** [step = 650] loss: 1.271, acc: 0.740
******** [step = 700] loss: 1.273, acc: 0.740
******** [step = 750] loss: 1.275, acc: 0.740
******** [step = 800] loss: 1.276, acc: 0.740
******** [step = 850] loss: 1.277, acc: 0.740
EPOCH = 10 loss: 1.277, acc: 0.740, val_loss: 1.356, val_acc: 0.746

================================================================================2025-08_11 00:02:28
******** [step = 50] loss: 1.182, acc: 0.753
******** [step = 100] loss: 1.172, acc: 0.755
******** [step = 150] loss: 1.175, acc: 0.755
******** [step = 200] loss: 1.182, acc: 0.754
******** [step = 250] loss: 1.188, acc: 0.754
******** [step = 300] loss: 1.191, acc: 0.753
******** [step = 350] loss: 1.194, acc: 0.753
******** [step = 400] loss: 1.197, acc: 0.752
******** [step = 450] loss: 1.199, acc: 0.752
******** [step = 500] loss: 1.201, acc: 0.752
******** [step = 550] loss: 1.205, acc: 0.751
******** [step = 600] loss: 1.209, acc: 0.751
******** [step = 650] loss: 1.209, acc: 0.751
******** [step = 700] loss: 1.211, acc: 0.751
******** [step = 750] loss: 1.213, acc: 0.750
******** [step = 800] loss: 1.217, acc: 0.750
******** [step = 850] loss: 1.219, acc: 0.750
EPOCH = 11 loss: 1.219, acc: 0.750, val_loss: 1.319, val_acc: 0.756

================================================================================2025-08_11 00:03:48
******** [step = 50] loss: 1.117, acc: 0.761
******** [step = 100] loss: 1.119, acc: 0.762
******** [step = 150] loss: 1.120, acc: 0.763
******** [step = 200] loss: 1.127, acc: 0.762
******** [step = 250] loss: 1.130, acc: 0.762
******** [step = 300] loss: 1.131, acc: 0.762
******** [step = 350] loss: 1.135, acc: 0.762
******** [step = 400] loss: 1.138, acc: 0.761
******** [step = 450] loss: 1.142, acc: 0.761
******** [step = 500] loss: 1.148, acc: 0.760
******** [step = 550] loss: 1.152, acc: 0.760
******** [step = 600] loss: 1.155, acc: 0.760
******** [step = 650] loss: 1.158, acc: 0.759
******** [step = 700] loss: 1.160, acc: 0.759
******** [step = 750] loss: 1.162, acc: 0.759
******** [step = 800] loss: 1.165, acc: 0.759
******** [step = 850] loss: 1.165, acc: 0.759
EPOCH = 12 loss: 1.165, acc: 0.759, val_loss: 1.295, val_acc: 0.762

================================================================================2025-08_11 00:05:09
******** [step = 50] loss: 1.067, acc: 0.772
******** [step = 100] loss: 1.051, acc: 0.775
******** [step = 150] loss: 1.058, acc: 0.774
******** [step = 200] loss: 1.068, acc: 0.772
******** [step = 250] loss: 1.075, acc: 0.771
******** [step = 300] loss: 1.081, acc: 0.771
******** [step = 350] loss: 1.091, acc: 0.769
******** [step = 400] loss: 1.095, acc: 0.769
******** [step = 450] loss: 1.101, acc: 0.768
******** [step = 500] loss: 1.104, acc: 0.768
******** [step = 550] loss: 1.105, acc: 0.767
******** [step = 600] loss: 1.107, acc: 0.767
******** [step = 650] loss: 1.111, acc: 0.767
******** [step = 700] loss: 1.113, acc: 0.767
******** [step = 750] loss: 1.116, acc: 0.767
******** [step = 800] loss: 1.119, acc: 0.766
******** [step = 850] loss: 1.122, acc: 0.766
EPOCH = 13 loss: 1.122, acc: 0.766, val_loss: 1.269, val_acc: 0.765

================================================================================2025-08_11 00:06:30
******** [step = 50] loss: 1.046, acc: 0.773
******** [step = 100] loss: 1.037, acc: 0.777
******** [step = 150] loss: 1.043, acc: 0.776
******** [step = 200] loss: 1.047, acc: 0.775
******** [step = 250] loss: 1.046, acc: 0.776
******** [step = 300] loss: 1.049, acc: 0.776
******** [step = 350] loss: 1.052, acc: 0.775
******** [step = 400] loss: 1.056, acc: 0.775
******** [step = 450] loss: 1.060, acc: 0.775
******** [step = 500] loss: 1.061, acc: 0.775
******** [step = 550] loss: 1.065, acc: 0.774
******** [step = 600] loss: 1.068, acc: 0.774
******** [step = 650] loss: 1.072, acc: 0.774
******** [step = 700] loss: 1.075, acc: 0.773
******** [step = 750] loss: 1.078, acc: 0.773
******** [step = 800] loss: 1.081, acc: 0.773
******** [step = 850] loss: 1.083, acc: 0.773
EPOCH = 14 loss: 1.083, acc: 0.773, val_loss: 1.247, val_acc: 0.770

================================================================================2025-08_11 00:07:51
******** [step = 50] loss: 1.003, acc: 0.783
******** [step = 100] loss: 0.995, acc: 0.785
******** [step = 150] loss: 0.994, acc: 0.785
******** [step = 200] loss: 0.994, acc: 0.785
******** [step = 250] loss: 0.997, acc: 0.785
******** [step = 300] loss: 1.007, acc: 0.783
******** [step = 350] loss: 1.009, acc: 0.783
******** [step = 400] loss: 1.017, acc: 0.782
******** [step = 450] loss: 1.020, acc: 0.782
******** [step = 500] loss: 1.025, acc: 0.781
******** [step = 550] loss: 1.031, acc: 0.781
******** [step = 600] loss: 1.034, acc: 0.780
******** [step = 650] loss: 1.037, acc: 0.780
******** [step = 700] loss: 1.041, acc: 0.780
******** [step = 750] loss: 1.043, acc: 0.780
******** [step = 800] loss: 1.046, acc: 0.779
******** [step = 850] loss: 1.048, acc: 0.779
EPOCH = 15 loss: 1.048, acc: 0.779, val_loss: 1.237, val_acc: 0.774

================================================================================2025-08_11 00:09:12
******** [step = 50] loss: 0.964, acc: 0.790
******** [step = 100] loss: 0.963, acc: 0.791
******** [step = 150] loss: 0.970, acc: 0.790
******** [step = 200] loss: 0.973, acc: 0.789
******** [step = 250] loss: 0.979, acc: 0.788
******** [step = 300] loss: 0.983, acc: 0.788
******** [step = 350] loss: 0.987, acc: 0.787
******** [step = 400] loss: 0.993, acc: 0.786
******** [step = 450] loss: 0.996, acc: 0.786
******** [step = 500] loss: 0.999, acc: 0.786
******** [step = 550] loss: 1.004, acc: 0.785
******** [step = 600] loss: 1.006, acc: 0.785
******** [step = 650] loss: 1.009, acc: 0.785
******** [step = 700] loss: 1.013, acc: 0.784
******** [step = 750] loss: 1.016, acc: 0.784
******** [step = 800] loss: 1.017, acc: 0.784
******** [step = 850] loss: 1.018, acc: 0.784
EPOCH = 16 loss: 1.018, acc: 0.784, val_loss: 1.225, val_acc: 0.775

================================================================================2025-08_11 00:10:32
******** [step = 50] loss: 0.916, acc: 0.797
******** [step = 100] loss: 0.919, acc: 0.798
******** [step = 150] loss: 0.923, acc: 0.797
******** [step = 200] loss: 0.925, acc: 0.797
******** [step = 250] loss: 0.938, acc: 0.795
******** [step = 300] loss: 0.940, acc: 0.795
******** [step = 350] loss: 0.947, acc: 0.794
******** [step = 400] loss: 0.954, acc: 0.793
******** [step = 450] loss: 0.959, acc: 0.793
******** [step = 500] loss: 0.965, acc: 0.792
******** [step = 550] loss: 0.968, acc: 0.792
******** [step = 600] loss: 0.971, acc: 0.792
******** [step = 650] loss: 0.976, acc: 0.791
******** [step = 700] loss: 0.980, acc: 0.791
******** [step = 750] loss: 0.982, acc: 0.790
******** [step = 800] loss: 0.987, acc: 0.790
******** [step = 850] loss: 0.988, acc: 0.790
EPOCH = 17 loss: 0.988, acc: 0.790, val_loss: 1.203, val_acc: 0.781

================================================================================2025-08_11 00:11:53
******** [step = 50] loss: 0.914, acc: 0.800
******** [step = 100] loss: 0.916, acc: 0.799
******** [step = 150] loss: 0.910, acc: 0.800
******** [step = 200] loss: 0.914, acc: 0.799
******** [step = 250] loss: 0.918, acc: 0.799
******** [step = 300] loss: 0.926, acc: 0.798
******** [step = 350] loss: 0.930, acc: 0.797
******** [step = 400] loss: 0.933, acc: 0.797
******** [step = 450] loss: 0.938, acc: 0.796
******** [step = 500] loss: 0.943, acc: 0.796
******** [step = 550] loss: 0.945, acc: 0.796
******** [step = 600] loss: 0.947, acc: 0.796
******** [step = 650] loss: 0.951, acc: 0.795
******** [step = 700] loss: 0.956, acc: 0.795
******** [step = 750] loss: 0.958, acc: 0.794
******** [step = 800] loss: 0.961, acc: 0.794
******** [step = 850] loss: 0.966, acc: 0.793
EPOCH = 18 loss: 0.966, acc: 0.793, val_loss: 1.193, val_acc: 0.783

================================================================================2025-08_11 00:13:13
******** [step = 50] loss: 0.877, acc: 0.806
******** [step = 100] loss: 0.875, acc: 0.807
******** [step = 150] loss: 0.882, acc: 0.805
******** [step = 200] loss: 0.890, acc: 0.804
******** [step = 250] loss: 0.899, acc: 0.803
******** [step = 300] loss: 0.905, acc: 0.802
******** [step = 350] loss: 0.908, acc: 0.802
******** [step = 400] loss: 0.913, acc: 0.801
******** [step = 450] loss: 0.918, acc: 0.801
******** [step = 500] loss: 0.922, acc: 0.800
******** [step = 550] loss: 0.925, acc: 0.800
******** [step = 600] loss: 0.928, acc: 0.799
******** [step = 650] loss: 0.932, acc: 0.799
******** [step = 700] loss: 0.934, acc: 0.799
******** [step = 750] loss: 0.937, acc: 0.798
******** [step = 800] loss: 0.939, acc: 0.798
******** [step = 850] loss: 0.944, acc: 0.797
EPOCH = 19 loss: 0.944, acc: 0.797, val_loss: 1.190, val_acc: 0.783

================================================================================2025-08_11 00:14:34
******** [step = 50] loss: 0.875, acc: 0.807
******** [step = 100] loss: 0.879, acc: 0.807
******** [step = 150] loss: 0.876, acc: 0.807
******** [step = 200] loss: 0.875, acc: 0.807
******** [step = 250] loss: 0.883, acc: 0.806
******** [step = 300] loss: 0.886, acc: 0.805
******** [step = 350] loss: 0.889, acc: 0.805
******** [step = 400] loss: 0.892, acc: 0.805
******** [step = 450] loss: 0.897, acc: 0.804
******** [step = 500] loss: 0.899, acc: 0.804
******** [step = 550] loss: 0.904, acc: 0.803
******** [step = 600] loss: 0.905, acc: 0.803
******** [step = 650] loss: 0.909, acc: 0.803
******** [step = 700] loss: 0.913, acc: 0.802
******** [step = 750] loss: 0.916, acc: 0.802
******** [step = 800] loss: 0.918, acc: 0.802
******** [step = 850] loss: 0.920, acc: 0.802
EPOCH = 20 loss: 0.920, acc: 0.802, val_loss: 1.179, val_acc: 0.785

================================================================================2025-08_11 00:16:01
******** [step = 50] loss: 0.823, acc: 0.815
******** [step = 100] loss: 0.840, acc: 0.813
******** [step = 150] loss: 0.838, acc: 0.813
******** [step = 200] loss: 0.843, acc: 0.812
******** [step = 250] loss: 0.851, acc: 0.811
******** [step = 300] loss: 0.856, acc: 0.811
******** [step = 350] loss: 0.863, acc: 0.810
******** [step = 400] loss: 0.870, acc: 0.809
******** [step = 450] loss: 0.874, acc: 0.808
******** [step = 500] loss: 0.879, acc: 0.807
******** [step = 550] loss: 0.883, acc: 0.807
******** [step = 600] loss: 0.888, acc: 0.806
******** [step = 650] loss: 0.891, acc: 0.806
******** [step = 700] loss: 0.894, acc: 0.806
******** [step = 750] loss: 0.897, acc: 0.805
******** [step = 800] loss: 0.900, acc: 0.805
******** [step = 850] loss: 0.904, acc: 0.805
EPOCH = 21 loss: 0.904, acc: 0.805, val_loss: 1.173, val_acc: 0.788

================================================================================2025-08_11 00:17:25
******** [step = 50] loss: 0.800, acc: 0.821
******** [step = 100] loss: 0.812, acc: 0.818
******** [step = 150] loss: 0.813, acc: 0.818
******** [step = 200] loss: 0.821, acc: 0.816
******** [step = 250] loss: 0.833, acc: 0.814
******** [step = 300] loss: 0.839, acc: 0.814
******** [step = 350] loss: 0.844, acc: 0.813
******** [step = 400] loss: 0.852, acc: 0.812
******** [step = 450] loss: 0.855, acc: 0.811
******** [step = 500] loss: 0.859, acc: 0.811
******** [step = 550] loss: 0.861, acc: 0.811
******** [step = 600] loss: 0.866, acc: 0.810
******** [step = 650] loss: 0.872, acc: 0.810
******** [step = 700] loss: 0.875, acc: 0.809
******** [step = 750] loss: 0.880, acc: 0.809
******** [step = 800] loss: 0.884, acc: 0.808
******** [step = 850] loss: 0.887, acc: 0.808
EPOCH = 22 loss: 0.887, acc: 0.808, val_loss: 1.171, val_acc: 0.789

================================================================================2025-08_11 00:18:46
******** [step = 50] loss: 0.799, acc: 0.822
******** [step = 100] loss: 0.796, acc: 0.823
******** [step = 150] loss: 0.809, acc: 0.821
******** [step = 200] loss: 0.818, acc: 0.819
******** [step = 250] loss: 0.822, acc: 0.818
******** [step = 300] loss: 0.828, acc: 0.817
******** [step = 350] loss: 0.833, acc: 0.816
******** [step = 400] loss: 0.836, acc: 0.816
******** [step = 450] loss: 0.839, acc: 0.815
******** [step = 500] loss: 0.844, acc: 0.815
******** [step = 550] loss: 0.850, acc: 0.814
******** [step = 600] loss: 0.852, acc: 0.814
******** [step = 650] loss: 0.855, acc: 0.813
******** [step = 700] loss: 0.860, acc: 0.812
******** [step = 750] loss: 0.863, acc: 0.812
******** [step = 800] loss: 0.866, acc: 0.812
******** [step = 850] loss: 0.867, acc: 0.811
EPOCH = 23 loss: 0.867, acc: 0.811, val_loss: 1.164, val_acc: 0.789

================================================================================2025-08_11 00:20:05
******** [step = 50] loss: 0.791, acc: 0.825
******** [step = 100] loss: 0.796, acc: 0.823
******** [step = 150] loss: 0.796, acc: 0.822
******** [step = 200] loss: 0.809, acc: 0.820
******** [step = 250] loss: 0.812, acc: 0.819
******** [step = 300] loss: 0.817, acc: 0.818
******** [step = 350] loss: 0.819, acc: 0.818
******** [step = 400] loss: 0.822, acc: 0.818
******** [step = 450] loss: 0.828, acc: 0.817
******** [step = 500] loss: 0.833, acc: 0.816
******** [step = 550] loss: 0.837, acc: 0.816
******** [step = 600] loss: 0.841, acc: 0.815
******** [step = 650] loss: 0.846, acc: 0.814
******** [step = 700] loss: 0.848, acc: 0.814
******** [step = 750] loss: 0.850, acc: 0.814
******** [step = 800] loss: 0.853, acc: 0.814
******** [step = 850] loss: 0.854, acc: 0.814
EPOCH = 24 loss: 0.854, acc: 0.814, val_loss: 1.163, val_acc: 0.791

================================================================================2025-08_11 00:21:29
******** [step = 50] loss: 0.774, acc: 0.823
******** [step = 100] loss: 0.781, acc: 0.823
******** [step = 150] loss: 0.789, acc: 0.822
******** [step = 200] loss: 0.792, acc: 0.822
******** [step = 250] loss: 0.802, acc: 0.821
******** [step = 300] loss: 0.801, acc: 0.821
******** [step = 350] loss: 0.805, acc: 0.820
******** [step = 400] loss: 0.810, acc: 0.820
******** [step = 450] loss: 0.813, acc: 0.820
******** [step = 500] loss: 0.816, acc: 0.819
******** [step = 550] loss: 0.821, acc: 0.819
******** [step = 600] loss: 0.824, acc: 0.819
******** [step = 650] loss: 0.826, acc: 0.818
******** [step = 700] loss: 0.830, acc: 0.818
******** [step = 750] loss: 0.834, acc: 0.817
******** [step = 800] loss: 0.837, acc: 0.817
******** [step = 850] loss: 0.840, acc: 0.817
EPOCH = 25 loss: 0.840, acc: 0.817, val_loss: 1.144, val_acc: 0.794

================================================================================2025-08_11 00:22:49
******** [step = 50] loss: 0.755, acc: 0.829
******** [step = 100] loss: 0.767, acc: 0.827
******** [step = 150] loss: 0.770, acc: 0.826
******** [step = 200] loss: 0.776, acc: 0.825
******** [step = 250] loss: 0.783, acc: 0.824
******** [step = 300] loss: 0.785, acc: 0.824
******** [step = 350] loss: 0.789, acc: 0.823
******** [step = 400] loss: 0.793, acc: 0.823
******** [step = 450] loss: 0.797, acc: 0.822
******** [step = 500] loss: 0.802, acc: 0.822
******** [step = 550] loss: 0.806, acc: 0.821
******** [step = 600] loss: 0.809, acc: 0.821
******** [step = 650] loss: 0.812, acc: 0.821
******** [step = 700] loss: 0.816, acc: 0.820
******** [step = 750] loss: 0.820, acc: 0.820
******** [step = 800] loss: 0.824, acc: 0.819
******** [step = 850] loss: 0.827, acc: 0.819
EPOCH = 26 loss: 0.827, acc: 0.819, val_loss: 1.144, val_acc: 0.796

================================================================================2025-08_11 00:24:10
******** [step = 50] loss: 0.750, acc: 0.829
******** [step = 100] loss: 0.743, acc: 0.832
******** [step = 150] loss: 0.747, acc: 0.831
******** [step = 200] loss: 0.755, acc: 0.829
******** [step = 250] loss: 0.765, acc: 0.828
******** [step = 300] loss: 0.769, acc: 0.827
******** [step = 350] loss: 0.775, acc: 0.827
******** [step = 400] loss: 0.780, acc: 0.826
******** [step = 450] loss: 0.785, acc: 0.825
******** [step = 500] loss: 0.790, acc: 0.824
******** [step = 550] loss: 0.796, acc: 0.824
******** [step = 600] loss: 0.799, acc: 0.823
******** [step = 650] loss: 0.801, acc: 0.823
******** [step = 700] loss: 0.805, acc: 0.823
******** [step = 750] loss: 0.807, acc: 0.822
******** [step = 800] loss: 0.811, acc: 0.822
******** [step = 850] loss: 0.814, acc: 0.822
EPOCH = 27 loss: 0.814, acc: 0.822, val_loss: 1.131, val_acc: 0.797

================================================================================2025-08_11 00:25:30
******** [step = 50] loss: 0.744, acc: 0.831
******** [step = 100] loss: 0.741, acc: 0.833
******** [step = 150] loss: 0.752, acc: 0.831
******** [step = 200] loss: 0.754, acc: 0.831
******** [step = 250] loss: 0.755, acc: 0.831
******** [step = 300] loss: 0.760, acc: 0.830
******** [step = 350] loss: 0.764, acc: 0.829
******** [step = 400] loss: 0.767, acc: 0.828
******** [step = 450] loss: 0.771, acc: 0.827
******** [step = 500] loss: 0.776, acc: 0.827
******** [step = 550] loss: 0.782, acc: 0.826
******** [step = 600] loss: 0.787, acc: 0.825
******** [step = 650] loss: 0.790, acc: 0.825
******** [step = 700] loss: 0.794, acc: 0.824
******** [step = 750] loss: 0.798, acc: 0.824
******** [step = 800] loss: 0.801, acc: 0.824
******** [step = 850] loss: 0.803, acc: 0.824
EPOCH = 28 loss: 0.803, acc: 0.824, val_loss: 1.137, val_acc: 0.797

================================================================================2025-08_11 00:26:50
******** [step = 50] loss: 0.703, acc: 0.841
******** [step = 100] loss: 0.721, acc: 0.836
******** [step = 150] loss: 0.731, acc: 0.834
******** [step = 200] loss: 0.743, acc: 0.832
******** [step = 250] loss: 0.746, acc: 0.832
******** [step = 300] loss: 0.748, acc: 0.831
******** [step = 350] loss: 0.751, acc: 0.831
******** [step = 400] loss: 0.757, acc: 0.830
******** [step = 450] loss: 0.759, acc: 0.830
******** [step = 500] loss: 0.765, acc: 0.829
******** [step = 550] loss: 0.768, acc: 0.828
******** [step = 600] loss: 0.773, acc: 0.828
******** [step = 650] loss: 0.778, acc: 0.827
******** [step = 700] loss: 0.782, acc: 0.827
******** [step = 750] loss: 0.785, acc: 0.826
******** [step = 800] loss: 0.789, acc: 0.826
******** [step = 850] loss: 0.794, acc: 0.825
EPOCH = 29 loss: 0.794, acc: 0.825, val_loss: 1.130, val_acc: 0.799

================================================================================2025-08_11 00:28:10
******** [step = 50] loss: 0.707, acc: 0.838
******** [step = 100] loss: 0.715, acc: 0.837
******** [step = 150] loss: 0.720, acc: 0.836
******** [step = 200] loss: 0.725, acc: 0.836
******** [step = 250] loss: 0.731, acc: 0.835
******** [step = 300] loss: 0.732, acc: 0.835
******** [step = 350] loss: 0.739, acc: 0.834
******** [step = 400] loss: 0.744, acc: 0.833
******** [step = 450] loss: 0.750, acc: 0.832
******** [step = 500] loss: 0.754, acc: 0.831
******** [step = 550] loss: 0.757, acc: 0.831
******** [step = 600] loss: 0.761, acc: 0.830
******** [step = 650] loss: 0.765, acc: 0.830
******** [step = 700] loss: 0.769, acc: 0.829
******** [step = 750] loss: 0.774, acc: 0.829
******** [step = 800] loss: 0.777, acc: 0.828
******** [step = 850] loss: 0.780, acc: 0.828
EPOCH = 30 loss: 0.780, acc: 0.828, val_loss: 1.133, val_acc: 0.799

================================================================================2025-08_11 00:29:30
******** [step = 50] loss: 0.684, acc: 0.843
******** [step = 100] loss: 0.691, acc: 0.842
******** [step = 150] loss: 0.701, acc: 0.840
******** [step = 200] loss: 0.710, acc: 0.839
******** [step = 250] loss: 0.717, acc: 0.838
******** [step = 300] loss: 0.723, acc: 0.837
******** [step = 350] loss: 0.728, acc: 0.836
******** [step = 400] loss: 0.734, acc: 0.835
******** [step = 450] loss: 0.739, acc: 0.834
******** [step = 500] loss: 0.743, acc: 0.833
******** [step = 550] loss: 0.747, acc: 0.833
******** [step = 600] loss: 0.752, acc: 0.832
******** [step = 650] loss: 0.755, acc: 0.832
******** [step = 700] loss: 0.761, acc: 0.831
******** [step = 750] loss: 0.764, acc: 0.831
******** [step = 800] loss: 0.768, acc: 0.830
******** [step = 850] loss: 0.771, acc: 0.830
EPOCH = 31 loss: 0.771, acc: 0.830, val_loss: 1.124, val_acc: 0.800

================================================================================2025-08_11 00:30:50
******** [step = 50] loss: 0.688, acc: 0.840
******** [step = 100] loss: 0.698, acc: 0.839
******** [step = 150] loss: 0.706, acc: 0.838
******** [step = 200] loss: 0.710, acc: 0.837
******** [step = 250] loss: 0.717, acc: 0.836
******** [step = 300] loss: 0.723, acc: 0.835
******** [step = 350] loss: 0.729, acc: 0.835
******** [step = 400] loss: 0.732, acc: 0.834
******** [step = 450] loss: 0.734, acc: 0.834
******** [step = 500] loss: 0.738, acc: 0.834
******** [step = 550] loss: 0.742, acc: 0.833
******** [step = 600] loss: 0.747, acc: 0.832
******** [step = 650] loss: 0.750, acc: 0.832
******** [step = 700] loss: 0.754, acc: 0.832
******** [step = 750] loss: 0.757, acc: 0.831
******** [step = 800] loss: 0.758, acc: 0.831
******** [step = 850] loss: 0.761, acc: 0.831
EPOCH = 32 loss: 0.761, acc: 0.831, val_loss: 1.125, val_acc: 0.801

================================================================================2025-08_11 00:32:17
******** [step = 50] loss: 0.695, acc: 0.839
******** [step = 100] loss: 0.696, acc: 0.840
******** [step = 150] loss: 0.697, acc: 0.839
******** [step = 200] loss: 0.702, acc: 0.838
******** [step = 250] loss: 0.710, acc: 0.837
******** [step = 300] loss: 0.712, acc: 0.837
******** [step = 350] loss: 0.716, acc: 0.837
******** [step = 400] loss: 0.723, acc: 0.836
******** [step = 450] loss: 0.727, acc: 0.835
******** [step = 500] loss: 0.731, acc: 0.835
******** [step = 550] loss: 0.735, acc: 0.834
******** [step = 600] loss: 0.739, acc: 0.834
******** [step = 650] loss: 0.743, acc: 0.833
******** [step = 700] loss: 0.746, acc: 0.833
******** [step = 750] loss: 0.749, acc: 0.833
******** [step = 800] loss: 0.752, acc: 0.833
******** [step = 850] loss: 0.754, acc: 0.833
EPOCH = 33 loss: 0.754, acc: 0.833, val_loss: 1.116, val_acc: 0.802

================================================================================2025-08_11 00:33:37
******** [step = 50] loss: 0.668, acc: 0.846
******** [step = 100] loss: 0.671, acc: 0.845
******** [step = 150] loss: 0.677, acc: 0.844
******** [step = 200] loss: 0.684, acc: 0.842
******** [step = 250] loss: 0.690, acc: 0.842
******** [step = 300] loss: 0.697, acc: 0.841
******** [step = 350] loss: 0.706, acc: 0.840
******** [step = 400] loss: 0.712, acc: 0.839
******** [step = 450] loss: 0.717, acc: 0.838
******** [step = 500] loss: 0.722, acc: 0.837
******** [step = 550] loss: 0.727, acc: 0.837
******** [step = 600] loss: 0.731, acc: 0.836
******** [step = 650] loss: 0.733, acc: 0.836
******** [step = 700] loss: 0.736, acc: 0.836
******** [step = 750] loss: 0.740, acc: 0.835
******** [step = 800] loss: 0.743, acc: 0.835
******** [step = 850] loss: 0.746, acc: 0.835
EPOCH = 34 loss: 0.746, acc: 0.835, val_loss: 1.119, val_acc: 0.801

================================================================================2025-08_11 00:34:57
******** [step = 50] loss: 0.670, acc: 0.845
******** [step = 100] loss: 0.679, acc: 0.844
******** [step = 150] loss: 0.683, acc: 0.843
******** [step = 200] loss: 0.686, acc: 0.843
******** [step = 250] loss: 0.693, acc: 0.842
******** [step = 300] loss: 0.698, acc: 0.841
******** [step = 350] loss: 0.700, acc: 0.841
******** [step = 400] loss: 0.704, acc: 0.841
******** [step = 450] loss: 0.709, acc: 0.840
******** [step = 500] loss: 0.713, acc: 0.839
******** [step = 550] loss: 0.718, acc: 0.839
******** [step = 600] loss: 0.721, acc: 0.838
******** [step = 650] loss: 0.724, acc: 0.838
******** [step = 700] loss: 0.727, acc: 0.838
******** [step = 750] loss: 0.729, acc: 0.837
******** [step = 800] loss: 0.733, acc: 0.837
******** [step = 850] loss: 0.736, acc: 0.837
EPOCH = 35 loss: 0.736, acc: 0.837, val_loss: 1.110, val_acc: 0.802

================================================================================2025-08_11 00:36:17
******** [step = 50] loss: 0.669, acc: 0.846
******** [step = 100] loss: 0.663, acc: 0.847
******** [step = 150] loss: 0.667, acc: 0.847
******** [step = 200] loss: 0.672, acc: 0.847
******** [step = 250] loss: 0.678, acc: 0.846
******** [step = 300] loss: 0.683, acc: 0.845
******** [step = 350] loss: 0.689, acc: 0.844
******** [step = 400] loss: 0.694, acc: 0.843
******** [step = 450] loss: 0.698, acc: 0.842
******** [step = 500] loss: 0.702, acc: 0.842
******** [step = 550] loss: 0.706, acc: 0.841
******** [step = 600] loss: 0.710, acc: 0.841
******** [step = 650] loss: 0.715, acc: 0.840
******** [step = 700] loss: 0.719, acc: 0.839
******** [step = 750] loss: 0.723, acc: 0.839
******** [step = 800] loss: 0.727, acc: 0.838
******** [step = 850] loss: 0.731, acc: 0.838
EPOCH = 36 loss: 0.731, acc: 0.838, val_loss: 1.108, val_acc: 0.804

================================================================================2025-08_11 00:37:41
******** [step = 50] loss: 0.652, acc: 0.848
******** [step = 100] loss: 0.654, acc: 0.849
******** [step = 150] loss: 0.663, acc: 0.847
******** [step = 200] loss: 0.666, acc: 0.847
******** [step = 250] loss: 0.671, acc: 0.846
******** [step = 300] loss: 0.679, acc: 0.845
******** [step = 350] loss: 0.681, acc: 0.845
******** [step = 400] loss: 0.688, acc: 0.844
******** [step = 450] loss: 0.691, acc: 0.844
******** [step = 500] loss: 0.695, acc: 0.843
******** [step = 550] loss: 0.700, acc: 0.842
******** [step = 600] loss: 0.703, acc: 0.842
******** [step = 650] loss: 0.707, acc: 0.841
******** [step = 700] loss: 0.712, acc: 0.841
******** [step = 750] loss: 0.715, acc: 0.840
******** [step = 800] loss: 0.720, acc: 0.840
******** [step = 850] loss: 0.723, acc: 0.840
EPOCH = 37 loss: 0.723, acc: 0.840, val_loss: 1.106, val_acc: 0.805

================================================================================2025-08_11 00:39:04
******** [step = 50] loss: 0.638, acc: 0.851
******** [step = 100] loss: 0.650, acc: 0.848
******** [step = 150] loss: 0.654, acc: 0.849
******** [step = 200] loss: 0.657, acc: 0.848
******** [step = 250] loss: 0.664, acc: 0.847
******** [step = 300] loss: 0.668, acc: 0.847
******** [step = 350] loss: 0.674, acc: 0.846
******** [step = 400] loss: 0.680, acc: 0.845
******** [step = 450] loss: 0.687, acc: 0.844
******** [step = 500] loss: 0.689, acc: 0.844
******** [step = 550] loss: 0.695, acc: 0.843
******** [step = 600] loss: 0.699, acc: 0.843
******** [step = 650] loss: 0.702, acc: 0.842
******** [step = 700] loss: 0.707, acc: 0.842
******** [step = 750] loss: 0.710, acc: 0.841
******** [step = 800] loss: 0.713, acc: 0.841
******** [step = 850] loss: 0.716, acc: 0.841
EPOCH = 38 loss: 0.716, acc: 0.841, val_loss: 1.108, val_acc: 0.806

================================================================================2025-08_11 00:40:32
******** [step = 50] loss: 0.649, acc: 0.850
******** [step = 100] loss: 0.648, acc: 0.850
******** [step = 150] loss: 0.655, acc: 0.848
******** [step = 200] loss: 0.660, acc: 0.848
******** [step = 250] loss: 0.665, acc: 0.847
******** [step = 300] loss: 0.669, acc: 0.846
******** [step = 350] loss: 0.673, acc: 0.846
******** [step = 400] loss: 0.679, acc: 0.845
******** [step = 450] loss: 0.684, acc: 0.845
******** [step = 500] loss: 0.687, acc: 0.844
******** [step = 550] loss: 0.691, acc: 0.844
******** [step = 600] loss: 0.692, acc: 0.844
******** [step = 650] loss: 0.695, acc: 0.844
******** [step = 700] loss: 0.700, acc: 0.843
******** [step = 750] loss: 0.705, acc: 0.842
******** [step = 800] loss: 0.707, acc: 0.842
******** [step = 850] loss: 0.709, acc: 0.842
EPOCH = 39 loss: 0.709, acc: 0.842, val_loss: 1.107, val_acc: 0.805

================================================================================2025-08_11 00:41:53
******** [step = 50] loss: 0.624, acc: 0.854
******** [step = 100] loss: 0.638, acc: 0.852
******** [step = 150] loss: 0.642, acc: 0.851
******** [step = 200] loss: 0.646, acc: 0.850
******** [step = 250] loss: 0.652, acc: 0.850
******** [step = 300] loss: 0.657, acc: 0.849
******** [step = 350] loss: 0.661, acc: 0.848
******** [step = 400] loss: 0.666, acc: 0.848
******** [step = 450] loss: 0.671, acc: 0.847
******** [step = 500] loss: 0.675, acc: 0.846
******** [step = 550] loss: 0.680, acc: 0.845
******** [step = 600] loss: 0.684, acc: 0.845
******** [step = 650] loss: 0.690, acc: 0.845
******** [step = 700] loss: 0.693, acc: 0.844
******** [step = 750] loss: 0.697, acc: 0.844
******** [step = 800] loss: 0.700, acc: 0.843
******** [step = 850] loss: 0.702, acc: 0.843
EPOCH = 40 loss: 0.702, acc: 0.843, val_loss: 1.102, val_acc: 0.806

================================================================================2025-08_11 00:43:13
******** [step = 50] loss: 0.618, acc: 0.857
******** [step = 100] loss: 0.624, acc: 0.855
******** [step = 150] loss: 0.630, acc: 0.854
******** [step = 200] loss: 0.636, acc: 0.853
******** [step = 250] loss: 0.643, acc: 0.852
******** [step = 300] loss: 0.649, acc: 0.851
******** [step = 350] loss: 0.654, acc: 0.850
******** [step = 400] loss: 0.660, acc: 0.849
******** [step = 450] loss: 0.664, acc: 0.848
******** [step = 500] loss: 0.669, acc: 0.848
******** [step = 550] loss: 0.674, acc: 0.847
******** [step = 600] loss: 0.678, acc: 0.847
******** [step = 650] loss: 0.681, acc: 0.846
******** [step = 700] loss: 0.686, acc: 0.846
******** [step = 750] loss: 0.689, acc: 0.845
******** [step = 800] loss: 0.692, acc: 0.845
******** [step = 850] loss: 0.696, acc: 0.844
EPOCH = 41 loss: 0.696, acc: 0.844, val_loss: 1.100, val_acc: 0.807

================================================================================2025-08_11 00:44:35
******** [step = 50] loss: 0.631, acc: 0.854
******** [step = 100] loss: 0.628, acc: 0.854
******** [step = 150] loss: 0.634, acc: 0.853
******** [step = 200] loss: 0.638, acc: 0.853
******** [step = 250] loss: 0.641, acc: 0.852
******** [step = 300] loss: 0.648, acc: 0.851
******** [step = 350] loss: 0.655, acc: 0.850
******** [step = 400] loss: 0.658, acc: 0.850
******** [step = 450] loss: 0.662, acc: 0.849
******** [step = 500] loss: 0.665, acc: 0.849
******** [step = 550] loss: 0.669, acc: 0.848
******** [step = 600] loss: 0.672, acc: 0.848
******** [step = 650] loss: 0.676, acc: 0.848
******** [step = 700] loss: 0.680, acc: 0.847
******** [step = 750] loss: 0.684, acc: 0.847
******** [step = 800] loss: 0.686, acc: 0.846
******** [step = 850] loss: 0.689, acc: 0.846
EPOCH = 42 loss: 0.689, acc: 0.846, val_loss: 1.099, val_acc: 0.806

================================================================================2025-08_11 00:45:56
******** [step = 50] loss: 0.610, acc: 0.859
******** [step = 100] loss: 0.621, acc: 0.856
******** [step = 150] loss: 0.628, acc: 0.855
******** [step = 200] loss: 0.638, acc: 0.853
******** [step = 250] loss: 0.639, acc: 0.853
******** [step = 300] loss: 0.644, acc: 0.852
******** [step = 350] loss: 0.647, acc: 0.851
******** [step = 400] loss: 0.655, acc: 0.850
******** [step = 450] loss: 0.657, acc: 0.850
******** [step = 500] loss: 0.660, acc: 0.850
******** [step = 550] loss: 0.663, acc: 0.849
******** [step = 600] loss: 0.668, acc: 0.849
******** [step = 650] loss: 0.671, acc: 0.848
******** [step = 700] loss: 0.676, acc: 0.848
******** [step = 750] loss: 0.679, acc: 0.848
******** [step = 800] loss: 0.682, acc: 0.847
******** [step = 850] loss: 0.687, acc: 0.847
EPOCH = 43 loss: 0.687, acc: 0.847, val_loss: 1.103, val_acc: 0.808

================================================================================2025-08_11 00:47:17
******** [step = 50] loss: 0.605, acc: 0.857
******** [step = 100] loss: 0.620, acc: 0.856
******** [step = 150] loss: 0.621, acc: 0.856
******** [step = 200] loss: 0.628, acc: 0.855
******** [step = 250] loss: 0.632, acc: 0.854
******** [step = 300] loss: 0.635, acc: 0.853
******** [step = 350] loss: 0.637, acc: 0.853
******** [step = 400] loss: 0.641, acc: 0.853
******** [step = 450] loss: 0.646, acc: 0.852
******** [step = 500] loss: 0.652, acc: 0.851
******** [step = 550] loss: 0.656, acc: 0.851
******** [step = 600] loss: 0.660, acc: 0.850
******** [step = 650] loss: 0.663, acc: 0.850
******** [step = 700] loss: 0.667, acc: 0.849
******** [step = 750] loss: 0.672, acc: 0.848
******** [step = 800] loss: 0.676, acc: 0.848
******** [step = 850] loss: 0.677, acc: 0.848
EPOCH = 44 loss: 0.677, acc: 0.848, val_loss: 1.093, val_acc: 0.808

================================================================================2025-08_11 00:48:38
******** [step = 50] loss: 0.604, acc: 0.859
******** [step = 100] loss: 0.602, acc: 0.860
******** [step = 150] loss: 0.614, acc: 0.857
******** [step = 200] loss: 0.618, acc: 0.857
******** [step = 250] loss: 0.623, acc: 0.856
******** [step = 300] loss: 0.631, acc: 0.854
******** [step = 350] loss: 0.636, acc: 0.854
******** [step = 400] loss: 0.640, acc: 0.853
******** [step = 450] loss: 0.642, acc: 0.853
******** [step = 500] loss: 0.644, acc: 0.853
******** [step = 550] loss: 0.648, acc: 0.852
******** [step = 600] loss: 0.656, acc: 0.851
******** [step = 650] loss: 0.660, acc: 0.850
******** [step = 700] loss: 0.664, acc: 0.850
******** [step = 750] loss: 0.667, acc: 0.850
******** [step = 800] loss: 0.669, acc: 0.849
******** [step = 850] loss: 0.672, acc: 0.849
EPOCH = 45 loss: 0.672, acc: 0.849, val_loss: 1.095, val_acc: 0.808

================================================================================2025-08_11 00:49:59
******** [step = 50] loss: 0.601, acc: 0.859
******** [step = 100] loss: 0.603, acc: 0.859
******** [step = 150] loss: 0.604, acc: 0.859
******** [step = 200] loss: 0.605, acc: 0.859
******** [step = 250] loss: 0.611, acc: 0.858
******** [step = 300] loss: 0.619, acc: 0.857
******** [step = 350] loss: 0.624, acc: 0.856
******** [step = 400] loss: 0.630, acc: 0.855
******** [step = 450] loss: 0.636, acc: 0.855
******** [step = 500] loss: 0.640, acc: 0.854
******** [step = 550] loss: 0.644, acc: 0.853
******** [step = 600] loss: 0.649, acc: 0.853
******** [step = 650] loss: 0.653, acc: 0.852
******** [step = 700] loss: 0.658, acc: 0.851
******** [step = 750] loss: 0.661, acc: 0.851
******** [step = 800] loss: 0.664, acc: 0.851
******** [step = 850] loss: 0.666, acc: 0.851
EPOCH = 46 loss: 0.666, acc: 0.851, val_loss: 1.096, val_acc: 0.808

================================================================================2025-08_11 00:51:20
******** [step = 50] loss: 0.591, acc: 0.861
******** [step = 100] loss: 0.595, acc: 0.860
******** [step = 150] loss: 0.600, acc: 0.860
******** [step = 200] loss: 0.603, acc: 0.860
******** [step = 250] loss: 0.611, acc: 0.858
******** [step = 300] loss: 0.620, acc: 0.857
******** [step = 350] loss: 0.623, acc: 0.857
******** [step = 400] loss: 0.627, acc: 0.856
******** [step = 450] loss: 0.631, acc: 0.855
******** [step = 500] loss: 0.636, acc: 0.855
******** [step = 550] loss: 0.640, acc: 0.854
******** [step = 600] loss: 0.644, acc: 0.853
******** [step = 650] loss: 0.648, acc: 0.853
******** [step = 700] loss: 0.651, acc: 0.853
******** [step = 750] loss: 0.655, acc: 0.852
******** [step = 800] loss: 0.659, acc: 0.852
******** [step = 850] loss: 0.664, acc: 0.851
EPOCH = 47 loss: 0.664, acc: 0.851, val_loss: 1.095, val_acc: 0.809

================================================================================2025-08_11 00:52:40
******** [step = 50] loss: 0.587, acc: 0.861
******** [step = 100] loss: 0.590, acc: 0.861
******** [step = 150] loss: 0.596, acc: 0.860
******** [step = 200] loss: 0.601, acc: 0.860
******** [step = 250] loss: 0.604, acc: 0.859
******** [step = 300] loss: 0.610, acc: 0.858
******** [step = 350] loss: 0.615, acc: 0.857
******** [step = 400] loss: 0.621, acc: 0.856
******** [step = 450] loss: 0.627, acc: 0.855
******** [step = 500] loss: 0.630, acc: 0.855
******** [step = 550] loss: 0.635, acc: 0.855
******** [step = 600] loss: 0.640, acc: 0.854
******** [step = 650] loss: 0.644, acc: 0.853
******** [step = 700] loss: 0.648, acc: 0.853
******** [step = 750] loss: 0.651, acc: 0.853
******** [step = 800] loss: 0.654, acc: 0.852
******** [step = 850] loss: 0.657, acc: 0.852
EPOCH = 48 loss: 0.657, acc: 0.852, val_loss: 1.089, val_acc: 0.809

================================================================================2025-08_11 00:54:00
******** [step = 50] loss: 0.594, acc: 0.861
******** [step = 100] loss: 0.590, acc: 0.861
******** [step = 150] loss: 0.595, acc: 0.861
******** [step = 200] loss: 0.601, acc: 0.860
******** [step = 250] loss: 0.603, acc: 0.860
******** [step = 300] loss: 0.605, acc: 0.859
******** [step = 350] loss: 0.610, acc: 0.858
******** [step = 400] loss: 0.616, acc: 0.858
******** [step = 450] loss: 0.621, acc: 0.857
******** [step = 500] loss: 0.625, acc: 0.856
******** [step = 550] loss: 0.630, acc: 0.856
******** [step = 600] loss: 0.634, acc: 0.855
******** [step = 650] loss: 0.638, acc: 0.854
******** [step = 700] loss: 0.643, acc: 0.854
******** [step = 750] loss: 0.646, acc: 0.854
******** [step = 800] loss: 0.650, acc: 0.853
******** [step = 850] loss: 0.652, acc: 0.853
EPOCH = 49 loss: 0.652, acc: 0.853, val_loss: 1.091, val_acc: 0.810

================================================================================2025-08_11 00:55:24
******** [step = 50] loss: 0.589, acc: 0.862
******** [step = 100] loss: 0.586, acc: 0.863
******** [step = 150] loss: 0.591, acc: 0.862
******** [step = 200] loss: 0.598, acc: 0.861
******** [step = 250] loss: 0.604, acc: 0.860
******** [step = 300] loss: 0.609, acc: 0.859
******** [step = 350] loss: 0.611, acc: 0.859
******** [step = 400] loss: 0.617, acc: 0.858
******** [step = 450] loss: 0.623, acc: 0.857
******** [step = 500] loss: 0.627, acc: 0.857
******** [step = 550] loss: 0.629, acc: 0.857
******** [step = 600] loss: 0.632, acc: 0.856
******** [step = 650] loss: 0.635, acc: 0.856
******** [step = 700] loss: 0.637, acc: 0.856
******** [step = 750] loss: 0.639, acc: 0.855
******** [step = 800] loss: 0.644, acc: 0.855
******** [step = 850] loss: 0.647, acc: 0.854
EPOCH = 50 loss: 0.647, acc: 0.854, val_loss: 1.087, val_acc: 0.811

================================================================================2025-08_11 00:56:45
******** [step = 50] loss: 0.581, acc: 0.864
******** [step = 100] loss: 0.576, acc: 0.865
******** [step = 150] loss: 0.582, acc: 0.864
******** [step = 200] loss: 0.584, acc: 0.864
******** [step = 250] loss: 0.594, acc: 0.862
******** [step = 300] loss: 0.600, acc: 0.861
******** [step = 350] loss: 0.606, acc: 0.860
******** [step = 400] loss: 0.609, acc: 0.859
******** [step = 450] loss: 0.613, acc: 0.859
******** [step = 500] loss: 0.617, acc: 0.858
******** [step = 550] loss: 0.621, acc: 0.858
******** [step = 600] loss: 0.625, acc: 0.857
******** [step = 650] loss: 0.630, acc: 0.857
******** [step = 700] loss: 0.633, acc: 0.856
******** [step = 750] loss: 0.637, acc: 0.856
******** [step = 800] loss: 0.640, acc: 0.855
******** [step = 850] loss: 0.645, acc: 0.854
EPOCH = 51 loss: 0.645, acc: 0.854, val_loss: 1.090, val_acc: 0.811

================================================================================2025-08_11 00:58:05
******** [step = 50] loss: 0.563, acc: 0.866
******** [step = 100] loss: 0.571, acc: 0.865
******** [step = 150] loss: 0.579, acc: 0.864
******** [step = 200] loss: 0.583, acc: 0.864
******** [step = 250] loss: 0.587, acc: 0.863
******** [step = 300] loss: 0.592, acc: 0.862
******** [step = 350] loss: 0.597, acc: 0.861
******** [step = 400] loss: 0.604, acc: 0.860
******** [step = 450] loss: 0.610, acc: 0.859
******** [step = 500] loss: 0.613, acc: 0.859
******** [step = 550] loss: 0.618, acc: 0.859
******** [step = 600] loss: 0.620, acc: 0.858
******** [step = 650] loss: 0.623, acc: 0.858
******** [step = 700] loss: 0.629, acc: 0.857
******** [step = 750] loss: 0.634, acc: 0.857
******** [step = 800] loss: 0.637, acc: 0.856
******** [step = 850] loss: 0.639, acc: 0.856
EPOCH = 52 loss: 0.639, acc: 0.856, val_loss: 1.090, val_acc: 0.812

================================================================================2025-08_11 00:59:26
******** [step = 50] loss: 0.570, acc: 0.868
******** [step = 100] loss: 0.572, acc: 0.867
******** [step = 150] loss: 0.578, acc: 0.865
******** [step = 200] loss: 0.583, acc: 0.864
******** [step = 250] loss: 0.588, acc: 0.864
******** [step = 300] loss: 0.590, acc: 0.863
******** [step = 350] loss: 0.597, acc: 0.862
******** [step = 400] loss: 0.600, acc: 0.861
******** [step = 450] loss: 0.603, acc: 0.861
******** [step = 500] loss: 0.608, acc: 0.860
******** [step = 550] loss: 0.613, acc: 0.860
******** [step = 600] loss: 0.619, acc: 0.859
******** [step = 650] loss: 0.623, acc: 0.858
******** [step = 700] loss: 0.627, acc: 0.858
******** [step = 750] loss: 0.632, acc: 0.857
******** [step = 800] loss: 0.635, acc: 0.857
******** [step = 850] loss: 0.636, acc: 0.857
EPOCH = 53 loss: 0.636, acc: 0.857, val_loss: 1.085, val_acc: 0.812

================================================================================2025-08_11 01:00:46
******** [step = 50] loss: 0.561, acc: 0.868
******** [step = 100] loss: 0.563, acc: 0.868
******** [step = 150] loss: 0.570, acc: 0.866
******** [step = 200] loss: 0.571, acc: 0.866
******** [step = 250] loss: 0.577, acc: 0.865
******** [step = 300] loss: 0.582, acc: 0.864
******** [step = 350] loss: 0.586, acc: 0.864
******** [step = 400] loss: 0.593, acc: 0.863
******** [step = 450] loss: 0.597, acc: 0.862
******** [step = 500] loss: 0.603, acc: 0.861
******** [step = 550] loss: 0.609, acc: 0.860
******** [step = 600] loss: 0.613, acc: 0.860
******** [step = 650] loss: 0.617, acc: 0.859
******** [step = 700] loss: 0.621, acc: 0.859
******** [step = 750] loss: 0.624, acc: 0.859
******** [step = 800] loss: 0.627, acc: 0.858
******** [step = 850] loss: 0.631, acc: 0.858
EPOCH = 54 loss: 0.631, acc: 0.858, val_loss: 1.093, val_acc: 0.812

================================================================================2025-08_11 01:02:06
******** [step = 50] loss: 0.553, acc: 0.868
******** [step = 100] loss: 0.561, acc: 0.867
******** [step = 150] loss: 0.569, acc: 0.866
******** [step = 200] loss: 0.573, acc: 0.866
******** [step = 250] loss: 0.579, acc: 0.865
******** [step = 300] loss: 0.582, acc: 0.864
******** [step = 350] loss: 0.587, acc: 0.863
******** [step = 400] loss: 0.593, acc: 0.863
******** [step = 450] loss: 0.596, acc: 0.862
******** [step = 500] loss: 0.601, acc: 0.862
******** [step = 550] loss: 0.605, acc: 0.861
******** [step = 600] loss: 0.608, acc: 0.861
******** [step = 650] loss: 0.611, acc: 0.860
******** [step = 700] loss: 0.616, acc: 0.860
******** [step = 750] loss: 0.620, acc: 0.859
******** [step = 800] loss: 0.623, acc: 0.859
******** [step = 850] loss: 0.625, acc: 0.859
EPOCH = 55 loss: 0.625, acc: 0.859, val_loss: 1.084, val_acc: 0.812

================================================================================2025-08_11 01:03:26
******** [step = 50] loss: 0.534, acc: 0.873
******** [step = 100] loss: 0.555, acc: 0.870
******** [step = 150] loss: 0.568, acc: 0.867
******** [step = 200] loss: 0.570, acc: 0.867
******** [step = 250] loss: 0.573, acc: 0.866
******** [step = 300] loss: 0.578, acc: 0.865
******** [step = 350] loss: 0.584, acc: 0.865
******** [step = 400] loss: 0.589, acc: 0.864
******** [step = 450] loss: 0.592, acc: 0.863
******** [step = 500] loss: 0.596, acc: 0.863
******** [step = 550] loss: 0.599, acc: 0.863
******** [step = 600] loss: 0.603, acc: 0.862
******** [step = 650] loss: 0.607, acc: 0.862
******** [step = 700] loss: 0.610, acc: 0.861
******** [step = 750] loss: 0.615, acc: 0.861
******** [step = 800] loss: 0.620, acc: 0.860
******** [step = 850] loss: 0.622, acc: 0.860
EPOCH = 56 loss: 0.622, acc: 0.860, val_loss: 1.086, val_acc: 0.813

================================================================================2025-08_11 01:04:46
******** [step = 50] loss: 0.555, acc: 0.870
******** [step = 100] loss: 0.558, acc: 0.870
******** [step = 150] loss: 0.559, acc: 0.869
******** [step = 200] loss: 0.566, acc: 0.867
******** [step = 250] loss: 0.569, acc: 0.867
******** [step = 300] loss: 0.576, acc: 0.866
******** [step = 350] loss: 0.579, acc: 0.866
******** [step = 400] loss: 0.582, acc: 0.865
******** [step = 450] loss: 0.586, acc: 0.864
******** [step = 500] loss: 0.591, acc: 0.864
******** [step = 550] loss: 0.595, acc: 0.863
******** [step = 600] loss: 0.599, acc: 0.863
******** [step = 650] loss: 0.603, acc: 0.862
******** [step = 700] loss: 0.608, acc: 0.862
******** [step = 750] loss: 0.612, acc: 0.861
******** [step = 800] loss: 0.615, acc: 0.861
******** [step = 850] loss: 0.619, acc: 0.860
EPOCH = 57 loss: 0.619, acc: 0.860, val_loss: 1.086, val_acc: 0.813

================================================================================2025-08_11 01:06:06
******** [step = 50] loss: 0.542, acc: 0.873
******** [step = 100] loss: 0.549, acc: 0.871
******** [step = 150] loss: 0.553, acc: 0.870
******** [step = 200] loss: 0.558, acc: 0.869
******** [step = 250] loss: 0.565, acc: 0.868
******** [step = 300] loss: 0.568, acc: 0.868
******** [step = 350] loss: 0.571, acc: 0.867
******** [step = 400] loss: 0.575, acc: 0.866
******** [step = 450] loss: 0.582, acc: 0.865
******** [step = 500] loss: 0.587, acc: 0.865
******** [step = 550] loss: 0.592, acc: 0.864
******** [step = 600] loss: 0.598, acc: 0.863
******** [step = 650] loss: 0.600, acc: 0.863
******** [step = 700] loss: 0.604, acc: 0.862
******** [step = 750] loss: 0.608, acc: 0.862
******** [step = 800] loss: 0.611, acc: 0.862
******** [step = 850] loss: 0.616, acc: 0.861
EPOCH = 58 loss: 0.616, acc: 0.861, val_loss: 1.089, val_acc: 0.813

================================================================================2025-08_11 01:07:26
******** [step = 50] loss: 0.550, acc: 0.871
******** [step = 100] loss: 0.543, acc: 0.872
******** [step = 150] loss: 0.549, acc: 0.870
******** [step = 200] loss: 0.555, acc: 0.870
******** [step = 250] loss: 0.560, acc: 0.869
******** [step = 300] loss: 0.565, acc: 0.868
******** [step = 350] loss: 0.569, acc: 0.868
******** [step = 400] loss: 0.575, acc: 0.867
******** [step = 450] loss: 0.580, acc: 0.866
******** [step = 500] loss: 0.584, acc: 0.865
******** [step = 550] loss: 0.589, acc: 0.865
******** [step = 600] loss: 0.592, acc: 0.864
******** [step = 650] loss: 0.597, acc: 0.864
******** [step = 700] loss: 0.600, acc: 0.863
******** [step = 750] loss: 0.604, acc: 0.863
******** [step = 800] loss: 0.607, acc: 0.862
******** [step = 850] loss: 0.611, acc: 0.862
EPOCH = 59 loss: 0.611, acc: 0.862, val_loss: 1.085, val_acc: 0.813

================================================================================2025-08_11 01:08:46
******** [step = 50] loss: 0.543, acc: 0.873
******** [step = 100] loss: 0.547, acc: 0.871
******** [step = 150] loss: 0.548, acc: 0.871
******** [step = 200] loss: 0.556, acc: 0.870
******** [step = 250] loss: 0.560, acc: 0.869
******** [step = 300] loss: 0.565, acc: 0.869
******** [step = 350] loss: 0.569, acc: 0.868
******** [step = 400] loss: 0.572, acc: 0.867
******** [step = 450] loss: 0.577, acc: 0.867
******** [step = 500] loss: 0.582, acc: 0.866
******** [step = 550] loss: 0.585, acc: 0.866
******** [step = 600] loss: 0.588, acc: 0.865
******** [step = 650] loss: 0.592, acc: 0.865
******** [step = 700] loss: 0.596, acc: 0.864
******** [step = 750] loss: 0.600, acc: 0.864
******** [step = 800] loss: 0.604, acc: 0.863
******** [step = 850] loss: 0.609, acc: 0.863
EPOCH = 60 loss: 0.609, acc: 0.863, val_loss: 1.080, val_acc: 0.813

================================================================================2025-08_11 01:10:07
******** [step = 50] loss: 0.532, acc: 0.874
******** [step = 100] loss: 0.535, acc: 0.874
******** [step = 150] loss: 0.540, acc: 0.873
******** [step = 200] loss: 0.543, acc: 0.872
******** [step = 250] loss: 0.547, acc: 0.871
******** [step = 300] loss: 0.553, acc: 0.870
******** [step = 350] loss: 0.558, acc: 0.869
******** [step = 400] loss: 0.564, acc: 0.869
******** [step = 450] loss: 0.569, acc: 0.868
******** [step = 500] loss: 0.575, acc: 0.867
******** [step = 550] loss: 0.579, acc: 0.866
******** [step = 600] loss: 0.583, acc: 0.866
******** [step = 650] loss: 0.587, acc: 0.865
******** [step = 700] loss: 0.590, acc: 0.865
******** [step = 750] loss: 0.595, acc: 0.864
******** [step = 800] loss: 0.600, acc: 0.864
******** [step = 850] loss: 0.603, acc: 0.863
EPOCH = 61 loss: 0.603, acc: 0.863, val_loss: 1.082, val_acc: 0.814

================================================================================2025-08_11 01:11:27
******** [step = 50] loss: 0.534, acc: 0.874
******** [step = 100] loss: 0.538, acc: 0.873
******** [step = 150] loss: 0.544, acc: 0.872
******** [step = 200] loss: 0.547, acc: 0.871
******** [step = 250] loss: 0.548, acc: 0.871
******** [step = 300] loss: 0.554, acc: 0.870
******** [step = 350] loss: 0.560, acc: 0.869
******** [step = 400] loss: 0.565, acc: 0.868
******** [step = 450] loss: 0.567, acc: 0.868
******** [step = 500] loss: 0.571, acc: 0.867
******** [step = 550] loss: 0.577, acc: 0.867
******** [step = 600] loss: 0.583, acc: 0.866
******** [step = 650] loss: 0.586, acc: 0.866
******** [step = 700] loss: 0.590, acc: 0.865
******** [step = 750] loss: 0.592, acc: 0.865
******** [step = 800] loss: 0.595, acc: 0.864
******** [step = 850] loss: 0.599, acc: 0.864
EPOCH = 62 loss: 0.599, acc: 0.864, val_loss: 1.090, val_acc: 0.813

================================================================================2025-08_11 01:12:47
******** [step = 50] loss: 0.531, acc: 0.875
******** [step = 100] loss: 0.528, acc: 0.876
******** [step = 150] loss: 0.534, acc: 0.875
******** [step = 200] loss: 0.540, acc: 0.874
******** [step = 250] loss: 0.545, acc: 0.872
******** [step = 300] loss: 0.550, acc: 0.871
******** [step = 350] loss: 0.556, acc: 0.870
******** [step = 400] loss: 0.560, acc: 0.869
******** [step = 450] loss: 0.566, acc: 0.869
******** [step = 500] loss: 0.570, acc: 0.868
******** [step = 550] loss: 0.575, acc: 0.867
******** [step = 600] loss: 0.579, acc: 0.867
******** [step = 650] loss: 0.583, acc: 0.867
******** [step = 700] loss: 0.586, acc: 0.866
******** [step = 750] loss: 0.589, acc: 0.866
******** [step = 800] loss: 0.592, acc: 0.865
******** [step = 850] loss: 0.595, acc: 0.865
EPOCH = 63 loss: 0.595, acc: 0.865, val_loss: 1.085, val_acc: 0.814

================================================================================2025-08_11 01:14:07
******** [step = 50] loss: 0.510, acc: 0.879
******** [step = 100] loss: 0.523, acc: 0.877
******** [step = 150] loss: 0.527, acc: 0.876
******** [step = 200] loss: 0.535, acc: 0.874
******** [step = 250] loss: 0.541, acc: 0.872
******** [step = 300] loss: 0.547, acc: 0.871
******** [step = 350] loss: 0.552, acc: 0.871
******** [step = 400] loss: 0.558, acc: 0.870
******** [step = 450] loss: 0.562, acc: 0.869
******** [step = 500] loss: 0.565, acc: 0.869
******** [step = 550] loss: 0.570, acc: 0.868
******** [step = 600] loss: 0.575, acc: 0.867
******** [step = 650] loss: 0.578, acc: 0.867
******** [step = 700] loss: 0.582, acc: 0.866
******** [step = 750] loss: 0.586, acc: 0.866
******** [step = 800] loss: 0.590, acc: 0.866
******** [step = 850] loss: 0.592, acc: 0.865
EPOCH = 64 loss: 0.592, acc: 0.865, val_loss: 1.081, val_acc: 0.815

================================================================================2025-08_11 01:15:27
******** [step = 50] loss: 0.522, acc: 0.876
******** [step = 100] loss: 0.526, acc: 0.875
******** [step = 150] loss: 0.532, acc: 0.874
******** [step = 200] loss: 0.533, acc: 0.874
******** [step = 250] loss: 0.538, acc: 0.873
******** [step = 300] loss: 0.541, acc: 0.873
******** [step = 350] loss: 0.545, acc: 0.872
******** [step = 400] loss: 0.549, acc: 0.871
******** [step = 450] loss: 0.554, acc: 0.870
******** [step = 500] loss: 0.559, acc: 0.870
******** [step = 550] loss: 0.564, acc: 0.869
******** [step = 600] loss: 0.569, acc: 0.868
******** [step = 650] loss: 0.576, acc: 0.867
******** [step = 700] loss: 0.580, acc: 0.867
******** [step = 750] loss: 0.583, acc: 0.867
******** [step = 800] loss: 0.588, acc: 0.866
******** [step = 850] loss: 0.592, acc: 0.866
EPOCH = 65 loss: 0.592, acc: 0.866, val_loss: 1.080, val_acc: 0.815

================================================================================2025-08_11 01:16:50
******** [step = 50] loss: 0.516, acc: 0.876
******** [step = 100] loss: 0.525, acc: 0.874
******** [step = 150] loss: 0.529, acc: 0.874
******** [step = 200] loss: 0.532, acc: 0.873
******** [step = 250] loss: 0.537, acc: 0.873
******** [step = 300] loss: 0.540, acc: 0.873
******** [step = 350] loss: 0.546, acc: 0.872
******** [step = 400] loss: 0.553, acc: 0.871
******** [step = 450] loss: 0.556, acc: 0.871
******** [step = 500] loss: 0.561, acc: 0.870
******** [step = 550] loss: 0.565, acc: 0.869
******** [step = 600] loss: 0.569, acc: 0.869
******** [step = 650] loss: 0.573, acc: 0.868
******** [step = 700] loss: 0.577, acc: 0.868
******** [step = 750] loss: 0.580, acc: 0.867
******** [step = 800] loss: 0.585, acc: 0.867
******** [step = 850] loss: 0.587, acc: 0.866
EPOCH = 66 loss: 0.587, acc: 0.866, val_loss: 1.084, val_acc: 0.815

================================================================================2025-08_11 01:18:09
******** [step = 50] loss: 0.519, acc: 0.878
******** [step = 100] loss: 0.518, acc: 0.877
******** [step = 150] loss: 0.528, acc: 0.875
******** [step = 200] loss: 0.532, acc: 0.874
******** [step = 250] loss: 0.530, acc: 0.874
******** [step = 300] loss: 0.539, acc: 0.873
******** [step = 350] loss: 0.541, acc: 0.873
******** [step = 400] loss: 0.546, acc: 0.872
******** [step = 450] loss: 0.550, acc: 0.872
******** [step = 500] loss: 0.554, acc: 0.871
******** [step = 550] loss: 0.560, acc: 0.870
******** [step = 600] loss: 0.564, acc: 0.870
******** [step = 650] loss: 0.568, acc: 0.869
******** [step = 700] loss: 0.571, acc: 0.869
******** [step = 750] loss: 0.575, acc: 0.868
******** [step = 800] loss: 0.580, acc: 0.868
******** [step = 850] loss: 0.583, acc: 0.867
EPOCH = 67 loss: 0.583, acc: 0.867, val_loss: 1.080, val_acc: 0.817

================================================================================2025-08_11 01:19:30
******** [step = 50] loss: 0.515, acc: 0.878
******** [step = 100] loss: 0.524, acc: 0.877
******** [step = 150] loss: 0.530, acc: 0.875
******** [step = 200] loss: 0.535, acc: 0.874
******** [step = 250] loss: 0.541, acc: 0.873
******** [step = 300] loss: 0.543, acc: 0.873
******** [step = 350] loss: 0.546, acc: 0.873
******** [step = 400] loss: 0.550, acc: 0.872
******** [step = 450] loss: 0.552, acc: 0.872
******** [step = 500] loss: 0.556, acc: 0.871
******** [step = 550] loss: 0.560, acc: 0.870
******** [step = 600] loss: 0.565, acc: 0.870
******** [step = 650] loss: 0.569, acc: 0.869
******** [step = 700] loss: 0.572, acc: 0.869
******** [step = 750] loss: 0.575, acc: 0.868
******** [step = 800] loss: 0.579, acc: 0.868
******** [step = 850] loss: 0.580, acc: 0.868
EPOCH = 68 loss: 0.580, acc: 0.868, val_loss: 1.081, val_acc: 0.816

================================================================================2025-08_11 01:20:51
******** [step = 50] loss: 0.502, acc: 0.881
******** [step = 100] loss: 0.510, acc: 0.879
******** [step = 150] loss: 0.519, acc: 0.877
******** [step = 200] loss: 0.525, acc: 0.876
******** [step = 250] loss: 0.531, acc: 0.875
******** [step = 300] loss: 0.533, acc: 0.874
******** [step = 350] loss: 0.537, acc: 0.874
******** [step = 400] loss: 0.543, acc: 0.873
******** [step = 450] loss: 0.547, acc: 0.872
******** [step = 500] loss: 0.551, acc: 0.872
******** [step = 550] loss: 0.556, acc: 0.871
******** [step = 600] loss: 0.559, acc: 0.871
******** [step = 650] loss: 0.563, acc: 0.870
******** [step = 700] loss: 0.568, acc: 0.870
******** [step = 750] loss: 0.572, acc: 0.869
******** [step = 800] loss: 0.575, acc: 0.869
******** [step = 850] loss: 0.578, acc: 0.869
EPOCH = 69 loss: 0.578, acc: 0.869, val_loss: 1.082, val_acc: 0.815

================================================================================2025-08_11 01:22:14
******** [step = 50] loss: 0.512, acc: 0.880
******** [step = 100] loss: 0.516, acc: 0.878
******** [step = 150] loss: 0.523, acc: 0.877
******** [step = 200] loss: 0.526, acc: 0.876
******** [step = 250] loss: 0.529, acc: 0.876
******** [step = 300] loss: 0.531, acc: 0.875
******** [step = 350] loss: 0.535, acc: 0.875
******** [step = 400] loss: 0.540, acc: 0.874
******** [step = 450] loss: 0.543, acc: 0.874
******** [step = 500] loss: 0.547, acc: 0.873
******** [step = 550] loss: 0.551, acc: 0.872
******** [step = 600] loss: 0.555, acc: 0.872
******** [step = 650] loss: 0.559, acc: 0.871
******** [step = 700] loss: 0.563, acc: 0.871
******** [step = 750] loss: 0.567, acc: 0.870
******** [step = 800] loss: 0.572, acc: 0.870
******** [step = 850] loss: 0.575, acc: 0.869
EPOCH = 70 loss: 0.575, acc: 0.869, val_loss: 1.079, val_acc: 0.817

================================================================================2025-08_11 01:23:34
******** [step = 50] loss: 0.502, acc: 0.880
******** [step = 100] loss: 0.513, acc: 0.878
******** [step = 150] loss: 0.513, acc: 0.878
******** [step = 200] loss: 0.517, acc: 0.877
******** [step = 250] loss: 0.523, acc: 0.876
******** [step = 300] loss: 0.527, acc: 0.875
******** [step = 350] loss: 0.531, acc: 0.875
******** [step = 400] loss: 0.535, acc: 0.874
******** [step = 450] loss: 0.539, acc: 0.874
******** [step = 500] loss: 0.543, acc: 0.873
******** [step = 550] loss: 0.549, acc: 0.872
******** [step = 600] loss: 0.554, acc: 0.872
******** [step = 650] loss: 0.557, acc: 0.871
******** [step = 700] loss: 0.562, acc: 0.871
******** [step = 750] loss: 0.565, acc: 0.871
******** [step = 800] loss: 0.569, acc: 0.870
******** [step = 850] loss: 0.572, acc: 0.870
EPOCH = 71 loss: 0.572, acc: 0.870, val_loss: 1.080, val_acc: 0.816

================================================================================2025-08_11 01:24:59
******** [step = 50] loss: 0.498, acc: 0.881
******** [step = 100] loss: 0.502, acc: 0.880
******** [step = 150] loss: 0.503, acc: 0.880
******** [step = 200] loss: 0.510, acc: 0.879
******** [step = 250] loss: 0.515, acc: 0.878
******** [step = 300] loss: 0.521, acc: 0.877
******** [step = 350] loss: 0.527, acc: 0.876
******** [step = 400] loss: 0.532, acc: 0.875
******** [step = 450] loss: 0.537, acc: 0.874
******** [step = 500] loss: 0.542, acc: 0.874
******** [step = 550] loss: 0.547, acc: 0.873
******** [step = 600] loss: 0.550, acc: 0.873
******** [step = 650] loss: 0.553, acc: 0.872
******** [step = 700] loss: 0.557, acc: 0.872
******** [step = 750] loss: 0.562, acc: 0.871
******** [step = 800] loss: 0.566, acc: 0.871
******** [step = 850] loss: 0.570, acc: 0.870
EPOCH = 72 loss: 0.570, acc: 0.870, val_loss: 1.076, val_acc: 0.817

================================================================================2025-08_11 01:26:21
******** [step = 50] loss: 0.500, acc: 0.881
******** [step = 100] loss: 0.511, acc: 0.879
******** [step = 150] loss: 0.514, acc: 0.879
******** [step = 200] loss: 0.511, acc: 0.879
******** [step = 250] loss: 0.519, acc: 0.878
******** [step = 300] loss: 0.521, acc: 0.877
******** [step = 350] loss: 0.524, acc: 0.877
******** [step = 400] loss: 0.529, acc: 0.876
******** [step = 450] loss: 0.534, acc: 0.876
******** [step = 500] loss: 0.538, acc: 0.875
******** [step = 550] loss: 0.542, acc: 0.874
******** [step = 600] loss: 0.547, acc: 0.874
******** [step = 650] loss: 0.552, acc: 0.873
******** [step = 700] loss: 0.556, acc: 0.872
******** [step = 750] loss: 0.561, acc: 0.872
******** [step = 800] loss: 0.563, acc: 0.871
******** [step = 850] loss: 0.567, acc: 0.871
EPOCH = 73 loss: 0.567, acc: 0.871, val_loss: 1.076, val_acc: 0.817

================================================================================2025-08_11 01:27:44
******** [step = 50] loss: 0.489, acc: 0.882
******** [step = 100] loss: 0.500, acc: 0.881
******** [step = 150] loss: 0.502, acc: 0.880
******** [step = 200] loss: 0.509, acc: 0.879
******** [step = 250] loss: 0.515, acc: 0.878
******** [step = 300] loss: 0.522, acc: 0.877
******** [step = 350] loss: 0.526, acc: 0.876
******** [step = 400] loss: 0.530, acc: 0.876
******** [step = 450] loss: 0.534, acc: 0.875
******** [step = 500] loss: 0.539, acc: 0.875
******** [step = 550] loss: 0.544, acc: 0.874
******** [step = 600] loss: 0.547, acc: 0.874
******** [step = 650] loss: 0.552, acc: 0.873
******** [step = 700] loss: 0.555, acc: 0.873
******** [step = 750] loss: 0.559, acc: 0.872
******** [step = 800] loss: 0.561, acc: 0.872
******** [step = 850] loss: 0.565, acc: 0.871
EPOCH = 74 loss: 0.565, acc: 0.871, val_loss: 1.075, val_acc: 0.817

================================================================================2025-08_11 01:29:04
******** [step = 50] loss: 0.487, acc: 0.883
******** [step = 100] loss: 0.499, acc: 0.881
******** [step = 150] loss: 0.505, acc: 0.880
******** [step = 200] loss: 0.510, acc: 0.880
******** [step = 250] loss: 0.517, acc: 0.878
******** [step = 300] loss: 0.522, acc: 0.878
******** [step = 350] loss: 0.523, acc: 0.877
******** [step = 400] loss: 0.528, acc: 0.877
******** [step = 450] loss: 0.532, acc: 0.876
******** [step = 500] loss: 0.538, acc: 0.875
******** [step = 550] loss: 0.542, acc: 0.874
******** [step = 600] loss: 0.546, acc: 0.874
******** [step = 650] loss: 0.548, acc: 0.874
******** [step = 700] loss: 0.552, acc: 0.873
******** [step = 750] loss: 0.555, acc: 0.873
******** [step = 800] loss: 0.559, acc: 0.872
******** [step = 850] loss: 0.562, acc: 0.872
EPOCH = 75 loss: 0.562, acc: 0.872, val_loss: 1.074, val_acc: 0.818

================================================================================2025-08_11 01:30:24
******** [step = 50] loss: 0.487, acc: 0.884
******** [step = 100] loss: 0.494, acc: 0.883
******** [step = 150] loss: 0.497, acc: 0.882
******** [step = 200] loss: 0.508, acc: 0.880
******** [step = 250] loss: 0.510, acc: 0.879
******** [step = 300] loss: 0.515, acc: 0.879
******** [step = 350] loss: 0.520, acc: 0.878
******** [step = 400] loss: 0.525, acc: 0.877
******** [step = 450] loss: 0.529, acc: 0.876
******** [step = 500] loss: 0.533, acc: 0.876
******** [step = 550] loss: 0.538, acc: 0.875
******** [step = 600] loss: 0.541, acc: 0.875
******** [step = 650] loss: 0.545, acc: 0.874
******** [step = 700] loss: 0.549, acc: 0.874
******** [step = 750] loss: 0.553, acc: 0.873
******** [step = 800] loss: 0.557, acc: 0.873
******** [step = 850] loss: 0.565, acc: 0.872
EPOCH = 76 loss: 0.565, acc: 0.872, val_loss: 1.075, val_acc: 0.817

================================================================================2025-08_11 01:31:44
******** [step = 50] loss: 0.496, acc: 0.881
******** [step = 100] loss: 0.497, acc: 0.880
******** [step = 150] loss: 0.504, acc: 0.879
******** [step = 200] loss: 0.504, acc: 0.879
******** [step = 250] loss: 0.508, acc: 0.879
******** [step = 300] loss: 0.514, acc: 0.878
******** [step = 350] loss: 0.521, acc: 0.877
******** [step = 400] loss: 0.524, acc: 0.876
******** [step = 450] loss: 0.527, acc: 0.876
******** [step = 500] loss: 0.530, acc: 0.876
******** [step = 550] loss: 0.534, acc: 0.875
******** [step = 600] loss: 0.538, acc: 0.875
******** [step = 650] loss: 0.542, acc: 0.874
******** [step = 700] loss: 0.546, acc: 0.874
******** [step = 750] loss: 0.549, acc: 0.874
******** [step = 800] loss: 0.553, acc: 0.873
******** [step = 850] loss: 0.557, acc: 0.873
EPOCH = 77 loss: 0.557, acc: 0.873, val_loss: 1.076, val_acc: 0.817

================================================================================2025-08_11 01:33:07
******** [step = 50] loss: 0.509, acc: 0.879
******** [step = 100] loss: 0.501, acc: 0.881
******** [step = 150] loss: 0.499, acc: 0.881
******** [step = 200] loss: 0.505, acc: 0.880
******** [step = 250] loss: 0.510, acc: 0.879
******** [step = 300] loss: 0.511, acc: 0.879
******** [step = 350] loss: 0.516, acc: 0.879
******** [step = 400] loss: 0.518, acc: 0.878
******** [step = 450] loss: 0.521, acc: 0.878
******** [step = 500] loss: 0.526, acc: 0.877
******** [step = 550] loss: 0.530, acc: 0.876
******** [step = 600] loss: 0.535, acc: 0.876
******** [step = 650] loss: 0.539, acc: 0.875
******** [step = 700] loss: 0.543, acc: 0.875
******** [step = 750] loss: 0.547, acc: 0.874
******** [step = 800] loss: 0.550, acc: 0.874
******** [step = 850] loss: 0.554, acc: 0.873
EPOCH = 78 loss: 0.554, acc: 0.873, val_loss: 1.081, val_acc: 0.818

================================================================================2025-08_11 01:34:29
******** [step = 50] loss: 0.482, acc: 0.884
******** [step = 100] loss: 0.482, acc: 0.884
******** [step = 150] loss: 0.488, acc: 0.882
******** [step = 200] loss: 0.497, acc: 0.881
******** [step = 250] loss: 0.501, acc: 0.881
******** [step = 300] loss: 0.505, acc: 0.880
******** [step = 350] loss: 0.508, acc: 0.880
******** [step = 400] loss: 0.514, acc: 0.879
******** [step = 450] loss: 0.520, acc: 0.878
******** [step = 500] loss: 0.525, acc: 0.877
******** [step = 550] loss: 0.529, acc: 0.877
******** [step = 600] loss: 0.534, acc: 0.876
******** [step = 650] loss: 0.537, acc: 0.876
******** [step = 700] loss: 0.540, acc: 0.875
******** [step = 750] loss: 0.544, acc: 0.875
******** [step = 800] loss: 0.547, acc: 0.875
******** [step = 850] loss: 0.553, acc: 0.874
EPOCH = 79 loss: 0.553, acc: 0.874, val_loss: 1.078, val_acc: 0.818

================================================================================2025-08_11 01:35:50
******** [step = 50] loss: 0.480, acc: 0.885
******** [step = 100] loss: 0.487, acc: 0.883
******** [step = 150] loss: 0.489, acc: 0.883
******** [step = 200] loss: 0.497, acc: 0.881
******** [step = 250] loss: 0.503, acc: 0.880
******** [step = 300] loss: 0.507, acc: 0.879
******** [step = 350] loss: 0.512, acc: 0.879
******** [step = 400] loss: 0.515, acc: 0.878
******** [step = 450] loss: 0.519, acc: 0.878
******** [step = 500] loss: 0.523, acc: 0.877
******** [step = 550] loss: 0.527, acc: 0.877
******** [step = 600] loss: 0.531, acc: 0.876
******** [step = 650] loss: 0.535, acc: 0.876
******** [step = 700] loss: 0.539, acc: 0.875
******** [step = 750] loss: 0.541, acc: 0.875
******** [step = 800] loss: 0.545, acc: 0.874
******** [step = 850] loss: 0.548, acc: 0.874
EPOCH = 80 loss: 0.548, acc: 0.874, val_loss: 1.077, val_acc: 0.819

================================================================================2025-08_11 01:37:15
finishing training...
Training complete in 108m 47s
    epoch  ...   val_acc
0     1.0  ...  0.451495
1     2.0  ...  0.543780
2     3.0  ...  0.600934
3     4.0  ...  0.644118
4     5.0  ...  0.673163
..    ...  ...       ...
75   76.0  ...  0.816975
76   77.0  ...  0.817270
77   78.0  ...  0.817809
78   79.0  ...  0.818425
79   80.0  ...  0.818535

[80 rows x 5 columns]
== Done ==
Mon Aug 11 01:37:41 AM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-11-2025 01:37:41
Job ID:        6787012
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=07:27:28,vmem=0,walltime=01:51:52,mem=34916K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-03-006-31-0
---------------------------------------
