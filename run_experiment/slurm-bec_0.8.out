---------------------------------------
Begin Slurm Prolog: Aug-10-2025 00:36:07
Job ID:    6778588
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 12:36:07 AM EDT 2025
atl1-1-02-007-31-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 00:36:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   45C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 105994.47it/s] 28%|██▊       | 38431/135842 [00:00<00:00, 158087.40it/s] 41%|████      | 55501/135842 [00:00<00:00, 120176.98it/s] 51%|█████     | 69378/135842 [00:00<00:00, 94351.20it/s]  66%|██████▌   | 89314/135842 [00:00<00:00, 119777.98it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90756.75it/s] 92%|█████████▏| 125417/135842 [00:01<00:00, 109000.97it/s]100%|██████████| 135842/135842 [00:01<00:00, 111987.07it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47706.89it/s] 33%|███▎      | 35660/108673 [00:00<00:00, 85692.64it/s] 49%|████▉     | 53736/108673 [00:00<00:00, 112956.11it/s] 66%|██████▌   | 71935/108673 [00:00<00:00, 133016.47it/s] 81%|████████  | 88294/108673 [00:01<00:00, 71548.98it/s]  98%|█████████▊| 106248/108673 [00:01<00:00, 90378.74it/s]100%|██████████| 108673/108673 [00:01<00:00, 89551.91it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17875/27169 [00:00<00:00, 178740.80it/s]100%|██████████| 27169/27169 [00:00<00:00, 179795.69it/s]
tensor([   3,    5,  411,   13,    5, 1522,   66,   95,   14,  361,  123,    5,
          21, 5214,    9,    6,   42,   41,  668,    4,    2,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([  3,  84,   6, 282, 105,   7,  43,   7,  74,  33,  97,  82,  35,  84,
         15,  18, 132,  71,   4,   2,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1]) torch.int64
tensor([   3,    6,  280,   87,    7,  417,   96,   55,  173,  104,  107,    6,
        3958,    4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_10 00:36:36
******** [step = 50] loss: 9.431, acc: 0.001
******** [step = 100] loss: 9.037, acc: 0.073
******** [step = 150] loss: 8.679, acc: 0.118
******** [step = 200] loss: 8.352, acc: 0.150
******** [step = 250] loss: 8.036, acc: 0.170
******** [step = 300] loss: 7.729, acc: 0.184
******** [step = 350] loss: 7.427, acc: 0.196
******** [step = 400] loss: 7.147, acc: 0.206
******** [step = 450] loss: 6.898, acc: 0.216
******** [step = 500] loss: 6.677, acc: 0.225
******** [step = 550] loss: 6.481, acc: 0.234
******** [step = 600] loss: 6.309, acc: 0.242
******** [step = 650] loss: 6.155, acc: 0.249
******** [step = 700] loss: 6.018, acc: 0.255
******** [step = 750] loss: 5.894, acc: 0.261
******** [step = 800] loss: 5.782, acc: 0.267
******** [step = 850] loss: 5.680, acc: 0.272
EPOCH = 1 loss: 5.680, acc: 0.272, val_loss: 3.921, val_acc: 0.365

================================================================================2025-08_10 00:37:57
******** [step = 50] loss: 3.999, acc: 0.351
******** [step = 100] loss: 3.969, acc: 0.352
******** [step = 150] loss: 3.940, acc: 0.355
******** [step = 200] loss: 3.919, acc: 0.356
******** [step = 250] loss: 3.897, acc: 0.358
******** [step = 300] loss: 3.874, acc: 0.359
******** [step = 350] loss: 3.856, acc: 0.361
******** [step = 400] loss: 3.837, acc: 0.362
******** [step = 450] loss: 3.822, acc: 0.363
******** [step = 500] loss: 3.805, acc: 0.364
******** [step = 550] loss: 3.786, acc: 0.366
******** [step = 600] loss: 3.769, acc: 0.367
******** [step = 650] loss: 3.751, acc: 0.368
******** [step = 700] loss: 3.736, acc: 0.369
******** [step = 750] loss: 3.722, acc: 0.370
******** [step = 800] loss: 3.708, acc: 0.372
******** [step = 850] loss: 3.693, acc: 0.373
EPOCH = 2 loss: 3.693, acc: 0.373, val_loss: 3.437, val_acc: 0.400

================================================================================2025-08_10 00:39:16
******** [step = 50] loss: 3.452, acc: 0.389
******** [step = 100] loss: 3.435, acc: 0.390
******** [step = 150] loss: 3.423, acc: 0.391
******** [step = 200] loss: 3.410, acc: 0.393
******** [step = 250] loss: 3.402, acc: 0.394
******** [step = 300] loss: 3.395, acc: 0.395
******** [step = 350] loss: 3.388, acc: 0.397
******** [step = 400] loss: 3.380, acc: 0.398
******** [step = 450] loss: 3.377, acc: 0.398
******** [step = 500] loss: 3.371, acc: 0.399
******** [step = 550] loss: 3.365, acc: 0.400
******** [step = 600] loss: 3.363, acc: 0.401
******** [step = 650] loss: 3.358, acc: 0.401
******** [step = 700] loss: 3.353, acc: 0.402
******** [step = 750] loss: 3.349, acc: 0.403
******** [step = 800] loss: 3.343, acc: 0.404
******** [step = 850] loss: 3.336, acc: 0.404
EPOCH = 3 loss: 3.336, acc: 0.404, val_loss: 3.198, val_acc: 0.426

================================================================================2025-08_10 00:40:36
******** [step = 50] loss: 3.212, acc: 0.413
******** [step = 100] loss: 3.195, acc: 0.415
******** [step = 150] loss: 3.190, acc: 0.416
******** [step = 200] loss: 3.189, acc: 0.417
******** [step = 250] loss: 3.180, acc: 0.418
******** [step = 300] loss: 3.180, acc: 0.418
******** [step = 350] loss: 3.178, acc: 0.418
******** [step = 400] loss: 3.177, acc: 0.419
******** [step = 450] loss: 3.172, acc: 0.419
******** [step = 500] loss: 3.170, acc: 0.420
******** [step = 550] loss: 3.169, acc: 0.420
******** [step = 600] loss: 3.165, acc: 0.421
******** [step = 650] loss: 3.164, acc: 0.421
******** [step = 700] loss: 3.162, acc: 0.421
******** [step = 750] loss: 3.161, acc: 0.422
******** [step = 800] loss: 3.160, acc: 0.422
******** [step = 850] loss: 3.160, acc: 0.422
EPOCH = 4 loss: 3.160, acc: 0.422, val_loss: 3.088, val_acc: 0.437

================================================================================2025-08_10 00:41:55
******** [step = 50] loss: 3.073, acc: 0.424
******** [step = 100] loss: 3.046, acc: 0.429
******** [step = 150] loss: 3.041, acc: 0.431
******** [step = 200] loss: 3.043, acc: 0.430
******** [step = 250] loss: 3.046, acc: 0.430
******** [step = 300] loss: 3.041, acc: 0.431
******** [step = 350] loss: 3.041, acc: 0.432
******** [step = 400] loss: 3.043, acc: 0.432
******** [step = 450] loss: 3.042, acc: 0.432
******** [step = 500] loss: 3.045, acc: 0.432
******** [step = 550] loss: 3.046, acc: 0.432
******** [step = 600] loss: 3.044, acc: 0.432
******** [step = 650] loss: 3.045, acc: 0.433
******** [step = 700] loss: 3.044, acc: 0.433
******** [step = 750] loss: 3.044, acc: 0.433
******** [step = 800] loss: 3.043, acc: 0.433
******** [step = 850] loss: 3.043, acc: 0.433
EPOCH = 5 loss: 3.043, acc: 0.433, val_loss: 2.970, val_acc: 0.452

================================================================================2025-08_10 00:43:14
******** [step = 50] loss: 2.943, acc: 0.441
******** [step = 100] loss: 2.933, acc: 0.443
******** [step = 150] loss: 2.923, acc: 0.444
******** [step = 200] loss: 2.919, acc: 0.444
******** [step = 250] loss: 2.921, acc: 0.444
******** [step = 300] loss: 2.921, acc: 0.444
******** [step = 350] loss: 2.923, acc: 0.444
******** [step = 400] loss: 2.922, acc: 0.445
******** [step = 450] loss: 2.922, acc: 0.445
******** [step = 500] loss: 2.922, acc: 0.445
******** [step = 550] loss: 2.925, acc: 0.445
******** [step = 600] loss: 2.922, acc: 0.445
******** [step = 650] loss: 2.922, acc: 0.445
******** [step = 700] loss: 2.922, acc: 0.446
******** [step = 750] loss: 2.923, acc: 0.445
******** [step = 800] loss: 2.922, acc: 0.446
******** [step = 850] loss: 2.920, acc: 0.446
EPOCH = 6 loss: 2.920, acc: 0.446, val_loss: 2.900, val_acc: 0.460

================================================================================2025-08_10 00:44:33
******** [step = 50] loss: 2.811, acc: 0.452
******** [step = 100] loss: 2.796, acc: 0.456
******** [step = 150] loss: 2.799, acc: 0.455
******** [step = 200] loss: 2.804, acc: 0.454
******** [step = 250] loss: 2.819, acc: 0.451
******** [step = 300] loss: 2.823, acc: 0.450
******** [step = 350] loss: 2.825, acc: 0.450
******** [step = 400] loss: 2.824, acc: 0.451
******** [step = 450] loss: 2.823, acc: 0.452
******** [step = 500] loss: 2.823, acc: 0.452
******** [step = 550] loss: 2.823, acc: 0.453
******** [step = 600] loss: 2.823, acc: 0.454
******** [step = 650] loss: 2.823, acc: 0.454
******** [step = 700] loss: 2.823, acc: 0.454
******** [step = 750] loss: 2.821, acc: 0.455
******** [step = 800] loss: 2.821, acc: 0.455
******** [step = 850] loss: 2.822, acc: 0.455
EPOCH = 7 loss: 2.822, acc: 0.455, val_loss: 2.837, val_acc: 0.466

================================================================================2025-08_10 00:45:53
******** [step = 50] loss: 2.762, acc: 0.451
******** [step = 100] loss: 2.761, acc: 0.453
******** [step = 150] loss: 2.752, acc: 0.455
******** [step = 200] loss: 2.748, acc: 0.457
******** [step = 250] loss: 2.750, acc: 0.457
******** [step = 300] loss: 2.748, acc: 0.457
******** [step = 350] loss: 2.744, acc: 0.458
******** [step = 400] loss: 2.744, acc: 0.458
******** [step = 450] loss: 2.745, acc: 0.459
******** [step = 500] loss: 2.744, acc: 0.460
******** [step = 550] loss: 2.748, acc: 0.459
******** [step = 600] loss: 2.752, acc: 0.459
******** [step = 650] loss: 2.754, acc: 0.460
******** [step = 700] loss: 2.754, acc: 0.460
******** [step = 750] loss: 2.755, acc: 0.461
******** [step = 800] loss: 2.755, acc: 0.461
******** [step = 850] loss: 2.756, acc: 0.461
EPOCH = 8 loss: 2.756, acc: 0.461, val_loss: 2.815, val_acc: 0.466

================================================================================2025-08_10 00:47:12
******** [step = 50] loss: 2.671, acc: 0.466
******** [step = 100] loss: 2.674, acc: 0.468
******** [step = 150] loss: 2.667, acc: 0.469
******** [step = 200] loss: 2.665, acc: 0.469
******** [step = 250] loss: 2.669, acc: 0.469
******** [step = 300] loss: 2.673, acc: 0.469
******** [step = 350] loss: 2.678, acc: 0.469
******** [step = 400] loss: 2.678, acc: 0.469
******** [step = 450] loss: 2.680, acc: 0.469
******** [step = 500] loss: 2.680, acc: 0.470
******** [step = 550] loss: 2.680, acc: 0.470
******** [step = 600] loss: 2.682, acc: 0.470
******** [step = 650] loss: 2.684, acc: 0.470
******** [step = 700] loss: 2.685, acc: 0.470
******** [step = 750] loss: 2.689, acc: 0.469
******** [step = 800] loss: 2.689, acc: 0.470
******** [step = 850] loss: 2.690, acc: 0.470
EPOCH = 9 loss: 2.690, acc: 0.470, val_loss: 2.766, val_acc: 0.476

================================================================================2025-08_10 00:48:32
******** [step = 50] loss: 2.580, acc: 0.480
******** [step = 100] loss: 2.575, acc: 0.480
******** [step = 150] loss: 2.586, acc: 0.480
******** [step = 200] loss: 2.588, acc: 0.479
******** [step = 250] loss: 2.592, acc: 0.479
******** [step = 300] loss: 2.598, acc: 0.479
******** [step = 350] loss: 2.605, acc: 0.478
******** [step = 400] loss: 2.608, acc: 0.478
******** [step = 450] loss: 2.611, acc: 0.477
******** [step = 500] loss: 2.615, acc: 0.477
******** [step = 550] loss: 2.616, acc: 0.478
******** [step = 600] loss: 2.620, acc: 0.478
******** [step = 650] loss: 2.621, acc: 0.478
******** [step = 700] loss: 2.621, acc: 0.478
******** [step = 750] loss: 2.622, acc: 0.478
******** [step = 800] loss: 2.623, acc: 0.478
******** [step = 850] loss: 2.626, acc: 0.478
EPOCH = 10 loss: 2.626, acc: 0.478, val_loss: 2.749, val_acc: 0.482

================================================================================2025-08_10 00:49:51
******** [step = 50] loss: 2.527, acc: 0.486
******** [step = 100] loss: 2.527, acc: 0.486
******** [step = 150] loss: 2.526, acc: 0.487
******** [step = 200] loss: 2.532, acc: 0.486
******** [step = 250] loss: 2.537, acc: 0.486
******** [step = 300] loss: 2.541, acc: 0.486
******** [step = 350] loss: 2.544, acc: 0.486
******** [step = 400] loss: 2.544, acc: 0.486
******** [step = 450] loss: 2.549, acc: 0.486
******** [step = 500] loss: 2.556, acc: 0.486
******** [step = 550] loss: 2.565, acc: 0.485
******** [step = 600] loss: 2.569, acc: 0.485
******** [step = 650] loss: 2.573, acc: 0.485
******** [step = 700] loss: 2.574, acc: 0.485
******** [step = 750] loss: 2.577, acc: 0.485
******** [step = 800] loss: 2.579, acc: 0.485
******** [step = 850] loss: 2.579, acc: 0.485
EPOCH = 11 loss: 2.579, acc: 0.485, val_loss: 2.686, val_acc: 0.491

================================================================================2025-08_10 00:51:10
******** [step = 50] loss: 2.475, acc: 0.492
******** [step = 100] loss: 2.476, acc: 0.494
******** [step = 150] loss: 2.486, acc: 0.493
******** [step = 200] loss: 2.492, acc: 0.493
******** [step = 250] loss: 2.494, acc: 0.494
******** [step = 300] loss: 2.500, acc: 0.494
******** [step = 350] loss: 2.502, acc: 0.494
******** [step = 400] loss: 2.507, acc: 0.494
******** [step = 450] loss: 2.513, acc: 0.493
******** [step = 500] loss: 2.518, acc: 0.493
******** [step = 550] loss: 2.522, acc: 0.492
******** [step = 600] loss: 2.525, acc: 0.492
******** [step = 650] loss: 2.526, acc: 0.492
******** [step = 700] loss: 2.528, acc: 0.492
******** [step = 750] loss: 2.530, acc: 0.492
******** [step = 800] loss: 2.533, acc: 0.492
******** [step = 850] loss: 2.538, acc: 0.491
EPOCH = 12 loss: 2.538, acc: 0.491, val_loss: 2.697, val_acc: 0.490

================================================================================2025-08_10 00:52:30
******** [step = 50] loss: 2.506, acc: 0.490
******** [step = 100] loss: 2.500, acc: 0.491
******** [step = 150] loss: 2.496, acc: 0.492
******** [step = 200] loss: 2.499, acc: 0.492
******** [step = 250] loss: 2.501, acc: 0.492
******** [step = 300] loss: 2.504, acc: 0.492
******** [step = 350] loss: 2.504, acc: 0.492
******** [step = 400] loss: 2.506, acc: 0.492
******** [step = 450] loss: 2.507, acc: 0.492
******** [step = 500] loss: 2.508, acc: 0.492
******** [step = 550] loss: 2.509, acc: 0.493
******** [step = 600] loss: 2.514, acc: 0.493
******** [step = 650] loss: 2.515, acc: 0.493
******** [step = 700] loss: 2.517, acc: 0.493
******** [step = 750] loss: 2.517, acc: 0.493
******** [step = 800] loss: 2.519, acc: 0.493
******** [step = 850] loss: 2.521, acc: 0.493
EPOCH = 13 loss: 2.521, acc: 0.493, val_loss: 2.662, val_acc: 0.497

================================================================================2025-08_10 00:53:49
******** [step = 50] loss: 2.437, acc: 0.499
******** [step = 100] loss: 2.420, acc: 0.500
******** [step = 150] loss: 2.426, acc: 0.501
******** [step = 200] loss: 2.431, acc: 0.500
******** [step = 250] loss: 2.431, acc: 0.501
******** [step = 300] loss: 2.437, acc: 0.501
******** [step = 350] loss: 2.439, acc: 0.501
******** [step = 400] loss: 2.446, acc: 0.501
******** [step = 450] loss: 2.455, acc: 0.500
******** [step = 500] loss: 2.454, acc: 0.500
******** [step = 550] loss: 2.458, acc: 0.500
******** [step = 600] loss: 2.463, acc: 0.500
******** [step = 650] loss: 2.467, acc: 0.500
******** [step = 700] loss: 2.471, acc: 0.499
******** [step = 750] loss: 2.474, acc: 0.500
******** [step = 800] loss: 2.478, acc: 0.499
******** [step = 850] loss: 2.482, acc: 0.499
EPOCH = 14 loss: 2.482, acc: 0.499, val_loss: 2.643, val_acc: 0.503

================================================================================2025-08_10 00:55:08
******** [step = 50] loss: 2.380, acc: 0.509
******** [step = 100] loss: 2.400, acc: 0.507
******** [step = 150] loss: 2.398, acc: 0.507
******** [step = 200] loss: 2.408, acc: 0.506
******** [step = 250] loss: 2.407, acc: 0.507
******** [step = 300] loss: 2.414, acc: 0.506
******** [step = 350] loss: 2.421, acc: 0.505
******** [step = 400] loss: 2.423, acc: 0.506
******** [step = 450] loss: 2.425, acc: 0.506
******** [step = 500] loss: 2.431, acc: 0.505
******** [step = 550] loss: 2.435, acc: 0.505
******** [step = 600] loss: 2.440, acc: 0.505
******** [step = 650] loss: 2.444, acc: 0.504
******** [step = 700] loss: 2.445, acc: 0.504
******** [step = 750] loss: 2.448, acc: 0.504
******** [step = 800] loss: 2.449, acc: 0.504
******** [step = 850] loss: 2.452, acc: 0.504
EPOCH = 15 loss: 2.452, acc: 0.504, val_loss: 2.629, val_acc: 0.503

================================================================================2025-08_10 00:56:27
******** [step = 50] loss: 2.378, acc: 0.508
******** [step = 100] loss: 2.366, acc: 0.510
******** [step = 150] loss: 2.376, acc: 0.510
******** [step = 200] loss: 2.387, acc: 0.508
******** [step = 250] loss: 2.403, acc: 0.505
******** [step = 300] loss: 2.411, acc: 0.503
******** [step = 350] loss: 2.416, acc: 0.503
******** [step = 400] loss: 2.422, acc: 0.502
******** [step = 450] loss: 2.429, acc: 0.501
******** [step = 500] loss: 2.435, acc: 0.501
******** [step = 550] loss: 2.439, acc: 0.501
******** [step = 600] loss: 2.442, acc: 0.501
******** [step = 650] loss: 2.444, acc: 0.501
******** [step = 700] loss: 2.447, acc: 0.501
******** [step = 750] loss: 2.449, acc: 0.501
******** [step = 800] loss: 2.452, acc: 0.501
******** [step = 850] loss: 2.453, acc: 0.501
EPOCH = 16 loss: 2.453, acc: 0.501, val_loss: 2.638, val_acc: 0.503

================================================================================2025-08_10 00:57:47
******** [step = 50] loss: 2.350, acc: 0.513
******** [step = 100] loss: 2.349, acc: 0.514
******** [step = 150] loss: 2.355, acc: 0.513
******** [step = 200] loss: 2.364, acc: 0.513
******** [step = 250] loss: 2.368, acc: 0.512
******** [step = 300] loss: 2.371, acc: 0.512
******** [step = 350] loss: 2.375, acc: 0.512
******** [step = 400] loss: 2.380, acc: 0.512
******** [step = 450] loss: 2.383, acc: 0.512
******** [step = 500] loss: 2.385, acc: 0.512
******** [step = 550] loss: 2.387, acc: 0.512
******** [step = 600] loss: 2.388, acc: 0.512
******** [step = 650] loss: 2.394, acc: 0.511
******** [step = 700] loss: 2.396, acc: 0.511
******** [step = 750] loss: 2.399, acc: 0.511
******** [step = 800] loss: 2.402, acc: 0.511
******** [step = 850] loss: 2.405, acc: 0.511
EPOCH = 17 loss: 2.405, acc: 0.511, val_loss: 2.612, val_acc: 0.508

================================================================================2025-08_10 00:59:06
******** [step = 50] loss: 2.311, acc: 0.521
******** [step = 100] loss: 2.314, acc: 0.521
******** [step = 150] loss: 2.328, acc: 0.518
******** [step = 200] loss: 2.335, acc: 0.518
******** [step = 250] loss: 2.334, acc: 0.518
******** [step = 300] loss: 2.340, acc: 0.517
******** [step = 350] loss: 2.346, acc: 0.516
******** [step = 400] loss: 2.351, acc: 0.516
******** [step = 450] loss: 2.354, acc: 0.515
******** [step = 500] loss: 2.359, acc: 0.515
******** [step = 550] loss: 2.362, acc: 0.515
******** [step = 600] loss: 2.366, acc: 0.514
******** [step = 650] loss: 2.368, acc: 0.514
******** [step = 700] loss: 2.371, acc: 0.514
******** [step = 750] loss: 2.373, acc: 0.514
******** [step = 800] loss: 2.378, acc: 0.514
******** [step = 850] loss: 2.384, acc: 0.513
EPOCH = 18 loss: 2.384, acc: 0.513, val_loss: 2.659, val_acc: 0.502

================================================================================2025-08_10 01:00:25
******** [step = 50] loss: 2.324, acc: 0.518
******** [step = 100] loss: 2.327, acc: 0.518
******** [step = 150] loss: 2.324, acc: 0.519
******** [step = 200] loss: 2.328, acc: 0.519
******** [step = 250] loss: 2.334, acc: 0.518
******** [step = 300] loss: 2.339, acc: 0.517
******** [step = 350] loss: 2.339, acc: 0.517
******** [step = 400] loss: 2.345, acc: 0.517
******** [step = 450] loss: 2.353, acc: 0.516
******** [step = 500] loss: 2.359, acc: 0.515
******** [step = 550] loss: 2.363, acc: 0.515
******** [step = 600] loss: 2.365, acc: 0.514
******** [step = 650] loss: 2.368, acc: 0.514
******** [step = 700] loss: 2.370, acc: 0.514
******** [step = 750] loss: 2.373, acc: 0.514
******** [step = 800] loss: 2.374, acc: 0.514
******** [step = 850] loss: 2.376, acc: 0.514
EPOCH = 19 loss: 2.376, acc: 0.514, val_loss: 2.610, val_acc: 0.512

================================================================================2025-08_10 01:01:44
******** [step = 50] loss: 2.276, acc: 0.525
******** [step = 100] loss: 2.283, acc: 0.524
******** [step = 150] loss: 2.292, acc: 0.523
******** [step = 200] loss: 2.301, acc: 0.522
******** [step = 250] loss: 2.305, acc: 0.523
******** [step = 300] loss: 2.307, acc: 0.523
******** [step = 350] loss: 2.312, acc: 0.522
******** [step = 400] loss: 2.316, acc: 0.522
******** [step = 450] loss: 2.317, acc: 0.522
******** [step = 500] loss: 2.322, acc: 0.522
******** [step = 550] loss: 2.331, acc: 0.520
******** [step = 600] loss: 2.336, acc: 0.519
******** [step = 650] loss: 2.342, acc: 0.518
******** [step = 700] loss: 2.346, acc: 0.517
******** [step = 750] loss: 2.351, acc: 0.516
******** [step = 800] loss: 2.354, acc: 0.516
******** [step = 850] loss: 2.359, acc: 0.515
EPOCH = 20 loss: 2.359, acc: 0.515, val_loss: 2.649, val_acc: 0.504

================================================================================2025-08_10 01:03:04
******** [step = 50] loss: 2.293, acc: 0.517
******** [step = 100] loss: 2.280, acc: 0.520
******** [step = 150] loss: 2.286, acc: 0.521
******** [step = 200] loss: 2.287, acc: 0.522
******** [step = 250] loss: 2.291, acc: 0.521
******** [step = 300] loss: 2.299, acc: 0.520
******** [step = 350] loss: 2.305, acc: 0.520
******** [step = 400] loss: 2.313, acc: 0.519
******** [step = 450] loss: 2.317, acc: 0.519
******** [step = 500] loss: 2.321, acc: 0.519
******** [step = 550] loss: 2.325, acc: 0.519
******** [step = 600] loss: 2.327, acc: 0.519
******** [step = 650] loss: 2.329, acc: 0.519
******** [step = 700] loss: 2.330, acc: 0.519
******** [step = 750] loss: 2.333, acc: 0.519
******** [step = 800] loss: 2.336, acc: 0.519
******** [step = 850] loss: 2.338, acc: 0.519
EPOCH = 21 loss: 2.338, acc: 0.519, val_loss: 2.592, val_acc: 0.512

================================================================================2025-08_10 01:04:23
******** [step = 50] loss: 2.253, acc: 0.527
******** [step = 100] loss: 2.247, acc: 0.529
******** [step = 150] loss: 2.255, acc: 0.528
******** [step = 200] loss: 2.261, acc: 0.528
******** [step = 250] loss: 2.266, acc: 0.527
******** [step = 300] loss: 2.266, acc: 0.527
******** [step = 350] loss: 2.271, acc: 0.526
******** [step = 400] loss: 2.278, acc: 0.526
******** [step = 450] loss: 2.284, acc: 0.525
******** [step = 500] loss: 2.289, acc: 0.525
******** [step = 550] loss: 2.297, acc: 0.524
******** [step = 600] loss: 2.301, acc: 0.524
******** [step = 650] loss: 2.303, acc: 0.523
******** [step = 700] loss: 2.306, acc: 0.523
******** [step = 750] loss: 2.308, acc: 0.523
******** [step = 800] loss: 2.312, acc: 0.523
******** [step = 850] loss: 2.315, acc: 0.523
EPOCH = 22 loss: 2.315, acc: 0.523, val_loss: 2.627, val_acc: 0.507

================================================================================2025-08_10 01:05:43
******** [step = 50] loss: 2.246, acc: 0.530
******** [step = 100] loss: 2.253, acc: 0.529
******** [step = 150] loss: 2.259, acc: 0.527
******** [step = 200] loss: 2.264, acc: 0.527
******** [step = 250] loss: 2.270, acc: 0.526
******** [step = 300] loss: 2.276, acc: 0.526
******** [step = 350] loss: 2.280, acc: 0.527
******** [step = 400] loss: 2.284, acc: 0.526
******** [step = 450] loss: 2.291, acc: 0.525
******** [step = 500] loss: 2.299, acc: 0.524
******** [step = 550] loss: 2.308, acc: 0.522
******** [step = 600] loss: 2.314, acc: 0.521
******** [step = 650] loss: 2.318, acc: 0.521
******** [step = 700] loss: 2.320, acc: 0.520
******** [step = 750] loss: 2.325, acc: 0.520
******** [step = 800] loss: 2.327, acc: 0.520
******** [step = 850] loss: 2.327, acc: 0.520
EPOCH = 23 loss: 2.327, acc: 0.520, val_loss: 2.604, val_acc: 0.510

================================================================================2025-08_10 01:07:05
******** [step = 50] loss: 2.240, acc: 0.527
******** [step = 100] loss: 2.234, acc: 0.529
******** [step = 150] loss: 2.228, acc: 0.531
******** [step = 200] loss: 2.237, acc: 0.531
******** [step = 250] loss: 2.240, acc: 0.531
******** [step = 300] loss: 2.246, acc: 0.530
******** [step = 350] loss: 2.246, acc: 0.531
******** [step = 400] loss: 2.253, acc: 0.530
******** [step = 450] loss: 2.258, acc: 0.529
******** [step = 500] loss: 2.262, acc: 0.529
******** [step = 550] loss: 2.264, acc: 0.529
******** [step = 600] loss: 2.266, acc: 0.529
******** [step = 650] loss: 2.269, acc: 0.529
******** [step = 700] loss: 2.273, acc: 0.529
******** [step = 750] loss: 2.276, acc: 0.529
******** [step = 800] loss: 2.278, acc: 0.529
******** [step = 850] loss: 2.279, acc: 0.529
EPOCH = 24 loss: 2.279, acc: 0.529, val_loss: 2.569, val_acc: 0.519

================================================================================2025-08_10 01:08:25
******** [step = 50] loss: 2.213, acc: 0.536
******** [step = 100] loss: 2.211, acc: 0.538
******** [step = 150] loss: 2.209, acc: 0.538
******** [step = 200] loss: 2.207, acc: 0.538
******** [step = 250] loss: 2.210, acc: 0.538
******** [step = 300] loss: 2.215, acc: 0.537
******** [step = 350] loss: 2.221, acc: 0.536
******** [step = 400] loss: 2.226, acc: 0.536
******** [step = 450] loss: 2.229, acc: 0.535
******** [step = 500] loss: 2.233, acc: 0.535
******** [step = 550] loss: 2.235, acc: 0.535
******** [step = 600] loss: 2.238, acc: 0.535
******** [step = 650] loss: 2.242, acc: 0.534
******** [step = 700] loss: 2.245, acc: 0.534
******** [step = 750] loss: 2.249, acc: 0.534
******** [step = 800] loss: 2.252, acc: 0.533
******** [step = 850] loss: 2.254, acc: 0.533
EPOCH = 25 loss: 2.254, acc: 0.533, val_loss: 2.587, val_acc: 0.519

================================================================================2025-08_10 01:09:45
******** [step = 50] loss: 2.181, acc: 0.541
******** [step = 100] loss: 2.181, acc: 0.540
******** [step = 150] loss: 2.187, acc: 0.540
******** [step = 200] loss: 2.189, acc: 0.540
******** [step = 250] loss: 2.192, acc: 0.540
******** [step = 300] loss: 2.196, acc: 0.539
******** [step = 350] loss: 2.202, acc: 0.539
******** [step = 400] loss: 2.204, acc: 0.539
******** [step = 450] loss: 2.206, acc: 0.539
******** [step = 500] loss: 2.210, acc: 0.539
******** [step = 550] loss: 2.214, acc: 0.538
******** [step = 600] loss: 2.220, acc: 0.538
******** [step = 650] loss: 2.224, acc: 0.538
******** [step = 700] loss: 2.227, acc: 0.537
******** [step = 750] loss: 2.232, acc: 0.537
******** [step = 800] loss: 2.234, acc: 0.537
******** [step = 850] loss: 2.236, acc: 0.536
EPOCH = 26 loss: 2.236, acc: 0.536, val_loss: 2.584, val_acc: 0.524

================================================================================2025-08_10 01:11:04
******** [step = 50] loss: 2.157, acc: 0.544
******** [step = 100] loss: 2.155, acc: 0.547
******** [step = 150] loss: 2.160, acc: 0.546
******** [step = 200] loss: 2.164, acc: 0.546
******** [step = 250] loss: 2.168, acc: 0.545
******** [step = 300] loss: 2.174, acc: 0.544
******** [step = 350] loss: 2.181, acc: 0.543
******** [step = 400] loss: 2.187, acc: 0.543
******** [step = 450] loss: 2.187, acc: 0.543
******** [step = 500] loss: 2.194, acc: 0.542
******** [step = 550] loss: 2.198, acc: 0.542
******** [step = 600] loss: 2.200, acc: 0.542
******** [step = 650] loss: 2.205, acc: 0.542
******** [step = 700] loss: 2.210, acc: 0.541
******** [step = 750] loss: 2.213, acc: 0.541
******** [step = 800] loss: 2.217, acc: 0.540
******** [step = 850] loss: 2.219, acc: 0.540
EPOCH = 27 loss: 2.219, acc: 0.540, val_loss: 2.536, val_acc: 0.528

================================================================================2025-08_10 01:12:24
******** [step = 50] loss: 2.147, acc: 0.545
******** [step = 100] loss: 2.142, acc: 0.547
******** [step = 150] loss: 2.146, acc: 0.548
******** [step = 200] loss: 2.148, acc: 0.547
******** [step = 250] loss: 2.155, acc: 0.547
******** [step = 300] loss: 2.158, acc: 0.547
******** [step = 350] loss: 2.165, acc: 0.546
******** [step = 400] loss: 2.169, acc: 0.545
******** [step = 450] loss: 2.173, acc: 0.545
******** [step = 500] loss: 2.177, acc: 0.545
******** [step = 550] loss: 2.182, acc: 0.544
******** [step = 600] loss: 2.188, acc: 0.544
******** [step = 650] loss: 2.191, acc: 0.544
******** [step = 700] loss: 2.195, acc: 0.543
******** [step = 750] loss: 2.199, acc: 0.543
******** [step = 800] loss: 2.203, acc: 0.543
******** [step = 850] loss: 2.205, acc: 0.542
EPOCH = 28 loss: 2.205, acc: 0.542, val_loss: 2.524, val_acc: 0.530

================================================================================2025-08_10 01:13:43
******** [step = 50] loss: 2.121, acc: 0.552
******** [step = 100] loss: 2.127, acc: 0.551
******** [step = 150] loss: 2.128, acc: 0.550
******** [step = 200] loss: 2.135, acc: 0.550
******** [step = 250] loss: 2.143, acc: 0.549
******** [step = 300] loss: 2.147, acc: 0.549
******** [step = 350] loss: 2.152, acc: 0.549
******** [step = 400] loss: 2.159, acc: 0.548
******** [step = 450] loss: 2.163, acc: 0.548
******** [step = 500] loss: 2.168, acc: 0.547
******** [step = 550] loss: 2.170, acc: 0.547
******** [step = 600] loss: 2.174, acc: 0.547
******** [step = 650] loss: 2.178, acc: 0.546
******** [step = 700] loss: 2.181, acc: 0.546
******** [step = 750] loss: 2.186, acc: 0.545
******** [step = 800] loss: 2.190, acc: 0.545
******** [step = 850] loss: 2.191, acc: 0.545
EPOCH = 29 loss: 2.191, acc: 0.545, val_loss: 2.516, val_acc: 0.533

================================================================================2025-08_10 01:15:02
******** [step = 50] loss: 2.096, acc: 0.553
******** [step = 100] loss: 2.114, acc: 0.552
******** [step = 150] loss: 2.112, acc: 0.552
******** [step = 200] loss: 2.125, acc: 0.551
******** [step = 250] loss: 2.126, acc: 0.552
******** [step = 300] loss: 2.134, acc: 0.551
******** [step = 350] loss: 2.137, acc: 0.551
******** [step = 400] loss: 2.142, acc: 0.550
******** [step = 450] loss: 2.147, acc: 0.550
******** [step = 500] loss: 2.150, acc: 0.550
******** [step = 550] loss: 2.156, acc: 0.549
******** [step = 600] loss: 2.161, acc: 0.548
******** [step = 650] loss: 2.164, acc: 0.548
******** [step = 700] loss: 2.168, acc: 0.548
******** [step = 750] loss: 2.172, acc: 0.548
******** [step = 800] loss: 2.175, acc: 0.547
******** [step = 850] loss: 2.179, acc: 0.547
EPOCH = 30 loss: 2.179, acc: 0.547, val_loss: 2.548, val_acc: 0.530

================================================================================2025-08_10 01:16:21
******** [step = 50] loss: 2.098, acc: 0.555
******** [step = 100] loss: 2.101, acc: 0.554
******** [step = 150] loss: 2.114, acc: 0.554
******** [step = 200] loss: 2.119, acc: 0.553
******** [step = 250] loss: 2.125, acc: 0.552
******** [step = 300] loss: 2.127, acc: 0.552
******** [step = 350] loss: 2.130, acc: 0.552
******** [step = 400] loss: 2.133, acc: 0.552
******** [step = 450] loss: 2.136, acc: 0.551
******** [step = 500] loss: 2.139, acc: 0.551
******** [step = 550] loss: 2.144, acc: 0.551
******** [step = 600] loss: 2.150, acc: 0.550
******** [step = 650] loss: 2.154, acc: 0.550
******** [step = 700] loss: 2.158, acc: 0.550
******** [step = 750] loss: 2.162, acc: 0.549
******** [step = 800] loss: 2.165, acc: 0.549
******** [step = 850] loss: 2.167, acc: 0.549
EPOCH = 31 loss: 2.167, acc: 0.549, val_loss: 2.523, val_acc: 0.533

================================================================================2025-08_10 01:17:40
******** [step = 50] loss: 2.091, acc: 0.557
******** [step = 100] loss: 2.087, acc: 0.558
******** [step = 150] loss: 2.086, acc: 0.559
******** [step = 200] loss: 2.093, acc: 0.559
******** [step = 250] loss: 2.103, acc: 0.557
******** [step = 300] loss: 2.109, acc: 0.556
******** [step = 350] loss: 2.115, acc: 0.555
******** [step = 400] loss: 2.120, acc: 0.555
******** [step = 450] loss: 2.126, acc: 0.554
******** [step = 500] loss: 2.129, acc: 0.554
******** [step = 550] loss: 2.133, acc: 0.553
******** [step = 600] loss: 2.138, acc: 0.553
******** [step = 650] loss: 2.142, acc: 0.552
******** [step = 700] loss: 2.145, acc: 0.552
******** [step = 750] loss: 2.149, acc: 0.552
******** [step = 800] loss: 2.153, acc: 0.551
******** [step = 850] loss: 2.156, acc: 0.551
EPOCH = 32 loss: 2.156, acc: 0.551, val_loss: 2.505, val_acc: 0.536

================================================================================2025-08_10 01:18:59
******** [step = 50] loss: 2.064, acc: 0.560
******** [step = 100] loss: 2.081, acc: 0.558
******** [step = 150] loss: 2.088, acc: 0.558
******** [step = 200] loss: 2.090, acc: 0.558
******** [step = 250] loss: 2.097, acc: 0.558
******** [step = 300] loss: 2.106, acc: 0.556
******** [step = 350] loss: 2.110, acc: 0.556
******** [step = 400] loss: 2.114, acc: 0.556
******** [step = 450] loss: 2.118, acc: 0.555
******** [step = 500] loss: 2.121, acc: 0.555
******** [step = 550] loss: 2.126, acc: 0.555
******** [step = 600] loss: 2.131, acc: 0.555
******** [step = 650] loss: 2.134, acc: 0.554
******** [step = 700] loss: 2.138, acc: 0.554
******** [step = 750] loss: 2.141, acc: 0.554
******** [step = 800] loss: 2.145, acc: 0.553
******** [step = 850] loss: 2.146, acc: 0.553
EPOCH = 33 loss: 2.146, acc: 0.553, val_loss: 2.515, val_acc: 0.537

================================================================================2025-08_10 01:20:18
******** [step = 50] loss: 2.068, acc: 0.564
******** [step = 100] loss: 2.072, acc: 0.563
******** [step = 150] loss: 2.077, acc: 0.562
******** [step = 200] loss: 2.084, acc: 0.560
******** [step = 250] loss: 2.088, acc: 0.560
******** [step = 300] loss: 2.094, acc: 0.560
******** [step = 350] loss: 2.099, acc: 0.559
******** [step = 400] loss: 2.104, acc: 0.559
******** [step = 450] loss: 2.107, acc: 0.559
******** [step = 500] loss: 2.114, acc: 0.558
******** [step = 550] loss: 2.118, acc: 0.557
******** [step = 600] loss: 2.121, acc: 0.557
******** [step = 650] loss: 2.124, acc: 0.557
******** [step = 700] loss: 2.127, acc: 0.556
******** [step = 750] loss: 2.130, acc: 0.556
******** [step = 800] loss: 2.134, acc: 0.556
******** [step = 850] loss: 2.136, acc: 0.555
EPOCH = 34 loss: 2.136, acc: 0.555, val_loss: 2.499, val_acc: 0.537

================================================================================2025-08_10 01:21:38
******** [step = 50] loss: 2.077, acc: 0.562
******** [step = 100] loss: 2.068, acc: 0.563
******** [step = 150] loss: 2.063, acc: 0.563
******** [step = 200] loss: 2.070, acc: 0.562
******** [step = 250] loss: 2.075, acc: 0.561
******** [step = 300] loss: 2.080, acc: 0.561
******** [step = 350] loss: 2.088, acc: 0.560
******** [step = 400] loss: 2.094, acc: 0.559
******** [step = 450] loss: 2.098, acc: 0.559
******** [step = 500] loss: 2.103, acc: 0.558
******** [step = 550] loss: 2.108, acc: 0.558
******** [step = 600] loss: 2.111, acc: 0.558
******** [step = 650] loss: 2.116, acc: 0.557
******** [step = 700] loss: 2.119, acc: 0.557
******** [step = 750] loss: 2.121, acc: 0.557
******** [step = 800] loss: 2.123, acc: 0.557
******** [step = 850] loss: 2.126, acc: 0.557
EPOCH = 35 loss: 2.126, acc: 0.557, val_loss: 2.500, val_acc: 0.540

================================================================================2025-08_10 01:22:57
******** [step = 50] loss: 2.057, acc: 0.564
******** [step = 100] loss: 2.057, acc: 0.564
******** [step = 150] loss: 2.061, acc: 0.564
******** [step = 200] loss: 2.065, acc: 0.564
******** [step = 250] loss: 2.068, acc: 0.563
******** [step = 300] loss: 2.073, acc: 0.563
******** [step = 350] loss: 2.077, acc: 0.563
******** [step = 400] loss: 2.082, acc: 0.562
******** [step = 450] loss: 2.087, acc: 0.562
******** [step = 500] loss: 2.091, acc: 0.562
******** [step = 550] loss: 2.094, acc: 0.561
******** [step = 600] loss: 2.097, acc: 0.561
******** [step = 650] loss: 2.101, acc: 0.561
******** [step = 700] loss: 2.104, acc: 0.560
******** [step = 750] loss: 2.106, acc: 0.560
******** [step = 800] loss: 2.110, acc: 0.560
******** [step = 850] loss: 2.112, acc: 0.560
EPOCH = 36 loss: 2.112, acc: 0.560, val_loss: 2.482, val_acc: 0.541

================================================================================2025-08_10 01:24:16
******** [step = 50] loss: 2.042, acc: 0.566
******** [step = 100] loss: 2.039, acc: 0.567
******** [step = 150] loss: 2.051, acc: 0.566
******** [step = 200] loss: 2.052, acc: 0.566
******** [step = 250] loss: 2.056, acc: 0.565
******** [step = 300] loss: 2.060, acc: 0.564
******** [step = 350] loss: 2.061, acc: 0.564
******** [step = 400] loss: 2.065, acc: 0.564
******** [step = 450] loss: 2.072, acc: 0.563
******** [step = 500] loss: 2.077, acc: 0.563
******** [step = 550] loss: 2.082, acc: 0.563
******** [step = 600] loss: 2.084, acc: 0.563
******** [step = 650] loss: 2.089, acc: 0.562
******** [step = 700] loss: 2.094, acc: 0.562
******** [step = 750] loss: 2.098, acc: 0.561
******** [step = 800] loss: 2.099, acc: 0.561
******** [step = 850] loss: 2.104, acc: 0.561
EPOCH = 37 loss: 2.104, acc: 0.561, val_loss: 2.501, val_acc: 0.542

================================================================================2025-08_10 01:25:35
******** [step = 50] loss: 2.020, acc: 0.569
******** [step = 100] loss: 2.025, acc: 0.570
******** [step = 150] loss: 2.031, acc: 0.569
******** [step = 200] loss: 2.032, acc: 0.569
******** [step = 250] loss: 2.039, acc: 0.568
******** [step = 300] loss: 2.048, acc: 0.567
******** [step = 350] loss: 2.056, acc: 0.566
******** [step = 400] loss: 2.060, acc: 0.566
******** [step = 450] loss: 2.064, acc: 0.566
******** [step = 500] loss: 2.068, acc: 0.565
******** [step = 550] loss: 2.073, acc: 0.565
******** [step = 600] loss: 2.078, acc: 0.564
******** [step = 650] loss: 2.081, acc: 0.564
******** [step = 700] loss: 2.084, acc: 0.564
******** [step = 750] loss: 2.087, acc: 0.563
******** [step = 800] loss: 2.090, acc: 0.563
******** [step = 850] loss: 2.091, acc: 0.563
EPOCH = 38 loss: 2.091, acc: 0.563, val_loss: 2.474, val_acc: 0.545

================================================================================2025-08_10 01:26:55
******** [step = 50] loss: 2.008, acc: 0.571
******** [step = 100] loss: 2.014, acc: 0.571
******** [step = 150] loss: 2.021, acc: 0.570
******** [step = 200] loss: 2.024, acc: 0.571
******** [step = 250] loss: 2.031, acc: 0.570
******** [step = 300] loss: 2.037, acc: 0.569
******** [step = 350] loss: 2.042, acc: 0.569
******** [step = 400] loss: 2.048, acc: 0.568
******** [step = 450] loss: 2.053, acc: 0.567
******** [step = 500] loss: 2.057, acc: 0.567
******** [step = 550] loss: 2.061, acc: 0.567
******** [step = 600] loss: 2.065, acc: 0.566
******** [step = 650] loss: 2.070, acc: 0.566
******** [step = 700] loss: 2.072, acc: 0.566
******** [step = 750] loss: 2.075, acc: 0.565
******** [step = 800] loss: 2.079, acc: 0.565
******** [step = 850] loss: 2.082, acc: 0.565
EPOCH = 39 loss: 2.082, acc: 0.565, val_loss: 2.467, val_acc: 0.545

================================================================================2025-08_10 01:28:14
******** [step = 50] loss: 2.022, acc: 0.569
******** [step = 100] loss: 2.006, acc: 0.573
******** [step = 150] loss: 2.012, acc: 0.572
******** [step = 200] loss: 2.015, acc: 0.573
******** [step = 250] loss: 2.022, acc: 0.572
******** [step = 300] loss: 2.028, acc: 0.571
******** [step = 350] loss: 2.032, acc: 0.571
******** [step = 400] loss: 2.039, acc: 0.570
******** [step = 450] loss: 2.043, acc: 0.569
******** [step = 500] loss: 2.048, acc: 0.569
******** [step = 550] loss: 2.052, acc: 0.568
******** [step = 600] loss: 2.057, acc: 0.568
******** [step = 650] loss: 2.060, acc: 0.567
******** [step = 700] loss: 2.062, acc: 0.567
******** [step = 750] loss: 2.065, acc: 0.567
******** [step = 800] loss: 2.069, acc: 0.567
******** [step = 850] loss: 2.072, acc: 0.566
EPOCH = 40 loss: 2.072, acc: 0.566, val_loss: 2.454, val_acc: 0.548

================================================================================2025-08_10 01:29:33
******** [step = 50] loss: 2.007, acc: 0.569
******** [step = 100] loss: 2.000, acc: 0.573
******** [step = 150] loss: 2.003, acc: 0.573
******** [step = 200] loss: 2.010, acc: 0.573
******** [step = 250] loss: 2.016, acc: 0.572
******** [step = 300] loss: 2.020, acc: 0.572
******** [step = 350] loss: 2.023, acc: 0.571
******** [step = 400] loss: 2.030, acc: 0.571
******** [step = 450] loss: 2.034, acc: 0.570
******** [step = 500] loss: 2.039, acc: 0.570
******** [step = 550] loss: 2.042, acc: 0.570
******** [step = 600] loss: 2.045, acc: 0.570
******** [step = 650] loss: 2.049, acc: 0.569
******** [step = 700] loss: 2.053, acc: 0.569
******** [step = 750] loss: 2.058, acc: 0.568
******** [step = 800] loss: 2.062, acc: 0.568
******** [step = 850] loss: 2.064, acc: 0.568
EPOCH = 41 loss: 2.064, acc: 0.568, val_loss: 2.453, val_acc: 0.549

================================================================================2025-08_10 01:30:55
******** [step = 50] loss: 1.984, acc: 0.575
******** [step = 100] loss: 1.994, acc: 0.575
******** [step = 150] loss: 2.001, acc: 0.574
******** [step = 200] loss: 2.005, acc: 0.574
******** [step = 250] loss: 2.008, acc: 0.574
******** [step = 300] loss: 2.013, acc: 0.573
******** [step = 350] loss: 2.018, acc: 0.573
******** [step = 400] loss: 2.022, acc: 0.573
******** [step = 450] loss: 2.028, acc: 0.572
******** [step = 500] loss: 2.033, acc: 0.572
******** [step = 550] loss: 2.038, acc: 0.571
******** [step = 600] loss: 2.042, acc: 0.571
******** [step = 650] loss: 2.044, acc: 0.570
******** [step = 700] loss: 2.048, acc: 0.570
******** [step = 750] loss: 2.051, acc: 0.570
******** [step = 800] loss: 2.054, acc: 0.569
******** [step = 850] loss: 2.058, acc: 0.569
EPOCH = 42 loss: 2.058, acc: 0.569, val_loss: 2.444, val_acc: 0.551

================================================================================2025-08_10 01:32:14
******** [step = 50] loss: 1.976, acc: 0.578
******** [step = 100] loss: 1.979, acc: 0.577
******** [step = 150] loss: 1.983, acc: 0.577
******** [step = 200] loss: 1.985, acc: 0.577
******** [step = 250] loss: 1.996, acc: 0.576
******** [step = 300] loss: 2.004, acc: 0.575
******** [step = 350] loss: 2.008, acc: 0.574
******** [step = 400] loss: 2.014, acc: 0.573
******** [step = 450] loss: 2.017, acc: 0.573
******** [step = 500] loss: 2.022, acc: 0.573
******** [step = 550] loss: 2.025, acc: 0.572
******** [step = 600] loss: 2.027, acc: 0.572
******** [step = 650] loss: 2.033, acc: 0.572
******** [step = 700] loss: 2.038, acc: 0.571
******** [step = 750] loss: 2.041, acc: 0.571
******** [step = 800] loss: 2.044, acc: 0.571
******** [step = 850] loss: 2.047, acc: 0.571
EPOCH = 43 loss: 2.047, acc: 0.571, val_loss: 2.462, val_acc: 0.549

================================================================================2025-08_10 01:33:34
******** [step = 50] loss: 1.987, acc: 0.577
******** [step = 100] loss: 1.984, acc: 0.577
******** [step = 150] loss: 1.982, acc: 0.578
******** [step = 200] loss: 1.991, acc: 0.577
******** [step = 250] loss: 1.996, acc: 0.577
******** [step = 300] loss: 1.998, acc: 0.577
******** [step = 350] loss: 2.002, acc: 0.576
******** [step = 400] loss: 2.005, acc: 0.576
******** [step = 450] loss: 2.010, acc: 0.575
******** [step = 500] loss: 2.014, acc: 0.575
******** [step = 550] loss: 2.019, acc: 0.575
******** [step = 600] loss: 2.023, acc: 0.574
******** [step = 650] loss: 2.027, acc: 0.574
******** [step = 700] loss: 2.030, acc: 0.574
******** [step = 750] loss: 2.034, acc: 0.573
******** [step = 800] loss: 2.037, acc: 0.573
******** [step = 850] loss: 2.039, acc: 0.573
EPOCH = 44 loss: 2.039, acc: 0.573, val_loss: 2.440, val_acc: 0.553

================================================================================2025-08_10 01:34:53
******** [step = 50] loss: 1.946, acc: 0.582
******** [step = 100] loss: 1.954, acc: 0.583
******** [step = 150] loss: 1.966, acc: 0.581
******** [step = 200] loss: 1.972, acc: 0.580
******** [step = 250] loss: 1.980, acc: 0.579
******** [step = 300] loss: 1.983, acc: 0.578
******** [step = 350] loss: 1.989, acc: 0.578
******** [step = 400] loss: 1.995, acc: 0.577
******** [step = 450] loss: 2.001, acc: 0.576
******** [step = 500] loss: 2.006, acc: 0.576
******** [step = 550] loss: 2.010, acc: 0.576
******** [step = 600] loss: 2.013, acc: 0.575
******** [step = 650] loss: 2.017, acc: 0.575
******** [step = 700] loss: 2.022, acc: 0.575
******** [step = 750] loss: 2.026, acc: 0.575
******** [step = 800] loss: 2.029, acc: 0.574
******** [step = 850] loss: 2.033, acc: 0.574
EPOCH = 45 loss: 2.033, acc: 0.574, val_loss: 2.461, val_acc: 0.551

================================================================================2025-08_10 01:36:12
******** [step = 50] loss: 1.950, acc: 0.583
******** [step = 100] loss: 1.957, acc: 0.582
******** [step = 150] loss: 1.963, acc: 0.582
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 6778588 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T01:36:26 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 6778588.0 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T01:36:26 DUE TO TIME LIMIT ***
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 01:36:29
Job ID:        6778588
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=04:01:20,vmem=0,walltime=01:00:20,mem=35076K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-007-31-0
---------------------------------------
