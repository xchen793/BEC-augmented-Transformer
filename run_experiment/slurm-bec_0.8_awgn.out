---------------------------------------
Begin Slurm Prolog: Aug-13-2025 14:45:23
Job ID:    7013054
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Wed Aug 13 02:45:23 PM EDT 2025
atl1-1-02-005-30-0.pace.gatech.edu
== GPU check ==
Wed Aug 13 14:45:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   39C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 107940.75it/s] 28%|██▊       | 38506/135842 [00:00<00:00, 159961.94it/s] 41%|████      | 55701/135842 [00:00<00:00, 121750.73it/s] 51%|█████     | 69378/135842 [00:00<00:00, 95053.34it/s]  66%|██████▌   | 89317/135842 [00:00<00:00, 120494.62it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90983.57it/s] 92%|█████████▏| 125342/135842 [00:01<00:00, 109081.52it/s]100%|██████████| 135842/135842 [00:01<00:00, 112487.76it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47734.17it/s] 33%|███▎      | 35611/108673 [00:00<00:00, 85588.01it/s] 49%|████▉     | 53779/108673 [00:00<00:00, 113153.93it/s] 66%|██████▋   | 72048/108673 [00:00<00:00, 133363.22it/s] 81%|████████▏ | 88450/108673 [00:01<00:00, 71658.78it/s]  98%|█████████▊| 106512/108673 [00:01<00:00, 90642.42it/s]100%|██████████| 108673/108673 [00:01<00:00, 89609.69it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17865/27169 [00:00<00:00, 178630.15it/s]100%|██████████| 27169/27169 [00:00<00:00, 180239.89it/s]
tensor([   3,   12,   41,    6,   97,   82,   13,    5,  135,   49,   95,  195,
          13,   17,   15, 3466,    4,    2,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,    5,   31, 1175,   13,   10,   41, 1639,   73,    9,    4,   19,
          41,  146,    7,  155, 2199,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,    5,   22,   11,  264,   67,    6,   49,  187,   36,    4,    5,
          35,   77, 1390,    4,    2,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_13 14:46:45
******** [step = 50] loss: 9.350, acc: 0.005
******** [step = 100] loss: 8.963, acc: 0.090
******** [step = 150] loss: 8.598, acc: 0.128
******** [step = 200] loss: 8.256, acc: 0.153
******** [step = 250] loss: 7.937, acc: 0.172
******** [step = 300] loss: 7.632, acc: 0.185
******** [step = 350] loss: 7.340, acc: 0.196
******** [step = 400] loss: 7.068, acc: 0.206
******** [step = 450] loss: 6.825, acc: 0.215
******** [step = 500] loss: 6.611, acc: 0.224
******** [step = 550] loss: 6.423, acc: 0.233
******** [step = 600] loss: 6.256, acc: 0.241
******** [step = 650] loss: 6.108, acc: 0.248
******** [step = 700] loss: 5.975, acc: 0.254
******** [step = 750] loss: 5.854, acc: 0.261
******** [step = 800] loss: 5.744, acc: 0.266
******** [step = 850] loss: 5.644, acc: 0.271
EPOCH = 1 loss: 5.644, acc: 0.271, val_loss: 3.929, val_acc: 0.365

================================================================================2025-08_13 14:48:56
******** [step = 50] loss: 4.025, acc: 0.350
******** [step = 100] loss: 3.974, acc: 0.353
******** [step = 150] loss: 3.951, acc: 0.354
******** [step = 200] loss: 3.930, acc: 0.355
******** [step = 250] loss: 3.910, acc: 0.357
******** [step = 300] loss: 3.891, acc: 0.358
******** [step = 350] loss: 3.876, acc: 0.358
******** [step = 400] loss: 3.860, acc: 0.359
******** [step = 450] loss: 3.849, acc: 0.360
******** [step = 500] loss: 3.838, acc: 0.361
******** [step = 550] loss: 3.826, acc: 0.362
******** [step = 600] loss: 3.815, acc: 0.362
******** [step = 650] loss: 3.803, acc: 0.363
******** [step = 700] loss: 3.791, acc: 0.364
******** [step = 750] loss: 3.783, acc: 0.365
******** [step = 800] loss: 3.773, acc: 0.365
******** [step = 850] loss: 3.764, acc: 0.366
EPOCH = 2 loss: 3.764, acc: 0.366, val_loss: 3.531, val_acc: 0.388

================================================================================2025-08_13 14:51:03
******** [step = 50] loss: 3.589, acc: 0.375
******** [step = 100] loss: 3.567, acc: 0.377
******** [step = 150] loss: 3.552, acc: 0.378
******** [step = 200] loss: 3.545, acc: 0.379
******** [step = 250] loss: 3.536, acc: 0.379
******** [step = 300] loss: 3.532, acc: 0.380
******** [step = 350] loss: 3.527, acc: 0.381
******** [step = 400] loss: 3.523, acc: 0.381
******** [step = 450] loss: 3.517, acc: 0.382
******** [step = 500] loss: 3.514, acc: 0.382
******** [step = 550] loss: 3.510, acc: 0.383
******** [step = 600] loss: 3.505, acc: 0.383
******** [step = 650] loss: 3.500, acc: 0.383
******** [step = 700] loss: 3.495, acc: 0.384
******** [step = 750] loss: 3.491, acc: 0.384
******** [step = 800] loss: 3.487, acc: 0.384
******** [step = 850] loss: 3.483, acc: 0.385
EPOCH = 3 loss: 3.483, acc: 0.385, val_loss: 3.383, val_acc: 0.400

================================================================================2025-08_13 14:53:07
******** [step = 50] loss: 3.369, acc: 0.390
******** [step = 100] loss: 3.354, acc: 0.393
******** [step = 150] loss: 3.349, acc: 0.394
******** [step = 200] loss: 3.350, acc: 0.394
******** [step = 250] loss: 3.342, acc: 0.395
******** [step = 300] loss: 3.340, acc: 0.395
******** [step = 350] loss: 3.339, acc: 0.395
******** [step = 400] loss: 3.339, acc: 0.395
******** [step = 450] loss: 3.335, acc: 0.395
******** [step = 500] loss: 3.332, acc: 0.396
******** [step = 550] loss: 3.329, acc: 0.396
******** [step = 600] loss: 3.329, acc: 0.396
******** [step = 650] loss: 3.328, acc: 0.396
******** [step = 700] loss: 3.328, acc: 0.397
******** [step = 750] loss: 3.326, acc: 0.397
******** [step = 800] loss: 3.325, acc: 0.397
******** [step = 850] loss: 3.320, acc: 0.398
EPOCH = 4 loss: 3.320, acc: 0.398, val_loss: 3.240, val_acc: 0.413

================================================================================2025-08_13 14:55:23
******** [step = 50] loss: 3.216, acc: 0.402
******** [step = 100] loss: 3.206, acc: 0.404
******** [step = 150] loss: 3.207, acc: 0.404
******** [step = 200] loss: 3.213, acc: 0.404
******** [step = 250] loss: 3.211, acc: 0.404
******** [step = 300] loss: 3.213, acc: 0.404
******** [step = 350] loss: 3.213, acc: 0.404
******** [step = 400] loss: 3.214, acc: 0.405
******** [step = 450] loss: 3.215, acc: 0.405
******** [step = 500] loss: 3.214, acc: 0.405
******** [step = 550] loss: 3.216, acc: 0.405
******** [step = 600] loss: 3.216, acc: 0.405
******** [step = 650] loss: 3.217, acc: 0.406
******** [step = 700] loss: 3.216, acc: 0.406
******** [step = 750] loss: 3.215, acc: 0.406
******** [step = 800] loss: 3.216, acc: 0.406
******** [step = 850] loss: 3.214, acc: 0.406
EPOCH = 5 loss: 3.214, acc: 0.406, val_loss: 3.158, val_acc: 0.422

================================================================================2025-08_13 14:57:30
******** [step = 50] loss: 3.092, acc: 0.413
******** [step = 100] loss: 3.102, acc: 0.413
******** [step = 150] loss: 3.097, acc: 0.414
******** [step = 200] loss: 3.095, acc: 0.415
******** [step = 250] loss: 3.095, acc: 0.415
******** [step = 300] loss: 3.096, acc: 0.415
******** [step = 350] loss: 3.098, acc: 0.416
******** [step = 400] loss: 3.096, acc: 0.416
******** [step = 450] loss: 3.098, acc: 0.416
******** [step = 500] loss: 3.099, acc: 0.416
******** [step = 550] loss: 3.100, acc: 0.416
******** [step = 600] loss: 3.102, acc: 0.416
******** [step = 650] loss: 3.102, acc: 0.416
******** [step = 700] loss: 3.102, acc: 0.417
******** [step = 750] loss: 3.102, acc: 0.417
******** [step = 800] loss: 3.102, acc: 0.417
******** [step = 850] loss: 3.103, acc: 0.417
EPOCH = 6 loss: 3.103, acc: 0.417, val_loss: 3.070, val_acc: 0.432

================================================================================2025-08_13 14:59:37
******** [step = 50] loss: 2.991, acc: 0.424
******** [step = 100] loss: 2.982, acc: 0.426
******** [step = 150] loss: 2.985, acc: 0.427
******** [step = 200] loss: 2.980, acc: 0.427
******** [step = 250] loss: 2.982, acc: 0.427
******** [step = 300] loss: 2.989, acc: 0.426
******** [step = 350] loss: 2.990, acc: 0.427
******** [step = 400] loss: 2.993, acc: 0.426
******** [step = 450] loss: 2.993, acc: 0.427
******** [step = 500] loss: 2.996, acc: 0.427
******** [step = 550] loss: 2.997, acc: 0.427
******** [step = 600] loss: 2.998, acc: 0.427
******** [step = 650] loss: 3.000, acc: 0.427
******** [step = 700] loss: 3.000, acc: 0.427
******** [step = 750] loss: 2.999, acc: 0.427
******** [step = 800] loss: 2.998, acc: 0.427
******** [step = 850] loss: 2.999, acc: 0.427
EPOCH = 7 loss: 2.999, acc: 0.427, val_loss: 3.011, val_acc: 0.438

================================================================================2025-08_13 15:01:39
******** [step = 50] loss: 2.881, acc: 0.436
******** [step = 100] loss: 2.880, acc: 0.436
******** [step = 150] loss: 2.885, acc: 0.436
******** [step = 200] loss: 2.885, acc: 0.436
******** [step = 250] loss: 2.894, acc: 0.436
******** [step = 300] loss: 2.900, acc: 0.435
******** [step = 350] loss: 2.903, acc: 0.436
******** [step = 400] loss: 2.908, acc: 0.435
******** [step = 450] loss: 2.909, acc: 0.436
******** [step = 500] loss: 2.911, acc: 0.435
******** [step = 550] loss: 2.911, acc: 0.436
******** [step = 600] loss: 2.914, acc: 0.436
******** [step = 650] loss: 2.916, acc: 0.436
******** [step = 700] loss: 2.918, acc: 0.436
******** [step = 750] loss: 2.919, acc: 0.435
******** [step = 800] loss: 2.921, acc: 0.435
******** [step = 850] loss: 2.924, acc: 0.435
EPOCH = 8 loss: 2.924, acc: 0.435, val_loss: 2.975, val_acc: 0.446

================================================================================2025-08_13 15:03:40
******** [step = 50] loss: 2.837, acc: 0.442
******** [step = 100] loss: 2.829, acc: 0.444
******** [step = 150] loss: 2.832, acc: 0.443
******** [step = 200] loss: 2.825, acc: 0.445
******** [step = 250] loss: 2.827, acc: 0.444
******** [step = 300] loss: 2.829, acc: 0.444
******** [step = 350] loss: 2.834, acc: 0.444
******** [step = 400] loss: 2.838, acc: 0.444
******** [step = 450] loss: 2.842, acc: 0.444
******** [step = 500] loss: 2.846, acc: 0.444
******** [step = 550] loss: 2.847, acc: 0.444
******** [step = 600] loss: 2.848, acc: 0.443
******** [step = 650] loss: 2.850, acc: 0.443
******** [step = 700] loss: 2.852, acc: 0.443
******** [step = 750] loss: 2.852, acc: 0.443
******** [step = 800] loss: 2.851, acc: 0.443
******** [step = 850] loss: 2.851, acc: 0.443
EPOCH = 9 loss: 2.851, acc: 0.443, val_loss: 2.895, val_acc: 0.451

================================================================================2025-08_13 15:05:42
******** [step = 50] loss: 2.708, acc: 0.454
******** [step = 100] loss: 2.731, acc: 0.451
******** [step = 150] loss: 2.723, acc: 0.454
******** [step = 200] loss: 2.723, acc: 0.454
******** [step = 250] loss: 2.719, acc: 0.454
******** [step = 300] loss: 2.716, acc: 0.455
******** [step = 350] loss: 2.714, acc: 0.456
******** [step = 400] loss: 2.715, acc: 0.456
******** [step = 450] loss: 2.719, acc: 0.456
******** [step = 500] loss: 2.720, acc: 0.456
******** [step = 550] loss: 2.724, acc: 0.456
******** [step = 600] loss: 2.726, acc: 0.456
******** [step = 650] loss: 2.727, acc: 0.456
******** [step = 700] loss: 2.729, acc: 0.456
******** [step = 750] loss: 2.732, acc: 0.456
******** [step = 800] loss: 2.734, acc: 0.456
******** [step = 850] loss: 2.735, acc: 0.456
EPOCH = 10 loss: 2.735, acc: 0.456, val_loss: 2.814, val_acc: 0.461

================================================================================2025-08_13 15:07:44
******** [step = 50] loss: 2.626, acc: 0.464
******** [step = 100] loss: 2.628, acc: 0.465
******** [step = 150] loss: 2.625, acc: 0.466
******** [step = 200] loss: 2.627, acc: 0.466
******** [step = 250] loss: 2.631, acc: 0.466
******** [step = 300] loss: 2.641, acc: 0.465
******** [step = 350] loss: 2.647, acc: 0.465
******** [step = 400] loss: 2.654, acc: 0.464
******** [step = 450] loss: 2.658, acc: 0.464
******** [step = 500] loss: 2.659, acc: 0.464
******** [step = 550] loss: 2.662, acc: 0.464
******** [step = 600] loss: 2.665, acc: 0.464
******** [step = 650] loss: 2.668, acc: 0.463
******** [step = 700] loss: 2.669, acc: 0.464
******** [step = 750] loss: 2.671, acc: 0.464
******** [step = 800] loss: 2.673, acc: 0.464
******** [step = 850] loss: 2.676, acc: 0.463
EPOCH = 11 loss: 2.676, acc: 0.463, val_loss: 2.773, val_acc: 0.470

================================================================================2025-08_13 15:09:51
******** [step = 50] loss: 2.584, acc: 0.473
******** [step = 100] loss: 2.577, acc: 0.473
******** [step = 150] loss: 2.583, acc: 0.473
******** [step = 200] loss: 2.584, acc: 0.473
******** [step = 250] loss: 2.586, acc: 0.473
******** [step = 300] loss: 2.590, acc: 0.472
******** [step = 350] loss: 2.594, acc: 0.472
******** [step = 400] loss: 2.599, acc: 0.472
******** [step = 450] loss: 2.603, acc: 0.471
******** [step = 500] loss: 2.605, acc: 0.471
******** [step = 550] loss: 2.608, acc: 0.471
******** [step = 600] loss: 2.613, acc: 0.471
******** [step = 650] loss: 2.616, acc: 0.471
******** [step = 700] loss: 2.619, acc: 0.471
******** [step = 750] loss: 2.622, acc: 0.470
******** [step = 800] loss: 2.625, acc: 0.470
******** [step = 850] loss: 2.631, acc: 0.470
EPOCH = 12 loss: 2.631, acc: 0.470, val_loss: 2.767, val_acc: 0.472

================================================================================2025-08_13 15:11:58
******** [step = 50] loss: 2.548, acc: 0.480
******** [step = 100] loss: 2.536, acc: 0.482
******** [step = 150] loss: 2.536, acc: 0.482
******** [step = 200] loss: 2.542, acc: 0.482
******** [step = 250] loss: 2.551, acc: 0.480
******** [step = 300] loss: 2.557, acc: 0.480
******** [step = 350] loss: 2.560, acc: 0.480
******** [step = 400] loss: 2.564, acc: 0.480
******** [step = 450] loss: 2.569, acc: 0.479
******** [step = 500] loss: 2.573, acc: 0.479
******** [step = 550] loss: 2.575, acc: 0.479
******** [step = 600] loss: 2.578, acc: 0.479
******** [step = 650] loss: 2.581, acc: 0.479
******** [step = 700] loss: 2.582, acc: 0.479
******** [step = 750] loss: 2.585, acc: 0.479
******** [step = 800] loss: 2.585, acc: 0.479
******** [step = 850] loss: 2.587, acc: 0.479
EPOCH = 13 loss: 2.587, acc: 0.479, val_loss: 2.730, val_acc: 0.483

================================================================================2025-08_13 15:14:07
******** [step = 50] loss: 2.500, acc: 0.486
******** [step = 100] loss: 2.499, acc: 0.486
******** [step = 150] loss: 2.505, acc: 0.487
******** [step = 200] loss: 2.512, acc: 0.487
******** [step = 250] loss: 2.521, acc: 0.486
******** [step = 300] loss: 2.525, acc: 0.486
******** [step = 350] loss: 2.529, acc: 0.486
******** [step = 400] loss: 2.531, acc: 0.486
******** [step = 450] loss: 2.531, acc: 0.486
******** [step = 500] loss: 2.535, acc: 0.485
******** [step = 550] loss: 2.536, acc: 0.485
******** [step = 600] loss: 2.538, acc: 0.485
******** [step = 650] loss: 2.539, acc: 0.485
******** [step = 700] loss: 2.543, acc: 0.485
******** [step = 750] loss: 2.546, acc: 0.485
******** [step = 800] loss: 2.548, acc: 0.485
******** [step = 850] loss: 2.551, acc: 0.484
EPOCH = 14 loss: 2.551, acc: 0.484, val_loss: 2.715, val_acc: 0.486

================================================================================2025-08_13 15:16:14
******** [step = 50] loss: 2.490, acc: 0.488
******** [step = 100] loss: 2.469, acc: 0.492
******** [step = 150] loss: 2.468, acc: 0.492
******** [step = 200] loss: 2.471, acc: 0.492
******** [step = 250] loss: 2.477, acc: 0.491
******** [step = 300] loss: 2.483, acc: 0.491
******** [step = 350] loss: 2.487, acc: 0.490
******** [step = 400] loss: 2.490, acc: 0.490
******** [step = 450] loss: 2.495, acc: 0.490
******** [step = 500] loss: 2.498, acc: 0.490
******** [step = 550] loss: 2.503, acc: 0.489
******** [step = 600] loss: 2.506, acc: 0.490
******** [step = 650] loss: 2.509, acc: 0.489
******** [step = 700] loss: 2.512, acc: 0.489
******** [step = 750] loss: 2.517, acc: 0.489
******** [step = 800] loss: 2.520, acc: 0.489
******** [step = 850] loss: 2.523, acc: 0.489
EPOCH = 15 loss: 2.523, acc: 0.489, val_loss: 2.722, val_acc: 0.487

================================================================================2025-08_13 15:18:15
******** [step = 50] loss: 2.429, acc: 0.496
******** [step = 100] loss: 2.437, acc: 0.496
******** [step = 150] loss: 2.443, acc: 0.495
******** [step = 200] loss: 2.452, acc: 0.494
******** [step = 250] loss: 2.459, acc: 0.494
******** [step = 300] loss: 2.463, acc: 0.493
******** [step = 350] loss: 2.470, acc: 0.493
******** [step = 400] loss: 2.470, acc: 0.493
******** [step = 450] loss: 2.474, acc: 0.493
******** [step = 500] loss: 2.478, acc: 0.493
******** [step = 550] loss: 2.481, acc: 0.493
******** [step = 600] loss: 2.484, acc: 0.492
******** [step = 650] loss: 2.486, acc: 0.492
******** [step = 700] loss: 2.489, acc: 0.492
******** [step = 750] loss: 2.492, acc: 0.492
******** [step = 800] loss: 2.494, acc: 0.492
******** [step = 850] loss: 2.496, acc: 0.492
EPOCH = 16 loss: 2.496, acc: 0.492, val_loss: 2.690, val_acc: 0.491

================================================================================2025-08_13 15:20:16
******** [step = 50] loss: 2.410, acc: 0.500
******** [step = 100] loss: 2.406, acc: 0.502
******** [step = 150] loss: 2.407, acc: 0.501
******** [step = 200] loss: 2.417, acc: 0.500
******** [step = 250] loss: 2.422, acc: 0.499
******** [step = 300] loss: 2.427, acc: 0.499
******** [step = 350] loss: 2.431, acc: 0.498
******** [step = 400] loss: 2.435, acc: 0.498
******** [step = 450] loss: 2.440, acc: 0.498
******** [step = 500] loss: 2.446, acc: 0.498
******** [step = 550] loss: 2.449, acc: 0.497
******** [step = 600] loss: 2.454, acc: 0.497
******** [step = 650] loss: 2.458, acc: 0.497
******** [step = 700] loss: 2.459, acc: 0.497
******** [step = 750] loss: 2.460, acc: 0.497
******** [step = 800] loss: 2.462, acc: 0.497
******** [step = 850] loss: 2.463, acc: 0.496
EPOCH = 17 loss: 2.463, acc: 0.496, val_loss: 2.667, val_acc: 0.493

================================================================================2025-08_13 15:22:16
******** [step = 50] loss: 2.382, acc: 0.504
******** [step = 100] loss: 2.380, acc: 0.506
******** [step = 150] loss: 2.383, acc: 0.505
******** [step = 200] loss: 2.386, acc: 0.506
******** [step = 250] loss: 2.393, acc: 0.505
******** [step = 300] loss: 2.393, acc: 0.504
******** [step = 350] loss: 2.403, acc: 0.504
******** [step = 400] loss: 2.409, acc: 0.503
******** [step = 450] loss: 2.415, acc: 0.503
******** [step = 500] loss: 2.417, acc: 0.502
******** [step = 550] loss: 2.420, acc: 0.502
******** [step = 600] loss: 2.422, acc: 0.502
******** [step = 650] loss: 2.425, acc: 0.502
******** [step = 700] loss: 2.427, acc: 0.502
******** [step = 750] loss: 2.431, acc: 0.502
******** [step = 800] loss: 2.435, acc: 0.501
******** [step = 850] loss: 2.439, acc: 0.501
EPOCH = 18 loss: 2.439, acc: 0.501, val_loss: 2.657, val_acc: 0.495

================================================================================2025-08_13 15:24:18
******** [step = 50] loss: 2.360, acc: 0.504
******** [step = 100] loss: 2.372, acc: 0.505
******** [step = 150] loss: 2.373, acc: 0.506
******** [step = 200] loss: 2.376, acc: 0.506
******** [step = 250] loss: 2.378, acc: 0.506
******** [step = 300] loss: 2.381, acc: 0.506
******** [step = 350] loss: 2.384, acc: 0.506
******** [step = 400] loss: 2.387, acc: 0.505
******** [step = 450] loss: 2.392, acc: 0.505
******** [step = 500] loss: 2.396, acc: 0.505
******** [step = 550] loss: 2.398, acc: 0.504
******** [step = 600] loss: 2.402, acc: 0.504
******** [step = 650] loss: 2.405, acc: 0.504
******** [step = 700] loss: 2.408, acc: 0.504
******** [step = 750] loss: 2.410, acc: 0.504
******** [step = 800] loss: 2.413, acc: 0.503
******** [step = 850] loss: 2.415, acc: 0.504
EPOCH = 19 loss: 2.415, acc: 0.504, val_loss: 2.646, val_acc: 0.499

================================================================================2025-08_13 15:26:25
******** [step = 50] loss: 2.326, acc: 0.512
******** [step = 100] loss: 2.338, acc: 0.510
******** [step = 150] loss: 2.344, acc: 0.509
******** [step = 200] loss: 2.348, acc: 0.510
******** [step = 250] loss: 2.358, acc: 0.509
******** [step = 300] loss: 2.362, acc: 0.509
******** [step = 350] loss: 2.363, acc: 0.509
******** [step = 400] loss: 2.364, acc: 0.509
******** [step = 450] loss: 2.368, acc: 0.509
******** [step = 500] loss: 2.374, acc: 0.508
******** [step = 550] loss: 2.378, acc: 0.508
******** [step = 600] loss: 2.381, acc: 0.507
******** [step = 650] loss: 2.385, acc: 0.507
******** [step = 700] loss: 2.387, acc: 0.507
******** [step = 750] loss: 2.391, acc: 0.507
******** [step = 800] loss: 2.394, acc: 0.507
******** [step = 850] loss: 2.397, acc: 0.506
EPOCH = 20 loss: 2.397, acc: 0.506, val_loss: 2.639, val_acc: 0.500

================================================================================2025-08_13 15:28:32
******** [step = 50] loss: 2.303, acc: 0.515
******** [step = 100] loss: 2.308, acc: 0.515
******** [step = 150] loss: 2.319, acc: 0.514
******** [step = 200] loss: 2.327, acc: 0.513
******** [step = 250] loss: 2.334, acc: 0.513
******** [step = 300] loss: 2.339, acc: 0.512
******** [step = 350] loss: 2.343, acc: 0.512
******** [step = 400] loss: 2.348, acc: 0.511
******** [step = 450] loss: 2.352, acc: 0.511
******** [step = 500] loss: 2.357, acc: 0.510
******** [step = 550] loss: 2.361, acc: 0.510
******** [step = 600] loss: 2.364, acc: 0.510
******** [step = 650] loss: 2.366, acc: 0.510
******** [step = 700] loss: 2.367, acc: 0.510
******** [step = 750] loss: 2.371, acc: 0.509
******** [step = 800] loss: 2.373, acc: 0.509
******** [step = 850] loss: 2.376, acc: 0.509
EPOCH = 21 loss: 2.376, acc: 0.509, val_loss: 2.632, val_acc: 0.498

================================================================================2025-08_13 15:30:38
******** [step = 50] loss: 2.291, acc: 0.518
******** [step = 100] loss: 2.300, acc: 0.517
******** [step = 150] loss: 2.307, acc: 0.517
******** [step = 200] loss: 2.311, acc: 0.517
******** [step = 250] loss: 2.320, acc: 0.515
******** [step = 300] loss: 2.324, acc: 0.515
******** [step = 350] loss: 2.328, acc: 0.515
******** [step = 400] loss: 2.329, acc: 0.515
******** [step = 450] loss: 2.333, acc: 0.514
******** [step = 500] loss: 2.337, acc: 0.514
******** [step = 550] loss: 2.340, acc: 0.513
******** [step = 600] loss: 2.344, acc: 0.513
******** [step = 650] loss: 2.348, acc: 0.513
******** [step = 700] loss: 2.352, acc: 0.512
******** [step = 750] loss: 2.355, acc: 0.512
******** [step = 800] loss: 2.357, acc: 0.512
******** [step = 850] loss: 2.358, acc: 0.512
EPOCH = 22 loss: 2.358, acc: 0.512, val_loss: 2.625, val_acc: 0.504

================================================================================2025-08_13 15:32:38
******** [step = 50] loss: 2.277, acc: 0.521
******** [step = 100] loss: 2.280, acc: 0.520
******** [step = 150] loss: 2.281, acc: 0.519
******** [step = 200] loss: 2.291, acc: 0.517
******** [step = 250] loss: 2.296, acc: 0.517
******** [step = 300] loss: 2.303, acc: 0.516
******** [step = 350] loss: 2.307, acc: 0.516
******** [step = 400] loss: 2.310, acc: 0.516
******** [step = 450] loss: 2.316, acc: 0.516
******** [step = 500] loss: 2.319, acc: 0.516
******** [step = 550] loss: 2.324, acc: 0.515
******** [step = 600] loss: 2.329, acc: 0.515
******** [step = 650] loss: 2.332, acc: 0.515
******** [step = 700] loss: 2.335, acc: 0.515
******** [step = 750] loss: 2.338, acc: 0.514
******** [step = 800] loss: 2.340, acc: 0.514
******** [step = 850] loss: 2.344, acc: 0.514
EPOCH = 23 loss: 2.344, acc: 0.514, val_loss: 2.626, val_acc: 0.504

================================================================================2025-08_13 15:34:43
******** [step = 50] loss: 2.260, acc: 0.519
******** [step = 100] loss: 2.271, acc: 0.520
******** [step = 150] loss: 2.272, acc: 0.520
******** [step = 200] loss: 2.278, acc: 0.520
******** [step = 250] loss: 2.281, acc: 0.520
******** [step = 300] loss: 2.288, acc: 0.520
******** [step = 350] loss: 2.292, acc: 0.519
******** [step = 400] loss: 2.296, acc: 0.519
******** [step = 450] loss: 2.301, acc: 0.518
******** [step = 500] loss: 2.305, acc: 0.518
******** [step = 550] loss: 2.309, acc: 0.518
******** [step = 600] loss: 2.313, acc: 0.518
******** [step = 650] loss: 2.315, acc: 0.517
******** [step = 700] loss: 2.318, acc: 0.517
******** [step = 750] loss: 2.321, acc: 0.517
******** [step = 800] loss: 2.324, acc: 0.517
******** [step = 850] loss: 2.327, acc: 0.517
EPOCH = 24 loss: 2.327, acc: 0.517, val_loss: 2.611, val_acc: 0.506

================================================================================2025-08_13 15:36:55
******** [step = 50] loss: 2.231, acc: 0.526
******** [step = 100] loss: 2.236, acc: 0.525
******** [step = 150] loss: 2.255, acc: 0.523
******** [step = 200] loss: 2.258, acc: 0.522
******** [step = 250] loss: 2.264, acc: 0.522
******** [step = 300] loss: 2.267, acc: 0.522
******** [step = 350] loss: 2.273, acc: 0.521
******** [step = 400] loss: 2.277, acc: 0.521
******** [step = 450] loss: 2.282, acc: 0.520
******** [step = 500] loss: 2.285, acc: 0.520
******** [step = 550] loss: 2.293, acc: 0.519
******** [step = 600] loss: 2.295, acc: 0.519
******** [step = 650] loss: 2.300, acc: 0.518
******** [step = 700] loss: 2.304, acc: 0.518
******** [step = 750] loss: 2.308, acc: 0.518
******** [step = 800] loss: 2.310, acc: 0.518
******** [step = 850] loss: 2.314, acc: 0.518
EPOCH = 25 loss: 2.314, acc: 0.518, val_loss: 2.606, val_acc: 0.505

================================================================================2025-08_13 15:38:58
******** [step = 50] loss: 2.226, acc: 0.528
******** [step = 100] loss: 2.222, acc: 0.528
******** [step = 150] loss: 2.226, acc: 0.528
******** [step = 200] loss: 2.242, acc: 0.526
******** [step = 250] loss: 2.251, acc: 0.525
******** [step = 300] loss: 2.258, acc: 0.524
******** [step = 350] loss: 2.264, acc: 0.523
******** [step = 400] loss: 2.266, acc: 0.523
******** [step = 450] loss: 2.269, acc: 0.523
******** [step = 500] loss: 2.273, acc: 0.522
******** [step = 550] loss: 2.278, acc: 0.522
******** [step = 600] loss: 2.281, acc: 0.522
******** [step = 650] loss: 2.286, acc: 0.521
******** [step = 700] loss: 2.291, acc: 0.520
******** [step = 750] loss: 2.293, acc: 0.520
******** [step = 800] loss: 2.297, acc: 0.520
******** [step = 850] loss: 2.301, acc: 0.520
EPOCH = 26 loss: 2.301, acc: 0.520, val_loss: 2.600, val_acc: 0.509

================================================================================2025-08_13 15:40:58
******** [step = 50] loss: 2.215, acc: 0.529
******** [step = 100] loss: 2.224, acc: 0.528
******** [step = 150] loss: 2.235, acc: 0.527
******** [step = 200] loss: 2.240, acc: 0.527
******** [step = 250] loss: 2.243, acc: 0.527
******** [step = 300] loss: 2.248, acc: 0.526
******** [step = 350] loss: 2.252, acc: 0.525
******** [step = 400] loss: 2.253, acc: 0.525
******** [step = 450] loss: 2.258, acc: 0.525
******** [step = 500] loss: 2.262, acc: 0.525
******** [step = 550] loss: 2.266, acc: 0.524
******** [step = 600] loss: 2.269, acc: 0.524
******** [step = 650] loss: 2.274, acc: 0.523
******** [step = 700] loss: 2.278, acc: 0.523
******** [step = 750] loss: 2.281, acc: 0.523
******** [step = 800] loss: 2.285, acc: 0.522
******** [step = 850] loss: 2.287, acc: 0.522
EPOCH = 27 loss: 2.287, acc: 0.522, val_loss: 2.598, val_acc: 0.508

================================================================================2025-08_13 15:42:59
******** [step = 50] loss: 2.207, acc: 0.528
******** [step = 100] loss: 2.225, acc: 0.527
******** [step = 150] loss: 2.223, acc: 0.529
******** [step = 200] loss: 2.229, acc: 0.528
******** [step = 250] loss: 2.233, acc: 0.528
******** [step = 300] loss: 2.240, acc: 0.527
******** [step = 350] loss: 2.248, acc: 0.526
******** [step = 400] loss: 2.250, acc: 0.526
******** [step = 450] loss: 2.252, acc: 0.526
******** [step = 500] loss: 2.256, acc: 0.525
******** [step = 550] loss: 2.260, acc: 0.525
******** [step = 600] loss: 2.262, acc: 0.525
******** [step = 650] loss: 2.264, acc: 0.524
******** [step = 700] loss: 2.267, acc: 0.524
******** [step = 750] loss: 2.269, acc: 0.524
******** [step = 800] loss: 2.272, acc: 0.524
******** [step = 850] loss: 2.275, acc: 0.524
EPOCH = 28 loss: 2.275, acc: 0.524, val_loss: 2.603, val_acc: 0.509

================================================================================2025-08_13 15:45:05
******** [step = 50] loss: 2.183, acc: 0.533
******** [step = 100] loss: 2.192, acc: 0.532
******** [step = 150] loss: 2.205, acc: 0.530
******** [step = 200] loss: 2.215, acc: 0.529
******** [step = 250] loss: 2.220, acc: 0.529
******** [step = 300] loss: 2.222, acc: 0.529
******** [step = 350] loss: 2.226, acc: 0.528
******** [step = 400] loss: 2.231, acc: 0.528
******** [step = 450] loss: 2.236, acc: 0.527
******** [step = 500] loss: 2.241, acc: 0.527
******** [step = 550] loss: 2.246, acc: 0.526
******** [step = 600] loss: 2.249, acc: 0.526
******** [step = 650] loss: 2.252, acc: 0.526
******** [step = 700] loss: 2.255, acc: 0.526
******** [step = 750] loss: 2.257, acc: 0.526
******** [step = 800] loss: 2.261, acc: 0.525
******** [step = 850] loss: 2.265, acc: 0.525
EPOCH = 29 loss: 2.265, acc: 0.525, val_loss: 2.591, val_acc: 0.512

================================================================================2025-08_13 15:47:14
******** [step = 50] loss: 2.188, acc: 0.531
******** [step = 100] loss: 2.193, acc: 0.532
******** [step = 150] loss: 2.199, acc: 0.531
******** [step = 200] loss: 2.202, acc: 0.531
******** [step = 250] loss: 2.210, acc: 0.530
******** [step = 300] loss: 2.218, acc: 0.529
******** [step = 350] loss: 2.223, acc: 0.529
******** [step = 400] loss: 2.227, acc: 0.529
******** [step = 450] loss: 2.232, acc: 0.528
******** [step = 500] loss: 2.236, acc: 0.528
******** [step = 550] loss: 2.238, acc: 0.528
******** [step = 600] loss: 2.240, acc: 0.528
******** [step = 650] loss: 2.243, acc: 0.528
******** [step = 700] loss: 2.246, acc: 0.528
******** [step = 750] loss: 2.249, acc: 0.527
******** [step = 800] loss: 2.252, acc: 0.527
******** [step = 850] loss: 2.256, acc: 0.526
EPOCH = 30 loss: 2.256, acc: 0.526, val_loss: 2.589, val_acc: 0.512

================================================================================2025-08_13 15:49:15
******** [step = 50] loss: 2.176, acc: 0.534
******** [step = 100] loss: 2.181, acc: 0.534
******** [step = 150] loss: 2.187, acc: 0.534
******** [step = 200] loss: 2.192, acc: 0.533
******** [step = 250] loss: 2.196, acc: 0.532
******** [step = 300] loss: 2.201, acc: 0.532
******** [step = 350] loss: 2.206, acc: 0.531
******** [step = 400] loss: 2.213, acc: 0.531
******** [step = 450] loss: 2.216, acc: 0.531
******** [step = 500] loss: 2.221, acc: 0.530
******** [step = 550] loss: 2.225, acc: 0.530
******** [step = 600] loss: 2.229, acc: 0.529
******** [step = 650] loss: 2.233, acc: 0.529
******** [step = 700] loss: 2.235, acc: 0.529
******** [step = 750] loss: 2.239, acc: 0.529
******** [step = 800] loss: 2.243, acc: 0.528
******** [step = 850] loss: 2.244, acc: 0.528
EPOCH = 31 loss: 2.244, acc: 0.528, val_loss: 2.582, val_acc: 0.512

================================================================================2025-08_13 15:51:15
******** [step = 50] loss: 2.181, acc: 0.538
******** [step = 100] loss: 2.176, acc: 0.536
******** [step = 150] loss: 2.184, acc: 0.536
******** [step = 200] loss: 2.186, acc: 0.535
******** [step = 250] loss: 2.190, acc: 0.534
******** [step = 300] loss: 2.193, acc: 0.534
******** [step = 350] loss: 2.199, acc: 0.533
******** [step = 400] loss: 2.201, acc: 0.533
******** [step = 450] loss: 2.206, acc: 0.533
******** [step = 500] loss: 2.208, acc: 0.533
******** [step = 550] loss: 2.213, acc: 0.532
******** [step = 600] loss: 2.216, acc: 0.532
******** [step = 650] loss: 2.219, acc: 0.531
******** [step = 700] loss: 2.225, acc: 0.531
******** [step = 750] loss: 2.229, acc: 0.530
******** [step = 800] loss: 2.233, acc: 0.530
******** [step = 850] loss: 2.236, acc: 0.530
EPOCH = 32 loss: 2.236, acc: 0.530, val_loss: 2.584, val_acc: 0.516

================================================================================2025-08_13 15:53:19
******** [step = 50] loss: 2.155, acc: 0.535
******** [step = 100] loss: 2.158, acc: 0.536
******** [step = 150] loss: 2.161, acc: 0.536
******** [step = 200] loss: 2.167, acc: 0.536
******** [step = 250] loss: 2.177, acc: 0.535
******** [step = 300] loss: 2.183, acc: 0.535
******** [step = 350] loss: 2.187, acc: 0.534
******** [step = 400] loss: 2.194, acc: 0.534
******** [step = 450] loss: 2.197, acc: 0.533
******** [step = 500] loss: 2.200, acc: 0.533
******** [step = 550] loss: 2.205, acc: 0.533
******** [step = 600] loss: 2.209, acc: 0.532
******** [step = 650] loss: 2.212, acc: 0.532
******** [step = 700] loss: 2.216, acc: 0.532
******** [step = 750] loss: 2.221, acc: 0.531
******** [step = 800] loss: 2.223, acc: 0.531
******** [step = 850] loss: 2.227, acc: 0.531
EPOCH = 33 loss: 2.227, acc: 0.531, val_loss: 2.580, val_acc: 0.515

================================================================================2025-08_13 15:55:20
******** [step = 50] loss: 2.167, acc: 0.537
******** [step = 100] loss: 2.164, acc: 0.538
******** [step = 150] loss: 2.159, acc: 0.538
******** [step = 200] loss: 2.166, acc: 0.538
******** [step = 250] loss: 2.172, acc: 0.537
******** [step = 300] loss: 2.175, acc: 0.537
******** [step = 350] loss: 2.177, acc: 0.537
******** [step = 400] loss: 2.180, acc: 0.536
******** [step = 450] loss: 2.185, acc: 0.536
******** [step = 500] loss: 2.195, acc: 0.534
******** [step = 550] loss: 2.203, acc: 0.532
******** [step = 600] loss: 2.207, acc: 0.532
******** [step = 650] loss: 2.215, acc: 0.531
******** [step = 700] loss: 2.221, acc: 0.530
******** [step = 750] loss: 2.226, acc: 0.529
******** [step = 800] loss: 2.230, acc: 0.529
******** [step = 850] loss: 2.233, acc: 0.529
EPOCH = 34 loss: 2.233, acc: 0.529, val_loss: 2.597, val_acc: 0.509

================================================================================2025-08_13 15:57:19
******** [step = 50] loss: 2.165, acc: 0.533
******** [step = 100] loss: 2.163, acc: 0.533
******** [step = 150] loss: 2.170, acc: 0.534
******** [step = 200] loss: 2.176, acc: 0.534
******** [step = 250] loss: 2.184, acc: 0.533
******** [step = 300] loss: 2.186, acc: 0.533
******** [step = 350] loss: 2.189, acc: 0.533
******** [step = 400] loss: 2.196, acc: 0.533
******** [step = 450] loss: 2.200, acc: 0.532
******** [step = 500] loss: 2.205, acc: 0.532
******** [step = 550] loss: 2.210, acc: 0.532
******** [step = 600] loss: 2.211, acc: 0.532
******** [step = 650] loss: 2.214, acc: 0.532
******** [step = 700] loss: 2.217, acc: 0.531
******** [step = 750] loss: 2.221, acc: 0.531
******** [step = 800] loss: 2.224, acc: 0.531
******** [step = 850] loss: 2.226, acc: 0.531
EPOCH = 35 loss: 2.226, acc: 0.531, val_loss: 2.599, val_acc: 0.513

================================================================================2025-08_13 15:59:19
******** [step = 50] loss: 2.166, acc: 0.538
******** [step = 100] loss: 2.154, acc: 0.539
******** [step = 150] loss: 2.158, acc: 0.538
******** [step = 200] loss: 2.165, acc: 0.538
******** [step = 250] loss: 2.168, acc: 0.537
******** [step = 300] loss: 2.167, acc: 0.537
******** [step = 350] loss: 2.174, acc: 0.537
******** [step = 400] loss: 2.178, acc: 0.537
******** [step = 450] loss: 2.182, acc: 0.536
******** [step = 500] loss: 2.185, acc: 0.536
******** [step = 550] loss: 2.189, acc: 0.536
******** [step = 600] loss: 2.190, acc: 0.536
******** [step = 650] loss: 2.196, acc: 0.536
******** [step = 700] loss: 2.200, acc: 0.536
******** [step = 750] loss: 2.202, acc: 0.536
******** [step = 800] loss: 2.205, acc: 0.536
******** [step = 850] loss: 2.209, acc: 0.535
EPOCH = 36 loss: 2.209, acc: 0.535, val_loss: 2.572, val_acc: 0.520

================================================================================2025-08_13 16:01:19
******** [step = 50] loss: 2.131, acc: 0.546
******** [step = 100] loss: 2.129, acc: 0.546
******** [step = 150] loss: 2.130, acc: 0.546
******** [step = 200] loss: 2.134, acc: 0.545
******** [step = 250] loss: 2.142, acc: 0.544
******** [step = 300] loss: 2.146, acc: 0.544
******** [step = 350] loss: 2.152, acc: 0.543
******** [step = 400] loss: 2.157, acc: 0.542
******** [step = 450] loss: 2.163, acc: 0.542
******** [step = 500] loss: 2.167, acc: 0.541
******** [step = 550] loss: 2.172, acc: 0.541
******** [step = 600] loss: 2.175, acc: 0.541
******** [step = 650] loss: 2.181, acc: 0.540
******** [step = 700] loss: 2.185, acc: 0.540
******** [step = 750] loss: 2.186, acc: 0.540
******** [step = 800] loss: 2.190, acc: 0.540
******** [step = 850] loss: 2.193, acc: 0.540
EPOCH = 37 loss: 2.193, acc: 0.540, val_loss: 2.556, val_acc: 0.522

================================================================================2025-08_13 16:03:20
******** [step = 50] loss: 2.115, acc: 0.547
******** [step = 100] loss: 2.121, acc: 0.546
******** [step = 150] loss: 2.132, acc: 0.546
******** [step = 200] loss: 2.135, acc: 0.545
******** [step = 250] loss: 2.135, acc: 0.546
******** [step = 300] loss: 2.141, acc: 0.545
******** [step = 350] loss: 2.143, acc: 0.545
******** [step = 400] loss: 2.146, acc: 0.545
******** [step = 450] loss: 2.150, acc: 0.544
******** [step = 500] loss: 2.155, acc: 0.544
******** [step = 550] loss: 2.159, acc: 0.543
******** [step = 600] loss: 2.162, acc: 0.543
******** [step = 650] loss: 2.167, acc: 0.543
******** [step = 700] loss: 2.169, acc: 0.543
******** [step = 750] loss: 2.172, acc: 0.542
******** [step = 800] loss: 2.175, acc: 0.542
******** [step = 850] loss: 2.178, acc: 0.542
EPOCH = 38 loss: 2.178, acc: 0.542, val_loss: 2.555, val_acc: 0.521

================================================================================2025-08_13 16:05:24
******** [step = 50] loss: 2.118, acc: 0.546
******** [step = 100] loss: 2.114, acc: 0.548
******** [step = 150] loss: 2.117, acc: 0.548
******** [step = 200] loss: 2.114, acc: 0.548
******** [step = 250] loss: 2.122, acc: 0.547
******** [step = 300] loss: 2.125, acc: 0.547
******** [step = 350] loss: 2.130, acc: 0.546
******** [step = 400] loss: 2.135, acc: 0.546
******** [step = 450] loss: 2.142, acc: 0.545
******** [step = 500] loss: 2.143, acc: 0.545
******** [step = 550] loss: 2.147, acc: 0.545
******** [step = 600] loss: 2.150, acc: 0.545
******** [step = 650] loss: 2.155, acc: 0.544
******** [step = 700] loss: 2.160, acc: 0.544
******** [step = 750] loss: 2.164, acc: 0.544
******** [step = 800] loss: 2.166, acc: 0.544
******** [step = 850] loss: 2.171, acc: 0.543
EPOCH = 39 loss: 2.171, acc: 0.543, val_loss: 2.559, val_acc: 0.521

================================================================================2025-08_13 16:07:23
******** [step = 50] loss: 2.087, acc: 0.550
******** [step = 100] loss: 2.087, acc: 0.551
******** [step = 150] loss: 2.096, acc: 0.551
******** [step = 200] loss: 2.102, acc: 0.550
******** [step = 250] loss: 2.110, acc: 0.549
******** [step = 300] loss: 2.120, acc: 0.548
******** [step = 350] loss: 2.123, acc: 0.548
******** [step = 400] loss: 2.129, acc: 0.547
******** [step = 450] loss: 2.133, acc: 0.547
******** [step = 500] loss: 2.138, acc: 0.547
******** [step = 550] loss: 2.141, acc: 0.546
******** [step = 600] loss: 2.146, acc: 0.546
******** [step = 650] loss: 2.149, acc: 0.546
******** [step = 700] loss: 2.152, acc: 0.545
******** [step = 750] loss: 2.157, acc: 0.545
******** [step = 800] loss: 2.160, acc: 0.544
******** [step = 850] loss: 2.163, acc: 0.544
EPOCH = 40 loss: 2.163, acc: 0.544, val_loss: 2.547, val_acc: 0.523

================================================================================2025-08_13 16:09:23
******** [step = 50] loss: 2.112, acc: 0.547
******** [step = 100] loss: 2.102, acc: 0.551
******** [step = 150] loss: 2.101, acc: 0.551
******** [step = 200] loss: 2.106, acc: 0.550
******** [step = 250] loss: 2.108, acc: 0.550
******** [step = 300] loss: 2.114, acc: 0.549
******** [step = 350] loss: 2.118, acc: 0.549
******** [step = 400] loss: 2.123, acc: 0.548
******** [step = 450] loss: 2.125, acc: 0.548
******** [step = 500] loss: 2.131, acc: 0.547
******** [step = 550] loss: 2.135, acc: 0.547
******** [step = 600] loss: 2.139, acc: 0.547
******** [step = 650] loss: 2.141, acc: 0.546
******** [step = 700] loss: 2.145, acc: 0.546
******** [step = 750] loss: 2.148, acc: 0.546
******** [step = 800] loss: 2.151, acc: 0.546
******** [step = 850] loss: 2.155, acc: 0.545
EPOCH = 41 loss: 2.155, acc: 0.545, val_loss: 2.552, val_acc: 0.525

================================================================================2025-08_13 16:11:24
******** [step = 50] loss: 2.078, acc: 0.553
******** [step = 100] loss: 2.082, acc: 0.553
******** [step = 150] loss: 2.093, acc: 0.552
******** [step = 200] loss: 2.094, acc: 0.551
******** [step = 250] loss: 2.099, acc: 0.550
******** [step = 300] loss: 2.102, acc: 0.550
******** [step = 350] loss: 2.106, acc: 0.550
******** [step = 400] loss: 2.111, acc: 0.550
******** [step = 450] loss: 2.113, acc: 0.550
******** [step = 500] loss: 2.120, acc: 0.549
******** [step = 550] loss: 2.124, acc: 0.549
******** [step = 600] loss: 2.127, acc: 0.549
******** [step = 650] loss: 2.131, acc: 0.548
******** [step = 700] loss: 2.135, acc: 0.548
******** [step = 750] loss: 2.140, acc: 0.548
******** [step = 800] loss: 2.143, acc: 0.547
******** [step = 850] loss: 2.148, acc: 0.547
EPOCH = 42 loss: 2.148, acc: 0.547, val_loss: 2.538, val_acc: 0.524

================================================================================2025-08_13 16:13:24
******** [step = 50] loss: 2.083, acc: 0.553
******** [step = 100] loss: 2.072, acc: 0.554
******** [step = 150] loss: 2.081, acc: 0.553
******** [step = 200] loss: 2.086, acc: 0.553
******** [step = 250] loss: 2.093, acc: 0.552
******** [step = 300] loss: 2.100, acc: 0.551
******** [step = 350] loss: 2.101, acc: 0.551
******** [step = 400] loss: 2.104, acc: 0.550
******** [step = 450] loss: 2.107, acc: 0.550
******** [step = 500] loss: 2.112, acc: 0.550
******** [step = 550] loss: 2.117, acc: 0.549
******** [step = 600] loss: 2.123, acc: 0.549
******** [step = 650] loss: 2.127, acc: 0.549
******** [step = 700] loss: 2.131, acc: 0.549
******** [step = 750] loss: 2.133, acc: 0.548
******** [step = 800] loss: 2.136, acc: 0.548
******** [step = 850] loss: 2.140, acc: 0.548
EPOCH = 43 loss: 2.140, acc: 0.548, val_loss: 2.546, val_acc: 0.525

================================================================================2025-08_13 16:15:24
******** [step = 50] loss: 2.061, acc: 0.557
******** [step = 100] loss: 2.061, acc: 0.556
******** [step = 150] loss: 2.066, acc: 0.555
******** [step = 200] loss: 2.075, acc: 0.554
******** [step = 250] loss: 2.079, acc: 0.554
******** [step = 300] loss: 2.088, acc: 0.552
******** [step = 350] loss: 2.093, acc: 0.552
******** [step = 400] loss: 2.098, acc: 0.552
******** [step = 450] loss: 2.101, acc: 0.552
******** [step = 500] loss: 2.106, acc: 0.551
******** [step = 550] loss: 2.110, acc: 0.551
******** [step = 600] loss: 2.115, acc: 0.550
******** [step = 650] loss: 2.120, acc: 0.550
******** [step = 700] loss: 2.123, acc: 0.549
******** [step = 750] loss: 2.127, acc: 0.549
******** [step = 800] loss: 2.131, acc: 0.549
******** [step = 850] loss: 2.134, acc: 0.549
EPOCH = 44 loss: 2.134, acc: 0.549, val_loss: 2.541, val_acc: 0.526

================================================================================2025-08_13 16:17:24
******** [step = 50] loss: 2.061, acc: 0.555
******** [step = 100] loss: 2.062, acc: 0.556
******** [step = 150] loss: 2.069, acc: 0.556
******** [step = 200] loss: 2.074, acc: 0.555
******** [step = 250] loss: 2.077, acc: 0.555
******** [step = 300] loss: 2.080, acc: 0.555
******** [step = 350] loss: 2.083, acc: 0.555
******** [step = 400] loss: 2.088, acc: 0.554
******** [step = 450] loss: 2.091, acc: 0.553
******** [step = 500] loss: 2.096, acc: 0.553
******** [step = 550] loss: 2.102, acc: 0.552
******** [step = 600] loss: 2.109, acc: 0.552
******** [step = 650] loss: 2.113, acc: 0.551
******** [step = 700] loss: 2.116, acc: 0.551
******** [step = 750] loss: 2.120, acc: 0.550
******** [step = 800] loss: 2.123, acc: 0.550
******** [step = 850] loss: 2.126, acc: 0.550
EPOCH = 45 loss: 2.126, acc: 0.550, val_loss: 2.532, val_acc: 0.526

================================================================================2025-08_13 16:19:24
******** [step = 50] loss: 2.053, acc: 0.558
******** [step = 100] loss: 2.054, acc: 0.558
******** [step = 150] loss: 2.059, acc: 0.558
******** [step = 200] loss: 2.063, acc: 0.557
******** [step = 250] loss: 2.067, acc: 0.557
******** [step = 300] loss: 2.071, acc: 0.556
******** [step = 350] loss: 2.078, acc: 0.555
******** [step = 400] loss: 2.084, acc: 0.554
******** [step = 450] loss: 2.089, acc: 0.554
******** [step = 500] loss: 2.094, acc: 0.553
******** [step = 550] loss: 2.100, acc: 0.552
******** [step = 600] loss: 2.105, acc: 0.552
******** [step = 650] loss: 2.108, acc: 0.552
******** [step = 700] loss: 2.112, acc: 0.551
******** [step = 750] loss: 2.116, acc: 0.551
******** [step = 800] loss: 2.118, acc: 0.551
******** [step = 850] loss: 2.121, acc: 0.550
EPOCH = 46 loss: 2.121, acc: 0.550, val_loss: 2.531, val_acc: 0.528

================================================================================2025-08_13 16:21:24
******** [step = 50] loss: 2.044, acc: 0.559
******** [step = 100] loss: 2.050, acc: 0.558
******** [step = 150] loss: 2.056, acc: 0.558
******** [step = 200] loss: 2.066, acc: 0.556
******** [step = 250] loss: 2.070, acc: 0.556
******** [step = 300] loss: 2.073, acc: 0.556
******** [step = 350] loss: 2.077, acc: 0.556
******** [step = 400] loss: 2.078, acc: 0.555
******** [step = 450] loss: 2.082, acc: 0.554
******** [step = 500] loss: 2.086, acc: 0.554
******** [step = 550] loss: 2.091, acc: 0.554
******** [step = 600] loss: 2.093, acc: 0.554
******** [step = 650] loss: 2.099, acc: 0.553
******** [step = 700] loss: 2.104, acc: 0.553
******** [step = 750] loss: 2.107, acc: 0.552
******** [step = 800] loss: 2.110, acc: 0.552
******** [step = 850] loss: 2.113, acc: 0.552
EPOCH = 47 loss: 2.113, acc: 0.552, val_loss: 2.536, val_acc: 0.528

================================================================================2025-08_13 16:23:30
******** [step = 50] loss: 2.027, acc: 0.561
******** [step = 100] loss: 2.036, acc: 0.561
******** [step = 150] loss: 2.046, acc: 0.560
******** [step = 200] loss: 2.051, acc: 0.560
******** [step = 250] loss: 2.052, acc: 0.560
******** [step = 300] loss: 2.056, acc: 0.559
******** [step = 350] loss: 2.064, acc: 0.558
******** [step = 400] loss: 2.069, acc: 0.557
******** [step = 450] loss: 2.075, acc: 0.556
******** [step = 500] loss: 2.081, acc: 0.555
******** [step = 550] loss: 2.084, acc: 0.555
******** [step = 600] loss: 2.090, acc: 0.554
******** [step = 650] loss: 2.095, acc: 0.554
******** [step = 700] loss: 2.099, acc: 0.553
******** [step = 750] loss: 2.101, acc: 0.553
******** [step = 800] loss: 2.105, acc: 0.553
******** [step = 850] loss: 2.111, acc: 0.552
EPOCH = 48 loss: 2.111, acc: 0.552, val_loss: 2.539, val_acc: 0.527

================================================================================2025-08_13 16:25:30
******** [step = 50] loss: 2.028, acc: 0.559
******** [step = 100] loss: 2.031, acc: 0.560
******** [step = 150] loss: 2.038, acc: 0.560
******** [step = 200] loss: 2.046, acc: 0.560
******** [step = 250] loss: 2.049, acc: 0.559
******** [step = 300] loss: 2.056, acc: 0.559
******** [step = 350] loss: 2.060, acc: 0.558
******** [step = 400] loss: 2.067, acc: 0.557
******** [step = 450] loss: 2.070, acc: 0.557
******** [step = 500] loss: 2.076, acc: 0.556
******** [step = 550] loss: 2.080, acc: 0.556
******** [step = 600] loss: 2.082, acc: 0.555
******** [step = 650] loss: 2.086, acc: 0.555
******** [step = 700] loss: 2.091, acc: 0.555
******** [step = 750] loss: 2.096, acc: 0.555
******** [step = 800] loss: 2.099, acc: 0.554
******** [step = 850] loss: 2.102, acc: 0.554
EPOCH = 49 loss: 2.102, acc: 0.554, val_loss: 2.526, val_acc: 0.528

================================================================================2025-08_13 16:27:31
******** [step = 50] loss: 2.019, acc: 0.559
******** [step = 100] loss: 2.028, acc: 0.561
******** [step = 150] loss: 2.033, acc: 0.561
******** [step = 200] loss: 2.043, acc: 0.560
******** [step = 250] loss: 2.047, acc: 0.559
******** [step = 300] loss: 2.052, acc: 0.558
******** [step = 350] loss: 2.058, acc: 0.558
******** [step = 400] loss: 2.063, acc: 0.557
******** [step = 450] loss: 2.066, acc: 0.557
******** [step = 500] loss: 2.070, acc: 0.556
******** [step = 550] loss: 2.075, acc: 0.556
******** [step = 600] loss: 2.080, acc: 0.555
******** [step = 650] loss: 2.085, acc: 0.555
******** [step = 700] loss: 2.088, acc: 0.555
******** [step = 750] loss: 2.092, acc: 0.554
******** [step = 800] loss: 2.095, acc: 0.554
******** [step = 850] loss: 2.098, acc: 0.554
EPOCH = 50 loss: 2.098, acc: 0.554, val_loss: 2.530, val_acc: 0.529

================================================================================2025-08_13 16:29:37
******** [step = 50] loss: 2.022, acc: 0.560
******** [step = 100] loss: 2.028, acc: 0.560
******** [step = 150] loss: 2.034, acc: 0.560
******** [step = 200] loss: 2.038, acc: 0.560
******** [step = 250] loss: 2.045, acc: 0.559
******** [step = 300] loss: 2.053, acc: 0.558
******** [step = 350] loss: 2.059, acc: 0.557
******** [step = 400] loss: 2.062, acc: 0.557
******** [step = 450] loss: 2.066, acc: 0.557
******** [step = 500] loss: 2.069, acc: 0.557
******** [step = 550] loss: 2.074, acc: 0.556
******** [step = 600] loss: 2.077, acc: 0.556
******** [step = 650] loss: 2.080, acc: 0.556
******** [step = 700] loss: 2.083, acc: 0.555
******** [step = 750] loss: 2.085, acc: 0.555
******** [step = 800] loss: 2.089, acc: 0.555
******** [step = 850] loss: 2.093, acc: 0.554
EPOCH = 51 loss: 2.093, acc: 0.554, val_loss: 2.525, val_acc: 0.529

================================================================================2025-08_13 16:31:37
******** [step = 50] loss: 2.023, acc: 0.563
******** [step = 100] loss: 2.024, acc: 0.562
******** [step = 150] loss: 2.026, acc: 0.563
******** [step = 200] loss: 2.029, acc: 0.563
******** [step = 250] loss: 2.037, acc: 0.562
******** [step = 300] loss: 2.046, acc: 0.561
******** [step = 350] loss: 2.050, acc: 0.560
******** [step = 400] loss: 2.053, acc: 0.559
******** [step = 450] loss: 2.060, acc: 0.559
******** [step = 500] loss: 2.064, acc: 0.559
******** [step = 550] loss: 2.067, acc: 0.558
******** [step = 600] loss: 2.071, acc: 0.558
******** [step = 650] loss: 2.073, acc: 0.558
******** [step = 700] loss: 2.079, acc: 0.557
******** [step = 750] loss: 2.082, acc: 0.557
******** [step = 800] loss: 2.086, acc: 0.556
******** [step = 850] loss: 2.089, acc: 0.556
EPOCH = 52 loss: 2.089, acc: 0.556, val_loss: 2.525, val_acc: 0.531

================================================================================2025-08_13 16:33:44
******** [step = 50] loss: 2.023, acc: 0.565
******** [step = 100] loss: 2.019, acc: 0.563
******** [step = 150] loss: 2.026, acc: 0.563
******** [step = 200] loss: 2.032, acc: 0.562
******** [step = 250] loss: 2.041, acc: 0.561
******** [step = 300] loss: 2.047, acc: 0.560
******** [step = 350] loss: 2.050, acc: 0.559
******** [step = 400] loss: 2.053, acc: 0.559
******** [step = 450] loss: 2.060, acc: 0.558
******** [step = 500] loss: 2.063, acc: 0.558
******** [step = 550] loss: 2.065, acc: 0.558
******** [step = 600] loss: 2.070, acc: 0.557
******** [step = 650] loss: 2.074, acc: 0.557
******** [step = 700] loss: 2.077, acc: 0.557
******** [step = 750] loss: 2.079, acc: 0.557
******** [step = 800] loss: 2.083, acc: 0.556
******** [step = 850] loss: 2.086, acc: 0.556
EPOCH = 53 loss: 2.086, acc: 0.556, val_loss: 2.529, val_acc: 0.529

================================================================================2025-08_13 16:35:49
******** [step = 50] loss: 2.007, acc: 0.564
******** [step = 100] loss: 2.019, acc: 0.562
******** [step = 150] loss: 2.024, acc: 0.562
******** [step = 200] loss: 2.028, acc: 0.562
******** [step = 250] loss: 2.033, acc: 0.561
******** [step = 300] loss: 2.033, acc: 0.561
******** [step = 350] loss: 2.038, acc: 0.561
******** [step = 400] loss: 2.042, acc: 0.560
******** [step = 450] loss: 2.047, acc: 0.560
******** [step = 500] loss: 2.052, acc: 0.560
******** [step = 550] loss: 2.058, acc: 0.559
******** [step = 600] loss: 2.062, acc: 0.559
******** [step = 650] loss: 2.067, acc: 0.558
******** [step = 700] loss: 2.071, acc: 0.558
******** [step = 750] loss: 2.073, acc: 0.558
******** [step = 800] loss: 2.077, acc: 0.557
******** [step = 850] loss: 2.080, acc: 0.557
EPOCH = 54 loss: 2.080, acc: 0.557, val_loss: 2.522, val_acc: 0.530

================================================================================2025-08_13 16:38:00
******** [step = 50] loss: 2.000, acc: 0.569
******** [step = 100] loss: 2.007, acc: 0.566
******** [step = 150] loss: 2.009, acc: 0.566
******** [step = 200] loss: 2.018, acc: 0.564
******** [step = 250] loss: 2.023, acc: 0.563
******** [step = 300] loss: 2.030, acc: 0.563
******** [step = 350] loss: 2.035, acc: 0.562
******** [step = 400] loss: 2.041, acc: 0.561
******** [step = 450] loss: 2.044, acc: 0.561
******** [step = 500] loss: 2.048, acc: 0.560
******** [step = 550] loss: 2.054, acc: 0.560
******** [step = 600] loss: 2.057, acc: 0.559
******** [step = 650] loss: 2.062, acc: 0.558
******** [step = 700] loss: 2.066, acc: 0.558
******** [step = 750] loss: 2.069, acc: 0.558
******** [step = 800] loss: 2.072, acc: 0.557
******** [step = 850] loss: 2.074, acc: 0.557
EPOCH = 55 loss: 2.074, acc: 0.557, val_loss: 2.535, val_acc: 0.531

================================================================================2025-08_13 16:40:10
******** [step = 50] loss: 1.997, acc: 0.566
******** [step = 100] loss: 1.996, acc: 0.566
******** [step = 150] loss: 2.003, acc: 0.564
******** [step = 200] loss: 2.007, acc: 0.564
******** [step = 250] loss: 2.015, acc: 0.564
******** [step = 300] loss: 2.025, acc: 0.562
******** [step = 350] loss: 2.028, acc: 0.562
******** [step = 400] loss: 2.034, acc: 0.561
******** [step = 450] loss: 2.040, acc: 0.561
******** [step = 500] loss: 2.043, acc: 0.561
******** [step = 550] loss: 2.047, acc: 0.560
******** [step = 600] loss: 2.052, acc: 0.560
******** [step = 650] loss: 2.056, acc: 0.560
******** [step = 700] loss: 2.061, acc: 0.559
******** [step = 750] loss: 2.065, acc: 0.559
******** [step = 800] loss: 2.069, acc: 0.558
******** [step = 850] loss: 2.071, acc: 0.559
EPOCH = 56 loss: 2.071, acc: 0.559, val_loss: 2.523, val_acc: 0.532

================================================================================2025-08_13 16:42:10
******** [step = 50] loss: 2.001, acc: 0.565
******** [step = 100] loss: 1.999, acc: 0.565
******** [step = 150] loss: 2.001, acc: 0.565
******** [step = 200] loss: 2.006, acc: 0.565
******** [step = 250] loss: 2.010, acc: 0.564
******** [step = 300] loss: 2.016, acc: 0.564
******** [step = 350] loss: 2.024, acc: 0.563
******** [step = 400] loss: 2.028, acc: 0.563
******** [step = 450] loss: 2.033, acc: 0.562
******** [step = 500] loss: 2.038, acc: 0.562
******** [step = 550] loss: 2.041, acc: 0.561
******** [step = 600] loss: 2.045, acc: 0.561
******** [step = 650] loss: 2.049, acc: 0.561
******** [step = 700] loss: 2.053, acc: 0.561
******** [step = 750] loss: 2.058, acc: 0.560
******** [step = 800] loss: 2.062, acc: 0.560
******** [step = 850] loss: 2.066, acc: 0.559
EPOCH = 57 loss: 2.066, acc: 0.559, val_loss: 2.518, val_acc: 0.531

================================================================================2025-08_13 16:44:15
******** [step = 50] loss: 1.998, acc: 0.564
******** [step = 100] loss: 1.992, acc: 0.566
******** [step = 150] loss: 1.994, acc: 0.567
******** [step = 200] loss: 2.005, acc: 0.566
******** [step = 250] loss: 2.017, acc: 0.564
******** [step = 300] loss: 2.025, acc: 0.564
******** [step = 350] loss: 2.032, acc: 0.563
******** [step = 400] loss: 2.034, acc: 0.562
******** [step = 450] loss: 2.038, acc: 0.562
******** [step = 500] loss: 2.042, acc: 0.562
******** [step = 550] loss: 2.044, acc: 0.561
******** [step = 600] loss: 2.048, acc: 0.561
******** [step = 650] loss: 2.052, acc: 0.560
******** [step = 700] loss: 2.055, acc: 0.560
******** [step = 750] loss: 2.059, acc: 0.560
******** [step = 800] loss: 2.063, acc: 0.559
******** [step = 850] loss: 2.066, acc: 0.559
EPOCH = 58 loss: 2.066, acc: 0.559, val_loss: 2.530, val_acc: 0.532

================================================================================2025-08_13 16:46:15
******** [step = 50] loss: 1.980, acc: 0.568
******** [step = 100] loss: 1.992, acc: 0.567
******** [step = 150] loss: 1.996, acc: 0.567
******** [step = 200] loss: 2.002, acc: 0.566
******** [step = 250] loss: 2.008, acc: 0.566
******** [step = 300] loss: 2.012, acc: 0.565
******** [step = 350] loss: 2.019, acc: 0.565
******** [step = 400] loss: 2.024, acc: 0.564
******** [step = 450] loss: 2.028, acc: 0.563
******** [step = 500] loss: 2.032, acc: 0.563
******** [step = 550] loss: 2.036, acc: 0.563
******** [step = 600] loss: 2.041, acc: 0.562
******** [step = 650] loss: 2.043, acc: 0.562
******** [step = 700] loss: 2.046, acc: 0.562
******** [step = 750] loss: 2.051, acc: 0.561
******** [step = 800] loss: 2.054, acc: 0.561
******** [step = 850] loss: 2.058, acc: 0.560
EPOCH = 59 loss: 2.058, acc: 0.560, val_loss: 2.520, val_acc: 0.533

================================================================================2025-08_13 16:48:14
******** [step = 50] loss: 1.994, acc: 0.566
******** [step = 100] loss: 2.003, acc: 0.566
******** [step = 150] loss: 2.003, acc: 0.567
******** [step = 200] loss: 2.003, acc: 0.567
******** [step = 250] loss: 2.009, acc: 0.566
******** [step = 300] loss: 2.010, acc: 0.566
******** [step = 350] loss: 2.016, acc: 0.565
******** [step = 400] loss: 2.021, acc: 0.565
******** [step = 450] loss: 2.025, acc: 0.564
******** [step = 500] loss: 2.029, acc: 0.564
******** [step = 550] loss: 2.033, acc: 0.563
******** [step = 600] loss: 2.037, acc: 0.563
******** [step = 650] loss: 2.040, acc: 0.563
******** [step = 700] loss: 2.044, acc: 0.562
******** [step = 750] loss: 2.048, acc: 0.562
******** [step = 800] loss: 2.050, acc: 0.562
******** [step = 850] loss: 2.053, acc: 0.562
EPOCH = 60 loss: 2.053, acc: 0.562, val_loss: 2.517, val_acc: 0.533

================================================================================2025-08_13 16:50:14
******** [step = 50] loss: 1.999, acc: 0.566
******** [step = 100] loss: 1.997, acc: 0.567
******** [step = 150] loss: 1.995, acc: 0.567
******** [step = 200] loss: 2.003, acc: 0.567
******** [step = 250] loss: 2.008, acc: 0.566
******** [step = 300] loss: 2.011, acc: 0.566
******** [step = 350] loss: 2.014, acc: 0.565
******** [step = 400] loss: 2.019, acc: 0.564
******** [step = 450] loss: 2.025, acc: 0.564
******** [step = 500] loss: 2.029, acc: 0.563
******** [step = 550] loss: 2.032, acc: 0.563
******** [step = 600] loss: 2.036, acc: 0.562
******** [step = 650] loss: 2.040, acc: 0.562
******** [step = 700] loss: 2.042, acc: 0.562
******** [step = 750] loss: 2.045, acc: 0.562
******** [step = 800] loss: 2.048, acc: 0.562
******** [step = 850] loss: 2.051, acc: 0.562
EPOCH = 61 loss: 2.051, acc: 0.562, val_loss: 2.518, val_acc: 0.532

================================================================================2025-08_13 16:52:24
******** [step = 50] loss: 1.966, acc: 0.570
******** [step = 100] loss: 1.975, acc: 0.569
******** [step = 150] loss: 1.984, acc: 0.568
******** [step = 200] loss: 1.988, acc: 0.567
******** [step = 250] loss: 1.995, acc: 0.566
******** [step = 300] loss: 1.997, acc: 0.567
******** [step = 350] loss: 2.003, acc: 0.566
******** [step = 400] loss: 2.007, acc: 0.566
******** [step = 450] loss: 2.014, acc: 0.565
******** [step = 500] loss: 2.017, acc: 0.565
******** [step = 550] loss: 2.022, acc: 0.565
******** [step = 600] loss: 2.026, acc: 0.565
******** [step = 650] loss: 2.031, acc: 0.564
******** [step = 700] loss: 2.035, acc: 0.564
******** [step = 750] loss: 2.040, acc: 0.563
******** [step = 800] loss: 2.042, acc: 0.563
******** [step = 850] loss: 2.047, acc: 0.563
EPOCH = 62 loss: 2.047, acc: 0.563, val_loss: 2.513, val_acc: 0.535

================================================================================2025-08_13 16:54:31
******** [step = 50] loss: 1.968, acc: 0.573
******** [step = 100] loss: 1.973, acc: 0.572
******** [step = 150] loss: 1.974, acc: 0.571
******** [step = 200] loss: 1.980, acc: 0.571
******** [step = 250] loss: 1.988, acc: 0.570
******** [step = 300] loss: 1.994, acc: 0.569
******** [step = 350] loss: 2.001, acc: 0.568
******** [step = 400] loss: 2.006, acc: 0.567
******** [step = 450] loss: 2.011, acc: 0.567
******** [step = 500] loss: 2.014, acc: 0.567
******** [step = 550] loss: 2.019, acc: 0.566
******** [step = 600] loss: 2.023, acc: 0.565
******** [step = 650] loss: 2.028, acc: 0.565
******** [step = 700] loss: 2.032, acc: 0.564
******** [step = 750] loss: 2.036, acc: 0.564
******** [step = 800] loss: 2.040, acc: 0.564
******** [step = 850] loss: 2.044, acc: 0.563
EPOCH = 63 loss: 2.044, acc: 0.563, val_loss: 2.512, val_acc: 0.535

================================================================================2025-08_13 16:56:31
******** [step = 50] loss: 1.972, acc: 0.569
******** [step = 100] loss: 1.973, acc: 0.570
******** [step = 150] loss: 1.971, acc: 0.571
******** [step = 200] loss: 1.978, acc: 0.571
******** [step = 250] loss: 1.985, acc: 0.570
******** [step = 300] loss: 1.993, acc: 0.569
******** [step = 350] loss: 1.999, acc: 0.568
******** [step = 400] loss: 2.002, acc: 0.568
******** [step = 450] loss: 2.007, acc: 0.567
******** [step = 500] loss: 2.011, acc: 0.567
******** [step = 550] loss: 2.017, acc: 0.566
******** [step = 600] loss: 2.022, acc: 0.566
******** [step = 650] loss: 2.025, acc: 0.565
******** [step = 700] loss: 2.029, acc: 0.565
******** [step = 750] loss: 2.032, acc: 0.565
******** [step = 800] loss: 2.035, acc: 0.565
******** [step = 850] loss: 2.040, acc: 0.564
EPOCH = 64 loss: 2.040, acc: 0.564, val_loss: 2.513, val_acc: 0.533

================================================================================2025-08_13 16:58:33
******** [step = 50] loss: 1.957, acc: 0.573
******** [step = 100] loss: 1.958, acc: 0.574
******** [step = 150] loss: 1.976, acc: 0.572
******** [step = 200] loss: 1.983, acc: 0.570
******** [step = 250] loss: 1.985, acc: 0.570
******** [step = 300] loss: 1.991, acc: 0.569
******** [step = 350] loss: 1.996, acc: 0.568
******** [step = 400] loss: 2.003, acc: 0.568
******** [step = 450] loss: 2.010, acc: 0.567
******** [step = 500] loss: 2.013, acc: 0.567
******** [step = 550] loss: 2.016, acc: 0.566
******** [step = 600] loss: 2.019, acc: 0.566
******** [step = 650] loss: 2.024, acc: 0.566
******** [step = 700] loss: 2.028, acc: 0.565
******** [step = 750] loss: 2.031, acc: 0.565
******** [step = 800] loss: 2.035, acc: 0.565
******** [step = 850] loss: 2.038, acc: 0.565
EPOCH = 65 loss: 2.038, acc: 0.565, val_loss: 2.517, val_acc: 0.532

================================================================================2025-08_13 17:00:32
******** [step = 50] loss: 1.971, acc: 0.571
******** [step = 100] loss: 1.974, acc: 0.572
******** [step = 150] loss: 1.987, acc: 0.570
******** [step = 200] loss: 1.994, acc: 0.568
******** [step = 250] loss: 2.001, acc: 0.567
******** [step = 300] loss: 2.010, acc: 0.566
******** [step = 350] loss: 2.016, acc: 0.566
******** [step = 400] loss: 2.022, acc: 0.565
******** [step = 450] loss: 2.027, acc: 0.564
******** [step = 500] loss: 2.032, acc: 0.563
******** [step = 550] loss: 2.035, acc: 0.563
******** [step = 600] loss: 2.038, acc: 0.563
******** [step = 650] loss: 2.041, acc: 0.562
******** [step = 700] loss: 2.044, acc: 0.562
******** [step = 750] loss: 2.048, acc: 0.562
******** [step = 800] loss: 2.052, acc: 0.561
******** [step = 850] loss: 2.054, acc: 0.561
EPOCH = 66 loss: 2.054, acc: 0.561, val_loss: 2.540, val_acc: 0.529

================================================================================2025-08_13 17:02:32
******** [step = 50] loss: 1.967, acc: 0.571
******** [step = 100] loss: 1.969, acc: 0.572
******** [step = 150] loss: 1.983, acc: 0.570
******** [step = 200] loss: 1.990, acc: 0.569
******** [step = 250] loss: 1.990, acc: 0.569
******** [step = 300] loss: 1.996, acc: 0.569
******** [step = 350] loss: 2.002, acc: 0.568
******** [step = 400] loss: 2.007, acc: 0.567
******** [step = 450] loss: 2.009, acc: 0.567
******** [step = 500] loss: 2.017, acc: 0.566
******** [step = 550] loss: 2.020, acc: 0.566
******** [step = 600] loss: 2.025, acc: 0.565
******** [step = 650] loss: 2.029, acc: 0.565
******** [step = 700] loss: 2.032, acc: 0.564
******** [step = 750] loss: 2.035, acc: 0.564
******** [step = 800] loss: 2.037, acc: 0.564
******** [step = 850] loss: 2.040, acc: 0.564
EPOCH = 67 loss: 2.040, acc: 0.564, val_loss: 2.518, val_acc: 0.533

================================================================================2025-08_13 17:04:32
******** [step = 50] loss: 1.961, acc: 0.573
******** [step = 100] loss: 1.963, acc: 0.572
******** [step = 150] loss: 1.975, acc: 0.571
******** [step = 200] loss: 1.980, acc: 0.570
******** [step = 250] loss: 1.982, acc: 0.570
******** [step = 300] loss: 1.988, acc: 0.570
******** [step = 350] loss: 1.992, acc: 0.569
******** [step = 400] loss: 1.996, acc: 0.568
******** [step = 450] loss: 2.000, acc: 0.568
******** [step = 500] loss: 2.004, acc: 0.568
******** [step = 550] loss: 2.011, acc: 0.567
******** [step = 600] loss: 2.016, acc: 0.566
******** [step = 650] loss: 2.020, acc: 0.566
******** [step = 700] loss: 2.024, acc: 0.566
******** [step = 750] loss: 2.027, acc: 0.565
******** [step = 800] loss: 2.031, acc: 0.565
******** [step = 850] loss: 2.034, acc: 0.565
EPOCH = 68 loss: 2.034, acc: 0.565, val_loss: 2.514, val_acc: 0.536

================================================================================2025-08_13 17:06:32
******** [step = 50] loss: 1.939, acc: 0.573
******** [step = 100] loss: 1.953, acc: 0.572
******** [step = 150] loss: 1.956, acc: 0.573
******** [step = 200] loss: 1.966, acc: 0.571
******** [step = 250] loss: 1.975, acc: 0.570
******** [step = 300] loss: 1.980, acc: 0.570
******** [step = 350] loss: 1.986, acc: 0.570
******** [step = 400] loss: 1.989, acc: 0.569
******** [step = 450] loss: 1.993, acc: 0.569
******** [step = 500] loss: 1.999, acc: 0.569
******** [step = 550] loss: 2.004, acc: 0.568
******** [step = 600] loss: 2.007, acc: 0.568
******** [step = 650] loss: 2.011, acc: 0.568
******** [step = 700] loss: 2.016, acc: 0.567
******** [step = 750] loss: 2.021, acc: 0.567
******** [step = 800] loss: 2.025, acc: 0.566
******** [step = 850] loss: 2.028, acc: 0.566
EPOCH = 69 loss: 2.028, acc: 0.566, val_loss: 2.516, val_acc: 0.535

================================================================================2025-08_13 17:08:35
******** [step = 50] loss: 1.947, acc: 0.573
******** [step = 100] loss: 1.942, acc: 0.575
******** [step = 150] loss: 1.946, acc: 0.574
******** [step = 200] loss: 1.957, acc: 0.573
******** [step = 250] loss: 1.966, acc: 0.572
******** [step = 300] loss: 1.972, acc: 0.571
******** [step = 350] loss: 1.977, acc: 0.571
******** [step = 400] loss: 1.984, acc: 0.570
******** [step = 450] loss: 1.991, acc: 0.570
******** [step = 500] loss: 1.997, acc: 0.569
******** [step = 550] loss: 2.000, acc: 0.569
******** [step = 600] loss: 2.005, acc: 0.568
******** [step = 650] loss: 2.008, acc: 0.568
******** [step = 700] loss: 2.013, acc: 0.567
******** [step = 750] loss: 2.016, acc: 0.567
******** [step = 800] loss: 2.020, acc: 0.567
******** [step = 850] loss: 2.024, acc: 0.566
EPOCH = 70 loss: 2.024, acc: 0.566, val_loss: 2.518, val_acc: 0.535

================================================================================2025-08_13 17:10:40
******** [step = 50] loss: 1.946, acc: 0.575
******** [step = 100] loss: 1.947, acc: 0.576
******** [step = 150] loss: 1.957, acc: 0.575
******** [step = 200] loss: 1.964, acc: 0.574
******** [step = 250] loss: 1.969, acc: 0.573
******** [step = 300] loss: 1.974, acc: 0.573
******** [step = 350] loss: 1.980, acc: 0.572
******** [step = 400] loss: 1.987, acc: 0.571
******** [step = 450] loss: 1.991, acc: 0.570
******** [step = 500] loss: 1.999, acc: 0.569
******** [step = 550] loss: 2.003, acc: 0.569
******** [step = 600] loss: 2.006, acc: 0.568
******** [step = 650] loss: 2.008, acc: 0.568
******** [step = 700] loss: 2.011, acc: 0.568
******** [step = 750] loss: 2.015, acc: 0.567
******** [step = 800] loss: 2.018, acc: 0.567
******** [step = 850] loss: 2.021, acc: 0.567
EPOCH = 71 loss: 2.021, acc: 0.567, val_loss: 2.522, val_acc: 0.536

================================================================================2025-08_13 17:12:44
******** [step = 50] loss: 1.926, acc: 0.577
******** [step = 100] loss: 1.947, acc: 0.575
******** [step = 150] loss: 1.953, acc: 0.574
******** [step = 200] loss: 1.960, acc: 0.573
******** [step = 250] loss: 1.968, acc: 0.573
******** [step = 300] loss: 1.972, acc: 0.572
******** [step = 350] loss: 1.977, acc: 0.572
******** [step = 400] loss: 1.981, acc: 0.571
******** [step = 450] loss: 1.987, acc: 0.571
******** [step = 500] loss: 1.988, acc: 0.571
******** [step = 550] loss: 1.992, acc: 0.571
******** [step = 600] loss: 1.997, acc: 0.570
******** [step = 650] loss: 2.002, acc: 0.569
******** [step = 700] loss: 2.007, acc: 0.568
******** [step = 750] loss: 2.012, acc: 0.568
******** [step = 800] loss: 2.018, acc: 0.567
******** [step = 850] loss: 2.021, acc: 0.567
EPOCH = 72 loss: 2.021, acc: 0.567, val_loss: 2.523, val_acc: 0.530

================================================================================2025-08_13 17:14:52
******** [step = 50] loss: 1.946, acc: 0.573
******** [step = 100] loss: 1.957, acc: 0.571
******** [step = 150] loss: 1.958, acc: 0.571
******** [step = 200] loss: 1.963, acc: 0.571
******** [step = 250] loss: 1.967, acc: 0.571
******** [step = 300] loss: 1.973, acc: 0.570
******** [step = 350] loss: 1.979, acc: 0.569
******** [step = 400] loss: 1.988, acc: 0.568
******** [step = 450] loss: 1.994, acc: 0.567
******** [step = 500] loss: 1.998, acc: 0.567
******** [step = 550] loss: 1.999, acc: 0.566
******** [step = 600] loss: 2.005, acc: 0.566
******** [step = 650] loss: 2.010, acc: 0.565
******** [step = 700] loss: 2.015, acc: 0.565
******** [step = 750] loss: 2.020, acc: 0.564
******** [step = 800] loss: 2.024, acc: 0.564
******** [step = 850] loss: 2.027, acc: 0.563
EPOCH = 73 loss: 2.027, acc: 0.563, val_loss: 2.526, val_acc: 0.533

================================================================================2025-08_13 17:16:51
******** [step = 50] loss: 1.955, acc: 0.570
******** [step = 100] loss: 1.956, acc: 0.571
******** [step = 150] loss: 1.960, acc: 0.570
******** [step = 200] loss: 1.959, acc: 0.571
******** [step = 250] loss: 1.965, acc: 0.571
******** [step = 300] loss: 1.971, acc: 0.570
******** [step = 350] loss: 1.975, acc: 0.570
******** [step = 400] loss: 1.983, acc: 0.569
******** [step = 450] loss: 1.987, acc: 0.569
******** [step = 500] loss: 1.993, acc: 0.568
******** [step = 550] loss: 1.998, acc: 0.568
******** [step = 600] loss: 2.003, acc: 0.568
******** [step = 650] loss: 2.007, acc: 0.567
******** [step = 700] loss: 2.010, acc: 0.567
******** [step = 750] loss: 2.015, acc: 0.567
******** [step = 800] loss: 2.018, acc: 0.566
******** [step = 850] loss: 2.021, acc: 0.566
EPOCH = 74 loss: 2.021, acc: 0.566, val_loss: 2.506, val_acc: 0.537

================================================================================2025-08_13 17:18:51
******** [step = 50] loss: 1.928, acc: 0.578
******** [step = 100] loss: 1.948, acc: 0.576
******** [step = 150] loss: 1.948, acc: 0.576
******** [step = 200] loss: 1.952, acc: 0.575
******** [step = 250] loss: 1.958, acc: 0.574
******** [step = 300] loss: 1.962, acc: 0.574
******** [step = 350] loss: 1.968, acc: 0.573
******** [step = 400] loss: 1.975, acc: 0.572
******** [step = 450] loss: 1.980, acc: 0.572
******** [step = 500] loss: 1.986, acc: 0.571
******** [step = 550] loss: 1.990, acc: 0.570
******** [step = 600] loss: 1.994, acc: 0.570
******** [step = 650] loss: 1.999, acc: 0.569
******** [step = 700] loss: 2.002, acc: 0.569
******** [step = 750] loss: 2.006, acc: 0.569
******** [step = 800] loss: 2.009, acc: 0.569
******** [step = 850] loss: 2.013, acc: 0.568
EPOCH = 75 loss: 2.013, acc: 0.568, val_loss: 2.521, val_acc: 0.535

================================================================================2025-08_13 17:20:52
******** [step = 50] loss: 1.948, acc: 0.575
******** [step = 100] loss: 1.950, acc: 0.575
******** [step = 150] loss: 1.951, acc: 0.575
******** [step = 200] loss: 1.954, acc: 0.574
******** [step = 250] loss: 1.958, acc: 0.574
******** [step = 300] loss: 1.965, acc: 0.573
******** [step = 350] loss: 1.970, acc: 0.573
******** [step = 400] loss: 1.975, acc: 0.572
******** [step = 450] loss: 1.978, acc: 0.572
******** [step = 500] loss: 1.981, acc: 0.572
******** [step = 550] loss: 1.986, acc: 0.571
******** [step = 600] loss: 1.991, acc: 0.571
******** [step = 650] loss: 1.996, acc: 0.570
******** [step = 700] loss: 1.999, acc: 0.570
******** [step = 750] loss: 2.002, acc: 0.570
******** [step = 800] loss: 2.006, acc: 0.569
******** [step = 850] loss: 2.010, acc: 0.569
EPOCH = 76 loss: 2.010, acc: 0.569, val_loss: 2.510, val_acc: 0.537

================================================================================2025-08_13 17:22:52
******** [step = 50] loss: 1.929, acc: 0.578
******** [step = 100] loss: 1.930, acc: 0.578
******** [step = 150] loss: 1.933, acc: 0.578
******** [step = 200] loss: 1.939, acc: 0.578
******** [step = 250] loss: 1.942, acc: 0.577
******** [step = 300] loss: 1.945, acc: 0.577
******** [step = 350] loss: 1.952, acc: 0.576
******** [step = 400] loss: 1.959, acc: 0.575
******** [step = 450] loss: 1.965, acc: 0.574
******** [step = 500] loss: 1.970, acc: 0.574
******** [step = 550] loss: 1.975, acc: 0.574
******** [step = 600] loss: 1.979, acc: 0.573
******** [step = 650] loss: 1.985, acc: 0.572
******** [step = 700] loss: 1.989, acc: 0.572
******** [step = 750] loss: 1.994, acc: 0.571
******** [step = 800] loss: 1.999, acc: 0.571
******** [step = 850] loss: 2.004, acc: 0.570
EPOCH = 77 loss: 2.004, acc: 0.570, val_loss: 2.509, val_acc: 0.537

================================================================================2025-08_13 17:24:52
******** [step = 50] loss: 1.919, acc: 0.580
******** [step = 100] loss: 1.929, acc: 0.578
******** [step = 150] loss: 1.929, acc: 0.579
******** [step = 200] loss: 1.935, acc: 0.578
******** [step = 250] loss: 1.946, acc: 0.576
******** [step = 300] loss: 1.954, acc: 0.576
******** [step = 350] loss: 1.959, acc: 0.574
******** [step = 400] loss: 1.965, acc: 0.574
******** [step = 450] loss: 1.971, acc: 0.573
******** [step = 500] loss: 1.978, acc: 0.572
******** [step = 550] loss: 1.984, acc: 0.572
******** [step = 600] loss: 1.987, acc: 0.571
******** [step = 650] loss: 1.992, acc: 0.571
******** [step = 700] loss: 1.998, acc: 0.570
******** [step = 750] loss: 2.004, acc: 0.569
******** [step = 800] loss: 2.007, acc: 0.569
******** [step = 850] loss: 2.011, acc: 0.569
EPOCH = 78 loss: 2.011, acc: 0.569, val_loss: 2.529, val_acc: 0.534

================================================================================2025-08_13 17:26:56
******** [step = 50] loss: 1.945, acc: 0.573
******** [step = 100] loss: 1.943, acc: 0.574
******** [step = 150] loss: 1.956, acc: 0.573
******** [step = 200] loss: 1.960, acc: 0.573
******** [step = 250] loss: 1.963, acc: 0.573
******** [step = 300] loss: 1.970, acc: 0.571
******** [step = 350] loss: 1.970, acc: 0.571
******** [step = 400] loss: 1.974, acc: 0.571
******** [step = 450] loss: 1.979, acc: 0.571
******** [step = 500] loss: 1.981, acc: 0.571
******** [step = 550] loss: 1.986, acc: 0.571
******** [step = 600] loss: 1.988, acc: 0.571
******** [step = 650] loss: 1.993, acc: 0.571
******** [step = 700] loss: 1.996, acc: 0.570
******** [step = 750] loss: 1.999, acc: 0.570
******** [step = 800] loss: 2.003, acc: 0.570
******** [step = 850] loss: 2.007, acc: 0.569
EPOCH = 79 loss: 2.007, acc: 0.569, val_loss: 2.524, val_acc: 0.537

================================================================================2025-08_13 17:28:56
******** [step = 50] loss: 1.924, acc: 0.579
******** [step = 100] loss: 1.929, acc: 0.579
******** [step = 150] loss: 1.934, acc: 0.578
******** [step = 200] loss: 1.941, acc: 0.577
******** [step = 250] loss: 1.948, acc: 0.575
******** [step = 300] loss: 1.956, acc: 0.575
******** [step = 350] loss: 1.958, acc: 0.574
******** [step = 400] loss: 1.965, acc: 0.573
******** [step = 450] loss: 1.970, acc: 0.573
******** [step = 500] loss: 1.973, acc: 0.573
******** [step = 550] loss: 1.979, acc: 0.572
******** [step = 600] loss: 1.983, acc: 0.572
******** [step = 650] loss: 1.986, acc: 0.572
******** [step = 700] loss: 1.990, acc: 0.572
******** [step = 750] loss: 1.993, acc: 0.572
******** [step = 800] loss: 1.997, acc: 0.571
******** [step = 850] loss: 2.000, acc: 0.571
EPOCH = 80 loss: 2.000, acc: 0.571, val_loss: 2.517, val_acc: 0.538

================================================================================2025-08_13 17:30:56
finishing training...
Training complete in 164m 12s
    epoch  ...   val_acc
0     1.0  ...  0.365462
1     2.0  ...  0.388163
2     3.0  ...  0.399604
3     4.0  ...  0.412765
4     5.0  ...  0.421626
..    ...  ...       ...
75   76.0  ...  0.536576
76   77.0  ...  0.536698
77   78.0  ...  0.533536
78   79.0  ...  0.537326
79   80.0  ...  0.537610

[80 rows x 5 columns]
== Done ==
Wed Aug 13 05:31:24 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-13-2025 17:31:25
Job ID:        7013054
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=11:04:04,vmem=0,walltime=02:46:01,mem=35188K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-005-30-0
---------------------------------------
