---------------------------------------
Begin Slurm Prolog: Aug-11-2025 12:47:58
Job ID:    6870201
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Mon Aug 11 12:47:58 PM EDT 2025
atl1-1-02-006-36-0.pace.gatech.edu
== GPU check ==
Mon Aug 11 12:48:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   37C    P0             24W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 62308.93it/s] 28%|██▊       | 38259/135842 [00:00<00:00, 114875.28it/s] 39%|███▉      | 53429/135842 [00:00<00:00, 100689.48it/s] 51%|█████     | 69378/135842 [00:00<00:00, 87621.95it/s]  66%|██████▌   | 89277/135842 [00:00<00:00, 112802.41it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 88097.89it/s] 92%|█████████▏| 125370/135842 [00:01<00:00, 106242.27it/s]100%|██████████| 135842/135842 [00:01<00:00, 102416.54it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47611.29it/s] 33%|███▎      | 35605/108673 [00:00<00:00, 85414.58it/s] 49%|████▉     | 53716/108673 [00:00<00:00, 112830.98it/s] 66%|██████▌   | 71879/108673 [00:00<00:00, 132809.82it/s] 81%|████████  | 88222/108673 [00:01<00:00, 71172.68it/s]  98%|█████████▊| 106269/108673 [00:01<00:00, 90161.17it/s]100%|██████████| 108673/108673 [00:01<00:00, 89290.30it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 65%|██████▌   | 17757/27169 [00:00<00:00, 177557.47it/s]100%|██████████| 27169/27169 [00:00<00:00, 179546.96it/s]
tensor([   3,   14,  323,   11,   76,  352,    6,  957,  311,   35,   12,   21,
          11,    9,  526,   26, 2237,   39, 2129,    4,    2,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,   157,   102,   147,    39,   209,    54,   505,    40,    16,
         1183,   409,    26,  1010,    78,  2859,  1617,   191, 15662,    65,
         2749,     4,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([   3,  189, 1557,  444,   68,   21,    8,  282,  928, 1145,   28, 2126,
         102,   44, 6624,   56, 1644,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_11 12:49:29
******** [step = 50] loss: 9.252, acc: 0.026
******** [step = 100] loss: 8.800, acc: 0.104
******** [step = 150] loss: 8.467, acc: 0.146
******** [step = 200] loss: 8.142, acc: 0.173
******** [step = 250] loss: 7.827, acc: 0.191
******** [step = 300] loss: 7.519, acc: 0.204
******** [step = 350] loss: 7.211, acc: 0.217
******** [step = 400] loss: 6.925, acc: 0.232
******** [step = 450] loss: 6.666, acc: 0.247
******** [step = 500] loss: 6.437, acc: 0.260
******** [step = 550] loss: 6.236, acc: 0.272
******** [step = 600] loss: 6.056, acc: 0.284
******** [step = 650] loss: 5.892, acc: 0.294
******** [step = 700] loss: 5.745, acc: 0.304
******** [step = 750] loss: 5.610, acc: 0.313
******** [step = 800] loss: 5.487, acc: 0.321
******** [step = 850] loss: 5.373, acc: 0.329
EPOCH = 1 loss: 5.373, acc: 0.329, val_loss: 3.365, val_acc: 0.479

================================================================================2025-08_11 12:50:52
******** [step = 50] loss: 3.472, acc: 0.460
******** [step = 100] loss: 3.424, acc: 0.465
******** [step = 150] loss: 3.384, acc: 0.470
******** [step = 200] loss: 3.351, acc: 0.473
******** [step = 250] loss: 3.319, acc: 0.477
******** [step = 300] loss: 3.292, acc: 0.480
******** [step = 350] loss: 3.260, acc: 0.484
******** [step = 400] loss: 3.225, acc: 0.488
******** [step = 450] loss: 3.197, acc: 0.491
******** [step = 500] loss: 3.168, acc: 0.495
******** [step = 550] loss: 3.139, acc: 0.498
******** [step = 600] loss: 3.112, acc: 0.501
******** [step = 650] loss: 3.086, acc: 0.504
******** [step = 700] loss: 3.060, acc: 0.507
******** [step = 750] loss: 3.035, acc: 0.510
******** [step = 800] loss: 3.012, acc: 0.513
******** [step = 850] loss: 2.986, acc: 0.517
EPOCH = 2 loss: 2.986, acc: 0.517, val_loss: 2.467, val_acc: 0.583

================================================================================2025-08_11 12:52:13
******** [step = 50] loss: 2.539, acc: 0.569
******** [step = 100] loss: 2.501, acc: 0.572
******** [step = 150] loss: 2.468, acc: 0.578
******** [step = 200] loss: 2.449, acc: 0.581
******** [step = 250] loss: 2.429, acc: 0.584
******** [step = 300] loss: 2.417, acc: 0.585
******** [step = 350] loss: 2.402, acc: 0.588
******** [step = 400] loss: 2.387, acc: 0.590
******** [step = 450] loss: 2.373, acc: 0.592
******** [step = 500] loss: 2.358, acc: 0.593
******** [step = 550] loss: 2.343, acc: 0.595
******** [step = 600] loss: 2.330, acc: 0.597
******** [step = 650] loss: 2.316, acc: 0.599
******** [step = 700] loss: 2.304, acc: 0.601
******** [step = 750] loss: 2.291, acc: 0.602
******** [step = 800] loss: 2.281, acc: 0.604
******** [step = 850] loss: 2.269, acc: 0.606
EPOCH = 3 loss: 2.269, acc: 0.606, val_loss: 1.979, val_acc: 0.650

================================================================================2025-08_11 12:53:34
******** [step = 50] loss: 1.988, acc: 0.635
******** [step = 100] loss: 1.957, acc: 0.641
******** [step = 150] loss: 1.950, acc: 0.644
******** [step = 200] loss: 1.941, acc: 0.645
******** [step = 250] loss: 1.935, acc: 0.647
******** [step = 300] loss: 1.933, acc: 0.647
******** [step = 350] loss: 1.927, acc: 0.648
******** [step = 400] loss: 1.923, acc: 0.649
******** [step = 450] loss: 1.916, acc: 0.649
******** [step = 500] loss: 1.915, acc: 0.650
******** [step = 550] loss: 1.906, acc: 0.651
******** [step = 600] loss: 1.900, acc: 0.652
******** [step = 650] loss: 1.895, acc: 0.653
******** [step = 700] loss: 1.888, acc: 0.654
******** [step = 750] loss: 1.881, acc: 0.655
******** [step = 800] loss: 1.875, acc: 0.656
******** [step = 850] loss: 1.871, acc: 0.657
EPOCH = 4 loss: 1.871, acc: 0.657, val_loss: 1.714, val_acc: 0.690

================================================================================2025-08_11 12:54:53
******** [step = 50] loss: 1.687, acc: 0.678
******** [step = 100] loss: 1.657, acc: 0.682
******** [step = 150] loss: 1.650, acc: 0.683
******** [step = 200] loss: 1.648, acc: 0.684
******** [step = 250] loss: 1.646, acc: 0.684
******** [step = 300] loss: 1.645, acc: 0.685
******** [step = 350] loss: 1.652, acc: 0.684
******** [step = 400] loss: 1.655, acc: 0.684
******** [step = 450] loss: 1.656, acc: 0.684
******** [step = 500] loss: 1.656, acc: 0.684
******** [step = 550] loss: 1.653, acc: 0.685
******** [step = 600] loss: 1.654, acc: 0.685
******** [step = 650] loss: 1.653, acc: 0.685
******** [step = 700] loss: 1.652, acc: 0.685
******** [step = 750] loss: 1.651, acc: 0.686
******** [step = 800] loss: 1.649, acc: 0.686
******** [step = 850] loss: 1.648, acc: 0.686
EPOCH = 5 loss: 1.648, acc: 0.686, val_loss: 1.545, val_acc: 0.711

================================================================================2025-08_11 12:56:15
******** [step = 50] loss: 1.495, acc: 0.702
******** [step = 100] loss: 1.467, acc: 0.707
******** [step = 150] loss: 1.472, acc: 0.706
******** [step = 200] loss: 1.466, acc: 0.707
******** [step = 250] loss: 1.468, acc: 0.708
******** [step = 300] loss: 1.468, acc: 0.708
******** [step = 350] loss: 1.466, acc: 0.709
******** [step = 400] loss: 1.467, acc: 0.709
******** [step = 450] loss: 1.466, acc: 0.709
******** [step = 500] loss: 1.469, acc: 0.709
******** [step = 550] loss: 1.466, acc: 0.710
******** [step = 600] loss: 1.465, acc: 0.711
******** [step = 650] loss: 1.463, acc: 0.711
******** [step = 700] loss: 1.462, acc: 0.712
******** [step = 750] loss: 1.461, acc: 0.712
******** [step = 800] loss: 1.460, acc: 0.713
******** [step = 850] loss: 1.456, acc: 0.713
EPOCH = 6 loss: 1.456, acc: 0.713, val_loss: 1.370, val_acc: 0.741

================================================================================2025-08_11 12:57:36
******** [step = 50] loss: 1.268, acc: 0.738
******** [step = 100] loss: 1.275, acc: 0.737
******** [step = 150] loss: 1.278, acc: 0.737
******** [step = 200] loss: 1.277, acc: 0.738
******** [step = 250] loss: 1.283, acc: 0.738
******** [step = 300] loss: 1.282, acc: 0.738
******** [step = 350] loss: 1.288, acc: 0.737
******** [step = 400] loss: 1.289, acc: 0.737
******** [step = 450] loss: 1.295, acc: 0.736
******** [step = 500] loss: 1.297, acc: 0.736
******** [step = 550] loss: 1.298, acc: 0.737
******** [step = 600] loss: 1.298, acc: 0.737
******** [step = 650] loss: 1.301, acc: 0.737
******** [step = 700] loss: 1.302, acc: 0.737
******** [step = 750] loss: 1.303, acc: 0.737
******** [step = 800] loss: 1.303, acc: 0.737
******** [step = 850] loss: 1.304, acc: 0.737
EPOCH = 7 loss: 1.304, acc: 0.737, val_loss: 1.303, val_acc: 0.754

================================================================================2025-08_11 12:58:55
******** [step = 50] loss: 1.189, acc: 0.752
******** [step = 100] loss: 1.160, acc: 0.758
******** [step = 150] loss: 1.159, acc: 0.758
******** [step = 200] loss: 1.164, acc: 0.758
******** [step = 250] loss: 1.169, acc: 0.757
******** [step = 300] loss: 1.170, acc: 0.757
******** [step = 350] loss: 1.173, acc: 0.757
******** [step = 400] loss: 1.176, acc: 0.756
******** [step = 450] loss: 1.179, acc: 0.756
******** [step = 500] loss: 1.181, acc: 0.756
******** [step = 550] loss: 1.184, acc: 0.755
******** [step = 600] loss: 1.187, acc: 0.755
******** [step = 650] loss: 1.189, acc: 0.755
******** [step = 700] loss: 1.191, acc: 0.755
******** [step = 750] loss: 1.194, acc: 0.755
******** [step = 800] loss: 1.196, acc: 0.755
******** [step = 850] loss: 1.198, acc: 0.755
EPOCH = 8 loss: 1.198, acc: 0.755, val_loss: 1.239, val_acc: 0.768

================================================================================2025-08_11 13:00:15
******** [step = 50] loss: 1.053, acc: 0.774
******** [step = 100] loss: 1.061, acc: 0.774
******** [step = 150] loss: 1.068, acc: 0.774
******** [step = 200] loss: 1.074, acc: 0.772
******** [step = 250] loss: 1.078, acc: 0.772
******** [step = 300] loss: 1.084, acc: 0.771
******** [step = 350] loss: 1.090, acc: 0.771
******** [step = 400] loss: 1.092, acc: 0.771
******** [step = 450] loss: 1.093, acc: 0.771
******** [step = 500] loss: 1.096, acc: 0.771
******** [step = 550] loss: 1.100, acc: 0.770
******** [step = 600] loss: 1.104, acc: 0.769
******** [step = 650] loss: 1.109, acc: 0.769
******** [step = 700] loss: 1.109, acc: 0.769
******** [step = 750] loss: 1.111, acc: 0.769
******** [step = 800] loss: 1.113, acc: 0.769
******** [step = 850] loss: 1.114, acc: 0.769
EPOCH = 9 loss: 1.114, acc: 0.769, val_loss: 1.199, val_acc: 0.775

================================================================================2025-08_11 13:01:37
******** [step = 50] loss: 1.007, acc: 0.783
******** [step = 100] loss: 0.992, acc: 0.786
******** [step = 150] loss: 0.994, acc: 0.787
******** [step = 200] loss: 1.000, acc: 0.786
******** [step = 250] loss: 1.006, acc: 0.785
******** [step = 300] loss: 1.009, acc: 0.785
******** [step = 350] loss: 1.013, acc: 0.785
******** [step = 400] loss: 1.013, acc: 0.785
******** [step = 450] loss: 1.017, acc: 0.784
******** [step = 500] loss: 1.023, acc: 0.783
******** [step = 550] loss: 1.027, acc: 0.783
******** [step = 600] loss: 1.031, acc: 0.783
******** [step = 650] loss: 1.034, acc: 0.782
******** [step = 700] loss: 1.038, acc: 0.782
******** [step = 750] loss: 1.041, acc: 0.781
******** [step = 800] loss: 1.044, acc: 0.781
******** [step = 850] loss: 1.046, acc: 0.781
EPOCH = 10 loss: 1.046, acc: 0.781, val_loss: 1.161, val_acc: 0.782

================================================================================2025-08_11 13:03:08
******** [step = 50] loss: 0.903, acc: 0.803
******** [step = 100] loss: 0.906, acc: 0.803
******** [step = 150] loss: 0.913, acc: 0.801
******** [step = 200] loss: 0.916, acc: 0.801
******** [step = 250] loss: 0.924, acc: 0.800
******** [step = 300] loss: 0.933, acc: 0.798
******** [step = 350] loss: 0.939, acc: 0.797
******** [step = 400] loss: 0.944, acc: 0.796
******** [step = 450] loss: 0.951, acc: 0.795
******** [step = 500] loss: 0.958, acc: 0.794
******** [step = 550] loss: 0.961, acc: 0.794
******** [step = 600] loss: 0.964, acc: 0.794
******** [step = 650] loss: 0.971, acc: 0.793
******** [step = 700] loss: 0.975, acc: 0.792
******** [step = 750] loss: 0.978, acc: 0.792
******** [step = 800] loss: 0.984, acc: 0.791
******** [step = 850] loss: 0.987, acc: 0.791
EPOCH = 11 loss: 0.987, acc: 0.791, val_loss: 1.146, val_acc: 0.786

================================================================================2025-08_11 13:04:36
******** [step = 50] loss: 0.885, acc: 0.805
******** [step = 100] loss: 0.880, acc: 0.807
******** [step = 150] loss: 0.879, acc: 0.807
******** [step = 200] loss: 0.883, acc: 0.807
******** [step = 250] loss: 0.885, acc: 0.806
******** [step = 300] loss: 0.887, acc: 0.806
******** [step = 350] loss: 0.892, acc: 0.805
******** [step = 400] loss: 0.898, acc: 0.804
******** [step = 450] loss: 0.905, acc: 0.804
******** [step = 500] loss: 0.907, acc: 0.804
******** [step = 550] loss: 0.910, acc: 0.803
******** [step = 600] loss: 0.914, acc: 0.803
******** [step = 650] loss: 0.919, acc: 0.802
******** [step = 700] loss: 0.925, acc: 0.801
******** [step = 750] loss: 0.929, acc: 0.801
******** [step = 800] loss: 0.932, acc: 0.801
******** [step = 850] loss: 0.936, acc: 0.800
EPOCH = 12 loss: 0.936, acc: 0.800, val_loss: 1.121, val_acc: 0.793

================================================================================2025-08_11 13:05:57
******** [step = 50] loss: 0.854, acc: 0.811
******** [step = 100] loss: 0.840, acc: 0.814
******** [step = 150] loss: 0.830, acc: 0.817
******** [step = 200] loss: 0.835, acc: 0.816
******** [step = 250] loss: 0.835, acc: 0.816
******** [step = 300] loss: 0.841, acc: 0.815
******** [step = 350] loss: 0.845, acc: 0.815
******** [step = 400] loss: 0.855, acc: 0.813
******** [step = 450] loss: 0.858, acc: 0.813
******** [step = 500] loss: 0.862, acc: 0.812
******** [step = 550] loss: 0.867, acc: 0.811
******** [step = 600] loss: 0.872, acc: 0.811
******** [step = 650] loss: 0.877, acc: 0.810
******** [step = 700] loss: 0.881, acc: 0.809
******** [step = 750] loss: 0.884, acc: 0.809
******** [step = 800] loss: 0.887, acc: 0.809
******** [step = 850] loss: 0.893, acc: 0.808
EPOCH = 13 loss: 0.893, acc: 0.808, val_loss: 1.119, val_acc: 0.793

================================================================================2025-08_11 13:07:17
******** [step = 50] loss: 0.795, acc: 0.822
******** [step = 100] loss: 0.783, acc: 0.825
******** [step = 150] loss: 0.788, acc: 0.824
******** [step = 200] loss: 0.788, acc: 0.823
******** [step = 250] loss: 0.795, acc: 0.822
******** [step = 300] loss: 0.804, acc: 0.821
******** [step = 350] loss: 0.812, acc: 0.820
******** [step = 400] loss: 0.817, acc: 0.819
******** [step = 450] loss: 0.825, acc: 0.818
******** [step = 500] loss: 0.828, acc: 0.818
******** [step = 550] loss: 0.831, acc: 0.817
******** [step = 600] loss: 0.836, acc: 0.816
******** [step = 650] loss: 0.841, acc: 0.816
******** [step = 700] loss: 0.843, acc: 0.816
******** [step = 750] loss: 0.847, acc: 0.815
******** [step = 800] loss: 0.850, acc: 0.815
******** [step = 850] loss: 0.855, acc: 0.814
EPOCH = 14 loss: 0.855, acc: 0.814, val_loss: 1.103, val_acc: 0.798

================================================================================2025-08_11 13:08:43
******** [step = 50] loss: 0.753, acc: 0.831
******** [step = 100] loss: 0.759, acc: 0.830
******** [step = 150] loss: 0.761, acc: 0.829
******** [step = 200] loss: 0.763, acc: 0.829
******** [step = 250] loss: 0.772, acc: 0.827
******** [step = 300] loss: 0.777, acc: 0.826
******** [step = 350] loss: 0.783, acc: 0.825
******** [step = 400] loss: 0.787, acc: 0.825
******** [step = 450] loss: 0.792, acc: 0.824
******** [step = 500] loss: 0.796, acc: 0.824
******** [step = 550] loss: 0.802, acc: 0.823
******** [step = 600] loss: 0.804, acc: 0.823
******** [step = 650] loss: 0.808, acc: 0.822
******** [step = 700] loss: 0.810, acc: 0.822
******** [step = 750] loss: 0.814, acc: 0.821
******** [step = 800] loss: 0.818, acc: 0.821
******** [step = 850] loss: 0.822, acc: 0.820
EPOCH = 15 loss: 0.822, acc: 0.820, val_loss: 1.087, val_acc: 0.801

================================================================================2025-08_11 13:10:03
******** [step = 50] loss: 0.735, acc: 0.834
******** [step = 100] loss: 0.736, acc: 0.834
******** [step = 150] loss: 0.735, acc: 0.834
******** [step = 200] loss: 0.739, acc: 0.833
******** [step = 250] loss: 0.745, acc: 0.832
******** [step = 300] loss: 0.747, acc: 0.832
******** [step = 350] loss: 0.751, acc: 0.831
******** [step = 400] loss: 0.757, acc: 0.830
******** [step = 450] loss: 0.764, acc: 0.829
******** [step = 500] loss: 0.769, acc: 0.829
******** [step = 550] loss: 0.774, acc: 0.828
******** [step = 600] loss: 0.778, acc: 0.827
******** [step = 650] loss: 0.782, acc: 0.827
******** [step = 700] loss: 0.785, acc: 0.827
******** [step = 750] loss: 0.787, acc: 0.826
******** [step = 800] loss: 0.789, acc: 0.826
******** [step = 850] loss: 0.792, acc: 0.826
EPOCH = 16 loss: 0.792, acc: 0.826, val_loss: 1.077, val_acc: 0.805

================================================================================2025-08_11 13:11:27
******** [step = 50] loss: 0.696, acc: 0.841
******** [step = 100] loss: 0.699, acc: 0.840
******** [step = 150] loss: 0.708, acc: 0.839
******** [step = 200] loss: 0.714, acc: 0.838
******** [step = 250] loss: 0.719, acc: 0.838
******** [step = 300] loss: 0.722, acc: 0.837
******** [step = 350] loss: 0.729, acc: 0.836
******** [step = 400] loss: 0.731, acc: 0.836
******** [step = 450] loss: 0.736, acc: 0.835
******** [step = 500] loss: 0.740, acc: 0.835
******** [step = 550] loss: 0.744, acc: 0.834
******** [step = 600] loss: 0.747, acc: 0.834
******** [step = 650] loss: 0.750, acc: 0.834
******** [step = 700] loss: 0.753, acc: 0.833
******** [step = 750] loss: 0.758, acc: 0.832
******** [step = 800] loss: 0.761, acc: 0.832
******** [step = 850] loss: 0.763, acc: 0.832
EPOCH = 17 loss: 0.763, acc: 0.832, val_loss: 1.080, val_acc: 0.804

================================================================================2025-08_11 13:12:48
******** [step = 50] loss: 0.674, acc: 0.846
******** [step = 100] loss: 0.671, acc: 0.847
******** [step = 150] loss: 0.673, acc: 0.846
******** [step = 200] loss: 0.675, acc: 0.846
******** [step = 250] loss: 0.680, acc: 0.845
******** [step = 300] loss: 0.686, acc: 0.843
******** [step = 350] loss: 0.691, acc: 0.843
******** [step = 400] loss: 0.697, acc: 0.842
******** [step = 450] loss: 0.703, acc: 0.841
******** [step = 500] loss: 0.709, acc: 0.840
******** [step = 550] loss: 0.714, acc: 0.839
******** [step = 600] loss: 0.718, acc: 0.839
******** [step = 650] loss: 0.724, acc: 0.838
******** [step = 700] loss: 0.727, acc: 0.838
******** [step = 750] loss: 0.732, acc: 0.837
******** [step = 800] loss: 0.737, acc: 0.836
******** [step = 850] loss: 0.741, acc: 0.836
EPOCH = 18 loss: 0.741, acc: 0.836, val_loss: 1.062, val_acc: 0.809

================================================================================2025-08_11 13:14:08
******** [step = 50] loss: 0.663, acc: 0.847
******** [step = 100] loss: 0.659, acc: 0.848
******** [step = 150] loss: 0.657, acc: 0.849
******** [step = 200] loss: 0.658, acc: 0.849
******** [step = 250] loss: 0.664, acc: 0.848
******** [step = 300] loss: 0.670, acc: 0.847
******** [step = 350] loss: 0.674, acc: 0.846
******** [step = 400] loss: 0.680, acc: 0.845
******** [step = 450] loss: 0.687, acc: 0.844
******** [step = 500] loss: 0.691, acc: 0.844
******** [step = 550] loss: 0.696, acc: 0.843
******** [step = 600] loss: 0.699, acc: 0.843
******** [step = 650] loss: 0.703, acc: 0.842
******** [step = 700] loss: 0.707, acc: 0.842
******** [step = 750] loss: 0.710, acc: 0.841
******** [step = 800] loss: 0.715, acc: 0.841
******** [step = 850] loss: 0.718, acc: 0.840
EPOCH = 19 loss: 0.718, acc: 0.840, val_loss: 1.057, val_acc: 0.811

================================================================================2025-08_11 13:15:31
******** [step = 50] loss: 0.633, acc: 0.854
******** [step = 100] loss: 0.639, acc: 0.853
******** [step = 150] loss: 0.636, acc: 0.854
******** [step = 200] loss: 0.636, acc: 0.854
******** [step = 250] loss: 0.642, acc: 0.852
******** [step = 300] loss: 0.648, acc: 0.851
******** [step = 350] loss: 0.655, acc: 0.850
******** [step = 400] loss: 0.660, acc: 0.849
******** [step = 450] loss: 0.667, acc: 0.848
******** [step = 500] loss: 0.672, acc: 0.848
******** [step = 550] loss: 0.677, acc: 0.847
******** [step = 600] loss: 0.681, acc: 0.846
******** [step = 650] loss: 0.684, acc: 0.846
******** [step = 700] loss: 0.689, acc: 0.845
******** [step = 750] loss: 0.693, acc: 0.845
******** [step = 800] loss: 0.696, acc: 0.844
******** [step = 850] loss: 0.698, acc: 0.844
EPOCH = 20 loss: 0.698, acc: 0.844, val_loss: 1.055, val_acc: 0.812

================================================================================2025-08_11 13:16:53
******** [step = 50] loss: 0.625, acc: 0.856
******** [step = 100] loss: 0.622, acc: 0.857
******** [step = 150] loss: 0.627, acc: 0.856
******** [step = 200] loss: 0.628, acc: 0.856
******** [step = 250] loss: 0.631, acc: 0.855
******** [step = 300] loss: 0.634, acc: 0.854
******** [step = 350] loss: 0.639, acc: 0.853
******** [step = 400] loss: 0.644, acc: 0.853
******** [step = 450] loss: 0.647, acc: 0.852
******** [step = 500] loss: 0.651, acc: 0.851
******** [step = 550] loss: 0.658, acc: 0.850
******** [step = 600] loss: 0.661, acc: 0.850
******** [step = 650] loss: 0.664, acc: 0.850
******** [step = 700] loss: 0.668, acc: 0.849
******** [step = 750] loss: 0.672, acc: 0.849
******** [step = 800] loss: 0.675, acc: 0.848
******** [step = 850] loss: 0.678, acc: 0.848
EPOCH = 21 loss: 0.678, acc: 0.848, val_loss: 1.049, val_acc: 0.813

================================================================================2025-08_11 13:18:14
******** [step = 50] loss: 0.589, acc: 0.863
******** [step = 100] loss: 0.588, acc: 0.862
******** [step = 150] loss: 0.594, acc: 0.862
******** [step = 200] loss: 0.600, acc: 0.861
******** [step = 250] loss: 0.606, acc: 0.860
******** [step = 300] loss: 0.613, acc: 0.858
******** [step = 350] loss: 0.618, acc: 0.858
******** [step = 400] loss: 0.623, acc: 0.857
******** [step = 450] loss: 0.628, acc: 0.856
******** [step = 500] loss: 0.633, acc: 0.855
******** [step = 550] loss: 0.637, acc: 0.855
******** [step = 600] loss: 0.642, acc: 0.854
******** [step = 650] loss: 0.647, acc: 0.854
******** [step = 700] loss: 0.650, acc: 0.853
******** [step = 750] loss: 0.654, acc: 0.852
******** [step = 800] loss: 0.658, acc: 0.852
******** [step = 850] loss: 0.661, acc: 0.851
EPOCH = 22 loss: 0.661, acc: 0.851, val_loss: 1.052, val_acc: 0.815

================================================================================2025-08_11 13:19:34
******** [step = 50] loss: 0.575, acc: 0.866
******** [step = 100] loss: 0.583, acc: 0.865
******** [step = 150] loss: 0.582, acc: 0.865
******** [step = 200] loss: 0.589, acc: 0.863
******** [step = 250] loss: 0.593, acc: 0.863
******** [step = 300] loss: 0.598, acc: 0.862
******** [step = 350] loss: 0.603, acc: 0.861
******** [step = 400] loss: 0.606, acc: 0.860
******** [step = 450] loss: 0.613, acc: 0.859
******** [step = 500] loss: 0.618, acc: 0.858
******** [step = 550] loss: 0.623, acc: 0.857
******** [step = 600] loss: 0.627, acc: 0.857
******** [step = 650] loss: 0.631, acc: 0.856
******** [step = 700] loss: 0.635, acc: 0.856
******** [step = 750] loss: 0.638, acc: 0.856
******** [step = 800] loss: 0.641, acc: 0.855
******** [step = 850] loss: 0.645, acc: 0.854
EPOCH = 23 loss: 0.645, acc: 0.854, val_loss: 1.044, val_acc: 0.816

================================================================================2025-08_11 13:20:55
******** [step = 50] loss: 0.556, acc: 0.870
******** [step = 100] loss: 0.555, acc: 0.870
******** [step = 150] loss: 0.563, acc: 0.869
******** [step = 200] loss: 0.572, acc: 0.867
******** [step = 250] loss: 0.576, acc: 0.866
******** [step = 300] loss: 0.582, acc: 0.865
******** [step = 350] loss: 0.586, acc: 0.864
******** [step = 400] loss: 0.592, acc: 0.863
******** [step = 450] loss: 0.598, acc: 0.862
******** [step = 500] loss: 0.602, acc: 0.862
******** [step = 550] loss: 0.607, acc: 0.861
******** [step = 600] loss: 0.611, acc: 0.860
******** [step = 650] loss: 0.614, acc: 0.860
******** [step = 700] loss: 0.619, acc: 0.859
******** [step = 750] loss: 0.621, acc: 0.859
******** [step = 800] loss: 0.626, acc: 0.858
******** [step = 850] loss: 0.629, acc: 0.858
EPOCH = 24 loss: 0.629, acc: 0.858, val_loss: 1.037, val_acc: 0.818

================================================================================2025-08_11 13:22:16
******** [step = 50] loss: 0.543, acc: 0.872
******** [step = 100] loss: 0.550, acc: 0.871
******** [step = 150] loss: 0.554, acc: 0.870
******** [step = 200] loss: 0.560, acc: 0.869
******** [step = 250] loss: 0.563, acc: 0.869
******** [step = 300] loss: 0.570, acc: 0.867
******** [step = 350] loss: 0.575, acc: 0.866
******** [step = 400] loss: 0.580, acc: 0.866
******** [step = 450] loss: 0.584, acc: 0.865
******** [step = 500] loss: 0.589, acc: 0.864
******** [step = 550] loss: 0.593, acc: 0.864
******** [step = 600] loss: 0.597, acc: 0.863
******** [step = 650] loss: 0.601, acc: 0.863
******** [step = 700] loss: 0.606, acc: 0.862
******** [step = 750] loss: 0.609, acc: 0.861
******** [step = 800] loss: 0.612, acc: 0.861
******** [step = 850] loss: 0.616, acc: 0.861
EPOCH = 25 loss: 0.616, acc: 0.861, val_loss: 1.040, val_acc: 0.818

================================================================================2025-08_11 13:23:37
******** [step = 50] loss: 0.534, acc: 0.875
******** [step = 100] loss: 0.534, acc: 0.875
******** [step = 150] loss: 0.540, acc: 0.873
******** [step = 200] loss: 0.546, acc: 0.872
******** [step = 250] loss: 0.549, acc: 0.872
******** [step = 300] loss: 0.552, acc: 0.871
******** [step = 350] loss: 0.557, acc: 0.871
******** [step = 400] loss: 0.564, acc: 0.870
******** [step = 450] loss: 0.569, acc: 0.869
******** [step = 500] loss: 0.574, acc: 0.868
******** [step = 550] loss: 0.580, acc: 0.867
******** [step = 600] loss: 0.585, acc: 0.866
******** [step = 650] loss: 0.588, acc: 0.866
******** [step = 700] loss: 0.593, acc: 0.865
******** [step = 750] loss: 0.596, acc: 0.865
******** [step = 800] loss: 0.600, acc: 0.864
******** [step = 850] loss: 0.603, acc: 0.864
EPOCH = 26 loss: 0.603, acc: 0.864, val_loss: 1.039, val_acc: 0.818

================================================================================2025-08_11 13:24:57
******** [step = 50] loss: 0.511, acc: 0.879
******** [step = 100] loss: 0.518, acc: 0.878
******** [step = 150] loss: 0.520, acc: 0.878
******** [step = 200] loss: 0.529, acc: 0.876
******** [step = 250] loss: 0.533, acc: 0.875
******** [step = 300] loss: 0.541, acc: 0.874
******** [step = 350] loss: 0.546, acc: 0.873
******** [step = 400] loss: 0.551, acc: 0.872
******** [step = 450] loss: 0.557, acc: 0.871
******** [step = 500] loss: 0.561, acc: 0.870
******** [step = 550] loss: 0.566, acc: 0.870
******** [step = 600] loss: 0.570, acc: 0.869
******** [step = 650] loss: 0.574, acc: 0.868
******** [step = 700] loss: 0.579, acc: 0.868
******** [step = 750] loss: 0.584, acc: 0.867
******** [step = 800] loss: 0.586, acc: 0.866
******** [step = 850] loss: 0.589, acc: 0.866
EPOCH = 27 loss: 0.589, acc: 0.866, val_loss: 1.036, val_acc: 0.819

================================================================================2025-08_11 13:26:17
******** [step = 50] loss: 0.511, acc: 0.880
******** [step = 100] loss: 0.517, acc: 0.879
******** [step = 150] loss: 0.519, acc: 0.879
******** [step = 200] loss: 0.524, acc: 0.877
******** [step = 250] loss: 0.528, acc: 0.876
******** [step = 300] loss: 0.532, acc: 0.875
******** [step = 350] loss: 0.534, acc: 0.875
******** [step = 400] loss: 0.539, acc: 0.874
******** [step = 450] loss: 0.546, acc: 0.873
******** [step = 500] loss: 0.551, acc: 0.872
******** [step = 550] loss: 0.555, acc: 0.872
******** [step = 600] loss: 0.561, acc: 0.871
******** [step = 650] loss: 0.564, acc: 0.870
******** [step = 700] loss: 0.568, acc: 0.870
******** [step = 750] loss: 0.572, acc: 0.869
******** [step = 800] loss: 0.575, acc: 0.869
******** [step = 850] loss: 0.579, acc: 0.868
EPOCH = 28 loss: 0.579, acc: 0.868, val_loss: 1.043, val_acc: 0.819

================================================================================2025-08_11 13:27:37
******** [step = 50] loss: 0.512, acc: 0.880
******** [step = 100] loss: 0.507, acc: 0.881
******** [step = 150] loss: 0.504, acc: 0.881
******** [step = 200] loss: 0.506, acc: 0.881
******** [step = 250] loss: 0.511, acc: 0.880
******** [step = 300] loss: 0.517, acc: 0.879
******** [step = 350] loss: 0.522, acc: 0.878
******** [step = 400] loss: 0.527, acc: 0.877
******** [step = 450] loss: 0.534, acc: 0.876
******** [step = 500] loss: 0.538, acc: 0.876
******** [step = 550] loss: 0.542, acc: 0.875
******** [step = 600] loss: 0.547, acc: 0.874
******** [step = 650] loss: 0.551, acc: 0.874
******** [step = 700] loss: 0.554, acc: 0.873
******** [step = 750] loss: 0.558, acc: 0.873
******** [step = 800] loss: 0.564, acc: 0.872
******** [step = 850] loss: 0.568, acc: 0.871
EPOCH = 29 loss: 0.568, acc: 0.871, val_loss: 1.042, val_acc: 0.819

================================================================================2025-08_11 13:28:57
******** [step = 50] loss: 0.499, acc: 0.882
******** [step = 100] loss: 0.499, acc: 0.882
******** [step = 150] loss: 0.498, acc: 0.882
******** [step = 200] loss: 0.503, acc: 0.881
******** [step = 250] loss: 0.510, acc: 0.880
******** [step = 300] loss: 0.513, acc: 0.880
******** [step = 350] loss: 0.518, acc: 0.879
******** [step = 400] loss: 0.521, acc: 0.878
******** [step = 450] loss: 0.526, acc: 0.877
******** [step = 500] loss: 0.531, acc: 0.877
******** [step = 550] loss: 0.535, acc: 0.876
******** [step = 600] loss: 0.539, acc: 0.875
******** [step = 650] loss: 0.543, acc: 0.875
******** [step = 700] loss: 0.547, acc: 0.874
******** [step = 750] loss: 0.550, acc: 0.874
******** [step = 800] loss: 0.553, acc: 0.873
******** [step = 850] loss: 0.558, acc: 0.873
EPOCH = 30 loss: 0.558, acc: 0.873, val_loss: 1.037, val_acc: 0.821

================================================================================2025-08_11 13:30:17
******** [step = 50] loss: 0.479, acc: 0.887
******** [step = 100] loss: 0.484, acc: 0.885
******** [step = 150] loss: 0.495, acc: 0.883
******** [step = 200] loss: 0.497, acc: 0.883
******** [step = 250] loss: 0.499, acc: 0.882
******** [step = 300] loss: 0.503, acc: 0.882
******** [step = 350] loss: 0.509, acc: 0.881
******** [step = 400] loss: 0.512, acc: 0.881
******** [step = 450] loss: 0.517, acc: 0.880
******** [step = 500] loss: 0.521, acc: 0.879
******** [step = 550] loss: 0.526, acc: 0.878
******** [step = 600] loss: 0.530, acc: 0.877
******** [step = 650] loss: 0.534, acc: 0.877
******** [step = 700] loss: 0.538, acc: 0.876
******** [step = 750] loss: 0.541, acc: 0.876
******** [step = 800] loss: 0.545, acc: 0.875
******** [step = 850] loss: 0.549, acc: 0.875
EPOCH = 31 loss: 0.549, acc: 0.875, val_loss: 1.037, val_acc: 0.820

================================================================================2025-08_11 13:31:37
******** [step = 50] loss: 0.458, acc: 0.889
******** [step = 100] loss: 0.470, acc: 0.887
******** [step = 150] loss: 0.476, acc: 0.886
******** [step = 200] loss: 0.481, acc: 0.885
******** [step = 250] loss: 0.488, acc: 0.884
******** [step = 300] loss: 0.491, acc: 0.883
******** [step = 350] loss: 0.496, acc: 0.883
******** [step = 400] loss: 0.501, acc: 0.882
******** [step = 450] loss: 0.507, acc: 0.881
******** [step = 500] loss: 0.512, acc: 0.880
******** [step = 550] loss: 0.517, acc: 0.880
******** [step = 600] loss: 0.522, acc: 0.879
******** [step = 650] loss: 0.524, acc: 0.879
******** [step = 700] loss: 0.529, acc: 0.878
******** [step = 750] loss: 0.532, acc: 0.877
******** [step = 800] loss: 0.537, acc: 0.877
******** [step = 850] loss: 0.539, acc: 0.877
EPOCH = 32 loss: 0.539, acc: 0.877, val_loss: 1.034, val_acc: 0.824

================================================================================2025-08_11 13:32:57
******** [step = 50] loss: 0.448, acc: 0.892
******** [step = 100] loss: 0.461, acc: 0.889
******** [step = 150] loss: 0.470, acc: 0.887
******** [step = 200] loss: 0.475, acc: 0.887
******** [step = 250] loss: 0.479, acc: 0.886
******** [step = 300] loss: 0.484, acc: 0.885
******** [step = 350] loss: 0.490, acc: 0.884
******** [step = 400] loss: 0.494, acc: 0.884
******** [step = 450] loss: 0.500, acc: 0.883
******** [step = 500] loss: 0.505, acc: 0.882
******** [step = 550] loss: 0.509, acc: 0.882
******** [step = 600] loss: 0.512, acc: 0.881
******** [step = 650] loss: 0.516, acc: 0.880
******** [step = 700] loss: 0.520, acc: 0.880
******** [step = 750] loss: 0.524, acc: 0.879
******** [step = 800] loss: 0.525, acc: 0.879
******** [step = 850] loss: 0.529, acc: 0.879
EPOCH = 33 loss: 0.529, acc: 0.879, val_loss: 1.036, val_acc: 0.823

================================================================================2025-08_11 13:34:17
******** [step = 50] loss: 0.464, acc: 0.890
******** [step = 100] loss: 0.464, acc: 0.890
******** [step = 150] loss: 0.462, acc: 0.890
******** [step = 200] loss: 0.465, acc: 0.890
******** [step = 250] loss: 0.470, acc: 0.889
******** [step = 300] loss: 0.477, acc: 0.888
******** [step = 350] loss: 0.482, acc: 0.887
******** [step = 400] loss: 0.487, acc: 0.886
******** [step = 450] loss: 0.491, acc: 0.885
******** [step = 500] loss: 0.495, acc: 0.884
******** [step = 550] loss: 0.501, acc: 0.883
******** [step = 600] loss: 0.504, acc: 0.883
******** [step = 650] loss: 0.508, acc: 0.882
******** [step = 700] loss: 0.513, acc: 0.882
******** [step = 750] loss: 0.517, acc: 0.881
******** [step = 800] loss: 0.520, acc: 0.881
******** [step = 850] loss: 0.522, acc: 0.880
EPOCH = 34 loss: 0.522, acc: 0.880, val_loss: 1.032, val_acc: 0.824

================================================================================2025-08_11 13:35:37
******** [step = 50] loss: 0.441, acc: 0.896
******** [step = 100] loss: 0.446, acc: 0.894
******** [step = 150] loss: 0.454, acc: 0.892
******** [step = 200] loss: 0.460, acc: 0.891
******** [step = 250] loss: 0.465, acc: 0.890
******** [step = 300] loss: 0.469, acc: 0.889
******** [step = 350] loss: 0.473, acc: 0.888
******** [step = 400] loss: 0.478, acc: 0.887
******** [step = 450] loss: 0.484, acc: 0.886
******** [step = 500] loss: 0.488, acc: 0.886
******** [step = 550] loss: 0.493, acc: 0.885
******** [step = 600] loss: 0.498, acc: 0.884
******** [step = 650] loss: 0.502, acc: 0.884
******** [step = 700] loss: 0.505, acc: 0.883
******** [step = 750] loss: 0.508, acc: 0.883
******** [step = 800] loss: 0.512, acc: 0.882
******** [step = 850] loss: 0.514, acc: 0.882
EPOCH = 35 loss: 0.514, acc: 0.882, val_loss: 1.033, val_acc: 0.825

================================================================================2025-08_11 13:36:57
******** [step = 50] loss: 0.442, acc: 0.896
******** [step = 100] loss: 0.440, acc: 0.895
******** [step = 150] loss: 0.445, acc: 0.894
******** [step = 200] loss: 0.451, acc: 0.893
******** [step = 250] loss: 0.452, acc: 0.893
******** [step = 300] loss: 0.457, acc: 0.892
******** [step = 350] loss: 0.461, acc: 0.891
******** [step = 400] loss: 0.468, acc: 0.890
******** [step = 450] loss: 0.474, acc: 0.889
******** [step = 500] loss: 0.480, acc: 0.888
******** [step = 550] loss: 0.484, acc: 0.888
******** [step = 600] loss: 0.489, acc: 0.887
******** [step = 650] loss: 0.492, acc: 0.886
******** [step = 700] loss: 0.496, acc: 0.886
******** [step = 750] loss: 0.500, acc: 0.885
******** [step = 800] loss: 0.505, acc: 0.884
******** [step = 850] loss: 0.508, acc: 0.884
EPOCH = 36 loss: 0.508, acc: 0.884, val_loss: 1.034, val_acc: 0.825

================================================================================2025-08_11 13:38:20
******** [step = 50] loss: 0.437, acc: 0.895
******** [step = 100] loss: 0.434, acc: 0.896
******** [step = 150] loss: 0.440, acc: 0.896
******** [step = 200] loss: 0.446, acc: 0.894
******** [step = 250] loss: 0.451, acc: 0.893
******** [step = 300] loss: 0.455, acc: 0.892
******** [step = 350] loss: 0.460, acc: 0.892
******** [step = 400] loss: 0.464, acc: 0.891
******** [step = 450] loss: 0.470, acc: 0.890
******** [step = 500] loss: 0.475, acc: 0.889
******** [step = 550] loss: 0.479, acc: 0.889
******** [step = 600] loss: 0.482, acc: 0.888
******** [step = 650] loss: 0.485, acc: 0.888
******** [step = 700] loss: 0.489, acc: 0.887
******** [step = 750] loss: 0.493, acc: 0.886
******** [step = 800] loss: 0.496, acc: 0.886
******** [step = 850] loss: 0.501, acc: 0.885
EPOCH = 37 loss: 0.501, acc: 0.885, val_loss: 1.031, val_acc: 0.826

================================================================================2025-08_11 13:39:40
******** [step = 50] loss: 0.435, acc: 0.898
******** [step = 100] loss: 0.430, acc: 0.898
******** [step = 150] loss: 0.433, acc: 0.897
******** [step = 200] loss: 0.437, acc: 0.896
******** [step = 250] loss: 0.440, acc: 0.896
******** [step = 300] loss: 0.445, acc: 0.895
******** [step = 350] loss: 0.451, acc: 0.894
******** [step = 400] loss: 0.456, acc: 0.893
******** [step = 450] loss: 0.461, acc: 0.892
******** [step = 500] loss: 0.466, acc: 0.891
******** [step = 550] loss: 0.470, acc: 0.891
******** [step = 600] loss: 0.475, acc: 0.890
******** [step = 650] loss: 0.479, acc: 0.889
******** [step = 700] loss: 0.484, acc: 0.888
******** [step = 750] loss: 0.487, acc: 0.888
******** [step = 800] loss: 0.489, acc: 0.888
******** [step = 850] loss: 0.493, acc: 0.887
EPOCH = 38 loss: 0.493, acc: 0.887, val_loss: 1.037, val_acc: 0.825

================================================================================2025-08_11 13:41:04
******** [step = 50] loss: 0.426, acc: 0.897
******** [step = 100] loss: 0.422, acc: 0.899
******** [step = 150] loss: 0.422, acc: 0.899
******** [step = 200] loss: 0.426, acc: 0.898
******** [step = 250] loss: 0.433, acc: 0.897
******** [step = 300] loss: 0.438, acc: 0.897
******** [step = 350] loss: 0.444, acc: 0.896
******** [step = 400] loss: 0.451, acc: 0.895
******** [step = 450] loss: 0.455, acc: 0.894
******** [step = 500] loss: 0.460, acc: 0.893
******** [step = 550] loss: 0.464, acc: 0.892
******** [step = 600] loss: 0.469, acc: 0.891
******** [step = 650] loss: 0.472, acc: 0.891
******** [step = 700] loss: 0.475, acc: 0.890
******** [step = 750] loss: 0.478, acc: 0.890
******** [step = 800] loss: 0.482, acc: 0.889
******** [step = 850] loss: 0.487, acc: 0.888
EPOCH = 39 loss: 0.487, acc: 0.888, val_loss: 1.030, val_acc: 0.827

================================================================================2025-08_11 13:42:23
******** [step = 50] loss: 0.426, acc: 0.898
******** [step = 100] loss: 0.427, acc: 0.898
******** [step = 150] loss: 0.429, acc: 0.898
******** [step = 200] loss: 0.432, acc: 0.897
******** [step = 250] loss: 0.435, acc: 0.897
******** [step = 300] loss: 0.439, acc: 0.896
******** [step = 350] loss: 0.444, acc: 0.896
******** [step = 400] loss: 0.450, acc: 0.895
******** [step = 450] loss: 0.453, acc: 0.894
******** [step = 500] loss: 0.457, acc: 0.894
******** [step = 550] loss: 0.460, acc: 0.893
******** [step = 600] loss: 0.464, acc: 0.892
******** [step = 650] loss: 0.468, acc: 0.892
******** [step = 700] loss: 0.473, acc: 0.891
******** [step = 750] loss: 0.476, acc: 0.891
******** [step = 800] loss: 0.478, acc: 0.890
******** [step = 850] loss: 0.481, acc: 0.890
EPOCH = 40 loss: 0.481, acc: 0.890, val_loss: 1.036, val_acc: 0.826

================================================================================2025-08_11 13:43:51
******** [step = 50] loss: 0.414, acc: 0.902
******** [step = 100] loss: 0.415, acc: 0.901
******** [step = 150] loss: 0.419, acc: 0.900
******** [step = 200] loss: 0.420, acc: 0.900
******** [step = 250] loss: 0.425, acc: 0.899
******** [step = 300] loss: 0.429, acc: 0.898
******** [step = 350] loss: 0.434, acc: 0.897
******** [step = 400] loss: 0.439, acc: 0.896
******** [step = 450] loss: 0.443, acc: 0.895
******** [step = 500] loss: 0.447, acc: 0.895
******** [step = 550] loss: 0.451, acc: 0.894
******** [step = 600] loss: 0.455, acc: 0.894
******** [step = 650] loss: 0.459, acc: 0.893
******** [step = 700] loss: 0.463, acc: 0.893
******** [step = 750] loss: 0.466, acc: 0.892
******** [step = 800] loss: 0.470, acc: 0.892
******** [step = 850] loss: 0.473, acc: 0.891
EPOCH = 41 loss: 0.473, acc: 0.891, val_loss: 1.037, val_acc: 0.828

================================================================================2025-08_11 13:45:11
******** [step = 50] loss: 0.410, acc: 0.903
******** [step = 100] loss: 0.410, acc: 0.902
******** [step = 150] loss: 0.410, acc: 0.902
******** [step = 200] loss: 0.416, acc: 0.901
******** [step = 250] loss: 0.423, acc: 0.900
******** [step = 300] loss: 0.428, acc: 0.899
******** [step = 350] loss: 0.434, acc: 0.898
******** [step = 400] loss: 0.437, acc: 0.897
******** [step = 450] loss: 0.440, acc: 0.897
******** [step = 500] loss: 0.444, acc: 0.896
******** [step = 550] loss: 0.448, acc: 0.895
******** [step = 600] loss: 0.451, acc: 0.895
******** [step = 650] loss: 0.455, acc: 0.894
******** [step = 700] loss: 0.458, acc: 0.894
******** [step = 750] loss: 0.461, acc: 0.894
******** [step = 800] loss: 0.465, acc: 0.893
******** [step = 850] loss: 0.467, acc: 0.893
EPOCH = 42 loss: 0.467, acc: 0.893, val_loss: 1.037, val_acc: 0.828

================================================================================2025-08_11 13:46:40
******** [step = 50] loss: 0.397, acc: 0.906
******** [step = 100] loss: 0.398, acc: 0.905
******** [step = 150] loss: 0.402, acc: 0.904
******** [step = 200] loss: 0.405, acc: 0.903
******** [step = 250] loss: 0.410, acc: 0.902
******** [step = 300] loss: 0.415, acc: 0.902
******** [step = 350] loss: 0.421, acc: 0.900
******** [step = 400] loss: 0.425, acc: 0.900
******** [step = 450] loss: 0.429, acc: 0.899
******** [step = 500] loss: 0.434, acc: 0.898
******** [step = 550] loss: 0.438, acc: 0.898
******** [step = 600] loss: 0.442, acc: 0.897
******** [step = 650] loss: 0.446, acc: 0.896
******** [step = 700] loss: 0.449, acc: 0.896
******** [step = 750] loss: 0.454, acc: 0.895
******** [step = 800] loss: 0.457, acc: 0.895
******** [step = 850] loss: 0.461, acc: 0.894
EPOCH = 43 loss: 0.461, acc: 0.894, val_loss: 1.037, val_acc: 0.828

================================================================================2025-08_11 13:48:00
******** [step = 50] loss: 0.396, acc: 0.905
******** [step = 100] loss: 0.395, acc: 0.906
******** [step = 150] loss: 0.403, acc: 0.905
******** [step = 200] loss: 0.409, acc: 0.903
******** [step = 250] loss: 0.413, acc: 0.902
******** [step = 300] loss: 0.417, acc: 0.902
******** [step = 350] loss: 0.422, acc: 0.900
******** [step = 400] loss: 0.426, acc: 0.900
******** [step = 450] loss: 0.429, acc: 0.899
******** [step = 500] loss: 0.432, acc: 0.899
******** [step = 550] loss: 0.435, acc: 0.898
******** [step = 600] loss: 0.438, acc: 0.898
******** [step = 650] loss: 0.442, acc: 0.897
******** [step = 700] loss: 0.447, acc: 0.896
******** [step = 750] loss: 0.450, acc: 0.896
******** [step = 800] loss: 0.455, acc: 0.895
******** [step = 850] loss: 0.457, acc: 0.895
EPOCH = 44 loss: 0.457, acc: 0.895, val_loss: 1.039, val_acc: 0.828

================================================================================2025-08_11 13:49:21
******** [step = 50] loss: 0.378, acc: 0.910
******** [step = 100] loss: 0.385, acc: 0.908
******** [step = 150] loss: 0.390, acc: 0.907
******** [step = 200] loss: 0.397, acc: 0.905
******** [step = 250] loss: 0.404, acc: 0.904
******** [step = 300] loss: 0.407, acc: 0.904
******** [step = 350] loss: 0.411, acc: 0.903
******** [step = 400] loss: 0.417, acc: 0.902
******** [step = 450] loss: 0.421, acc: 0.901
******** [step = 500] loss: 0.426, acc: 0.900
******** [step = 550] loss: 0.430, acc: 0.900
******** [step = 600] loss: 0.434, acc: 0.899
******** [step = 650] loss: 0.438, acc: 0.898
******** [step = 700] loss: 0.443, acc: 0.898
******** [step = 750] loss: 0.446, acc: 0.897
******** [step = 800] loss: 0.449, acc: 0.897
******** [step = 850] loss: 0.452, acc: 0.896
EPOCH = 45 loss: 0.452, acc: 0.896, val_loss: 1.041, val_acc: 0.827

================================================================================2025-08_11 13:50:43
******** [step = 50] loss: 0.378, acc: 0.908
******** [step = 100] loss: 0.386, acc: 0.907
******** [step = 150] loss: 0.389, acc: 0.907
******** [step = 200] loss: 0.392, acc: 0.906
******** [step = 250] loss: 0.398, acc: 0.905
******** [step = 300] loss: 0.406, acc: 0.904
******** [step = 350] loss: 0.411, acc: 0.903
******** [step = 400] loss: 0.413, acc: 0.902
******** [step = 450] loss: 0.417, acc: 0.902
******** [step = 500] loss: 0.422, acc: 0.901
******** [step = 550] loss: 0.426, acc: 0.900
******** [step = 600] loss: 0.430, acc: 0.900
******** [step = 650] loss: 0.434, acc: 0.899
******** [step = 700] loss: 0.437, acc: 0.899
******** [step = 750] loss: 0.439, acc: 0.898
******** [step = 800] loss: 0.442, acc: 0.898
******** [step = 850] loss: 0.446, acc: 0.897
EPOCH = 46 loss: 0.446, acc: 0.897, val_loss: 1.039, val_acc: 0.829

================================================================================2025-08_11 13:52:08
******** [step = 50] loss: 0.378, acc: 0.908
******** [step = 100] loss: 0.385, acc: 0.908
******** [step = 150] loss: 0.387, acc: 0.908
******** [step = 200] loss: 0.392, acc: 0.906
******** [step = 250] loss: 0.394, acc: 0.906
******** [step = 300] loss: 0.397, acc: 0.906
******** [step = 350] loss: 0.402, acc: 0.905
******** [step = 400] loss: 0.405, acc: 0.904
******** [step = 450] loss: 0.409, acc: 0.903
******** [step = 500] loss: 0.414, acc: 0.903
******** [step = 550] loss: 0.417, acc: 0.902
******** [step = 600] loss: 0.421, acc: 0.901
******** [step = 650] loss: 0.425, acc: 0.901
******** [step = 700] loss: 0.429, acc: 0.900
******** [step = 750] loss: 0.433, acc: 0.900
******** [step = 800] loss: 0.437, acc: 0.899
******** [step = 850] loss: 0.441, acc: 0.899
EPOCH = 47 loss: 0.441, acc: 0.899, val_loss: 1.038, val_acc: 0.828

================================================================================2025-08_11 13:53:34
******** [step = 50] loss: 0.371, acc: 0.911
******** [step = 100] loss: 0.378, acc: 0.910
******** [step = 150] loss: 0.384, acc: 0.909
******** [step = 200] loss: 0.387, acc: 0.909
******** [step = 250] loss: 0.390, acc: 0.908
******** [step = 300] loss: 0.394, acc: 0.907
******** [step = 350] loss: 0.401, acc: 0.906
******** [step = 400] loss: 0.405, acc: 0.905
******** [step = 450] loss: 0.408, acc: 0.904
******** [step = 500] loss: 0.411, acc: 0.904
******** [step = 550] loss: 0.417, acc: 0.903
******** [step = 600] loss: 0.420, acc: 0.902
******** [step = 650] loss: 0.424, acc: 0.902
******** [step = 700] loss: 0.427, acc: 0.901
******** [step = 750] loss: 0.430, acc: 0.901
******** [step = 800] loss: 0.433, acc: 0.900
******** [step = 850] loss: 0.436, acc: 0.900
EPOCH = 48 loss: 0.436, acc: 0.900, val_loss: 1.037, val_acc: 0.829

================================================================================2025-08_11 13:54:54
******** [step = 50] loss: 0.369, acc: 0.911
******** [step = 100] loss: 0.374, acc: 0.911
******** [step = 150] loss: 0.375, acc: 0.911
******** [step = 200] loss: 0.380, acc: 0.909
******** [step = 250] loss: 0.383, acc: 0.908
******** [step = 300] loss: 0.388, acc: 0.907
******** [step = 350] loss: 0.392, acc: 0.907
******** [step = 400] loss: 0.396, acc: 0.906
******** [step = 450] loss: 0.399, acc: 0.905
******** [step = 500] loss: 0.404, acc: 0.905
******** [step = 550] loss: 0.410, acc: 0.904
******** [step = 600] loss: 0.414, acc: 0.903
******** [step = 650] loss: 0.419, acc: 0.902
******** [step = 700] loss: 0.423, acc: 0.902
******** [step = 750] loss: 0.426, acc: 0.901
******** [step = 800] loss: 0.429, acc: 0.901
******** [step = 850] loss: 0.432, acc: 0.900
EPOCH = 49 loss: 0.432, acc: 0.900, val_loss: 1.042, val_acc: 0.829

================================================================================2025-08_11 13:56:15
******** [step = 50] loss: 0.376, acc: 0.911
******** [step = 100] loss: 0.378, acc: 0.910
******** [step = 150] loss: 0.377, acc: 0.910
******** [step = 200] loss: 0.378, acc: 0.910
******** [step = 250] loss: 0.380, acc: 0.910
******** [step = 300] loss: 0.386, acc: 0.909
******** [step = 350] loss: 0.390, acc: 0.908
******** [step = 400] loss: 0.394, acc: 0.907
******** [step = 450] loss: 0.399, acc: 0.906
******** [step = 500] loss: 0.403, acc: 0.905
******** [step = 550] loss: 0.406, acc: 0.905
******** [step = 600] loss: 0.410, acc: 0.904
******** [step = 650] loss: 0.415, acc: 0.904
******** [step = 700] loss: 0.418, acc: 0.903
******** [step = 750] loss: 0.421, acc: 0.903
******** [step = 800] loss: 0.424, acc: 0.902
******** [step = 850] loss: 0.427, acc: 0.902
EPOCH = 50 loss: 0.427, acc: 0.902, val_loss: 1.039, val_acc: 0.829

================================================================================2025-08_11 13:57:37
******** [step = 50] loss: 0.353, acc: 0.915
******** [step = 100] loss: 0.360, acc: 0.913
******** [step = 150] loss: 0.363, acc: 0.912
******** [step = 200] loss: 0.370, acc: 0.911
******** [step = 250] loss: 0.378, acc: 0.909
******** [step = 300] loss: 0.383, acc: 0.909
******** [step = 350] loss: 0.387, acc: 0.908
******** [step = 400] loss: 0.391, acc: 0.907
******** [step = 450] loss: 0.396, acc: 0.907
******** [step = 500] loss: 0.400, acc: 0.906
******** [step = 550] loss: 0.404, acc: 0.905
******** [step = 600] loss: 0.409, acc: 0.904
******** [step = 650] loss: 0.412, acc: 0.904
******** [step = 700] loss: 0.415, acc: 0.904
******** [step = 750] loss: 0.419, acc: 0.903
******** [step = 800] loss: 0.421, acc: 0.903
******** [step = 850] loss: 0.424, acc: 0.903
EPOCH = 51 loss: 0.424, acc: 0.903, val_loss: 1.042, val_acc: 0.830

================================================================================2025-08_11 13:58:59
******** [step = 50] loss: 0.367, acc: 0.913
******** [step = 100] loss: 0.360, acc: 0.913
******** [step = 150] loss: 0.366, acc: 0.912
******** [step = 200] loss: 0.369, acc: 0.911
******** [step = 250] loss: 0.375, acc: 0.910
******** [step = 300] loss: 0.380, acc: 0.909
******** [step = 350] loss: 0.385, acc: 0.908
******** [step = 400] loss: 0.388, acc: 0.908
******** [step = 450] loss: 0.392, acc: 0.907
******** [step = 500] loss: 0.396, acc: 0.907
******** [step = 550] loss: 0.399, acc: 0.906
******** [step = 600] loss: 0.402, acc: 0.906
******** [step = 650] loss: 0.405, acc: 0.905
******** [step = 700] loss: 0.409, acc: 0.905
******** [step = 750] loss: 0.412, acc: 0.904
******** [step = 800] loss: 0.415, acc: 0.904
******** [step = 850] loss: 0.418, acc: 0.903
EPOCH = 52 loss: 0.418, acc: 0.903, val_loss: 1.043, val_acc: 0.830

================================================================================2025-08_11 14:00:26
******** [step = 50] loss: 0.348, acc: 0.917
******** [step = 100] loss: 0.352, acc: 0.914
******** [step = 150] loss: 0.360, acc: 0.913
******** [step = 200] loss: 0.362, acc: 0.913
******** [step = 250] loss: 0.366, acc: 0.912
******** [step = 300] loss: 0.372, acc: 0.911
******** [step = 350] loss: 0.377, acc: 0.910
******** [step = 400] loss: 0.381, acc: 0.910
******** [step = 450] loss: 0.385, acc: 0.909
******** [step = 500] loss: 0.388, acc: 0.909
******** [step = 550] loss: 0.393, acc: 0.908
******** [step = 600] loss: 0.398, acc: 0.907
******** [step = 650] loss: 0.401, acc: 0.907
******** [step = 700] loss: 0.405, acc: 0.906
******** [step = 750] loss: 0.408, acc: 0.905
******** [step = 800] loss: 0.412, acc: 0.905
******** [step = 850] loss: 0.415, acc: 0.904
EPOCH = 53 loss: 0.415, acc: 0.904, val_loss: 1.049, val_acc: 0.830

================================================================================2025-08_11 14:01:57
******** [step = 50] loss: 0.363, acc: 0.914
******** [step = 100] loss: 0.361, acc: 0.914
******** [step = 150] loss: 0.360, acc: 0.914
******** [step = 200] loss: 0.364, acc: 0.913
******** [step = 250] loss: 0.367, acc: 0.912
******** [step = 300] loss: 0.370, acc: 0.912
******** [step = 350] loss: 0.375, acc: 0.911
******** [step = 400] loss: 0.376, acc: 0.911
******** [step = 450] loss: 0.380, acc: 0.910
******** [step = 500] loss: 0.385, acc: 0.910
******** [step = 550] loss: 0.390, acc: 0.909
******** [step = 600] loss: 0.392, acc: 0.909
******** [step = 650] loss: 0.395, acc: 0.908
******** [step = 700] loss: 0.399, acc: 0.908
******** [step = 750] loss: 0.403, acc: 0.907
******** [step = 800] loss: 0.406, acc: 0.906
******** [step = 850] loss: 0.409, acc: 0.906
EPOCH = 54 loss: 0.409, acc: 0.906, val_loss: 1.045, val_acc: 0.830

================================================================================2025-08_11 14:03:20
******** [step = 50] loss: 0.344, acc: 0.920
******** [step = 100] loss: 0.347, acc: 0.918
******** [step = 150] loss: 0.352, acc: 0.917
******** [step = 200] loss: 0.358, acc: 0.915
******** [step = 250] loss: 0.360, acc: 0.915
******** [step = 300] loss: 0.364, acc: 0.914
******** [step = 350] loss: 0.368, acc: 0.913
******** [step = 400] loss: 0.372, acc: 0.912
******** [step = 450] loss: 0.377, acc: 0.912
******** [step = 500] loss: 0.381, acc: 0.911
******** [step = 550] loss: 0.385, acc: 0.910
******** [step = 600] loss: 0.389, acc: 0.910
******** [step = 650] loss: 0.393, acc: 0.909
******** [step = 700] loss: 0.397, acc: 0.908
******** [step = 750] loss: 0.400, acc: 0.908
******** [step = 800] loss: 0.403, acc: 0.907
******** [step = 850] loss: 0.405, acc: 0.907
EPOCH = 55 loss: 0.405, acc: 0.907, val_loss: 1.049, val_acc: 0.831

================================================================================2025-08_11 14:04:41
******** [step = 50] loss: 0.336, acc: 0.920
******** [step = 100] loss: 0.334, acc: 0.920
******** [step = 150] loss: 0.341, acc: 0.918
******** [step = 200] loss: 0.345, acc: 0.917
******** [step = 250] loss: 0.348, acc: 0.916
******** [step = 300] loss: 0.354, acc: 0.915
******** [step = 350] loss: 0.358, acc: 0.914
******** [step = 400] loss: 0.364, acc: 0.914
******** [step = 450] loss: 0.370, acc: 0.913
******** [step = 500] loss: 0.375, acc: 0.912
******** [step = 550] loss: 0.380, acc: 0.911
******** [step = 600] loss: 0.385, acc: 0.910
******** [step = 650] loss: 0.390, acc: 0.909
******** [step = 700] loss: 0.394, acc: 0.909
******** [step = 750] loss: 0.398, acc: 0.908
******** [step = 800] loss: 0.402, acc: 0.908
******** [step = 850] loss: 0.404, acc: 0.907
EPOCH = 56 loss: 0.404, acc: 0.907, val_loss: 1.046, val_acc: 0.832

================================================================================2025-08_11 14:06:07
******** [step = 50] loss: 0.335, acc: 0.919
******** [step = 100] loss: 0.343, acc: 0.918
******** [step = 150] loss: 0.347, acc: 0.917
******** [step = 200] loss: 0.349, acc: 0.916
******** [step = 250] loss: 0.355, acc: 0.915
******** [step = 300] loss: 0.358, acc: 0.915
******** [step = 350] loss: 0.360, acc: 0.914
******** [step = 400] loss: 0.366, acc: 0.913
******** [step = 450] loss: 0.370, acc: 0.912
******** [step = 500] loss: 0.375, acc: 0.912
******** [step = 550] loss: 0.378, acc: 0.911
******** [step = 600] loss: 0.381, acc: 0.911
******** [step = 650] loss: 0.385, acc: 0.910
******** [step = 700] loss: 0.389, acc: 0.910
******** [step = 750] loss: 0.393, acc: 0.909
******** [step = 800] loss: 0.396, acc: 0.909
******** [step = 850] loss: 0.399, acc: 0.908
EPOCH = 57 loss: 0.399, acc: 0.908, val_loss: 1.050, val_acc: 0.831

================================================================================2025-08_11 14:07:32
******** [step = 50] loss: 0.330, acc: 0.919
******** [step = 100] loss: 0.337, acc: 0.918
******** [step = 150] loss: 0.340, acc: 0.918
******** [step = 200] loss: 0.341, acc: 0.917
******** [step = 250] loss: 0.347, acc: 0.916
******** [step = 300] loss: 0.352, acc: 0.916
******** [step = 350] loss: 0.356, acc: 0.915
******** [step = 400] loss: 0.361, acc: 0.914
******** [step = 450] loss: 0.364, acc: 0.914
******** [step = 500] loss: 0.369, acc: 0.913
******** [step = 550] loss: 0.371, acc: 0.912
******** [step = 600] loss: 0.376, acc: 0.912
******** [step = 650] loss: 0.380, acc: 0.911
******** [step = 700] loss: 0.385, acc: 0.910
******** [step = 750] loss: 0.389, acc: 0.910
******** [step = 800] loss: 0.393, acc: 0.909
******** [step = 850] loss: 0.396, acc: 0.909
EPOCH = 58 loss: 0.396, acc: 0.909, val_loss: 1.052, val_acc: 0.831

================================================================================2025-08_11 14:08:52
******** [step = 50] loss: 0.338, acc: 0.920
******** [step = 100] loss: 0.337, acc: 0.919
******** [step = 150] loss: 0.342, acc: 0.918
******** [step = 200] loss: 0.346, acc: 0.918
******** [step = 250] loss: 0.351, acc: 0.916
******** [step = 300] loss: 0.355, acc: 0.916
******** [step = 350] loss: 0.359, acc: 0.915
******** [step = 400] loss: 0.362, acc: 0.914
******** [step = 450] loss: 0.366, acc: 0.914
******** [step = 500] loss: 0.369, acc: 0.913
******** [step = 550] loss: 0.373, acc: 0.913
******** [step = 600] loss: 0.376, acc: 0.912
******** [step = 650] loss: 0.379, acc: 0.911
******** [step = 700] loss: 0.383, acc: 0.911
******** [step = 750] loss: 0.386, acc: 0.911
******** [step = 800] loss: 0.389, acc: 0.910
******** [step = 850] loss: 0.393, acc: 0.910
EPOCH = 59 loss: 0.393, acc: 0.910, val_loss: 1.054, val_acc: 0.831

================================================================================2025-08_11 14:10:16
******** [step = 50] loss: 0.346, acc: 0.916
******** [step = 100] loss: 0.339, acc: 0.919
******** [step = 150] loss: 0.341, acc: 0.919
******** [step = 200] loss: 0.344, acc: 0.918
******** [step = 250] loss: 0.347, acc: 0.917
******** [step = 300] loss: 0.349, acc: 0.917
******** [step = 350] loss: 0.352, acc: 0.916
******** [step = 400] loss: 0.358, acc: 0.915
******** [step = 450] loss: 0.362, acc: 0.915
******** [step = 500] loss: 0.367, acc: 0.914
******** [step = 550] loss: 0.371, acc: 0.913
******** [step = 600] loss: 0.374, acc: 0.913
******** [step = 650] loss: 0.378, acc: 0.912
******** [step = 700] loss: 0.381, acc: 0.912
******** [step = 750] loss: 0.384, acc: 0.911
******** [step = 800] loss: 0.386, acc: 0.911
******** [step = 850] loss: 0.389, acc: 0.911
EPOCH = 60 loss: 0.389, acc: 0.911, val_loss: 1.050, val_acc: 0.832

================================================================================2025-08_11 14:11:37
******** [step = 50] loss: 0.325, acc: 0.923
******** [step = 100] loss: 0.330, acc: 0.921
******** [step = 150] loss: 0.334, acc: 0.920
******** [step = 200] loss: 0.337, acc: 0.919
******** [step = 250] loss: 0.344, acc: 0.918
******** [step = 300] loss: 0.348, acc: 0.917
******** [step = 350] loss: 0.353, acc: 0.916
******** [step = 400] loss: 0.357, acc: 0.915
******** [step = 450] loss: 0.362, acc: 0.915
******** [step = 500] loss: 0.364, acc: 0.914
******** [step = 550] loss: 0.368, acc: 0.914
******** [step = 600] loss: 0.371, acc: 0.913
******** [step = 650] loss: 0.374, acc: 0.913
******** [step = 700] loss: 0.378, acc: 0.912
******** [step = 750] loss: 0.381, acc: 0.912
******** [step = 800] loss: 0.384, acc: 0.911
******** [step = 850] loss: 0.387, acc: 0.911
EPOCH = 61 loss: 0.387, acc: 0.911, val_loss: 1.048, val_acc: 0.831

================================================================================2025-08_11 14:12:57
******** [step = 50] loss: 0.327, acc: 0.922
******** [step = 100] loss: 0.327, acc: 0.922
******** [step = 150] loss: 0.332, acc: 0.921
******** [step = 200] loss: 0.336, acc: 0.920
******** [step = 250] loss: 0.338, acc: 0.920
******** [step = 300] loss: 0.342, acc: 0.919
******** [step = 350] loss: 0.347, acc: 0.918
******** [step = 400] loss: 0.352, acc: 0.918
******** [step = 450] loss: 0.356, acc: 0.917
******** [step = 500] loss: 0.360, acc: 0.916
******** [step = 550] loss: 0.363, acc: 0.915
******** [step = 600] loss: 0.367, acc: 0.915
******** [step = 650] loss: 0.370, acc: 0.914
******** [step = 700] loss: 0.375, acc: 0.914
******** [step = 750] loss: 0.378, acc: 0.913
******** [step = 800] loss: 0.382, acc: 0.913
******** [step = 850] loss: 0.384, acc: 0.912
EPOCH = 62 loss: 0.384, acc: 0.912, val_loss: 1.050, val_acc: 0.832

================================================================================2025-08_11 14:14:17
******** [step = 50] loss: 0.319, acc: 0.923
******** [step = 100] loss: 0.328, acc: 0.922
******** [step = 150] loss: 0.333, acc: 0.921
******** [step = 200] loss: 0.339, acc: 0.920
******** [step = 250] loss: 0.342, acc: 0.919
******** [step = 300] loss: 0.345, acc: 0.918
******** [step = 350] loss: 0.350, acc: 0.918
******** [step = 400] loss: 0.354, acc: 0.917
******** [step = 450] loss: 0.356, acc: 0.916
******** [step = 500] loss: 0.359, acc: 0.916
******** [step = 550] loss: 0.361, acc: 0.916
******** [step = 600] loss: 0.364, acc: 0.915
******** [step = 650] loss: 0.368, acc: 0.914
******** [step = 700] loss: 0.371, acc: 0.914
******** [step = 750] loss: 0.374, acc: 0.913
******** [step = 800] loss: 0.377, acc: 0.913
******** [step = 850] loss: 0.379, acc: 0.913
EPOCH = 63 loss: 0.379, acc: 0.913, val_loss: 1.056, val_acc: 0.831

================================================================================2025-08_11 14:15:38
******** [step = 50] loss: 0.315, acc: 0.924
******** [step = 100] loss: 0.317, acc: 0.924
******** [step = 150] loss: 0.322, acc: 0.923
******** [step = 200] loss: 0.326, acc: 0.921
******** [step = 250] loss: 0.332, acc: 0.920
******** [step = 300] loss: 0.337, acc: 0.919
******** [step = 350] loss: 0.341, acc: 0.919
******** [step = 400] loss: 0.345, acc: 0.918
******** [step = 450] loss: 0.349, acc: 0.917
******** [step = 500] loss: 0.353, acc: 0.917
******** [step = 550] loss: 0.357, acc: 0.916
******** [step = 600] loss: 0.362, acc: 0.915
******** [step = 650] loss: 0.365, acc: 0.915
******** [step = 700] loss: 0.369, acc: 0.914
******** [step = 750] loss: 0.372, acc: 0.914
******** [step = 800] loss: 0.375, acc: 0.913
******** [step = 850] loss: 0.379, acc: 0.913
EPOCH = 64 loss: 0.379, acc: 0.913, val_loss: 1.045, val_acc: 0.832

================================================================================2025-08_11 14:16:58
******** [step = 50] loss: 0.327, acc: 0.922
******** [step = 100] loss: 0.324, acc: 0.923
******** [step = 150] loss: 0.328, acc: 0.922
******** [step = 200] loss: 0.330, acc: 0.922
******** [step = 250] loss: 0.334, acc: 0.921
******** [step = 300] loss: 0.336, acc: 0.920
******** [step = 350] loss: 0.339, acc: 0.920
******** [step = 400] loss: 0.344, acc: 0.919
******** [step = 450] loss: 0.346, acc: 0.918
******** [step = 500] loss: 0.350, acc: 0.918
******** [step = 550] loss: 0.354, acc: 0.917
******** [step = 600] loss: 0.357, acc: 0.916
******** [step = 650] loss: 0.361, acc: 0.916
******** [step = 700] loss: 0.365, acc: 0.915
******** [step = 750] loss: 0.369, acc: 0.915
******** [step = 800] loss: 0.372, acc: 0.915
******** [step = 850] loss: 0.375, acc: 0.914
EPOCH = 65 loss: 0.375, acc: 0.914, val_loss: 1.051, val_acc: 0.832

================================================================================2025-08_11 14:18:18
******** [step = 50] loss: 0.313, acc: 0.924
******** [step = 100] loss: 0.317, acc: 0.924
******** [step = 150] loss: 0.320, acc: 0.923
******** [step = 200] loss: 0.326, acc: 0.922
******** [step = 250] loss: 0.328, acc: 0.921
******** [step = 300] loss: 0.332, acc: 0.920
******** [step = 350] loss: 0.337, acc: 0.920
******** [step = 400] loss: 0.341, acc: 0.919
******** [step = 450] loss: 0.344, acc: 0.919
******** [step = 500] loss: 0.348, acc: 0.918
******** [step = 550] loss: 0.352, acc: 0.918
******** [step = 600] loss: 0.356, acc: 0.917
******** [step = 650] loss: 0.359, acc: 0.916
******** [step = 700] loss: 0.363, acc: 0.916
******** [step = 750] loss: 0.366, acc: 0.915
******** [step = 800] loss: 0.369, acc: 0.915
******** [step = 850] loss: 0.372, acc: 0.915
EPOCH = 66 loss: 0.372, acc: 0.915, val_loss: 1.053, val_acc: 0.833

================================================================================2025-08_11 14:19:38
******** [step = 50] loss: 0.317, acc: 0.925
******** [step = 100] loss: 0.318, acc: 0.924
******** [step = 150] loss: 0.318, acc: 0.924
******** [step = 200] loss: 0.323, acc: 0.923
******** [step = 250] loss: 0.327, acc: 0.922
******** [step = 300] loss: 0.331, acc: 0.922
******** [step = 350] loss: 0.335, acc: 0.921
******** [step = 400] loss: 0.338, acc: 0.920
******** [step = 450] loss: 0.340, acc: 0.920
******** [step = 500] loss: 0.344, acc: 0.919
******** [step = 550] loss: 0.349, acc: 0.918
******** [step = 600] loss: 0.352, acc: 0.918
******** [step = 650] loss: 0.356, acc: 0.917
******** [step = 700] loss: 0.360, acc: 0.917
******** [step = 750] loss: 0.363, acc: 0.916
******** [step = 800] loss: 0.367, acc: 0.916
******** [step = 850] loss: 0.370, acc: 0.915
EPOCH = 67 loss: 0.370, acc: 0.915, val_loss: 1.052, val_acc: 0.832

================================================================================2025-08_11 14:21:01
******** [step = 50] loss: 0.302, acc: 0.926
******** [step = 100] loss: 0.305, acc: 0.926
******** [step = 150] loss: 0.311, acc: 0.925
******** [step = 200] loss: 0.317, acc: 0.924
******** [step = 250] loss: 0.322, acc: 0.923
******** [step = 300] loss: 0.324, acc: 0.923
******** [step = 350] loss: 0.328, acc: 0.922
******** [step = 400] loss: 0.332, acc: 0.921
******** [step = 450] loss: 0.336, acc: 0.921
******** [step = 500] loss: 0.340, acc: 0.920
******** [step = 550] loss: 0.344, acc: 0.920
******** [step = 600] loss: 0.349, acc: 0.919
******** [step = 650] loss: 0.353, acc: 0.918
******** [step = 700] loss: 0.357, acc: 0.917
******** [step = 750] loss: 0.361, acc: 0.917
******** [step = 800] loss: 0.365, acc: 0.917
******** [step = 850] loss: 0.367, acc: 0.916
EPOCH = 68 loss: 0.367, acc: 0.916, val_loss: 1.057, val_acc: 0.832

================================================================================2025-08_11 14:22:28
******** [step = 50] loss: 0.304, acc: 0.927
******** [step = 100] loss: 0.306, acc: 0.927
******** [step = 150] loss: 0.314, acc: 0.926
******** [step = 200] loss: 0.320, acc: 0.924
******** [step = 250] loss: 0.324, acc: 0.923
******** [step = 300] loss: 0.326, acc: 0.923
******** [step = 350] loss: 0.329, acc: 0.922
******** [step = 400] loss: 0.333, acc: 0.921
******** [step = 450] loss: 0.337, acc: 0.921
******** [step = 500] loss: 0.341, acc: 0.920
******** [step = 550] loss: 0.345, acc: 0.919
******** [step = 600] loss: 0.348, acc: 0.919
******** [step = 650] loss: 0.352, acc: 0.919
******** [step = 700] loss: 0.355, acc: 0.918
******** [step = 750] loss: 0.357, acc: 0.917
******** [step = 800] loss: 0.360, acc: 0.917
******** [step = 850] loss: 0.363, acc: 0.917
EPOCH = 69 loss: 0.363, acc: 0.917, val_loss: 1.055, val_acc: 0.831

================================================================================2025-08_11 14:23:47
******** [step = 50] loss: 0.310, acc: 0.925
******** [step = 100] loss: 0.308, acc: 0.926
******** [step = 150] loss: 0.307, acc: 0.926
******** [step = 200] loss: 0.310, acc: 0.925
******** [step = 250] loss: 0.314, acc: 0.924
******** [step = 300] loss: 0.319, acc: 0.924
******** [step = 350] loss: 0.322, acc: 0.923
******** [step = 400] loss: 0.327, acc: 0.922
******** [step = 450] loss: 0.331, acc: 0.922
******** [step = 500] loss: 0.335, acc: 0.921
******** [step = 550] loss: 0.338, acc: 0.921
******** [step = 600] loss: 0.342, acc: 0.920
******** [step = 650] loss: 0.346, acc: 0.919
******** [step = 700] loss: 0.351, acc: 0.919
******** [step = 750] loss: 0.355, acc: 0.918
******** [step = 800] loss: 0.358, acc: 0.918
******** [step = 850] loss: 0.361, acc: 0.917
EPOCH = 70 loss: 0.361, acc: 0.917, val_loss: 1.058, val_acc: 0.833

================================================================================2025-08_11 14:25:07
******** [step = 50] loss: 0.307, acc: 0.928
******** [step = 100] loss: 0.301, acc: 0.928
******** [step = 150] loss: 0.302, acc: 0.928
******** [step = 200] loss: 0.306, acc: 0.926
******** [step = 250] loss: 0.310, acc: 0.926
******** [step = 300] loss: 0.315, acc: 0.925
******** [step = 350] loss: 0.321, acc: 0.924
******** [step = 400] loss: 0.325, acc: 0.923
******** [step = 450] loss: 0.329, acc: 0.922
******** [step = 500] loss: 0.333, acc: 0.921
******** [step = 550] loss: 0.336, acc: 0.921
******** [step = 600] loss: 0.341, acc: 0.920
******** [step = 650] loss: 0.344, acc: 0.920
******** [step = 700] loss: 0.347, acc: 0.919
******** [step = 750] loss: 0.351, acc: 0.919
******** [step = 800] loss: 0.355, acc: 0.918
******** [step = 850] loss: 0.359, acc: 0.918
EPOCH = 71 loss: 0.359, acc: 0.918, val_loss: 1.054, val_acc: 0.833

================================================================================2025-08_11 14:26:29
******** [step = 50] loss: 0.308, acc: 0.926
******** [step = 100] loss: 0.303, acc: 0.927
******** [step = 150] loss: 0.306, acc: 0.927
******** [step = 200] loss: 0.312, acc: 0.925
******** [step = 250] loss: 0.315, acc: 0.925
******** [step = 300] loss: 0.317, acc: 0.924
******** [step = 350] loss: 0.320, acc: 0.924
******** [step = 400] loss: 0.325, acc: 0.923
******** [step = 450] loss: 0.328, acc: 0.923
******** [step = 500] loss: 0.331, acc: 0.922
******** [step = 550] loss: 0.336, acc: 0.921
******** [step = 600] loss: 0.339, acc: 0.921
******** [step = 650] loss: 0.343, acc: 0.920
******** [step = 700] loss: 0.346, acc: 0.920
******** [step = 750] loss: 0.349, acc: 0.920
******** [step = 800] loss: 0.353, acc: 0.919
******** [step = 850] loss: 0.355, acc: 0.919
EPOCH = 72 loss: 0.355, acc: 0.919, val_loss: 1.059, val_acc: 0.832

================================================================================2025-08_11 14:27:55
******** [step = 50] loss: 0.297, acc: 0.930
******** [step = 100] loss: 0.298, acc: 0.929
******** [step = 150] loss: 0.300, acc: 0.928
******** [step = 200] loss: 0.306, acc: 0.927
******** [step = 250] loss: 0.310, acc: 0.927
******** [step = 300] loss: 0.314, acc: 0.926
******** [step = 350] loss: 0.316, acc: 0.925
******** [step = 400] loss: 0.322, acc: 0.924
******** [step = 450] loss: 0.326, acc: 0.923
******** [step = 500] loss: 0.329, acc: 0.923
******** [step = 550] loss: 0.333, acc: 0.922
******** [step = 600] loss: 0.338, acc: 0.921
******** [step = 650] loss: 0.341, acc: 0.921
******** [step = 700] loss: 0.345, acc: 0.920
******** [step = 750] loss: 0.348, acc: 0.920
******** [step = 800] loss: 0.352, acc: 0.919
******** [step = 850] loss: 0.355, acc: 0.919
EPOCH = 73 loss: 0.355, acc: 0.919, val_loss: 1.057, val_acc: 0.833

================================================================================2025-08_11 14:29:17
******** [step = 50] loss: 0.290, acc: 0.930
******** [step = 100] loss: 0.292, acc: 0.930
******** [step = 150] loss: 0.296, acc: 0.929
******** [step = 200] loss: 0.301, acc: 0.928
******** [step = 250] loss: 0.306, acc: 0.927
******** [step = 300] loss: 0.311, acc: 0.926
******** [step = 350] loss: 0.315, acc: 0.925
******** [step = 400] loss: 0.318, acc: 0.925
******** [step = 450] loss: 0.322, acc: 0.924
******** [step = 500] loss: 0.325, acc: 0.924
******** [step = 550] loss: 0.330, acc: 0.923
******** [step = 600] loss: 0.334, acc: 0.922
******** [step = 650] loss: 0.337, acc: 0.922
******** [step = 700] loss: 0.341, acc: 0.921
******** [step = 750] loss: 0.345, acc: 0.921
******** [step = 800] loss: 0.348, acc: 0.920
******** [step = 850] loss: 0.351, acc: 0.920
EPOCH = 74 loss: 0.351, acc: 0.920, val_loss: 1.063, val_acc: 0.834

================================================================================2025-08_11 14:30:42
******** [step = 50] loss: 0.290, acc: 0.930
******** [step = 100] loss: 0.292, acc: 0.930
******** [step = 150] loss: 0.296, acc: 0.929
******** [step = 200] loss: 0.298, acc: 0.928
******** [step = 250] loss: 0.302, acc: 0.927
******** [step = 300] loss: 0.308, acc: 0.926
******** [step = 350] loss: 0.312, acc: 0.926
******** [step = 400] loss: 0.317, acc: 0.925
******** [step = 450] loss: 0.321, acc: 0.924
******** [step = 500] loss: 0.325, acc: 0.924
******** [step = 550] loss: 0.329, acc: 0.923
******** [step = 600] loss: 0.332, acc: 0.923
******** [step = 650] loss: 0.336, acc: 0.922
******** [step = 700] loss: 0.340, acc: 0.921
******** [step = 750] loss: 0.343, acc: 0.921
******** [step = 800] loss: 0.346, acc: 0.920
******** [step = 850] loss: 0.350, acc: 0.920
EPOCH = 75 loss: 0.350, acc: 0.920, val_loss: 1.062, val_acc: 0.834

================================================================================2025-08_11 14:32:12
******** [step = 50] loss: 0.293, acc: 0.929
******** [step = 100] loss: 0.292, acc: 0.930
******** [step = 150] loss: 0.294, acc: 0.930
******** [step = 200] loss: 0.297, acc: 0.929
******** [step = 250] loss: 0.301, acc: 0.929
******** [step = 300] loss: 0.305, acc: 0.928
******** [step = 350] loss: 0.309, acc: 0.927
******** [step = 400] loss: 0.314, acc: 0.926
******** [step = 450] loss: 0.318, acc: 0.925
******** [step = 500] loss: 0.322, acc: 0.925
******** [step = 550] loss: 0.326, acc: 0.924
******** [step = 600] loss: 0.330, acc: 0.923
******** [step = 650] loss: 0.333, acc: 0.923
******** [step = 700] loss: 0.336, acc: 0.922
******** [step = 750] loss: 0.340, acc: 0.922
******** [step = 800] loss: 0.344, acc: 0.921
******** [step = 850] loss: 0.347, acc: 0.921
EPOCH = 76 loss: 0.347, acc: 0.921, val_loss: 1.060, val_acc: 0.833

================================================================================2025-08_11 14:33:43
******** [step = 50] loss: 0.283, acc: 0.932
******** [step = 100] loss: 0.288, acc: 0.931
******** [step = 150] loss: 0.291, acc: 0.931
******** [step = 200] loss: 0.296, acc: 0.930
******** [step = 250] loss: 0.299, acc: 0.929
******** [step = 300] loss: 0.303, acc: 0.928
******** [step = 350] loss: 0.306, acc: 0.927
******** [step = 400] loss: 0.311, acc: 0.927
******** [step = 450] loss: 0.316, acc: 0.926
******** [step = 500] loss: 0.321, acc: 0.925
******** [step = 550] loss: 0.325, acc: 0.924
******** [step = 600] loss: 0.328, acc: 0.924
******** [step = 650] loss: 0.332, acc: 0.923
******** [step = 700] loss: 0.335, acc: 0.923
******** [step = 750] loss: 0.339, acc: 0.922
******** [step = 800] loss: 0.342, acc: 0.922
******** [step = 850] loss: 0.346, acc: 0.921
EPOCH = 77 loss: 0.346, acc: 0.921, val_loss: 1.060, val_acc: 0.834

================================================================================2025-08_11 14:35:13
******** [step = 50] loss: 0.282, acc: 0.932
******** [step = 100] loss: 0.289, acc: 0.930
******** [step = 150] loss: 0.292, acc: 0.929
******** [step = 200] loss: 0.295, acc: 0.929
******** [step = 250] loss: 0.300, acc: 0.928
******** [step = 300] loss: 0.304, acc: 0.928
******** [step = 350] loss: 0.305, acc: 0.927
******** [step = 400] loss: 0.311, acc: 0.926
******** [step = 450] loss: 0.314, acc: 0.926
******** [step = 500] loss: 0.318, acc: 0.925
******** [step = 550] loss: 0.321, acc: 0.925
******** [step = 600] loss: 0.325, acc: 0.924
******** [step = 650] loss: 0.329, acc: 0.924
******** [step = 700] loss: 0.333, acc: 0.923
******** [step = 750] loss: 0.337, acc: 0.922
******** [step = 800] loss: 0.340, acc: 0.922
******** [step = 850] loss: 0.343, acc: 0.922
EPOCH = 78 loss: 0.343, acc: 0.922, val_loss: 1.062, val_acc: 0.834

================================================================================2025-08_11 14:36:39
******** [step = 50] loss: 0.274, acc: 0.934
******** [step = 100] loss: 0.280, acc: 0.933
******** [step = 150] loss: 0.285, acc: 0.932
******** [step = 200] loss: 0.290, acc: 0.931
******** [step = 250] loss: 0.294, acc: 0.930
******** [step = 300] loss: 0.300, acc: 0.929
******** [step = 350] loss: 0.303, acc: 0.928
******** [step = 400] loss: 0.306, acc: 0.928
******** [step = 450] loss: 0.311, acc: 0.927
******** [step = 500] loss: 0.315, acc: 0.926
******** [step = 550] loss: 0.320, acc: 0.925
******** [step = 600] loss: 0.324, acc: 0.925
******** [step = 650] loss: 0.327, acc: 0.924
******** [step = 700] loss: 0.329, acc: 0.924
******** [step = 750] loss: 0.334, acc: 0.923
******** [step = 800] loss: 0.336, acc: 0.923
******** [step = 850] loss: 0.339, acc: 0.923
EPOCH = 79 loss: 0.339, acc: 0.923, val_loss: 1.065, val_acc: 0.834

================================================================================2025-08_11 14:38:06
******** [step = 50] loss: 0.278, acc: 0.933
******** [step = 100] loss: 0.284, acc: 0.932
******** [step = 150] loss: 0.287, acc: 0.931
******** [step = 200] loss: 0.289, acc: 0.930
******** [step = 250] loss: 0.293, acc: 0.930
******** [step = 300] loss: 0.297, acc: 0.929
******** [step = 350] loss: 0.302, acc: 0.928
******** [step = 400] loss: 0.307, acc: 0.927
******** [step = 450] loss: 0.311, acc: 0.927
******** [step = 500] loss: 0.315, acc: 0.926
******** [step = 550] loss: 0.317, acc: 0.926
******** [step = 600] loss: 0.322, acc: 0.925
******** [step = 650] loss: 0.324, acc: 0.925
******** [step = 700] loss: 0.328, acc: 0.924
******** [step = 750] loss: 0.332, acc: 0.924
******** [step = 800] loss: 0.335, acc: 0.923
******** [step = 850] loss: 0.338, acc: 0.923
EPOCH = 80 loss: 0.338, acc: 0.923, val_loss: 1.064, val_acc: 0.834

================================================================================2025-08_11 14:39:27
finishing training...
Training complete in 109m 58s
    epoch  ...   val_acc
0     1.0  ...  0.478722
1     2.0  ...  0.583441
2     3.0  ...  0.649578
3     4.0  ...  0.689662
4     5.0  ...  0.710904
..    ...  ...       ...
75   76.0  ...  0.832863
76   77.0  ...  0.833775
77   78.0  ...  0.833532
78   79.0  ...  0.834481
79   80.0  ...  0.834175

[80 rows x 5 columns]
== Done ==
Mon Aug 11 02:40:00 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-11-2025 14:40:00
Job ID:        6870201
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=07:28:12,vmem=0,walltime=01:52:03,mem=35424K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-006-36-0
---------------------------------------
