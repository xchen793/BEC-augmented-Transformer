---------------------------------------
Begin Slurm Prolog: Aug-09-2025 23:26:22
Job ID:    6778324
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sat Aug  9 11:26:22 PM EDT 2025
atl1-1-02-007-31-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Sat Aug  9 23:26:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   45C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 107658.28it/s] 28%|██▊       | 38330/135842 [00:00<00:00, 158902.33it/s] 41%|████      | 55409/135842 [00:00<00:00, 120827.21it/s] 51%|█████     | 69378/135842 [00:00<00:00, 94614.40it/s]  66%|██████▌   | 89271/135842 [00:00<00:00, 119904.10it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90628.22it/s] 92%|█████████▏| 125249/135842 [00:01<00:00, 108534.53it/s]100%|██████████| 135842/135842 [00:01<00:00, 111950.80it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47198.90it/s] 33%|███▎      | 35414/108673 [00:00<00:00, 84347.44it/s] 49%|████▉     | 53338/108673 [00:00<00:00, 111412.59it/s] 66%|██████▌   | 71496/108673 [00:00<00:00, 131723.42it/s] 81%|████████  | 87742/108673 [00:01<00:00, 70362.34it/s]  97%|█████████▋| 105754/108673 [00:01<00:00, 89334.68it/s]100%|██████████| 108673/108673 [00:01<00:00, 88631.39it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17853/27169 [00:00<00:00, 178519.96it/s]100%|██████████| 27169/27169 [00:00<00:00, 180388.82it/s]
tensor([   3,   19,   23,  484,   22, 1040,  202,   48,   81,   35,   12,   21,
           6,   83,  264,   40,   22,  749,    7,  191,   92,  187,    4,    2,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,    7,  155,   29,  228,   13,   27,    6,  231,   16,  804,  847,
         482,   12,   81,   13,   14, 6343,    6, 4325,   60,   14,   43, 4041,
           4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,   17,   44,    8,  229,   19,   50,  121,  732,   14,  466,   15,
        8092,   27,    8,  352,  326,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_09 23:26:54
******** [step = 50] loss: 9.361, acc: 0.022
******** [step = 100] loss: 8.973, acc: 0.086
******** [step = 150] loss: 8.615, acc: 0.135
******** [step = 200] loss: 8.283, acc: 0.163
******** [step = 250] loss: 7.969, acc: 0.180
******** [step = 300] loss: 7.661, acc: 0.193
******** [step = 350] loss: 7.364, acc: 0.205
******** [step = 400] loss: 7.085, acc: 0.215
******** [step = 450] loss: 6.837, acc: 0.224
******** [step = 500] loss: 6.619, acc: 0.232
******** [step = 550] loss: 6.429, acc: 0.240
******** [step = 600] loss: 6.261, acc: 0.248
******** [step = 650] loss: 6.111, acc: 0.254
******** [step = 700] loss: 5.977, acc: 0.260
******** [step = 750] loss: 5.856, acc: 0.266
******** [step = 800] loss: 5.746, acc: 0.271
******** [step = 850] loss: 5.648, acc: 0.275
EPOCH = 1 loss: 5.648, acc: 0.275, val_loss: 3.916, val_acc: 0.367

================================================================================2025-08_09 23:28:14
******** [step = 50] loss: 3.997, acc: 0.349
******** [step = 100] loss: 3.975, acc: 0.351
******** [step = 150] loss: 3.951, acc: 0.352
******** [step = 200] loss: 3.930, acc: 0.354
******** [step = 250] loss: 3.912, acc: 0.355
******** [step = 300] loss: 3.896, acc: 0.356
******** [step = 350] loss: 3.879, acc: 0.358
******** [step = 400] loss: 3.862, acc: 0.359
******** [step = 450] loss: 3.849, acc: 0.360
******** [step = 500] loss: 3.837, acc: 0.361
******** [step = 550] loss: 3.824, acc: 0.361
******** [step = 600] loss: 3.814, acc: 0.362
******** [step = 650] loss: 3.802, acc: 0.363
******** [step = 700] loss: 3.791, acc: 0.364
******** [step = 750] loss: 3.781, acc: 0.364
******** [step = 800] loss: 3.771, acc: 0.365
******** [step = 850] loss: 3.762, acc: 0.366
EPOCH = 2 loss: 3.762, acc: 0.366, val_loss: 3.508, val_acc: 0.392

================================================================================2025-08_09 23:29:34
******** [step = 50] loss: 3.577, acc: 0.374
******** [step = 100] loss: 3.548, acc: 0.379
******** [step = 150] loss: 3.545, acc: 0.379
******** [step = 200] loss: 3.534, acc: 0.380
******** [step = 250] loss: 3.531, acc: 0.380
******** [step = 300] loss: 3.523, acc: 0.381
******** [step = 350] loss: 3.521, acc: 0.381
******** [step = 400] loss: 3.514, acc: 0.382
******** [step = 450] loss: 3.509, acc: 0.382
******** [step = 500] loss: 3.505, acc: 0.382
******** [step = 550] loss: 3.501, acc: 0.383
******** [step = 600] loss: 3.496, acc: 0.383
******** [step = 650] loss: 3.492, acc: 0.384
******** [step = 700] loss: 3.487, acc: 0.384
******** [step = 750] loss: 3.483, acc: 0.385
******** [step = 800] loss: 3.479, acc: 0.385
******** [step = 850] loss: 3.476, acc: 0.385
EPOCH = 3 loss: 3.476, acc: 0.385, val_loss: 3.341, val_acc: 0.405

================================================================================2025-08_09 23:30:54
******** [step = 50] loss: 3.354, acc: 0.391
******** [step = 100] loss: 3.352, acc: 0.393
******** [step = 150] loss: 3.343, acc: 0.394
******** [step = 200] loss: 3.335, acc: 0.395
******** [step = 250] loss: 3.332, acc: 0.395
******** [step = 300] loss: 3.334, acc: 0.395
******** [step = 350] loss: 3.330, acc: 0.396
******** [step = 400] loss: 3.328, acc: 0.396
******** [step = 450] loss: 3.325, acc: 0.397
******** [step = 500] loss: 3.325, acc: 0.397
******** [step = 550] loss: 3.324, acc: 0.397
******** [step = 600] loss: 3.323, acc: 0.397
******** [step = 650] loss: 3.320, acc: 0.398
******** [step = 700] loss: 3.318, acc: 0.398
******** [step = 750] loss: 3.316, acc: 0.398
******** [step = 800] loss: 3.313, acc: 0.398
******** [step = 850] loss: 3.311, acc: 0.399
EPOCH = 4 loss: 3.311, acc: 0.399, val_loss: 3.223, val_acc: 0.416

================================================================================2025-08_09 23:32:13
******** [step = 50] loss: 3.210, acc: 0.406
******** [step = 100] loss: 3.191, acc: 0.408
******** [step = 150] loss: 3.193, acc: 0.407
******** [step = 200] loss: 3.194, acc: 0.407
******** [step = 250] loss: 3.198, acc: 0.407
******** [step = 300] loss: 3.197, acc: 0.407
******** [step = 350] loss: 3.200, acc: 0.407
******** [step = 400] loss: 3.200, acc: 0.407
******** [step = 450] loss: 3.199, acc: 0.407
******** [step = 500] loss: 3.202, acc: 0.407
******** [step = 550] loss: 3.203, acc: 0.407
******** [step = 600] loss: 3.201, acc: 0.408
******** [step = 650] loss: 3.202, acc: 0.408
******** [step = 700] loss: 3.202, acc: 0.408
******** [step = 750] loss: 3.202, acc: 0.408
******** [step = 800] loss: 3.200, acc: 0.408
******** [step = 850] loss: 3.200, acc: 0.408
EPOCH = 5 loss: 3.200, acc: 0.408, val_loss: 3.140, val_acc: 0.423

================================================================================2025-08_09 23:33:32
******** [step = 50] loss: 3.103, acc: 0.414
******** [step = 100] loss: 3.093, acc: 0.415
******** [step = 150] loss: 3.091, acc: 0.416
******** [step = 200] loss: 3.091, acc: 0.416
******** [step = 250] loss: 3.091, acc: 0.416
******** [step = 300] loss: 3.090, acc: 0.417
******** [step = 350] loss: 3.088, acc: 0.417
******** [step = 400] loss: 3.089, acc: 0.417
******** [step = 450] loss: 3.090, acc: 0.417
******** [step = 500] loss: 3.089, acc: 0.417
******** [step = 550] loss: 3.091, acc: 0.417
******** [step = 600] loss: 3.089, acc: 0.418
******** [step = 650] loss: 3.091, acc: 0.418
******** [step = 700] loss: 3.093, acc: 0.418
******** [step = 750] loss: 3.093, acc: 0.418
******** [step = 800] loss: 3.093, acc: 0.418
******** [step = 850] loss: 3.092, acc: 0.418
EPOCH = 6 loss: 3.092, acc: 0.418, val_loss: 3.064, val_acc: 0.432

================================================================================2025-08_09 23:34:51
******** [step = 50] loss: 2.973, acc: 0.426
******** [step = 100] loss: 2.979, acc: 0.427
******** [step = 150] loss: 2.968, acc: 0.428
******** [step = 200] loss: 2.970, acc: 0.428
******** [step = 250] loss: 2.975, acc: 0.428
******** [step = 300] loss: 2.977, acc: 0.428
******** [step = 350] loss: 2.982, acc: 0.428
******** [step = 400] loss: 2.984, acc: 0.427
******** [step = 450] loss: 2.985, acc: 0.427
******** [step = 500] loss: 2.986, acc: 0.428
******** [step = 550] loss: 2.989, acc: 0.428
******** [step = 600] loss: 2.989, acc: 0.428
******** [step = 650] loss: 2.990, acc: 0.428
******** [step = 700] loss: 2.990, acc: 0.428
******** [step = 750] loss: 2.991, acc: 0.428
******** [step = 800] loss: 2.992, acc: 0.428
******** [step = 850] loss: 2.991, acc: 0.428
EPOCH = 7 loss: 2.991, acc: 0.428, val_loss: 3.008, val_acc: 0.441

================================================================================2025-08_09 23:36:11
******** [step = 50] loss: 2.874, acc: 0.438
******** [step = 100] loss: 2.865, acc: 0.439
******** [step = 150] loss: 2.873, acc: 0.438
******** [step = 200] loss: 2.879, acc: 0.438
******** [step = 250] loss: 2.885, acc: 0.438
******** [step = 300] loss: 2.891, acc: 0.438
******** [step = 350] loss: 2.892, acc: 0.438
******** [step = 400] loss: 2.898, acc: 0.437
******** [step = 450] loss: 2.902, acc: 0.437
******** [step = 500] loss: 2.905, acc: 0.437
******** [step = 550] loss: 2.907, acc: 0.437
******** [step = 600] loss: 2.909, acc: 0.437
******** [step = 650] loss: 2.909, acc: 0.437
******** [step = 700] loss: 2.909, acc: 0.438
******** [step = 750] loss: 2.911, acc: 0.438
******** [step = 800] loss: 2.912, acc: 0.438
******** [step = 850] loss: 2.912, acc: 0.438
EPOCH = 8 loss: 2.912, acc: 0.438, val_loss: 2.957, val_acc: 0.446

================================================================================2025-08_09 23:37:30
******** [step = 50] loss: 2.789, acc: 0.450
******** [step = 100] loss: 2.802, acc: 0.448
******** [step = 150] loss: 2.813, acc: 0.446
******** [step = 200] loss: 2.817, acc: 0.446
******** [step = 250] loss: 2.819, acc: 0.447
******** [step = 300] loss: 2.825, acc: 0.446
******** [step = 350] loss: 2.827, acc: 0.446
******** [step = 400] loss: 2.830, acc: 0.446
******** [step = 450] loss: 2.832, acc: 0.446
******** [step = 500] loss: 2.836, acc: 0.446
******** [step = 550] loss: 2.836, acc: 0.446
******** [step = 600] loss: 2.839, acc: 0.445
******** [step = 650] loss: 2.843, acc: 0.445
******** [step = 700] loss: 2.846, acc: 0.445
******** [step = 750] loss: 2.848, acc: 0.445
******** [step = 800] loss: 2.850, acc: 0.445
******** [step = 850] loss: 2.852, acc: 0.445
EPOCH = 9 loss: 2.852, acc: 0.445, val_loss: 2.949, val_acc: 0.445

================================================================================2025-08_09 23:38:50
******** [step = 50] loss: 2.745, acc: 0.452
******** [step = 100] loss: 2.744, acc: 0.453
******** [step = 150] loss: 2.757, acc: 0.453
******** [step = 200] loss: 2.757, acc: 0.452
******** [step = 250] loss: 2.762, acc: 0.452
******** [step = 300] loss: 2.769, acc: 0.452
******** [step = 350] loss: 2.774, acc: 0.452
******** [step = 400] loss: 2.777, acc: 0.451
******** [step = 450] loss: 2.781, acc: 0.451
******** [step = 500] loss: 2.784, acc: 0.451
******** [step = 550] loss: 2.788, acc: 0.451
******** [step = 600] loss: 2.789, acc: 0.451
******** [step = 650] loss: 2.793, acc: 0.450
******** [step = 700] loss: 2.795, acc: 0.450
******** [step = 750] loss: 2.795, acc: 0.450
******** [step = 800] loss: 2.798, acc: 0.450
******** [step = 850] loss: 2.800, acc: 0.450
EPOCH = 10 loss: 2.800, acc: 0.450, val_loss: 2.903, val_acc: 0.455

================================================================================2025-08_09 23:40:09
******** [step = 50] loss: 2.691, acc: 0.460
******** [step = 100] loss: 2.704, acc: 0.459
******** [step = 150] loss: 2.710, acc: 0.458
******** [step = 200] loss: 2.709, acc: 0.458
******** [step = 250] loss: 2.713, acc: 0.458
******** [step = 300] loss: 2.718, acc: 0.458
******** [step = 350] loss: 2.723, acc: 0.458
******** [step = 400] loss: 2.727, acc: 0.457
******** [step = 450] loss: 2.733, acc: 0.457
******** [step = 500] loss: 2.735, acc: 0.457
******** [step = 550] loss: 2.739, acc: 0.456
******** [step = 600] loss: 2.742, acc: 0.456
******** [step = 650] loss: 2.746, acc: 0.456
******** [step = 700] loss: 2.746, acc: 0.456
******** [step = 750] loss: 2.749, acc: 0.456
******** [step = 800] loss: 2.752, acc: 0.456
******** [step = 850] loss: 2.755, acc: 0.456
EPOCH = 11 loss: 2.755, acc: 0.456, val_loss: 2.880, val_acc: 0.459

================================================================================2025-08_09 23:41:28
******** [step = 50] loss: 2.664, acc: 0.463
******** [step = 100] loss: 2.660, acc: 0.464
******** [step = 150] loss: 2.665, acc: 0.463
******** [step = 200] loss: 2.671, acc: 0.463
******** [step = 250] loss: 2.678, acc: 0.463
******** [step = 300] loss: 2.685, acc: 0.462
******** [step = 350] loss: 2.688, acc: 0.461
******** [step = 400] loss: 2.692, acc: 0.461
******** [step = 450] loss: 2.696, acc: 0.461
******** [step = 500] loss: 2.700, acc: 0.461
******** [step = 550] loss: 2.704, acc: 0.460
******** [step = 600] loss: 2.708, acc: 0.460
******** [step = 650] loss: 2.710, acc: 0.460
******** [step = 700] loss: 2.713, acc: 0.460
******** [step = 750] loss: 2.715, acc: 0.460
******** [step = 800] loss: 2.716, acc: 0.460
******** [step = 850] loss: 2.718, acc: 0.460
EPOCH = 12 loss: 2.718, acc: 0.460, val_loss: 2.872, val_acc: 0.461

================================================================================2025-08_09 23:42:47
******** [step = 50] loss: 2.626, acc: 0.469
******** [step = 100] loss: 2.633, acc: 0.469
******** [step = 150] loss: 2.630, acc: 0.470
******** [step = 200] loss: 2.634, acc: 0.469
******** [step = 250] loss: 2.646, acc: 0.468
******** [step = 300] loss: 2.649, acc: 0.468
******** [step = 350] loss: 2.655, acc: 0.467
******** [step = 400] loss: 2.658, acc: 0.467
******** [step = 450] loss: 2.660, acc: 0.467
******** [step = 500] loss: 2.664, acc: 0.467
******** [step = 550] loss: 2.667, acc: 0.467
******** [step = 600] loss: 2.670, acc: 0.467
******** [step = 650] loss: 2.675, acc: 0.466
******** [step = 700] loss: 2.677, acc: 0.466
******** [step = 750] loss: 2.679, acc: 0.466
******** [step = 800] loss: 2.682, acc: 0.465
******** [step = 850] loss: 2.684, acc: 0.465
EPOCH = 13 loss: 2.684, acc: 0.465, val_loss: 2.850, val_acc: 0.464

================================================================================2025-08_09 23:44:06
******** [step = 50] loss: 2.586, acc: 0.473
******** [step = 100] loss: 2.581, acc: 0.474
******** [step = 150] loss: 2.588, acc: 0.475
******** [step = 200] loss: 2.598, acc: 0.473
******** [step = 250] loss: 2.601, acc: 0.473
******** [step = 300] loss: 2.612, acc: 0.472
******** [step = 350] loss: 2.619, acc: 0.472
******** [step = 400] loss: 2.623, acc: 0.471
******** [step = 450] loss: 2.625, acc: 0.471
******** [step = 500] loss: 2.630, acc: 0.471
******** [step = 550] loss: 2.634, acc: 0.471
******** [step = 600] loss: 2.639, acc: 0.470
******** [step = 650] loss: 2.642, acc: 0.470
******** [step = 700] loss: 2.645, acc: 0.470
******** [step = 750] loss: 2.648, acc: 0.470
******** [step = 800] loss: 2.652, acc: 0.470
******** [step = 850] loss: 2.654, acc: 0.469
EPOCH = 14 loss: 2.654, acc: 0.469, val_loss: 2.841, val_acc: 0.468

================================================================================2025-08_09 23:45:26
******** [step = 50] loss: 2.554, acc: 0.480
******** [step = 100] loss: 2.559, acc: 0.479
******** [step = 150] loss: 2.570, acc: 0.478
******** [step = 200] loss: 2.574, acc: 0.478
******** [step = 250] loss: 2.585, acc: 0.476
******** [step = 300] loss: 2.592, acc: 0.476
******** [step = 350] loss: 2.595, acc: 0.476
******** [step = 400] loss: 2.603, acc: 0.475
******** [step = 450] loss: 2.604, acc: 0.475
******** [step = 500] loss: 2.609, acc: 0.475
******** [step = 550] loss: 2.614, acc: 0.474
******** [step = 600] loss: 2.616, acc: 0.474
******** [step = 650] loss: 2.620, acc: 0.474
******** [step = 700] loss: 2.623, acc: 0.474
******** [step = 750] loss: 2.624, acc: 0.473
******** [step = 800] loss: 2.627, acc: 0.473
******** [step = 850] loss: 2.629, acc: 0.473
EPOCH = 15 loss: 2.629, acc: 0.473, val_loss: 2.830, val_acc: 0.470

================================================================================2025-08_09 23:46:45
******** [step = 50] loss: 2.545, acc: 0.477
******** [step = 100] loss: 2.542, acc: 0.480
******** [step = 150] loss: 2.549, acc: 0.479
******** [step = 200] loss: 2.550, acc: 0.479
******** [step = 250] loss: 2.557, acc: 0.479
******** [step = 300] loss: 2.567, acc: 0.478
******** [step = 350] loss: 2.569, acc: 0.479
******** [step = 400] loss: 2.568, acc: 0.479
******** [step = 450] loss: 2.569, acc: 0.479
******** [step = 500] loss: 2.575, acc: 0.478
******** [step = 550] loss: 2.580, acc: 0.478
******** [step = 600] loss: 2.585, acc: 0.477
******** [step = 650] loss: 2.587, acc: 0.477
******** [step = 700] loss: 2.590, acc: 0.477
******** [step = 750] loss: 2.594, acc: 0.477
******** [step = 800] loss: 2.598, acc: 0.476
******** [step = 850] loss: 2.601, acc: 0.476
EPOCH = 16 loss: 2.601, acc: 0.476, val_loss: 2.828, val_acc: 0.472

================================================================================2025-08_09 23:48:04
******** [step = 50] loss: 2.520, acc: 0.486
******** [step = 100] loss: 2.510, acc: 0.486
******** [step = 150] loss: 2.517, acc: 0.485
******** [step = 200] loss: 2.526, acc: 0.483
******** [step = 250] loss: 2.531, acc: 0.483
******** [step = 300] loss: 2.537, acc: 0.483
******** [step = 350] loss: 2.543, acc: 0.483
******** [step = 400] loss: 2.547, acc: 0.482
******** [step = 450] loss: 2.550, acc: 0.482
******** [step = 500] loss: 2.555, acc: 0.482
******** [step = 550] loss: 2.560, acc: 0.481
******** [step = 600] loss: 2.565, acc: 0.481
******** [step = 650] loss: 2.566, acc: 0.481
******** [step = 700] loss: 2.569, acc: 0.481
******** [step = 750] loss: 2.572, acc: 0.480
******** [step = 800] loss: 2.576, acc: 0.480
******** [step = 850] loss: 2.578, acc: 0.480
EPOCH = 17 loss: 2.578, acc: 0.480, val_loss: 2.808, val_acc: 0.474

================================================================================2025-08_09 23:49:23
******** [step = 50] loss: 2.499, acc: 0.488
******** [step = 100] loss: 2.499, acc: 0.489
******** [step = 150] loss: 2.505, acc: 0.488
******** [step = 200] loss: 2.509, acc: 0.487
******** [step = 250] loss: 2.518, acc: 0.486
******** [step = 300] loss: 2.523, acc: 0.486
******** [step = 350] loss: 2.529, acc: 0.485
******** [step = 400] loss: 2.531, acc: 0.485
******** [step = 450] loss: 2.535, acc: 0.485
******** [step = 500] loss: 2.540, acc: 0.484
******** [step = 550] loss: 2.542, acc: 0.485
******** [step = 600] loss: 2.544, acc: 0.484
******** [step = 650] loss: 2.546, acc: 0.484
******** [step = 700] loss: 2.549, acc: 0.484
******** [step = 750] loss: 2.551, acc: 0.484
******** [step = 800] loss: 2.555, acc: 0.483
******** [step = 850] loss: 2.556, acc: 0.483
EPOCH = 18 loss: 2.556, acc: 0.483, val_loss: 2.803, val_acc: 0.475

================================================================================2025-08_09 23:50:43
******** [step = 50] loss: 2.480, acc: 0.487
******** [step = 100] loss: 2.491, acc: 0.489
******** [step = 150] loss: 2.491, acc: 0.490
******** [step = 200] loss: 2.491, acc: 0.490
******** [step = 250] loss: 2.494, acc: 0.489
******** [step = 300] loss: 2.499, acc: 0.489
******** [step = 350] loss: 2.501, acc: 0.488
******** [step = 400] loss: 2.504, acc: 0.488
******** [step = 450] loss: 2.508, acc: 0.488
******** [step = 500] loss: 2.514, acc: 0.487
******** [step = 550] loss: 2.517, acc: 0.487
******** [step = 600] loss: 2.522, acc: 0.486
******** [step = 650] loss: 2.525, acc: 0.486
******** [step = 700] loss: 2.528, acc: 0.486
******** [step = 750] loss: 2.531, acc: 0.486
******** [step = 800] loss: 2.536, acc: 0.485
******** [step = 850] loss: 2.540, acc: 0.485
EPOCH = 19 loss: 2.540, acc: 0.485, val_loss: 2.793, val_acc: 0.478

================================================================================2025-08_09 23:52:02
******** [step = 50] loss: 2.437, acc: 0.495
******** [step = 100] loss: 2.458, acc: 0.493
******** [step = 150] loss: 2.465, acc: 0.493
******** [step = 200] loss: 2.470, acc: 0.493
******** [step = 250] loss: 2.475, acc: 0.492
******** [step = 300] loss: 2.480, acc: 0.491
******** [step = 350] loss: 2.484, acc: 0.490
******** [step = 400] loss: 2.489, acc: 0.490
******** [step = 450] loss: 2.492, acc: 0.489
******** [step = 500] loss: 2.496, acc: 0.490
******** [step = 550] loss: 2.500, acc: 0.489
******** [step = 600] loss: 2.504, acc: 0.489
******** [step = 650] loss: 2.506, acc: 0.489
******** [step = 700] loss: 2.509, acc: 0.488
******** [step = 750] loss: 2.512, acc: 0.488
******** [step = 800] loss: 2.515, acc: 0.488
******** [step = 850] loss: 2.519, acc: 0.488
EPOCH = 20 loss: 2.519, acc: 0.488, val_loss: 2.786, val_acc: 0.480

================================================================================2025-08_09 23:53:21
******** [step = 50] loss: 2.430, acc: 0.500
******** [step = 100] loss: 2.437, acc: 0.498
******** [step = 150] loss: 2.441, acc: 0.496
******** [step = 200] loss: 2.448, acc: 0.495
******** [step = 250] loss: 2.456, acc: 0.494
******** [step = 300] loss: 2.459, acc: 0.493
******** [step = 350] loss: 2.464, acc: 0.493
******** [step = 400] loss: 2.468, acc: 0.493
******** [step = 450] loss: 2.472, acc: 0.492
******** [step = 500] loss: 2.476, acc: 0.492
******** [step = 550] loss: 2.482, acc: 0.491
******** [step = 600] loss: 2.484, acc: 0.491
******** [step = 650] loss: 2.490, acc: 0.491
******** [step = 700] loss: 2.491, acc: 0.491
******** [step = 750] loss: 2.494, acc: 0.491
******** [step = 800] loss: 2.500, acc: 0.490
******** [step = 850] loss: 2.503, acc: 0.490
EPOCH = 21 loss: 2.503, acc: 0.490, val_loss: 2.783, val_acc: 0.480

================================================================================2025-08_09 23:54:40
******** [step = 50] loss: 2.452, acc: 0.495
******** [step = 100] loss: 2.435, acc: 0.496
******** [step = 150] loss: 2.440, acc: 0.496
******** [step = 200] loss: 2.443, acc: 0.496
******** [step = 250] loss: 2.443, acc: 0.496
******** [step = 300] loss: 2.447, acc: 0.496
******** [step = 350] loss: 2.449, acc: 0.496
******** [step = 400] loss: 2.453, acc: 0.495
******** [step = 450] loss: 2.457, acc: 0.495
******** [step = 500] loss: 2.460, acc: 0.495
******** [step = 550] loss: 2.465, acc: 0.494
******** [step = 600] loss: 2.470, acc: 0.494
******** [step = 650] loss: 2.474, acc: 0.493
******** [step = 700] loss: 2.478, acc: 0.493
******** [step = 750] loss: 2.481, acc: 0.493
******** [step = 800] loss: 2.484, acc: 0.493
******** [step = 850] loss: 2.487, acc: 0.493
EPOCH = 22 loss: 2.487, acc: 0.493, val_loss: 2.779, val_acc: 0.482

================================================================================2025-08_09 23:56:00
******** [step = 50] loss: 2.410, acc: 0.498
******** [step = 100] loss: 2.404, acc: 0.499
******** [step = 150] loss: 2.403, acc: 0.500
******** [step = 200] loss: 2.412, acc: 0.499
******** [step = 250] loss: 2.415, acc: 0.499
******** [step = 300] loss: 2.421, acc: 0.499
******** [step = 350] loss: 2.425, acc: 0.499
******** [step = 400] loss: 2.431, acc: 0.498
******** [step = 450] loss: 2.436, acc: 0.498
******** [step = 500] loss: 2.441, acc: 0.497
******** [step = 550] loss: 2.449, acc: 0.496
******** [step = 600] loss: 2.452, acc: 0.496
******** [step = 650] loss: 2.457, acc: 0.496
******** [step = 700] loss: 2.461, acc: 0.495
******** [step = 750] loss: 2.465, acc: 0.495
******** [step = 800] loss: 2.469, acc: 0.495
******** [step = 850] loss: 2.475, acc: 0.494
EPOCH = 23 loss: 2.475, acc: 0.494, val_loss: 2.774, val_acc: 0.483

================================================================================2025-08_09 23:57:19
******** [step = 50] loss: 2.402, acc: 0.502
******** [step = 100] loss: 2.394, acc: 0.503
******** [step = 150] loss: 2.403, acc: 0.502
******** [step = 200] loss: 2.411, acc: 0.502
******** [step = 250] loss: 2.411, acc: 0.502
******** [step = 300] loss: 2.417, acc: 0.501
******** [step = 350] loss: 2.423, acc: 0.500
******** [step = 400] loss: 2.431, acc: 0.499
******** [step = 450] loss: 2.435, acc: 0.499
******** [step = 500] loss: 2.438, acc: 0.499
******** [step = 550] loss: 2.442, acc: 0.498
******** [step = 600] loss: 2.444, acc: 0.498
******** [step = 650] loss: 2.448, acc: 0.498
******** [step = 700] loss: 2.453, acc: 0.497
******** [step = 750] loss: 2.456, acc: 0.497
******** [step = 800] loss: 2.458, acc: 0.496
******** [step = 850] loss: 2.460, acc: 0.496
EPOCH = 24 loss: 2.460, acc: 0.496, val_loss: 2.771, val_acc: 0.483

================================================================================2025-08_09 23:58:38
******** [step = 50] loss: 2.384, acc: 0.504
******** [step = 100] loss: 2.381, acc: 0.504
******** [step = 150] loss: 2.396, acc: 0.502
******** [step = 200] loss: 2.401, acc: 0.502
******** [step = 250] loss: 2.403, acc: 0.502
******** [step = 300] loss: 2.404, acc: 0.502
******** [step = 350] loss: 2.411, acc: 0.501
******** [step = 400] loss: 2.416, acc: 0.501
******** [step = 450] loss: 2.421, acc: 0.500
******** [step = 500] loss: 2.425, acc: 0.500
******** [step = 550] loss: 2.428, acc: 0.500
******** [step = 600] loss: 2.431, acc: 0.500
******** [step = 650] loss: 2.434, acc: 0.499
******** [step = 700] loss: 2.438, acc: 0.499
******** [step = 750] loss: 2.443, acc: 0.498
******** [step = 800] loss: 2.446, acc: 0.498
******** [step = 850] loss: 2.446, acc: 0.498
EPOCH = 25 loss: 2.446, acc: 0.498, val_loss: 2.767, val_acc: 0.485

================================================================================2025-08_09 23:59:58
******** [step = 50] loss: 2.368, acc: 0.508
******** [step = 100] loss: 2.369, acc: 0.507
******** [step = 150] loss: 2.373, acc: 0.507
******** [step = 200] loss: 2.380, acc: 0.506
******** [step = 250] loss: 2.385, acc: 0.505
******** [step = 300] loss: 2.390, acc: 0.505
******** [step = 350] loss: 2.397, acc: 0.504
******** [step = 400] loss: 2.402, acc: 0.503
******** [step = 450] loss: 2.405, acc: 0.503
******** [step = 500] loss: 2.411, acc: 0.502
******** [step = 550] loss: 2.412, acc: 0.502
******** [step = 600] loss: 2.416, acc: 0.502
******** [step = 650] loss: 2.421, acc: 0.501
******** [step = 700] loss: 2.424, acc: 0.501
******** [step = 750] loss: 2.428, acc: 0.501
******** [step = 800] loss: 2.432, acc: 0.500
******** [step = 850] loss: 2.434, acc: 0.500
EPOCH = 26 loss: 2.434, acc: 0.500, val_loss: 2.766, val_acc: 0.486

================================================================================2025-08_10 00:01:17
******** [step = 50] loss: 2.359, acc: 0.509
******** [step = 100] loss: 2.359, acc: 0.509
******** [step = 150] loss: 2.361, acc: 0.508
******** [step = 200] loss: 2.367, acc: 0.507
******** [step = 250] loss: 2.372, acc: 0.507
******** [step = 300] loss: 2.376, acc: 0.506
******** [step = 350] loss: 2.383, acc: 0.506
******** [step = 400] loss: 2.388, acc: 0.505
******** [step = 450] loss: 2.391, acc: 0.505
******** [step = 500] loss: 2.398, acc: 0.504
******** [step = 550] loss: 2.404, acc: 0.503
******** [step = 600] loss: 2.407, acc: 0.503
******** [step = 650] loss: 2.411, acc: 0.503
******** [step = 700] loss: 2.414, acc: 0.502
******** [step = 750] loss: 2.418, acc: 0.502
******** [step = 800] loss: 2.420, acc: 0.502
******** [step = 850] loss: 2.424, acc: 0.501
EPOCH = 27 loss: 2.424, acc: 0.501, val_loss: 2.762, val_acc: 0.485

================================================================================2025-08_10 00:02:36
******** [step = 50] loss: 2.349, acc: 0.512
******** [step = 100] loss: 2.351, acc: 0.511
******** [step = 150] loss: 2.354, acc: 0.510
******** [step = 200] loss: 2.352, acc: 0.510
******** [step = 250] loss: 2.357, acc: 0.509
******** [step = 300] loss: 2.365, acc: 0.508
******** [step = 350] loss: 2.371, acc: 0.508
******** [step = 400] loss: 2.376, acc: 0.507
******** [step = 450] loss: 2.380, acc: 0.507
******** [step = 500] loss: 2.387, acc: 0.506
******** [step = 550] loss: 2.393, acc: 0.505
******** [step = 600] loss: 2.396, acc: 0.505
******** [step = 650] loss: 2.400, acc: 0.505
******** [step = 700] loss: 2.403, acc: 0.505
******** [step = 750] loss: 2.407, acc: 0.504
******** [step = 800] loss: 2.411, acc: 0.504
******** [step = 850] loss: 2.413, acc: 0.504
EPOCH = 28 loss: 2.413, acc: 0.504, val_loss: 2.759, val_acc: 0.485

================================================================================2025-08_10 00:03:55
******** [step = 50] loss: 2.337, acc: 0.510
******** [step = 100] loss: 2.339, acc: 0.510
******** [step = 150] loss: 2.347, acc: 0.509
******** [step = 200] loss: 2.350, acc: 0.509
******** [step = 250] loss: 2.358, acc: 0.508
******** [step = 300] loss: 2.362, acc: 0.508
******** [step = 350] loss: 2.368, acc: 0.507
******** [step = 400] loss: 2.369, acc: 0.507
******** [step = 450] loss: 2.374, acc: 0.507
******** [step = 500] loss: 2.380, acc: 0.506
******** [step = 550] loss: 2.385, acc: 0.506
******** [step = 600] loss: 2.387, acc: 0.506
******** [step = 650] loss: 2.389, acc: 0.506
******** [step = 700] loss: 2.394, acc: 0.505
******** [step = 750] loss: 2.396, acc: 0.505
******** [step = 800] loss: 2.400, acc: 0.505
******** [step = 850] loss: 2.403, acc: 0.505
EPOCH = 29 loss: 2.403, acc: 0.505, val_loss: 2.756, val_acc: 0.486

================================================================================2025-08_10 00:05:14
******** [step = 50] loss: 2.334, acc: 0.511
******** [step = 100] loss: 2.339, acc: 0.511
******** [step = 150] loss: 2.335, acc: 0.512
******** [step = 200] loss: 2.333, acc: 0.512
******** [step = 250] loss: 2.338, acc: 0.511
******** [step = 300] loss: 2.344, acc: 0.511
******** [step = 350] loss: 2.350, acc: 0.510
******** [step = 400] loss: 2.355, acc: 0.509
******** [step = 450] loss: 2.360, acc: 0.509
******** [step = 500] loss: 2.365, acc: 0.509
******** [step = 550] loss: 2.371, acc: 0.508
******** [step = 600] loss: 2.376, acc: 0.507
******** [step = 650] loss: 2.381, acc: 0.507
******** [step = 700] loss: 2.383, acc: 0.507
******** [step = 750] loss: 2.386, acc: 0.506
******** [step = 800] loss: 2.390, acc: 0.506
******** [step = 850] loss: 2.393, acc: 0.506
EPOCH = 30 loss: 2.393, acc: 0.506, val_loss: 2.752, val_acc: 0.488

================================================================================2025-08_10 00:06:33
******** [step = 50] loss: 2.329, acc: 0.510
******** [step = 100] loss: 2.324, acc: 0.512
******** [step = 150] loss: 2.320, acc: 0.514
******** [step = 200] loss: 2.326, acc: 0.513
******** [step = 250] loss: 2.332, acc: 0.512
******** [step = 300] loss: 2.338, acc: 0.511
******** [step = 350] loss: 2.343, acc: 0.511
******** [step = 400] loss: 2.350, acc: 0.510
******** [step = 450] loss: 2.358, acc: 0.509
******** [step = 500] loss: 2.363, acc: 0.509
******** [step = 550] loss: 2.367, acc: 0.509
******** [step = 600] loss: 2.369, acc: 0.509
******** [step = 650] loss: 2.373, acc: 0.508
******** [step = 700] loss: 2.377, acc: 0.508
******** [step = 750] loss: 2.380, acc: 0.508
******** [step = 800] loss: 2.383, acc: 0.507
******** [step = 850] loss: 2.384, acc: 0.507
EPOCH = 31 loss: 2.384, acc: 0.507, val_loss: 2.748, val_acc: 0.489

================================================================================2025-08_10 00:07:53
******** [step = 50] loss: 2.329, acc: 0.514
******** [step = 100] loss: 2.320, acc: 0.513
******** [step = 150] loss: 2.322, acc: 0.512
******** [step = 200] loss: 2.323, acc: 0.513
******** [step = 250] loss: 2.328, acc: 0.513
******** [step = 300] loss: 2.329, acc: 0.513
******** [step = 350] loss: 2.332, acc: 0.513
******** [step = 400] loss: 2.334, acc: 0.512
******** [step = 450] loss: 2.340, acc: 0.512
******** [step = 500] loss: 2.344, acc: 0.512
******** [step = 550] loss: 2.348, acc: 0.511
******** [step = 600] loss: 2.356, acc: 0.510
******** [step = 650] loss: 2.360, acc: 0.510
******** [step = 700] loss: 2.365, acc: 0.509
******** [step = 750] loss: 2.368, acc: 0.509
******** [step = 800] loss: 2.372, acc: 0.509
******** [step = 850] loss: 2.376, acc: 0.508
EPOCH = 32 loss: 2.376, acc: 0.508, val_loss: 2.747, val_acc: 0.490

================================================================================2025-08_10 00:09:12
******** [step = 50] loss: 2.300, acc: 0.517
******** [step = 100] loss: 2.303, acc: 0.516
******** [step = 150] loss: 2.307, acc: 0.516
******** [step = 200] loss: 2.314, acc: 0.515
******** [step = 250] loss: 2.318, acc: 0.515
******** [step = 300] loss: 2.323, acc: 0.514
******** [step = 350] loss: 2.327, acc: 0.514
******** [step = 400] loss: 2.330, acc: 0.513
******** [step = 450] loss: 2.335, acc: 0.513
******** [step = 500] loss: 2.340, acc: 0.512
******** [step = 550] loss: 2.346, acc: 0.512
******** [step = 600] loss: 2.350, acc: 0.511
******** [step = 650] loss: 2.353, acc: 0.511
******** [step = 700] loss: 2.357, acc: 0.511
******** [step = 750] loss: 2.359, acc: 0.510
******** [step = 800] loss: 2.363, acc: 0.510
******** [step = 850] loss: 2.365, acc: 0.510
EPOCH = 33 loss: 2.365, acc: 0.510, val_loss: 2.741, val_acc: 0.491

================================================================================2025-08_10 00:10:31
******** [step = 50] loss: 2.296, acc: 0.516
******** [step = 100] loss: 2.306, acc: 0.516
******** [step = 150] loss: 2.311, acc: 0.516
******** [step = 200] loss: 2.316, acc: 0.515
******** [step = 250] loss: 2.317, acc: 0.515
******** [step = 300] loss: 2.317, acc: 0.515
******** [step = 350] loss: 2.323, acc: 0.514
******** [step = 400] loss: 2.329, acc: 0.514
******** [step = 450] loss: 2.334, acc: 0.513
******** [step = 500] loss: 2.336, acc: 0.513
******** [step = 550] loss: 2.339, acc: 0.513
******** [step = 600] loss: 2.343, acc: 0.512
******** [step = 650] loss: 2.346, acc: 0.512
******** [step = 700] loss: 2.348, acc: 0.512
******** [step = 750] loss: 2.351, acc: 0.512
******** [step = 800] loss: 2.355, acc: 0.511
******** [step = 850] loss: 2.359, acc: 0.511
EPOCH = 34 loss: 2.359, acc: 0.511, val_loss: 2.743, val_acc: 0.490

================================================================================2025-08_10 00:11:50
******** [step = 50] loss: 2.271, acc: 0.522
******** [step = 100] loss: 2.277, acc: 0.520
******** [step = 150] loss: 2.288, acc: 0.519
******** [step = 200] loss: 2.299, acc: 0.517
******** [step = 250] loss: 2.304, acc: 0.516
******** [step = 300] loss: 2.310, acc: 0.516
******** [step = 350] loss: 2.316, acc: 0.515
******** [step = 400] loss: 2.322, acc: 0.514
******** [step = 450] loss: 2.325, acc: 0.514
******** [step = 500] loss: 2.330, acc: 0.514
******** [step = 550] loss: 2.335, acc: 0.513
******** [step = 600] loss: 2.339, acc: 0.513
******** [step = 650] loss: 2.342, acc: 0.513
******** [step = 700] loss: 2.344, acc: 0.513
******** [step = 750] loss: 2.347, acc: 0.512
******** [step = 800] loss: 2.350, acc: 0.512
******** [step = 850] loss: 2.352, acc: 0.512
EPOCH = 35 loss: 2.352, acc: 0.512, val_loss: 2.736, val_acc: 0.492

================================================================================2025-08_10 00:13:09
******** [step = 50] loss: 2.268, acc: 0.519
******** [step = 100] loss: 2.276, acc: 0.519
******** [step = 150] loss: 2.278, acc: 0.520
******** [step = 200] loss: 2.281, acc: 0.520
******** [step = 250] loss: 2.287, acc: 0.520
******** [step = 300] loss: 2.293, acc: 0.519
******** [step = 350] loss: 2.301, acc: 0.518
******** [step = 400] loss: 2.307, acc: 0.517
******** [step = 450] loss: 2.310, acc: 0.517
******** [step = 500] loss: 2.315, acc: 0.516
******** [step = 550] loss: 2.317, acc: 0.516
******** [step = 600] loss: 2.322, acc: 0.516
******** [step = 650] loss: 2.327, acc: 0.515
******** [step = 700] loss: 2.332, acc: 0.515
******** [step = 750] loss: 2.337, acc: 0.514
******** [step = 800] loss: 2.341, acc: 0.514
******** [step = 850] loss: 2.344, acc: 0.513
EPOCH = 36 loss: 2.344, acc: 0.513, val_loss: 2.736, val_acc: 0.493

================================================================================2025-08_10 00:14:28
******** [step = 50] loss: 2.240, acc: 0.528
******** [step = 100] loss: 2.271, acc: 0.523
******** [step = 150] loss: 2.276, acc: 0.522
******** [step = 200] loss: 2.284, acc: 0.521
******** [step = 250] loss: 2.290, acc: 0.519
******** [step = 300] loss: 2.296, acc: 0.519
******** [step = 350] loss: 2.302, acc: 0.518
******** [step = 400] loss: 2.307, acc: 0.517
******** [step = 450] loss: 2.312, acc: 0.517
******** [step = 500] loss: 2.314, acc: 0.516
******** [step = 550] loss: 2.319, acc: 0.516
******** [step = 600] loss: 2.323, acc: 0.516
******** [step = 650] loss: 2.327, acc: 0.515
******** [step = 700] loss: 2.330, acc: 0.515
******** [step = 750] loss: 2.334, acc: 0.514
******** [step = 800] loss: 2.337, acc: 0.514
******** [step = 850] loss: 2.339, acc: 0.514
EPOCH = 37 loss: 2.339, acc: 0.514, val_loss: 2.739, val_acc: 0.492

================================================================================2025-08_10 00:15:47
******** [step = 50] loss: 2.250, acc: 0.525
******** [step = 100] loss: 2.265, acc: 0.523
******** [step = 150] loss: 2.273, acc: 0.521
******** [step = 200] loss: 2.277, acc: 0.520
******** [step = 250] loss: 2.281, acc: 0.520
******** [step = 300] loss: 2.289, acc: 0.519
******** [step = 350] loss: 2.292, acc: 0.519
******** [step = 400] loss: 2.295, acc: 0.518
******** [step = 450] loss: 2.299, acc: 0.518
******** [step = 500] loss: 2.303, acc: 0.518
******** [step = 550] loss: 2.309, acc: 0.517
******** [step = 600] loss: 2.314, acc: 0.517
******** [step = 650] loss: 2.316, acc: 0.516
******** [step = 700] loss: 2.319, acc: 0.516
******** [step = 750] loss: 2.324, acc: 0.516
******** [step = 800] loss: 2.329, acc: 0.515
******** [step = 850] loss: 2.330, acc: 0.515
EPOCH = 38 loss: 2.330, acc: 0.515, val_loss: 2.739, val_acc: 0.492

================================================================================2025-08_10 00:17:09
******** [step = 50] loss: 2.235, acc: 0.524
******** [step = 100] loss: 2.250, acc: 0.524
******** [step = 150] loss: 2.263, acc: 0.522
******** [step = 200] loss: 2.270, acc: 0.521
******** [step = 250] loss: 2.279, acc: 0.520
******** [step = 300] loss: 2.283, acc: 0.520
******** [step = 350] loss: 2.287, acc: 0.519
******** [step = 400] loss: 2.290, acc: 0.519
******** [step = 450] loss: 2.294, acc: 0.519
******** [step = 500] loss: 2.297, acc: 0.519
******** [step = 550] loss: 2.301, acc: 0.518
******** [step = 600] loss: 2.305, acc: 0.518
******** [step = 650] loss: 2.308, acc: 0.517
******** [step = 700] loss: 2.313, acc: 0.517
******** [step = 750] loss: 2.317, acc: 0.517
******** [step = 800] loss: 2.322, acc: 0.516
******** [step = 850] loss: 2.325, acc: 0.516
EPOCH = 39 loss: 2.325, acc: 0.516, val_loss: 2.734, val_acc: 0.493

================================================================================2025-08_10 00:18:28
******** [step = 50] loss: 2.249, acc: 0.523
******** [step = 100] loss: 2.250, acc: 0.523
******** [step = 150] loss: 2.254, acc: 0.523
******** [step = 200] loss: 2.263, acc: 0.522
******** [step = 250] loss: 2.265, acc: 0.521
******** [step = 300] loss: 2.271, acc: 0.521
******** [step = 350] loss: 2.278, acc: 0.521
******** [step = 400] loss: 2.284, acc: 0.520
******** [step = 450] loss: 2.287, acc: 0.519
******** [step = 500] loss: 2.292, acc: 0.519
******** [step = 550] loss: 2.298, acc: 0.518
******** [step = 600] loss: 2.301, acc: 0.518
******** [step = 650] loss: 2.306, acc: 0.518
******** [step = 700] loss: 2.309, acc: 0.518
******** [step = 750] loss: 2.312, acc: 0.517
******** [step = 800] loss: 2.315, acc: 0.517
******** [step = 850] loss: 2.318, acc: 0.517
EPOCH = 40 loss: 2.318, acc: 0.517, val_loss: 2.735, val_acc: 0.493

================================================================================2025-08_10 00:19:47
******** [step = 50] loss: 2.234, acc: 0.529
******** [step = 100] loss: 2.243, acc: 0.527
******** [step = 150] loss: 2.249, acc: 0.527
******** [step = 200] loss: 2.256, acc: 0.526
******** [step = 250] loss: 2.261, acc: 0.524
******** [step = 300] loss: 2.266, acc: 0.523
******** [step = 350] loss: 2.271, acc: 0.523
******** [step = 400] loss: 2.277, acc: 0.522
******** [step = 450] loss: 2.283, acc: 0.522
******** [step = 500] loss: 2.286, acc: 0.521
******** [step = 550] loss: 2.292, acc: 0.521
******** [step = 600] loss: 2.295, acc: 0.520
******** [step = 650] loss: 2.299, acc: 0.520
******** [step = 700] loss: 2.303, acc: 0.519
******** [step = 750] loss: 2.308, acc: 0.519
******** [step = 800] loss: 2.310, acc: 0.519
******** [step = 850] loss: 2.313, acc: 0.519
EPOCH = 41 loss: 2.313, acc: 0.519, val_loss: 2.731, val_acc: 0.493

================================================================================2025-08_10 00:21:06
******** [step = 50] loss: 2.213, acc: 0.529
******** [step = 100] loss: 2.223, acc: 0.527
******** [step = 150] loss: 2.238, acc: 0.525
******** [step = 200] loss: 2.244, acc: 0.524
******** [step = 250] loss: 2.251, acc: 0.524
******** [step = 300] loss: 2.256, acc: 0.523
******** [step = 350] loss: 2.262, acc: 0.523
******** [step = 400] loss: 2.266, acc: 0.523
******** [step = 450] loss: 2.272, acc: 0.522
******** [step = 500] loss: 2.277, acc: 0.522
******** [step = 550] loss: 2.282, acc: 0.521
******** [step = 600] loss: 2.286, acc: 0.521
******** [step = 650] loss: 2.292, acc: 0.520
******** [step = 700] loss: 2.298, acc: 0.520
******** [step = 750] loss: 2.300, acc: 0.519
******** [step = 800] loss: 2.304, acc: 0.519
******** [step = 850] loss: 2.307, acc: 0.519
EPOCH = 42 loss: 2.307, acc: 0.519, val_loss: 2.727, val_acc: 0.494

================================================================================2025-08_10 00:22:25
******** [step = 50] loss: 2.227, acc: 0.527
******** [step = 100] loss: 2.236, acc: 0.527
******** [step = 150] loss: 2.246, acc: 0.525
******** [step = 200] loss: 2.251, acc: 0.525
******** [step = 250] loss: 2.256, acc: 0.524
******** [step = 300] loss: 2.260, acc: 0.523
******** [step = 350] loss: 2.262, acc: 0.523
******** [step = 400] loss: 2.268, acc: 0.523
******** [step = 450] loss: 2.271, acc: 0.523
******** [step = 500] loss: 2.275, acc: 0.522
******** [step = 550] loss: 2.279, acc: 0.522
******** [step = 600] loss: 2.285, acc: 0.521
******** [step = 650] loss: 2.287, acc: 0.521
******** [step = 700] loss: 2.292, acc: 0.520
******** [step = 750] loss: 2.296, acc: 0.520
******** [step = 800] loss: 2.300, acc: 0.520
******** [step = 850] loss: 2.302, acc: 0.519
EPOCH = 43 loss: 2.302, acc: 0.519, val_loss: 2.727, val_acc: 0.495

================================================================================2025-08_10 00:23:44
******** [step = 50] loss: 2.233, acc: 0.527
******** [step = 100] loss: 2.227, acc: 0.529
******** [step = 150] loss: 2.231, acc: 0.527
******** [step = 200] loss: 2.235, acc: 0.526
******** [step = 250] loss: 2.241, acc: 0.526
******** [step = 300] loss: 2.246, acc: 0.525
******** [step = 350] loss: 2.249, acc: 0.525
******** [step = 400] loss: 2.254, acc: 0.524
******** [step = 450] loss: 2.261, acc: 0.524
******** [step = 500] loss: 2.267, acc: 0.524
******** [step = 550] loss: 2.273, acc: 0.523
******** [step = 600] loss: 2.278, acc: 0.522
******** [step = 650] loss: 2.283, acc: 0.522
******** [step = 700] loss: 2.287, acc: 0.521
******** [step = 750] loss: 2.289, acc: 0.521
******** [step = 800] loss: 2.293, acc: 0.521
******** [step = 850] loss: 2.297, acc: 0.520
EPOCH = 44 loss: 2.297, acc: 0.520, val_loss: 2.729, val_acc: 0.496

================================================================================2025-08_10 00:25:04
******** [step = 50] loss: 2.226, acc: 0.528
******** [step = 100] loss: 2.224, acc: 0.527
******** [step = 150] loss: 2.233, acc: 0.527
******** [step = 200] loss: 2.236, acc: 0.527
******** [step = 250] loss: 2.246, acc: 0.526
******** [step = 300] loss: 2.250, acc: 0.525
******** [step = 350] loss: 2.253, acc: 0.525
******** [step = 400] loss: 2.257, acc: 0.525
******** [step = 450] loss: 2.262, acc: 0.524
******** [step = 500] loss: 2.265, acc: 0.524
******** [step = 550] loss: 2.269, acc: 0.524
******** [step = 600] loss: 2.272, acc: 0.523
******** [step = 650] loss: 2.277, acc: 0.523
******** [step = 700] loss: 2.282, acc: 0.522
******** [step = 750] loss: 2.286, acc: 0.522
******** [step = 800] loss: 2.288, acc: 0.522
******** [step = 850] loss: 2.291, acc: 0.521
EPOCH = 45 loss: 2.291, acc: 0.521, val_loss: 2.734, val_acc: 0.496

================================================================================2025-08_10 00:26:23
******** [step = 50] loss: 2.220, acc: 0.528
******** [step = 100] loss: 2.222, acc: 0.528
******** [step = 150] loss: 2.223, acc: 0.528
******** [step = 200] loss: 2.225, acc: 0.528
******** [step = 250] loss: 2.230, acc: 0.527
******** [step = 300] loss: 2.241, acc: 0.526
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 6778324 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T00:26:51 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 6778324.0 ON atl1-1-02-007-31-0 CANCELLED AT 2025-08-10T00:26:51 DUE TO TIME LIMIT ***
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 00:26:54
Job ID:        6778324
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=04:01:56,vmem=0,walltime=01:00:29,mem=36288K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-007-31-0
---------------------------------------
