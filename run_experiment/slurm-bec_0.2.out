---------------------------------------
Begin Slurm Prolog: Aug-10-2025 21:29:44
Job ID:    6785413
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 09:29:44 PM EDT 2025
atl1-1-02-005-29-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 21:30:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   41C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 98189.78it/s] 28%|██▊       | 38198/135842 [00:00<00:00, 150680.04it/s] 40%|████      | 54810/135842 [00:00<00:00, 117227.01it/s] 51%|█████     | 69378/135842 [00:00<00:00, 93993.92it/s]  66%|██████▌   | 89206/135842 [00:00<00:00, 119057.34it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90509.73it/s] 92%|█████████▏| 125355/135842 [00:01<00:00, 108596.92it/s]100%|██████████| 135842/135842 [00:01<00:00, 110750.47it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47593.21it/s] 33%|███▎      | 35525/108673 [00:00<00:00, 85165.71it/s] 49%|████▉     | 53587/108673 [00:00<00:00, 112496.54it/s] 66%|██████▌   | 71746/108673 [00:00<00:00, 132556.38it/s] 81%|████████  | 88056/108673 [00:01<00:00, 71145.14it/s]  98%|█████████▊| 106135/108673 [00:01<00:00, 90212.35it/s]100%|██████████| 108673/108673 [00:01<00:00, 89310.61it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17905/27169 [00:00<00:00, 179040.35it/s]100%|██████████| 27169/27169 [00:00<00:00, 180165.23it/s]
tensor([    3,    14,  4083,     7,   421,  3835,    11,    18,    22,    39,
         7340,     7,    14,   187,    29,    43, 16252,    65, 17561,     4,
            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,  106,   15,   51,    6,  168,   26,   52,   26,  332,  123,   15,
          75,   17,    9,   54,  284, 1095,   35,   12,   41,    6,   39,  193,
          48,   91,   22,  311,    7,  384, 1143,   45, 1662,    8,    2,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,  385,   73,  199,    6,    7,  524,  111,   29,   11,    6,   47,
         769,  717,   15,   52,   33,  126,   91,  654,  520, 8048,   82,    6,
           9,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_10 21:31:47
******** [step = 50] loss: 9.382, acc: 0.009
******** [step = 100] loss: 8.932, acc: 0.091
******** [step = 150] loss: 8.579, acc: 0.134
******** [step = 200] loss: 8.239, acc: 0.163
******** [step = 250] loss: 7.919, acc: 0.181
******** [step = 300] loss: 7.609, acc: 0.195
******** [step = 350] loss: 7.308, acc: 0.207
******** [step = 400] loss: 7.028, acc: 0.218
******** [step = 450] loss: 6.774, acc: 0.230
******** [step = 500] loss: 6.551, acc: 0.242
******** [step = 550] loss: 6.351, acc: 0.253
******** [step = 600] loss: 6.175, acc: 0.264
******** [step = 650] loss: 6.013, acc: 0.274
******** [step = 700] loss: 5.869, acc: 0.282
******** [step = 750] loss: 5.737, acc: 0.291
******** [step = 800] loss: 5.618, acc: 0.298
******** [step = 850] loss: 5.506, acc: 0.305
EPOCH = 1 loss: 5.506, acc: 0.305, val_loss: 3.569, val_acc: 0.438

================================================================================2025-08_10 21:33:14
******** [step = 50] loss: 3.679, acc: 0.417
******** [step = 100] loss: 3.647, acc: 0.421
******** [step = 150] loss: 3.617, acc: 0.424
******** [step = 200] loss: 3.583, acc: 0.426
******** [step = 250] loss: 3.562, acc: 0.428
******** [step = 300] loss: 3.534, acc: 0.431
******** [step = 350] loss: 3.510, acc: 0.433
******** [step = 400] loss: 3.487, acc: 0.435
******** [step = 450] loss: 3.466, acc: 0.438
******** [step = 500] loss: 3.444, acc: 0.440
******** [step = 550] loss: 3.424, acc: 0.442
******** [step = 600] loss: 3.404, acc: 0.444
******** [step = 650] loss: 3.385, acc: 0.446
******** [step = 700] loss: 3.368, acc: 0.448
******** [step = 750] loss: 3.351, acc: 0.450
******** [step = 800] loss: 3.333, acc: 0.452
******** [step = 850] loss: 3.317, acc: 0.453
EPOCH = 2 loss: 3.317, acc: 0.453, val_loss: 2.938, val_acc: 0.504

================================================================================2025-08_10 21:34:34
******** [step = 50] loss: 3.001, acc: 0.485
******** [step = 100] loss: 2.977, acc: 0.488
******** [step = 150] loss: 2.963, acc: 0.489
******** [step = 200] loss: 2.948, acc: 0.492
******** [step = 250] loss: 2.937, acc: 0.494
******** [step = 300] loss: 2.925, acc: 0.495
******** [step = 350] loss: 2.914, acc: 0.497
******** [step = 400] loss: 2.900, acc: 0.499
******** [step = 450] loss: 2.884, acc: 0.502
******** [step = 500] loss: 2.872, acc: 0.504
******** [step = 550] loss: 2.861, acc: 0.505
******** [step = 600] loss: 2.849, acc: 0.507
******** [step = 650] loss: 2.842, acc: 0.508
******** [step = 700] loss: 2.830, acc: 0.510
******** [step = 750] loss: 2.819, acc: 0.511
******** [step = 800] loss: 2.809, acc: 0.513
******** [step = 850] loss: 2.803, acc: 0.514
EPOCH = 3 loss: 2.803, acc: 0.514, val_loss: 2.524, val_acc: 0.560

================================================================================2025-08_10 21:35:54
******** [step = 50] loss: 2.580, acc: 0.537
******** [step = 100] loss: 2.536, acc: 0.544
******** [step = 150] loss: 2.528, acc: 0.546
******** [step = 200] loss: 2.519, acc: 0.547
******** [step = 250] loss: 2.515, acc: 0.548
******** [step = 300] loss: 2.509, acc: 0.549
******** [step = 350] loss: 2.499, acc: 0.550
******** [step = 400] loss: 2.492, acc: 0.551
******** [step = 450] loss: 2.484, acc: 0.553
******** [step = 500] loss: 2.479, acc: 0.554
******** [step = 550] loss: 2.474, acc: 0.555
******** [step = 600] loss: 2.465, acc: 0.556
******** [step = 650] loss: 2.458, acc: 0.558
******** [step = 700] loss: 2.450, acc: 0.559
******** [step = 750] loss: 2.440, acc: 0.561
******** [step = 800] loss: 2.434, acc: 0.562
******** [step = 850] loss: 2.432, acc: 0.562
EPOCH = 4 loss: 2.432, acc: 0.562, val_loss: 2.211, val_acc: 0.602

================================================================================2025-08_10 21:37:18
******** [step = 50] loss: 2.249, acc: 0.580
******** [step = 100] loss: 2.226, acc: 0.586
******** [step = 150] loss: 2.216, acc: 0.588
******** [step = 200] loss: 2.207, acc: 0.589
******** [step = 250] loss: 2.201, acc: 0.590
******** [step = 300] loss: 2.198, acc: 0.591
******** [step = 350] loss: 2.198, acc: 0.591
******** [step = 400] loss: 2.194, acc: 0.592
******** [step = 450] loss: 2.189, acc: 0.593
******** [step = 500] loss: 2.184, acc: 0.594
******** [step = 550] loss: 2.180, acc: 0.595
******** [step = 600] loss: 2.176, acc: 0.596
******** [step = 650] loss: 2.173, acc: 0.596
******** [step = 700] loss: 2.169, acc: 0.597
******** [step = 750] loss: 2.166, acc: 0.598
******** [step = 800] loss: 2.162, acc: 0.599
******** [step = 850] loss: 2.161, acc: 0.599
EPOCH = 5 loss: 2.161, acc: 0.599, val_loss: 2.007, val_acc: 0.634

================================================================================2025-08_10 21:38:38
******** [step = 50] loss: 2.026, acc: 0.614
******** [step = 100] loss: 1.998, acc: 0.619
******** [step = 150] loss: 1.983, acc: 0.621
******** [step = 200] loss: 1.975, acc: 0.623
******** [step = 250] loss: 1.972, acc: 0.623
******** [step = 300] loss: 1.969, acc: 0.624
******** [step = 350] loss: 1.967, acc: 0.625
******** [step = 400] loss: 1.966, acc: 0.626
******** [step = 450] loss: 1.964, acc: 0.626
******** [step = 500] loss: 1.957, acc: 0.627
******** [step = 550] loss: 1.954, acc: 0.628
******** [step = 600] loss: 1.953, acc: 0.628
******** [step = 650] loss: 1.951, acc: 0.628
******** [step = 700] loss: 1.948, acc: 0.629
******** [step = 750] loss: 1.945, acc: 0.630
******** [step = 800] loss: 1.942, acc: 0.630
******** [step = 850] loss: 1.938, acc: 0.631
EPOCH = 6 loss: 1.938, acc: 0.631, val_loss: 1.798, val_acc: 0.665

================================================================================2025-08_10 21:40:08
******** [step = 50] loss: 1.757, acc: 0.650
******** [step = 100] loss: 1.738, acc: 0.656
******** [step = 150] loss: 1.748, acc: 0.655
******** [step = 200] loss: 1.748, acc: 0.656
******** [step = 250] loss: 1.750, acc: 0.656
******** [step = 300] loss: 1.751, acc: 0.656
******** [step = 350] loss: 1.756, acc: 0.656
******** [step = 400] loss: 1.759, acc: 0.656
******** [step = 450] loss: 1.761, acc: 0.656
******** [step = 500] loss: 1.762, acc: 0.656
******** [step = 550] loss: 1.765, acc: 0.655
******** [step = 600] loss: 1.766, acc: 0.656
******** [step = 650] loss: 1.765, acc: 0.656
******** [step = 700] loss: 1.763, acc: 0.656
******** [step = 750] loss: 1.763, acc: 0.656
******** [step = 800] loss: 1.761, acc: 0.657
******** [step = 850] loss: 1.759, acc: 0.657
EPOCH = 7 loss: 1.759, acc: 0.657, val_loss: 1.678, val_acc: 0.685

================================================================================2025-08_10 21:41:29
******** [step = 50] loss: 1.610, acc: 0.675
******** [step = 100] loss: 1.607, acc: 0.676
******** [step = 150] loss: 1.621, acc: 0.676
******** [step = 200] loss: 1.624, acc: 0.675
******** [step = 250] loss: 1.622, acc: 0.676
******** [step = 300] loss: 1.623, acc: 0.676
******** [step = 350] loss: 1.625, acc: 0.676
******** [step = 400] loss: 1.628, acc: 0.676
******** [step = 450] loss: 1.628, acc: 0.676
******** [step = 500] loss: 1.627, acc: 0.676
******** [step = 550] loss: 1.627, acc: 0.677
******** [step = 600] loss: 1.628, acc: 0.677
******** [step = 650] loss: 1.630, acc: 0.677
******** [step = 700] loss: 1.632, acc: 0.677
******** [step = 750] loss: 1.631, acc: 0.677
******** [step = 800] loss: 1.632, acc: 0.677
******** [step = 850] loss: 1.629, acc: 0.678
EPOCH = 8 loss: 1.629, acc: 0.678, val_loss: 1.593, val_acc: 0.699

================================================================================2025-08_10 21:42:58
******** [step = 50] loss: 1.480, acc: 0.696
******** [step = 100] loss: 1.496, acc: 0.695
******** [step = 150] loss: 1.498, acc: 0.695
******** [step = 200] loss: 1.506, acc: 0.694
******** [step = 250] loss: 1.512, acc: 0.694
******** [step = 300] loss: 1.512, acc: 0.694
******** [step = 350] loss: 1.514, acc: 0.694
******** [step = 400] loss: 1.518, acc: 0.694
******** [step = 450] loss: 1.520, acc: 0.694
******** [step = 500] loss: 1.523, acc: 0.693
******** [step = 550] loss: 1.524, acc: 0.694
******** [step = 600] loss: 1.525, acc: 0.694
******** [step = 650] loss: 1.527, acc: 0.694
******** [step = 700] loss: 1.526, acc: 0.694
******** [step = 750] loss: 1.528, acc: 0.694
******** [step = 800] loss: 1.530, acc: 0.694
******** [step = 850] loss: 1.531, acc: 0.694
EPOCH = 9 loss: 1.531, acc: 0.694, val_loss: 1.555, val_acc: 0.708

================================================================================2025-08_10 21:44:29
******** [step = 50] loss: 1.444, acc: 0.704
******** [step = 100] loss: 1.440, acc: 0.705
******** [step = 150] loss: 1.431, acc: 0.706
******** [step = 200] loss: 1.431, acc: 0.707
******** [step = 250] loss: 1.433, acc: 0.706
******** [step = 300] loss: 1.438, acc: 0.706
******** [step = 350] loss: 1.439, acc: 0.706
******** [step = 400] loss: 1.442, acc: 0.706
******** [step = 450] loss: 1.443, acc: 0.706
******** [step = 500] loss: 1.445, acc: 0.706
******** [step = 550] loss: 1.446, acc: 0.706
******** [step = 600] loss: 1.451, acc: 0.706
******** [step = 650] loss: 1.452, acc: 0.706
******** [step = 700] loss: 1.453, acc: 0.706
******** [step = 750] loss: 1.454, acc: 0.706
******** [step = 800] loss: 1.455, acc: 0.706
******** [step = 850] loss: 1.458, acc: 0.706
EPOCH = 10 loss: 1.458, acc: 0.706, val_loss: 1.506, val_acc: 0.720

================================================================================2025-08_10 21:45:49
******** [step = 50] loss: 1.347, acc: 0.720
******** [step = 100] loss: 1.349, acc: 0.720
******** [step = 150] loss: 1.353, acc: 0.719
******** [step = 200] loss: 1.356, acc: 0.719
******** [step = 250] loss: 1.360, acc: 0.719
******** [step = 300] loss: 1.366, acc: 0.719
******** [step = 350] loss: 1.368, acc: 0.718
******** [step = 400] loss: 1.369, acc: 0.719
******** [step = 450] loss: 1.375, acc: 0.718
******** [step = 500] loss: 1.379, acc: 0.718
******** [step = 550] loss: 1.381, acc: 0.718
******** [step = 600] loss: 1.385, acc: 0.717
******** [step = 650] loss: 1.388, acc: 0.717
******** [step = 700] loss: 1.390, acc: 0.717
******** [step = 750] loss: 1.392, acc: 0.717
******** [step = 800] loss: 1.394, acc: 0.717
******** [step = 850] loss: 1.394, acc: 0.717
EPOCH = 11 loss: 1.394, acc: 0.717, val_loss: 1.466, val_acc: 0.728

================================================================================2025-08_10 21:47:10
******** [step = 50] loss: 1.290, acc: 0.730
******** [step = 100] loss: 1.283, acc: 0.732
******** [step = 150] loss: 1.291, acc: 0.731
******** [step = 200] loss: 1.300, acc: 0.730
******** [step = 250] loss: 1.305, acc: 0.728
******** [step = 300] loss: 1.309, acc: 0.729
******** [step = 350] loss: 1.314, acc: 0.728
******** [step = 400] loss: 1.316, acc: 0.728
******** [step = 450] loss: 1.319, acc: 0.728
******** [step = 500] loss: 1.321, acc: 0.728
******** [step = 550] loss: 1.324, acc: 0.728
******** [step = 600] loss: 1.326, acc: 0.727
******** [step = 650] loss: 1.328, acc: 0.727
******** [step = 700] loss: 1.331, acc: 0.727
******** [step = 750] loss: 1.333, acc: 0.727
******** [step = 800] loss: 1.337, acc: 0.727
******** [step = 850] loss: 1.339, acc: 0.726
EPOCH = 12 loss: 1.339, acc: 0.726, val_loss: 1.430, val_acc: 0.734

================================================================================2025-08_10 21:48:32
******** [step = 50] loss: 1.242, acc: 0.737
******** [step = 100] loss: 1.227, acc: 0.739
******** [step = 150] loss: 1.230, acc: 0.739
******** [step = 200] loss: 1.232, acc: 0.740
******** [step = 250] loss: 1.239, acc: 0.739
******** [step = 300] loss: 1.248, acc: 0.738
******** [step = 350] loss: 1.250, acc: 0.738
******** [step = 400] loss: 1.257, acc: 0.737
******** [step = 450] loss: 1.258, acc: 0.737
******** [step = 500] loss: 1.265, acc: 0.737
******** [step = 550] loss: 1.270, acc: 0.736
******** [step = 600] loss: 1.272, acc: 0.736
******** [step = 650] loss: 1.278, acc: 0.735
******** [step = 700] loss: 1.282, acc: 0.735
******** [step = 750] loss: 1.284, acc: 0.734
******** [step = 800] loss: 1.286, acc: 0.734
******** [step = 850] loss: 1.289, acc: 0.734
EPOCH = 13 loss: 1.289, acc: 0.734, val_loss: 1.408, val_acc: 0.739

================================================================================2025-08_10 21:49:54
******** [step = 50] loss: 1.187, acc: 0.747
******** [step = 100] loss: 1.196, acc: 0.747
******** [step = 150] loss: 1.193, acc: 0.747
******** [step = 200] loss: 1.202, acc: 0.745
******** [step = 250] loss: 1.209, acc: 0.745
******** [step = 300] loss: 1.215, acc: 0.744
******** [step = 350] loss: 1.219, acc: 0.743
******** [step = 400] loss: 1.222, acc: 0.743
******** [step = 450] loss: 1.225, acc: 0.743
******** [step = 500] loss: 1.229, acc: 0.742
******** [step = 550] loss: 1.233, acc: 0.742
******** [step = 600] loss: 1.236, acc: 0.742
******** [step = 650] loss: 1.237, acc: 0.742
******** [step = 700] loss: 1.239, acc: 0.742
******** [step = 750] loss: 1.243, acc: 0.741
******** [step = 800] loss: 1.245, acc: 0.741
******** [step = 850] loss: 1.247, acc: 0.741
EPOCH = 14 loss: 1.247, acc: 0.741, val_loss: 1.389, val_acc: 0.743

================================================================================2025-08_10 21:51:21
******** [step = 50] loss: 1.169, acc: 0.750
******** [step = 100] loss: 1.163, acc: 0.752
******** [step = 150] loss: 1.159, acc: 0.752
******** [step = 200] loss: 1.167, acc: 0.752
******** [step = 250] loss: 1.172, acc: 0.751
******** [step = 300] loss: 1.176, acc: 0.751
******** [step = 350] loss: 1.179, acc: 0.750
******** [step = 400] loss: 1.180, acc: 0.750
******** [step = 450] loss: 1.183, acc: 0.750
******** [step = 500] loss: 1.187, acc: 0.749
******** [step = 550] loss: 1.190, acc: 0.749
******** [step = 600] loss: 1.193, acc: 0.749
******** [step = 650] loss: 1.195, acc: 0.749
******** [step = 700] loss: 1.198, acc: 0.748
******** [step = 750] loss: 1.203, acc: 0.748
******** [step = 800] loss: 1.206, acc: 0.748
******** [step = 850] loss: 1.208, acc: 0.748
EPOCH = 15 loss: 1.208, acc: 0.748, val_loss: 1.371, val_acc: 0.748

================================================================================2025-08_10 21:52:42
******** [step = 50] loss: 1.102, acc: 0.759
******** [step = 100] loss: 1.109, acc: 0.759
******** [step = 150] loss: 1.119, acc: 0.759
******** [step = 200] loss: 1.126, acc: 0.759
******** [step = 250] loss: 1.129, acc: 0.758
******** [step = 300] loss: 1.133, acc: 0.758
******** [step = 350] loss: 1.139, acc: 0.757
******** [step = 400] loss: 1.144, acc: 0.757
******** [step = 450] loss: 1.147, acc: 0.757
******** [step = 500] loss: 1.152, acc: 0.756
******** [step = 550] loss: 1.155, acc: 0.755
******** [step = 600] loss: 1.158, acc: 0.755
******** [step = 650] loss: 1.162, acc: 0.755
******** [step = 700] loss: 1.166, acc: 0.755
******** [step = 750] loss: 1.168, acc: 0.754
******** [step = 800] loss: 1.172, acc: 0.754
******** [step = 850] loss: 1.177, acc: 0.753
EPOCH = 16 loss: 1.177, acc: 0.753, val_loss: 1.351, val_acc: 0.751

================================================================================2025-08_10 21:54:12
******** [step = 50] loss: 1.067, acc: 0.767
******** [step = 100] loss: 1.080, acc: 0.765
******** [step = 150] loss: 1.093, acc: 0.763
******** [step = 200] loss: 1.098, acc: 0.762
******** [step = 250] loss: 1.098, acc: 0.763
******** [step = 300] loss: 1.104, acc: 0.762
******** [step = 350] loss: 1.109, acc: 0.761
******** [step = 400] loss: 1.114, acc: 0.761
******** [step = 450] loss: 1.118, acc: 0.760
******** [step = 500] loss: 1.122, acc: 0.760
******** [step = 550] loss: 1.127, acc: 0.759
******** [step = 600] loss: 1.131, acc: 0.759
******** [step = 650] loss: 1.136, acc: 0.758
******** [step = 700] loss: 1.137, acc: 0.759
******** [step = 750] loss: 1.141, acc: 0.758
******** [step = 800] loss: 1.142, acc: 0.758
******** [step = 850] loss: 1.144, acc: 0.758
EPOCH = 17 loss: 1.144, acc: 0.758, val_loss: 1.336, val_acc: 0.754

================================================================================2025-08_10 21:55:36
******** [step = 50] loss: 1.065, acc: 0.767
******** [step = 100] loss: 1.060, acc: 0.769
******** [step = 150] loss: 1.070, acc: 0.768
******** [step = 200] loss: 1.074, acc: 0.768
******** [step = 250] loss: 1.078, acc: 0.767
******** [step = 300] loss: 1.082, acc: 0.767
******** [step = 350] loss: 1.088, acc: 0.766
******** [step = 400] loss: 1.091, acc: 0.766
******** [step = 450] loss: 1.095, acc: 0.765
******** [step = 500] loss: 1.097, acc: 0.765
******** [step = 550] loss: 1.098, acc: 0.765
******** [step = 600] loss: 1.101, acc: 0.765
******** [step = 650] loss: 1.104, acc: 0.764
******** [step = 700] loss: 1.106, acc: 0.764
******** [step = 750] loss: 1.110, acc: 0.764
******** [step = 800] loss: 1.113, acc: 0.763
******** [step = 850] loss: 1.115, acc: 0.763
EPOCH = 18 loss: 1.115, acc: 0.763, val_loss: 1.318, val_acc: 0.758

================================================================================2025-08_10 21:57:01
******** [step = 50] loss: 1.022, acc: 0.775
******** [step = 100] loss: 1.017, acc: 0.776
******** [step = 150] loss: 1.026, acc: 0.775
******** [step = 200] loss: 1.033, acc: 0.775
******** [step = 250] loss: 1.043, acc: 0.774
******** [step = 300] loss: 1.053, acc: 0.772
******** [step = 350] loss: 1.055, acc: 0.772
******** [step = 400] loss: 1.060, acc: 0.771
******** [step = 450] loss: 1.062, acc: 0.771
******** [step = 500] loss: 1.068, acc: 0.770
******** [step = 550] loss: 1.073, acc: 0.770
******** [step = 600] loss: 1.075, acc: 0.769
******** [step = 650] loss: 1.078, acc: 0.769
******** [step = 700] loss: 1.082, acc: 0.768
******** [step = 750] loss: 1.085, acc: 0.768
******** [step = 800] loss: 1.089, acc: 0.768
******** [step = 850] loss: 1.093, acc: 0.767
EPOCH = 19 loss: 1.093, acc: 0.767, val_loss: 1.320, val_acc: 0.756

================================================================================2025-08_10 21:58:29
******** [step = 50] loss: 1.014, acc: 0.776
******** [step = 100] loss: 1.016, acc: 0.777
******** [step = 150] loss: 1.017, acc: 0.777
******** [step = 200] loss: 1.025, acc: 0.776
******** [step = 250] loss: 1.029, acc: 0.776
******** [step = 300] loss: 1.033, acc: 0.775
******** [step = 350] loss: 1.037, acc: 0.774
******** [step = 400] loss: 1.040, acc: 0.774
******** [step = 450] loss: 1.044, acc: 0.774
******** [step = 500] loss: 1.048, acc: 0.773
******** [step = 550] loss: 1.050, acc: 0.773
******** [step = 600] loss: 1.054, acc: 0.773
******** [step = 650] loss: 1.057, acc: 0.772
******** [step = 700] loss: 1.060, acc: 0.772
******** [step = 750] loss: 1.063, acc: 0.772
******** [step = 800] loss: 1.066, acc: 0.772
******** [step = 850] loss: 1.069, acc: 0.771
EPOCH = 20 loss: 1.069, acc: 0.771, val_loss: 1.301, val_acc: 0.762

================================================================================2025-08_10 21:59:50
******** [step = 50] loss: 0.984, acc: 0.785
******** [step = 100] loss: 0.981, acc: 0.785
******** [step = 150] loss: 0.995, acc: 0.782
******** [step = 200] loss: 1.002, acc: 0.781
******** [step = 250] loss: 1.003, acc: 0.781
******** [step = 300] loss: 1.011, acc: 0.779
******** [step = 350] loss: 1.015, acc: 0.779
******** [step = 400] loss: 1.018, acc: 0.778
******** [step = 450] loss: 1.022, acc: 0.778
******** [step = 500] loss: 1.023, acc: 0.778
******** [step = 550] loss: 1.027, acc: 0.778
******** [step = 600] loss: 1.032, acc: 0.777
******** [step = 650] loss: 1.036, acc: 0.777
******** [step = 700] loss: 1.039, acc: 0.776
******** [step = 750] loss: 1.041, acc: 0.776
******** [step = 800] loss: 1.045, acc: 0.776
******** [step = 850] loss: 1.047, acc: 0.776
EPOCH = 21 loss: 1.047, acc: 0.776, val_loss: 1.291, val_acc: 0.764

================================================================================2025-08_10 22:01:14
******** [step = 50] loss: 0.941, acc: 0.790
******** [step = 100] loss: 0.965, acc: 0.786
******** [step = 150] loss: 0.974, acc: 0.785
******** [step = 200] loss: 0.980, acc: 0.784
******** [step = 250] loss: 0.986, acc: 0.783
******** [step = 300] loss: 0.990, acc: 0.782
******** [step = 350] loss: 0.997, acc: 0.782
******** [step = 400] loss: 1.004, acc: 0.781
******** [step = 450] loss: 1.008, acc: 0.780
******** [step = 500] loss: 1.010, acc: 0.780
******** [step = 550] loss: 1.014, acc: 0.780
******** [step = 600] loss: 1.017, acc: 0.780
******** [step = 650] loss: 1.019, acc: 0.779
******** [step = 700] loss: 1.023, acc: 0.779
******** [step = 750] loss: 1.024, acc: 0.779
******** [step = 800] loss: 1.026, acc: 0.778
******** [step = 850] loss: 1.028, acc: 0.779
EPOCH = 22 loss: 1.028, acc: 0.779, val_loss: 1.284, val_acc: 0.765

================================================================================2025-08_10 22:02:40
******** [step = 50] loss: 0.950, acc: 0.787
******** [step = 100] loss: 0.940, acc: 0.790
******** [step = 150] loss: 0.944, acc: 0.789
******** [step = 200] loss: 0.949, acc: 0.789
******** [step = 250] loss: 0.961, acc: 0.787
******** [step = 300] loss: 0.966, acc: 0.787
******** [step = 350] loss: 0.971, acc: 0.786
******** [step = 400] loss: 0.976, acc: 0.786
******** [step = 450] loss: 0.981, acc: 0.785
******** [step = 500] loss: 0.984, acc: 0.785
******** [step = 550] loss: 0.989, acc: 0.784
******** [step = 600] loss: 0.993, acc: 0.783
******** [step = 650] loss: 0.998, acc: 0.783
******** [step = 700] loss: 1.001, acc: 0.783
******** [step = 750] loss: 1.005, acc: 0.782
******** [step = 800] loss: 1.008, acc: 0.782
******** [step = 850] loss: 1.012, acc: 0.781
EPOCH = 23 loss: 1.012, acc: 0.781, val_loss: 1.274, val_acc: 0.768

================================================================================2025-08_10 22:04:02
******** [step = 50] loss: 0.929, acc: 0.791
******** [step = 100] loss: 0.937, acc: 0.791
******** [step = 150] loss: 0.932, acc: 0.792
******** [step = 200] loss: 0.932, acc: 0.792
******** [step = 250] loss: 0.938, acc: 0.791
******** [step = 300] loss: 0.944, acc: 0.790
******** [step = 350] loss: 0.948, acc: 0.789
******** [step = 400] loss: 0.953, acc: 0.789
******** [step = 450] loss: 0.962, acc: 0.788
******** [step = 500] loss: 0.965, acc: 0.788
******** [step = 550] loss: 0.971, acc: 0.787
******** [step = 600] loss: 0.976, acc: 0.786
******** [step = 650] loss: 0.983, acc: 0.786
******** [step = 700] loss: 0.986, acc: 0.785
******** [step = 750] loss: 0.990, acc: 0.785
******** [step = 800] loss: 0.991, acc: 0.785
******** [step = 850] loss: 0.994, acc: 0.785
EPOCH = 24 loss: 0.994, acc: 0.785, val_loss: 1.267, val_acc: 0.770

================================================================================2025-08_10 22:05:26
******** [step = 50] loss: 0.910, acc: 0.797
******** [step = 100] loss: 0.911, acc: 0.797
******** [step = 150] loss: 0.925, acc: 0.794
******** [step = 200] loss: 0.926, acc: 0.794
******** [step = 250] loss: 0.929, acc: 0.794
******** [step = 300] loss: 0.935, acc: 0.793
******** [step = 350] loss: 0.938, acc: 0.792
******** [step = 400] loss: 0.944, acc: 0.792
******** [step = 450] loss: 0.948, acc: 0.791
******** [step = 500] loss: 0.954, acc: 0.791
******** [step = 550] loss: 0.960, acc: 0.790
******** [step = 600] loss: 0.962, acc: 0.790
******** [step = 650] loss: 0.965, acc: 0.789
******** [step = 700] loss: 0.970, acc: 0.789
******** [step = 750] loss: 0.974, acc: 0.788
******** [step = 800] loss: 0.977, acc: 0.788
******** [step = 850] loss: 0.981, acc: 0.787
EPOCH = 25 loss: 0.981, acc: 0.787, val_loss: 1.262, val_acc: 0.769

================================================================================2025-08_10 22:06:50
******** [step = 50] loss: 0.920, acc: 0.794
******** [step = 100] loss: 0.904, acc: 0.798
******** [step = 150] loss: 0.914, acc: 0.796
******** [step = 200] loss: 0.918, acc: 0.796
******** [step = 250] loss: 0.921, acc: 0.795
******** [step = 300] loss: 0.924, acc: 0.795
******** [step = 350] loss: 0.931, acc: 0.794
******** [step = 400] loss: 0.934, acc: 0.793
******** [step = 450] loss: 0.937, acc: 0.793
******** [step = 500] loss: 0.941, acc: 0.793
******** [step = 550] loss: 0.944, acc: 0.792
******** [step = 600] loss: 0.948, acc: 0.792
******** [step = 650] loss: 0.953, acc: 0.791
******** [step = 700] loss: 0.956, acc: 0.791
******** [step = 750] loss: 0.960, acc: 0.790
******** [step = 800] loss: 0.962, acc: 0.790
******** [step = 850] loss: 0.967, acc: 0.790
EPOCH = 26 loss: 0.967, acc: 0.790, val_loss: 1.257, val_acc: 0.772

================================================================================2025-08_10 22:08:16
******** [step = 50] loss: 0.910, acc: 0.797
******** [step = 100] loss: 0.899, acc: 0.798
******** [step = 150] loss: 0.897, acc: 0.799
******** [step = 200] loss: 0.896, acc: 0.799
******** [step = 250] loss: 0.903, acc: 0.798
******** [step = 300] loss: 0.909, acc: 0.797
******** [step = 350] loss: 0.912, acc: 0.797
******** [step = 400] loss: 0.916, acc: 0.796
******** [step = 450] loss: 0.923, acc: 0.795
******** [step = 500] loss: 0.928, acc: 0.795
******** [step = 550] loss: 0.932, acc: 0.794
******** [step = 600] loss: 0.937, acc: 0.794
******** [step = 650] loss: 0.940, acc: 0.794
******** [step = 700] loss: 0.946, acc: 0.793
******** [step = 750] loss: 0.948, acc: 0.793
******** [step = 800] loss: 0.951, acc: 0.793
******** [step = 850] loss: 0.953, acc: 0.792
EPOCH = 27 loss: 0.953, acc: 0.792, val_loss: 1.256, val_acc: 0.772

================================================================================2025-08_10 22:09:41
******** [step = 50] loss: 0.862, acc: 0.804
******** [step = 100] loss: 0.876, acc: 0.803
******** [step = 150] loss: 0.880, acc: 0.802
******** [step = 200] loss: 0.885, acc: 0.801
******** [step = 250] loss: 0.890, acc: 0.801
******** [step = 300] loss: 0.893, acc: 0.801
******** [step = 350] loss: 0.899, acc: 0.800
******** [step = 400] loss: 0.906, acc: 0.799
******** [step = 450] loss: 0.910, acc: 0.798
******** [step = 500] loss: 0.915, acc: 0.798
******** [step = 550] loss: 0.919, acc: 0.797
******** [step = 600] loss: 0.924, acc: 0.796
******** [step = 650] loss: 0.928, acc: 0.796
******** [step = 700] loss: 0.932, acc: 0.795
******** [step = 750] loss: 0.935, acc: 0.795
******** [step = 800] loss: 0.938, acc: 0.795
******** [step = 850] loss: 0.941, acc: 0.794
EPOCH = 28 loss: 0.941, acc: 0.794, val_loss: 1.246, val_acc: 0.775

================================================================================2025-08_10 22:11:03
******** [step = 50] loss: 0.856, acc: 0.808
******** [step = 100] loss: 0.858, acc: 0.808
******** [step = 150] loss: 0.861, acc: 0.807
******** [step = 200] loss: 0.870, acc: 0.805
******** [step = 250] loss: 0.874, acc: 0.804
******** [step = 300] loss: 0.880, acc: 0.804
******** [step = 350] loss: 0.887, acc: 0.802
******** [step = 400] loss: 0.892, acc: 0.802
******** [step = 450] loss: 0.897, acc: 0.801
******** [step = 500] loss: 0.901, acc: 0.801
******** [step = 550] loss: 0.903, acc: 0.800
******** [step = 600] loss: 0.908, acc: 0.800
******** [step = 650] loss: 0.912, acc: 0.799
******** [step = 700] loss: 0.916, acc: 0.799
******** [step = 750] loss: 0.920, acc: 0.798
******** [step = 800] loss: 0.925, acc: 0.798
******** [step = 850] loss: 0.929, acc: 0.797
EPOCH = 29 loss: 0.929, acc: 0.797, val_loss: 1.240, val_acc: 0.775

================================================================================2025-08_10 22:12:23
******** [step = 50] loss: 0.871, acc: 0.804
******** [step = 100] loss: 0.864, acc: 0.806
******** [step = 150] loss: 0.860, acc: 0.806
******** [step = 200] loss: 0.861, acc: 0.806
******** [step = 250] loss: 0.865, acc: 0.806
******** [step = 300] loss: 0.868, acc: 0.806
******** [step = 350] loss: 0.874, acc: 0.805
******** [step = 400] loss: 0.879, acc: 0.804
******** [step = 450] loss: 0.885, acc: 0.803
******** [step = 500] loss: 0.889, acc: 0.803
******** [step = 550] loss: 0.893, acc: 0.802
******** [step = 600] loss: 0.897, acc: 0.801
******** [step = 650] loss: 0.903, acc: 0.801
******** [step = 700] loss: 0.906, acc: 0.800
******** [step = 750] loss: 0.910, acc: 0.800
******** [step = 800] loss: 0.912, acc: 0.799
******** [step = 850] loss: 0.918, acc: 0.799
EPOCH = 30 loss: 0.918, acc: 0.799, val_loss: 1.233, val_acc: 0.777

================================================================================2025-08_10 22:13:47
******** [step = 50] loss: 0.855, acc: 0.807
******** [step = 100] loss: 0.852, acc: 0.808
******** [step = 150] loss: 0.852, acc: 0.809
******** [step = 200] loss: 0.860, acc: 0.807
******** [step = 250] loss: 0.862, acc: 0.807
******** [step = 300] loss: 0.868, acc: 0.806
******** [step = 350] loss: 0.872, acc: 0.805
******** [step = 400] loss: 0.877, acc: 0.804
******** [step = 450] loss: 0.881, acc: 0.804
******** [step = 500] loss: 0.884, acc: 0.804
******** [step = 550] loss: 0.888, acc: 0.803
******** [step = 600] loss: 0.892, acc: 0.802
******** [step = 650] loss: 0.895, acc: 0.802
******** [step = 700] loss: 0.899, acc: 0.802
******** [step = 750] loss: 0.902, acc: 0.802
******** [step = 800] loss: 0.905, acc: 0.801
******** [step = 850] loss: 0.907, acc: 0.801
EPOCH = 31 loss: 0.907, acc: 0.801, val_loss: 1.231, val_acc: 0.779

================================================================================2025-08_10 22:15:07
******** [step = 50] loss: 0.814, acc: 0.815
******** [step = 100] loss: 0.819, acc: 0.814
******** [step = 150] loss: 0.827, acc: 0.813
******** [step = 200] loss: 0.833, acc: 0.812
******** [step = 250] loss: 0.843, acc: 0.810
******** [step = 300] loss: 0.850, acc: 0.809
******** [step = 350] loss: 0.857, acc: 0.808
******** [step = 400] loss: 0.863, acc: 0.807
******** [step = 450] loss: 0.867, acc: 0.806
******** [step = 500] loss: 0.873, acc: 0.806
******** [step = 550] loss: 0.879, acc: 0.805
******** [step = 600] loss: 0.884, acc: 0.804
******** [step = 650] loss: 0.887, acc: 0.804
******** [step = 700] loss: 0.890, acc: 0.804
******** [step = 750] loss: 0.892, acc: 0.804
******** [step = 800] loss: 0.896, acc: 0.803
******** [step = 850] loss: 0.898, acc: 0.803
EPOCH = 32 loss: 0.898, acc: 0.803, val_loss: 1.230, val_acc: 0.779

================================================================================2025-08_10 22:16:30
******** [step = 50] loss: 0.814, acc: 0.816
******** [step = 100] loss: 0.818, acc: 0.814
******** [step = 150] loss: 0.821, acc: 0.813
******** [step = 200] loss: 0.828, acc: 0.812
******** [step = 250] loss: 0.835, acc: 0.811
******** [step = 300] loss: 0.841, acc: 0.810
******** [step = 350] loss: 0.845, acc: 0.810
******** [step = 400] loss: 0.849, acc: 0.809
******** [step = 450] loss: 0.854, acc: 0.809
******** [step = 500] loss: 0.860, acc: 0.808
******** [step = 550] loss: 0.863, acc: 0.808
******** [step = 600] loss: 0.868, acc: 0.807
******** [step = 650] loss: 0.872, acc: 0.807
******** [step = 700] loss: 0.876, acc: 0.806
******** [step = 750] loss: 0.880, acc: 0.806
******** [step = 800] loss: 0.883, acc: 0.806
******** [step = 850] loss: 0.886, acc: 0.805
EPOCH = 33 loss: 0.886, acc: 0.805, val_loss: 1.223, val_acc: 0.780

================================================================================2025-08_10 22:17:57
******** [step = 50] loss: 0.803, acc: 0.816
******** [step = 100] loss: 0.810, acc: 0.814
******** [step = 150] loss: 0.820, acc: 0.813
******** [step = 200] loss: 0.824, acc: 0.812
******** [step = 250] loss: 0.833, acc: 0.811
******** [step = 300] loss: 0.837, acc: 0.811
******** [step = 350] loss: 0.840, acc: 0.810
******** [step = 400] loss: 0.846, acc: 0.810
******** [step = 450] loss: 0.850, acc: 0.809
******** [step = 500] loss: 0.853, acc: 0.809
******** [step = 550] loss: 0.859, acc: 0.808
******** [step = 600] loss: 0.862, acc: 0.808
******** [step = 650] loss: 0.865, acc: 0.807
******** [step = 700] loss: 0.869, acc: 0.807
******** [step = 750] loss: 0.872, acc: 0.807
******** [step = 800] loss: 0.876, acc: 0.806
******** [step = 850] loss: 0.884, acc: 0.806
EPOCH = 34 loss: 0.884, acc: 0.806, val_loss: 1.221, val_acc: 0.781

================================================================================2025-08_10 22:19:24
******** [step = 50] loss: 0.803, acc: 0.815
******** [step = 100] loss: 0.805, acc: 0.816
******** [step = 150] loss: 0.808, acc: 0.815
******** [step = 200] loss: 0.810, acc: 0.816
******** [step = 250] loss: 0.816, acc: 0.815
******** [step = 300] loss: 0.821, acc: 0.814
******** [step = 350] loss: 0.826, acc: 0.813
******** [step = 400] loss: 0.832, acc: 0.812
******** [step = 450] loss: 0.838, acc: 0.812
******** [step = 500] loss: 0.843, acc: 0.811
******** [step = 550] loss: 0.847, acc: 0.811
******** [step = 600] loss: 0.851, acc: 0.810
******** [step = 650] loss: 0.855, acc: 0.810
******** [step = 700] loss: 0.857, acc: 0.810
******** [step = 750] loss: 0.861, acc: 0.809
******** [step = 800] loss: 0.865, acc: 0.809
******** [step = 850] loss: 0.871, acc: 0.808
EPOCH = 35 loss: 0.871, acc: 0.808, val_loss: 1.220, val_acc: 0.782

================================================================================2025-08_10 22:20:51
******** [step = 50] loss: 0.797, acc: 0.819
******** [step = 100] loss: 0.799, acc: 0.818
******** [step = 150] loss: 0.802, acc: 0.818
******** [step = 200] loss: 0.810, acc: 0.816
******** [step = 250] loss: 0.811, acc: 0.816
******** [step = 300] loss: 0.817, acc: 0.815
******** [step = 350] loss: 0.819, acc: 0.815
******** [step = 400] loss: 0.825, acc: 0.814
******** [step = 450] loss: 0.829, acc: 0.814
******** [step = 500] loss: 0.834, acc: 0.813
******** [step = 550] loss: 0.841, acc: 0.812
******** [step = 600] loss: 0.845, acc: 0.812
******** [step = 650] loss: 0.849, acc: 0.811
******** [step = 700] loss: 0.851, acc: 0.811
******** [step = 750] loss: 0.855, acc: 0.810
******** [step = 800] loss: 0.859, acc: 0.810
******** [step = 850] loss: 0.862, acc: 0.810
EPOCH = 36 loss: 0.862, acc: 0.810, val_loss: 1.215, val_acc: 0.782

================================================================================2025-08_10 22:22:14
******** [step = 50] loss: 0.790, acc: 0.818
******** [step = 100] loss: 0.790, acc: 0.819
******** [step = 150] loss: 0.788, acc: 0.820
******** [step = 200] loss: 0.794, acc: 0.819
******** [step = 250] loss: 0.797, acc: 0.818
******** [step = 300] loss: 0.804, acc: 0.817
******** [step = 350] loss: 0.808, acc: 0.817
******** [step = 400] loss: 0.815, acc: 0.816
******** [step = 450] loss: 0.820, acc: 0.815
******** [step = 500] loss: 0.826, acc: 0.814
******** [step = 550] loss: 0.832, acc: 0.814
******** [step = 600] loss: 0.834, acc: 0.813
******** [step = 650] loss: 0.839, acc: 0.813
******** [step = 700] loss: 0.844, acc: 0.812
******** [step = 750] loss: 0.848, acc: 0.812
******** [step = 800] loss: 0.852, acc: 0.811
******** [step = 850] loss: 0.855, acc: 0.811
EPOCH = 37 loss: 0.855, acc: 0.811, val_loss: 1.210, val_acc: 0.784

================================================================================2025-08_10 22:23:38
******** [step = 50] loss: 0.770, acc: 0.822
******** [step = 100] loss: 0.779, acc: 0.821
******** [step = 150] loss: 0.783, acc: 0.821
******** [step = 200] loss: 0.792, acc: 0.819
******** [step = 250] loss: 0.794, acc: 0.819
******** [step = 300] loss: 0.802, acc: 0.817
******** [step = 350] loss: 0.807, acc: 0.817
******** [step = 400] loss: 0.811, acc: 0.817
******** [step = 450] loss: 0.816, acc: 0.816
******** [step = 500] loss: 0.821, acc: 0.816
******** [step = 550] loss: 0.824, acc: 0.815
******** [step = 600] loss: 0.829, acc: 0.814
******** [step = 650] loss: 0.834, acc: 0.814
******** [step = 700] loss: 0.837, acc: 0.814
******** [step = 750] loss: 0.839, acc: 0.813
******** [step = 800] loss: 0.843, acc: 0.813
******** [step = 850] loss: 0.849, acc: 0.812
EPOCH = 38 loss: 0.849, acc: 0.812, val_loss: 1.209, val_acc: 0.784

================================================================================2025-08_10 22:25:03
******** [step = 50] loss: 0.755, acc: 0.826
******** [step = 100] loss: 0.773, acc: 0.823
******** [step = 150] loss: 0.776, acc: 0.823
******** [step = 200] loss: 0.777, acc: 0.823
******** [step = 250] loss: 0.786, acc: 0.821
******** [step = 300] loss: 0.792, acc: 0.820
******** [step = 350] loss: 0.802, acc: 0.818
******** [step = 400] loss: 0.807, acc: 0.818
******** [step = 450] loss: 0.811, acc: 0.817
******** [step = 500] loss: 0.815, acc: 0.817
******** [step = 550] loss: 0.818, acc: 0.816
******** [step = 600] loss: 0.822, acc: 0.816
******** [step = 650] loss: 0.826, acc: 0.815
******** [step = 700] loss: 0.831, acc: 0.815
******** [step = 750] loss: 0.834, acc: 0.815
******** [step = 800] loss: 0.838, acc: 0.814
******** [step = 850] loss: 0.842, acc: 0.813
EPOCH = 39 loss: 0.842, acc: 0.813, val_loss: 1.208, val_acc: 0.784

================================================================================2025-08_10 22:26:34
******** [step = 50] loss: 0.769, acc: 0.824
******** [step = 100] loss: 0.776, acc: 0.824
******** [step = 150] loss: 0.775, acc: 0.824
******** [step = 200] loss: 0.782, acc: 0.822
******** [step = 250] loss: 0.784, acc: 0.822
******** [step = 300] loss: 0.787, acc: 0.821
******** [step = 350] loss: 0.794, acc: 0.820
******** [step = 400] loss: 0.798, acc: 0.820
******** [step = 450] loss: 0.802, acc: 0.819
******** [step = 500] loss: 0.805, acc: 0.819
******** [step = 550] loss: 0.809, acc: 0.818
******** [step = 600] loss: 0.814, acc: 0.817
******** [step = 650] loss: 0.819, acc: 0.817
******** [step = 700] loss: 0.822, acc: 0.816
******** [step = 750] loss: 0.827, acc: 0.816
******** [step = 800] loss: 0.830, acc: 0.815
******** [step = 850] loss: 0.832, acc: 0.815
EPOCH = 40 loss: 0.832, acc: 0.815, val_loss: 1.207, val_acc: 0.785

================================================================================2025-08_10 22:27:55
******** [step = 50] loss: 0.765, acc: 0.824
******** [step = 100] loss: 0.760, acc: 0.825
******** [step = 150] loss: 0.764, acc: 0.824
******** [step = 200] loss: 0.769, acc: 0.823
******** [step = 250] loss: 0.774, acc: 0.823
******** [step = 300] loss: 0.779, acc: 0.822
******** [step = 350] loss: 0.785, acc: 0.822
******** [step = 400] loss: 0.790, acc: 0.821
******** [step = 450] loss: 0.797, acc: 0.820
******** [step = 500] loss: 0.801, acc: 0.820
******** [step = 550] loss: 0.805, acc: 0.819
******** [step = 600] loss: 0.808, acc: 0.819
******** [step = 650] loss: 0.811, acc: 0.818
******** [step = 700] loss: 0.815, acc: 0.818
******** [step = 750] loss: 0.819, acc: 0.818
******** [step = 800] loss: 0.822, acc: 0.817
******** [step = 850] loss: 0.829, acc: 0.816
EPOCH = 41 loss: 0.829, acc: 0.816, val_loss: 1.195, val_acc: 0.787

================================================================================2025-08_10 22:29:15
******** [step = 50] loss: 0.752, acc: 0.825
******** [step = 100] loss: 0.751, acc: 0.827
******** [step = 150] loss: 0.757, acc: 0.826
******** [step = 200] loss: 0.761, acc: 0.825
******** [step = 250] loss: 0.765, acc: 0.824
******** [step = 300] loss: 0.771, acc: 0.824
******** [step = 350] loss: 0.780, acc: 0.823
******** [step = 400] loss: 0.787, acc: 0.822
******** [step = 450] loss: 0.789, acc: 0.822
******** [step = 500] loss: 0.793, acc: 0.821
******** [step = 550] loss: 0.799, acc: 0.820
******** [step = 600] loss: 0.803, acc: 0.820
******** [step = 650] loss: 0.808, acc: 0.820
******** [step = 700] loss: 0.811, acc: 0.819
******** [step = 750] loss: 0.815, acc: 0.819
******** [step = 800] loss: 0.818, acc: 0.819
******** [step = 850] loss: 0.819, acc: 0.818
EPOCH = 42 loss: 0.819, acc: 0.818, val_loss: 1.204, val_acc: 0.787

================================================================================2025-08_10 22:30:41
******** [step = 50] loss: 0.745, acc: 0.828
******** [step = 100] loss: 0.747, acc: 0.828
******** [step = 150] loss: 0.756, acc: 0.827
******** [step = 200] loss: 0.762, acc: 0.826
******** [step = 250] loss: 0.769, acc: 0.825
******** [step = 300] loss: 0.775, acc: 0.824
******** [step = 350] loss: 0.778, acc: 0.823
******** [step = 400] loss: 0.781, acc: 0.823
******** [step = 450] loss: 0.787, acc: 0.822
******** [step = 500] loss: 0.789, acc: 0.822
******** [step = 550] loss: 0.795, acc: 0.821
******** [step = 600] loss: 0.797, acc: 0.821
******** [step = 650] loss: 0.801, acc: 0.820
******** [step = 700] loss: 0.804, acc: 0.820
******** [step = 750] loss: 0.808, acc: 0.820
******** [step = 800] loss: 0.810, acc: 0.819
******** [step = 850] loss: 0.815, acc: 0.818
EPOCH = 43 loss: 0.815, acc: 0.818, val_loss: 1.199, val_acc: 0.787

================================================================================2025-08_10 22:32:01
******** [step = 50] loss: 0.741, acc: 0.832
******** [step = 100] loss: 0.738, acc: 0.832
******** [step = 150] loss: 0.744, acc: 0.830
******** [step = 200] loss: 0.747, acc: 0.829
******** [step = 250] loss: 0.755, acc: 0.828
******** [step = 300] loss: 0.762, acc: 0.827
******** [step = 350] loss: 0.767, acc: 0.826
******** [step = 400] loss: 0.772, acc: 0.825
******** [step = 450] loss: 0.776, acc: 0.825
******** [step = 500] loss: 0.782, acc: 0.824
******** [step = 550] loss: 0.786, acc: 0.823
******** [step = 600] loss: 0.790, acc: 0.823
******** [step = 650] loss: 0.793, acc: 0.822
******** [step = 700] loss: 0.797, acc: 0.822
******** [step = 750] loss: 0.800, acc: 0.821
******** [step = 800] loss: 0.804, acc: 0.821
******** [step = 850] loss: 0.807, acc: 0.820
EPOCH = 44 loss: 0.807, acc: 0.820, val_loss: 1.198, val_acc: 0.789

================================================================================2025-08_10 22:33:23
******** [step = 50] loss: 0.729, acc: 0.831
******** [step = 100] loss: 0.736, acc: 0.830
******** [step = 150] loss: 0.741, acc: 0.829
******** [step = 200] loss: 0.746, acc: 0.828
******** [step = 250] loss: 0.752, acc: 0.828
******** [step = 300] loss: 0.759, acc: 0.826
******** [step = 350] loss: 0.761, acc: 0.826
******** [step = 400] loss: 0.767, acc: 0.825
******** [step = 450] loss: 0.771, acc: 0.825
******** [step = 500] loss: 0.777, acc: 0.824
******** [step = 550] loss: 0.778, acc: 0.824
******** [step = 600] loss: 0.782, acc: 0.824
******** [step = 650] loss: 0.786, acc: 0.823
******** [step = 700] loss: 0.789, acc: 0.823
******** [step = 750] loss: 0.793, acc: 0.822
******** [step = 800] loss: 0.798, acc: 0.822
******** [step = 850] loss: 0.801, acc: 0.821
EPOCH = 45 loss: 0.801, acc: 0.821, val_loss: 1.198, val_acc: 0.787

================================================================================2025-08_10 22:34:43
******** [step = 50] loss: 0.720, acc: 0.833
******** [step = 100] loss: 0.726, acc: 0.832
******** [step = 150] loss: 0.733, acc: 0.830
******** [step = 200] loss: 0.741, acc: 0.829
******** [step = 250] loss: 0.745, acc: 0.829
******** [step = 300] loss: 0.750, acc: 0.828
******** [step = 350] loss: 0.756, acc: 0.827
******** [step = 400] loss: 0.759, acc: 0.827
******** [step = 450] loss: 0.765, acc: 0.826
******** [step = 500] loss: 0.770, acc: 0.826
******** [step = 550] loss: 0.775, acc: 0.825
******** [step = 600] loss: 0.778, acc: 0.825
******** [step = 650] loss: 0.783, acc: 0.824
******** [step = 700] loss: 0.787, acc: 0.823
******** [step = 750] loss: 0.788, acc: 0.823
******** [step = 800] loss: 0.792, acc: 0.823
******** [step = 850] loss: 0.795, acc: 0.823
EPOCH = 46 loss: 0.795, acc: 0.823, val_loss: 1.194, val_acc: 0.788

================================================================================2025-08_10 22:36:03
******** [step = 50] loss: 0.719, acc: 0.835
******** [step = 100] loss: 0.718, acc: 0.835
******** [step = 150] loss: 0.722, acc: 0.834
******** [step = 200] loss: 0.733, acc: 0.832
******** [step = 250] loss: 0.740, acc: 0.831
******** [step = 300] loss: 0.746, acc: 0.830
******** [step = 350] loss: 0.751, acc: 0.829
******** [step = 400] loss: 0.755, acc: 0.828
******** [step = 450] loss: 0.760, acc: 0.828
******** [step = 500] loss: 0.763, acc: 0.827
******** [step = 550] loss: 0.767, acc: 0.827
******** [step = 600] loss: 0.772, acc: 0.826
******** [step = 650] loss: 0.776, acc: 0.826
******** [step = 700] loss: 0.780, acc: 0.825
******** [step = 750] loss: 0.784, acc: 0.824
******** [step = 800] loss: 0.787, acc: 0.824
******** [step = 850] loss: 0.792, acc: 0.823
EPOCH = 47 loss: 0.792, acc: 0.823, val_loss: 1.190, val_acc: 0.789

================================================================================2025-08_10 22:37:24
******** [step = 50] loss: 0.725, acc: 0.831
******** [step = 100] loss: 0.713, acc: 0.834
******** [step = 150] loss: 0.719, acc: 0.833
******** [step = 200] loss: 0.723, acc: 0.832
******** [step = 250] loss: 0.729, acc: 0.832
******** [step = 300] loss: 0.737, acc: 0.831
******** [step = 350] loss: 0.744, acc: 0.830
******** [step = 400] loss: 0.749, acc: 0.829
******** [step = 450] loss: 0.753, acc: 0.828
******** [step = 500] loss: 0.758, acc: 0.828
******** [step = 550] loss: 0.763, acc: 0.827
******** [step = 600] loss: 0.767, acc: 0.827
******** [step = 650] loss: 0.770, acc: 0.826
******** [step = 700] loss: 0.774, acc: 0.826
******** [step = 750] loss: 0.777, acc: 0.825
******** [step = 800] loss: 0.780, acc: 0.825
******** [step = 850] loss: 0.782, acc: 0.825
EPOCH = 48 loss: 0.782, acc: 0.825, val_loss: 1.189, val_acc: 0.790

================================================================================2025-08_10 22:38:48
******** [step = 50] loss: 0.721, acc: 0.834
******** [step = 100] loss: 0.720, acc: 0.835
******** [step = 150] loss: 0.720, acc: 0.834
******** [step = 200] loss: 0.725, acc: 0.833
******** [step = 250] loss: 0.733, acc: 0.832
******** [step = 300] loss: 0.736, acc: 0.832
******** [step = 350] loss: 0.741, acc: 0.831
******** [step = 400] loss: 0.745, acc: 0.830
******** [step = 450] loss: 0.751, acc: 0.830
******** [step = 500] loss: 0.756, acc: 0.829
******** [step = 550] loss: 0.760, acc: 0.828
******** [step = 600] loss: 0.764, acc: 0.828
******** [step = 650] loss: 0.768, acc: 0.827
******** [step = 700] loss: 0.773, acc: 0.826
******** [step = 750] loss: 0.775, acc: 0.826
******** [step = 800] loss: 0.778, acc: 0.826
******** [step = 850] loss: 0.780, acc: 0.826
EPOCH = 49 loss: 0.780, acc: 0.826, val_loss: 1.188, val_acc: 0.791

================================================================================2025-08_10 22:40:08
******** [step = 50] loss: 0.687, acc: 0.841
******** [step = 100] loss: 0.697, acc: 0.838
******** [step = 150] loss: 0.709, acc: 0.836
******** [step = 200] loss: 0.715, acc: 0.835
******** [step = 250] loss: 0.721, acc: 0.834
******** [step = 300] loss: 0.728, acc: 0.832
******** [step = 350] loss: 0.733, acc: 0.832
******** [step = 400] loss: 0.738, acc: 0.831
******** [step = 450] loss: 0.744, acc: 0.830
******** [step = 500] loss: 0.748, acc: 0.830
******** [step = 550] loss: 0.753, acc: 0.829
******** [step = 600] loss: 0.757, acc: 0.829
******** [step = 650] loss: 0.762, acc: 0.828
******** [step = 700] loss: 0.766, acc: 0.828
******** [step = 750] loss: 0.769, acc: 0.827
******** [step = 800] loss: 0.772, acc: 0.827
******** [step = 850] loss: 0.776, acc: 0.826
EPOCH = 50 loss: 0.776, acc: 0.826, val_loss: 1.189, val_acc: 0.791

================================================================================2025-08_10 22:41:27
******** [step = 50] loss: 0.701, acc: 0.837
******** [step = 100] loss: 0.709, acc: 0.836
******** [step = 150] loss: 0.718, acc: 0.834
******** [step = 200] loss: 0.718, acc: 0.834
******** [step = 250] loss: 0.724, acc: 0.833
******** [step = 300] loss: 0.728, acc: 0.833
******** [step = 350] loss: 0.731, acc: 0.833
******** [step = 400] loss: 0.738, acc: 0.831
******** [step = 450] loss: 0.740, acc: 0.831
******** [step = 500] loss: 0.745, acc: 0.830
******** [step = 550] loss: 0.748, acc: 0.830
******** [step = 600] loss: 0.750, acc: 0.830
******** [step = 650] loss: 0.752, acc: 0.830
******** [step = 700] loss: 0.756, acc: 0.829
******** [step = 750] loss: 0.761, acc: 0.829
******** [step = 800] loss: 0.766, acc: 0.828
******** [step = 850] loss: 0.769, acc: 0.828
EPOCH = 51 loss: 0.769, acc: 0.828, val_loss: 1.189, val_acc: 0.791

================================================================================2025-08_10 22:42:47
******** [step = 50] loss: 0.702, acc: 0.836
******** [step = 100] loss: 0.702, acc: 0.836
******** [step = 150] loss: 0.707, acc: 0.836
******** [step = 200] loss: 0.713, acc: 0.834
******** [step = 250] loss: 0.719, acc: 0.834
******** [step = 300] loss: 0.723, acc: 0.833
******** [step = 350] loss: 0.729, acc: 0.832
******** [step = 400] loss: 0.732, acc: 0.832
******** [step = 450] loss: 0.736, acc: 0.831
******** [step = 500] loss: 0.739, acc: 0.831
******** [step = 550] loss: 0.744, acc: 0.830
******** [step = 600] loss: 0.747, acc: 0.830
******** [step = 650] loss: 0.751, acc: 0.830
******** [step = 700] loss: 0.756, acc: 0.829
******** [step = 750] loss: 0.760, acc: 0.829
******** [step = 800] loss: 0.763, acc: 0.828
******** [step = 850] loss: 0.767, acc: 0.828
EPOCH = 52 loss: 0.767, acc: 0.828, val_loss: 1.185, val_acc: 0.792

================================================================================2025-08_10 22:44:12
******** [step = 50] loss: 0.686, acc: 0.841
******** [step = 100] loss: 0.692, acc: 0.839
******** [step = 150] loss: 0.694, acc: 0.839
******** [step = 200] loss: 0.703, acc: 0.837
******** [step = 250] loss: 0.710, acc: 0.836
******** [step = 300] loss: 0.716, acc: 0.835
******** [step = 350] loss: 0.719, acc: 0.835
******** [step = 400] loss: 0.722, acc: 0.834
******** [step = 450] loss: 0.726, acc: 0.834
******** [step = 500] loss: 0.730, acc: 0.833
******** [step = 550] loss: 0.736, acc: 0.833
******** [step = 600] loss: 0.739, acc: 0.832
******** [step = 650] loss: 0.746, acc: 0.831
******** [step = 700] loss: 0.750, acc: 0.831
******** [step = 750] loss: 0.753, acc: 0.830
******** [step = 800] loss: 0.758, acc: 0.830
******** [step = 850] loss: 0.760, acc: 0.829
EPOCH = 53 loss: 0.760, acc: 0.829, val_loss: 1.180, val_acc: 0.793

================================================================================2025-08_10 22:45:34
******** [step = 50] loss: 0.693, acc: 0.839
******** [step = 100] loss: 0.695, acc: 0.839
******** [step = 150] loss: 0.698, acc: 0.838
******** [step = 200] loss: 0.700, acc: 0.838
******** [step = 250] loss: 0.708, acc: 0.836
******** [step = 300] loss: 0.711, acc: 0.836
******** [step = 350] loss: 0.716, acc: 0.835
******** [step = 400] loss: 0.721, acc: 0.835
******** [step = 450] loss: 0.726, acc: 0.834
******** [step = 500] loss: 0.730, acc: 0.833
******** [step = 550] loss: 0.735, acc: 0.833
******** [step = 600] loss: 0.739, acc: 0.832
******** [step = 650] loss: 0.743, acc: 0.832
******** [step = 700] loss: 0.746, acc: 0.832
******** [step = 750] loss: 0.750, acc: 0.831
******** [step = 800] loss: 0.753, acc: 0.831
******** [step = 850] loss: 0.755, acc: 0.831
EPOCH = 54 loss: 0.755, acc: 0.831, val_loss: 1.179, val_acc: 0.792

================================================================================2025-08_10 22:46:55
******** [step = 50] loss: 0.683, acc: 0.839
******** [step = 100] loss: 0.686, acc: 0.839
******** [step = 150] loss: 0.684, acc: 0.840
******** [step = 200] loss: 0.694, acc: 0.838
******** [step = 250] loss: 0.699, acc: 0.837
******** [step = 300] loss: 0.703, acc: 0.837
******** [step = 350] loss: 0.709, acc: 0.837
******** [step = 400] loss: 0.713, acc: 0.836
******** [step = 450] loss: 0.720, acc: 0.835
******** [step = 500] loss: 0.724, acc: 0.835
******** [step = 550] loss: 0.728, acc: 0.834
******** [step = 600] loss: 0.734, acc: 0.833
******** [step = 650] loss: 0.738, acc: 0.833
******** [step = 700] loss: 0.741, acc: 0.832
******** [step = 750] loss: 0.745, acc: 0.832
******** [step = 800] loss: 0.749, acc: 0.832
******** [step = 850] loss: 0.752, acc: 0.831
EPOCH = 55 loss: 0.752, acc: 0.831, val_loss: 1.180, val_acc: 0.793

================================================================================2025-08_10 22:48:14
******** [step = 50] loss: 0.673, acc: 0.845
******** [step = 100] loss: 0.681, acc: 0.842
******** [step = 150] loss: 0.683, acc: 0.841
******** [step = 200] loss: 0.690, acc: 0.840
******** [step = 250] loss: 0.695, acc: 0.839
******** [step = 300] loss: 0.700, acc: 0.838
******** [step = 350] loss: 0.707, acc: 0.837
******** [step = 400] loss: 0.712, acc: 0.837
******** [step = 450] loss: 0.715, acc: 0.836
******** [step = 500] loss: 0.721, acc: 0.835
******** [step = 550] loss: 0.726, acc: 0.835
******** [step = 600] loss: 0.729, acc: 0.834
******** [step = 650] loss: 0.735, acc: 0.834
******** [step = 700] loss: 0.739, acc: 0.833
******** [step = 750] loss: 0.744, acc: 0.833
******** [step = 800] loss: 0.747, acc: 0.832
******** [step = 850] loss: 0.749, acc: 0.832
EPOCH = 56 loss: 0.749, acc: 0.832, val_loss: 1.182, val_acc: 0.792

================================================================================2025-08_10 22:49:34
******** [step = 50] loss: 0.669, acc: 0.845
******** [step = 100] loss: 0.668, acc: 0.844
******** [step = 150] loss: 0.685, acc: 0.841
******** [step = 200] loss: 0.686, acc: 0.841
******** [step = 250] loss: 0.695, acc: 0.839
******** [step = 300] loss: 0.702, acc: 0.838
******** [step = 350] loss: 0.704, acc: 0.838
******** [step = 400] loss: 0.707, acc: 0.837
******** [step = 450] loss: 0.710, acc: 0.837
******** [step = 500] loss: 0.712, acc: 0.837
******** [step = 550] loss: 0.718, acc: 0.836
******** [step = 600] loss: 0.724, acc: 0.835
******** [step = 650] loss: 0.728, acc: 0.835
******** [step = 700] loss: 0.733, acc: 0.834
******** [step = 750] loss: 0.737, acc: 0.834
******** [step = 800] loss: 0.740, acc: 0.833
******** [step = 850] loss: 0.743, acc: 0.833
EPOCH = 57 loss: 0.743, acc: 0.833, val_loss: 1.178, val_acc: 0.794

================================================================================2025-08_10 22:50:54
******** [step = 50] loss: 0.682, acc: 0.840
******** [step = 100] loss: 0.684, acc: 0.841
******** [step = 150] loss: 0.686, acc: 0.841
******** [step = 200] loss: 0.685, acc: 0.841
******** [step = 250] loss: 0.688, acc: 0.840
******** [step = 300] loss: 0.694, acc: 0.839
******** [step = 350] loss: 0.700, acc: 0.838
******** [step = 400] loss: 0.704, acc: 0.838
******** [step = 450] loss: 0.709, acc: 0.837
******** [step = 500] loss: 0.713, acc: 0.836
******** [step = 550] loss: 0.719, acc: 0.836
******** [step = 600] loss: 0.721, acc: 0.836
******** [step = 650] loss: 0.724, acc: 0.835
******** [step = 700] loss: 0.728, acc: 0.835
******** [step = 750] loss: 0.732, acc: 0.834
******** [step = 800] loss: 0.737, acc: 0.834
******** [step = 850] loss: 0.739, acc: 0.833
EPOCH = 58 loss: 0.739, acc: 0.833, val_loss: 1.181, val_acc: 0.794

================================================================================2025-08_10 22:52:15
******** [step = 50] loss: 0.657, acc: 0.845
******** [step = 100] loss: 0.660, acc: 0.846
******** [step = 150] loss: 0.667, acc: 0.845
******** [step = 200] loss: 0.673, acc: 0.844
******** [step = 250] loss: 0.681, acc: 0.843
******** [step = 300] loss: 0.692, acc: 0.841
******** [step = 350] loss: 0.699, acc: 0.840
******** [step = 400] loss: 0.704, acc: 0.839
******** [step = 450] loss: 0.708, acc: 0.838
******** [step = 500] loss: 0.712, acc: 0.837
******** [step = 550] loss: 0.716, acc: 0.837
******** [step = 600] loss: 0.720, acc: 0.836
******** [step = 650] loss: 0.723, acc: 0.836
******** [step = 700] loss: 0.726, acc: 0.836
******** [step = 750] loss: 0.729, acc: 0.835
******** [step = 800] loss: 0.732, acc: 0.835
******** [step = 850] loss: 0.735, acc: 0.835
EPOCH = 59 loss: 0.735, acc: 0.835, val_loss: 1.176, val_acc: 0.794

================================================================================2025-08_10 22:53:39
******** [step = 50] loss: 0.667, acc: 0.844
******** [step = 100] loss: 0.668, acc: 0.844
******** [step = 150] loss: 0.669, acc: 0.844
******** [step = 200] loss: 0.675, acc: 0.842
******** [step = 250] loss: 0.679, acc: 0.842
******** [step = 300] loss: 0.687, acc: 0.841
******** [step = 350] loss: 0.692, acc: 0.840
******** [step = 400] loss: 0.696, acc: 0.839
******** [step = 450] loss: 0.701, acc: 0.839
******** [step = 500] loss: 0.705, acc: 0.838
******** [step = 550] loss: 0.708, acc: 0.838
******** [step = 600] loss: 0.713, acc: 0.837
******** [step = 650] loss: 0.718, acc: 0.836
******** [step = 700] loss: 0.721, acc: 0.836
******** [step = 750] loss: 0.725, acc: 0.836
******** [step = 800] loss: 0.727, acc: 0.835
******** [step = 850] loss: 0.732, acc: 0.835
EPOCH = 60 loss: 0.732, acc: 0.835, val_loss: 1.177, val_acc: 0.794

================================================================================2025-08_10 22:55:01
******** [step = 50] loss: 0.651, acc: 0.845
******** [step = 100] loss: 0.652, acc: 0.845
******** [step = 150] loss: 0.664, acc: 0.844
******** [step = 200] loss: 0.670, acc: 0.843
******** [step = 250] loss: 0.674, acc: 0.843
******** [step = 300] loss: 0.679, acc: 0.842
******** [step = 350] loss: 0.685, acc: 0.841
******** [step = 400] loss: 0.690, acc: 0.841
******** [step = 450] loss: 0.695, acc: 0.840
******** [step = 500] loss: 0.699, acc: 0.839
******** [step = 550] loss: 0.703, acc: 0.839
******** [step = 600] loss: 0.708, acc: 0.838
******** [step = 650] loss: 0.714, acc: 0.837
******** [step = 700] loss: 0.718, acc: 0.837
******** [step = 750] loss: 0.722, acc: 0.836
******** [step = 800] loss: 0.726, acc: 0.836
******** [step = 850] loss: 0.728, acc: 0.836
EPOCH = 61 loss: 0.728, acc: 0.836, val_loss: 1.175, val_acc: 0.795

================================================================================2025-08_10 22:56:23
******** [step = 50] loss: 0.652, acc: 0.848
******** [step = 100] loss: 0.666, acc: 0.844
******** [step = 150] loss: 0.668, acc: 0.844
******** [step = 200] loss: 0.670, acc: 0.844
******** [step = 250] loss: 0.673, acc: 0.844
******** [step = 300] loss: 0.676, acc: 0.843
******** [step = 350] loss: 0.684, acc: 0.842
******** [step = 400] loss: 0.689, acc: 0.841
******** [step = 450] loss: 0.694, acc: 0.840
******** [step = 500] loss: 0.697, acc: 0.840
******** [step = 550] loss: 0.702, acc: 0.839
******** [step = 600] loss: 0.705, acc: 0.839
******** [step = 650] loss: 0.709, acc: 0.838
******** [step = 700] loss: 0.714, acc: 0.838
******** [step = 750] loss: 0.719, acc: 0.837
******** [step = 800] loss: 0.722, acc: 0.837
******** [step = 850] loss: 0.724, acc: 0.837
EPOCH = 62 loss: 0.724, acc: 0.837, val_loss: 1.175, val_acc: 0.794

================================================================================2025-08_10 22:57:43
******** [step = 50] loss: 0.655, acc: 0.846
******** [step = 100] loss: 0.654, acc: 0.846
******** [step = 150] loss: 0.663, acc: 0.845
******** [step = 200] loss: 0.663, acc: 0.845
******** [step = 250] loss: 0.670, acc: 0.844
******** [step = 300] loss: 0.674, acc: 0.844
******** [step = 350] loss: 0.678, acc: 0.843
******** [step = 400] loss: 0.684, acc: 0.842
******** [step = 450] loss: 0.689, acc: 0.841
******** [step = 500] loss: 0.694, acc: 0.840
******** [step = 550] loss: 0.700, acc: 0.840
******** [step = 600] loss: 0.705, acc: 0.839
******** [step = 650] loss: 0.707, acc: 0.839
******** [step = 700] loss: 0.712, acc: 0.839
******** [step = 750] loss: 0.715, acc: 0.838
******** [step = 800] loss: 0.717, acc: 0.838
******** [step = 850] loss: 0.721, acc: 0.838
EPOCH = 63 loss: 0.721, acc: 0.838, val_loss: 1.174, val_acc: 0.797

================================================================================2025-08_10 22:59:03
******** [step = 50] loss: 0.645, acc: 0.850
******** [step = 100] loss: 0.645, acc: 0.850
******** [step = 150] loss: 0.651, acc: 0.848
******** [step = 200] loss: 0.658, acc: 0.846
******** [step = 250] loss: 0.661, acc: 0.845
******** [step = 300] loss: 0.668, acc: 0.844
******** [step = 350] loss: 0.675, acc: 0.843
******** [step = 400] loss: 0.677, acc: 0.843
******** [step = 450] loss: 0.682, acc: 0.842
******** [step = 500] loss: 0.686, acc: 0.842
******** [step = 550] loss: 0.692, acc: 0.841
******** [step = 600] loss: 0.696, acc: 0.841
******** [step = 650] loss: 0.702, acc: 0.840
******** [step = 700] loss: 0.705, acc: 0.840
******** [step = 750] loss: 0.710, acc: 0.839
******** [step = 800] loss: 0.714, acc: 0.838
******** [step = 850] loss: 0.718, acc: 0.838
EPOCH = 64 loss: 0.718, acc: 0.838, val_loss: 1.170, val_acc: 0.796

================================================================================2025-08_10 23:00:24
******** [step = 50] loss: 0.644, acc: 0.848
******** [step = 100] loss: 0.647, acc: 0.848
******** [step = 150] loss: 0.652, acc: 0.847
******** [step = 200] loss: 0.660, acc: 0.845
******** [step = 250] loss: 0.663, acc: 0.845
******** [step = 300] loss: 0.669, acc: 0.844
******** [step = 350] loss: 0.676, acc: 0.843
******** [step = 400] loss: 0.679, acc: 0.843
******** [step = 450] loss: 0.684, acc: 0.842
******** [step = 500] loss: 0.689, acc: 0.841
******** [step = 550] loss: 0.692, acc: 0.841
******** [step = 600] loss: 0.695, acc: 0.841
******** [step = 650] loss: 0.699, acc: 0.840
******** [step = 700] loss: 0.704, acc: 0.840
******** [step = 750] loss: 0.708, acc: 0.839
******** [step = 800] loss: 0.711, acc: 0.839
******** [step = 850] loss: 0.714, acc: 0.839
EPOCH = 65 loss: 0.714, acc: 0.839, val_loss: 1.170, val_acc: 0.797

================================================================================2025-08_10 23:01:45
******** [step = 50] loss: 0.637, acc: 0.851
******** [step = 100] loss: 0.648, acc: 0.848
******** [step = 150] loss: 0.657, acc: 0.846
******** [step = 200] loss: 0.660, acc: 0.846
******** [step = 250] loss: 0.664, acc: 0.845
******** [step = 300] loss: 0.671, acc: 0.844
******** [step = 350] loss: 0.675, acc: 0.843
******** [step = 400] loss: 0.678, acc: 0.843
******** [step = 450] loss: 0.682, acc: 0.843
******** [step = 500] loss: 0.685, acc: 0.842
******** [step = 550] loss: 0.690, acc: 0.842
******** [step = 600] loss: 0.693, acc: 0.841
******** [step = 650] loss: 0.696, acc: 0.841
******** [step = 700] loss: 0.699, acc: 0.841
******** [step = 750] loss: 0.705, acc: 0.840
******** [step = 800] loss: 0.708, acc: 0.840
******** [step = 850] loss: 0.711, acc: 0.839
EPOCH = 66 loss: 0.711, acc: 0.839, val_loss: 1.170, val_acc: 0.797

================================================================================2025-08_10 23:03:04
******** [step = 50] loss: 0.641, acc: 0.849
******** [step = 100] loss: 0.647, acc: 0.849
******** [step = 150] loss: 0.645, acc: 0.849
******** [step = 200] loss: 0.651, acc: 0.848
******** [step = 250] loss: 0.657, acc: 0.847
******** [step = 300] loss: 0.662, acc: 0.846
******** [step = 350] loss: 0.668, acc: 0.845
******** [step = 400] loss: 0.673, acc: 0.844
******** [step = 450] loss: 0.676, acc: 0.844
******** [step = 500] loss: 0.682, acc: 0.843
******** [step = 550] loss: 0.686, acc: 0.843
******** [step = 600] loss: 0.688, acc: 0.842
******** [step = 650] loss: 0.693, acc: 0.842
******** [step = 700] loss: 0.697, acc: 0.841
******** [step = 750] loss: 0.701, acc: 0.841
******** [step = 800] loss: 0.704, acc: 0.841
******** [step = 850] loss: 0.709, acc: 0.840
EPOCH = 67 loss: 0.709, acc: 0.840, val_loss: 1.171, val_acc: 0.796

================================================================================2025-08_10 23:04:24
******** [step = 50] loss: 0.628, acc: 0.851
******** [step = 100] loss: 0.633, acc: 0.850
******** [step = 150] loss: 0.642, acc: 0.849
******** [step = 200] loss: 0.645, acc: 0.848
******** [step = 250] loss: 0.654, acc: 0.847
******** [step = 300] loss: 0.657, acc: 0.847
******** [step = 350] loss: 0.663, acc: 0.846
******** [step = 400] loss: 0.669, acc: 0.845
******** [step = 450] loss: 0.674, acc: 0.845
******** [step = 500] loss: 0.679, acc: 0.844
******** [step = 550] loss: 0.682, acc: 0.843
******** [step = 600] loss: 0.685, acc: 0.843
******** [step = 650] loss: 0.691, acc: 0.842
******** [step = 700] loss: 0.695, acc: 0.842
******** [step = 750] loss: 0.698, acc: 0.841
******** [step = 800] loss: 0.701, acc: 0.841
******** [step = 850] loss: 0.704, acc: 0.841
EPOCH = 68 loss: 0.704, acc: 0.841, val_loss: 1.167, val_acc: 0.798

================================================================================2025-08_10 23:05:51
******** [step = 50] loss: 0.637, acc: 0.848
******** [step = 100] loss: 0.632, acc: 0.850
******** [step = 150] loss: 0.633, acc: 0.851
******** [step = 200] loss: 0.641, acc: 0.850
******** [step = 250] loss: 0.645, acc: 0.849
******** [step = 300] loss: 0.649, acc: 0.848
******** [step = 350] loss: 0.656, acc: 0.847
******** [step = 400] loss: 0.664, acc: 0.846
******** [step = 450] loss: 0.668, acc: 0.845
******** [step = 500] loss: 0.673, acc: 0.844
******** [step = 550] loss: 0.679, acc: 0.844
******** [step = 600] loss: 0.683, acc: 0.843
******** [step = 650] loss: 0.687, acc: 0.843
******** [step = 700] loss: 0.691, acc: 0.842
******** [step = 750] loss: 0.694, acc: 0.842
******** [step = 800] loss: 0.698, acc: 0.842
******** [step = 850] loss: 0.700, acc: 0.841
EPOCH = 69 loss: 0.700, acc: 0.841, val_loss: 1.175, val_acc: 0.797

================================================================================2025-08_10 23:07:20
******** [step = 50] loss: 0.642, acc: 0.850
******** [step = 100] loss: 0.637, acc: 0.850
******** [step = 150] loss: 0.644, acc: 0.849
******** [step = 200] loss: 0.646, acc: 0.848
******** [step = 250] loss: 0.655, acc: 0.847
******** [step = 300] loss: 0.655, acc: 0.847
******** [step = 350] loss: 0.658, acc: 0.847
******** [step = 400] loss: 0.662, acc: 0.846
******** [step = 450] loss: 0.666, acc: 0.846
******** [step = 500] loss: 0.670, acc: 0.845
******** [step = 550] loss: 0.673, acc: 0.845
******** [step = 600] loss: 0.678, acc: 0.844
******** [step = 650] loss: 0.682, acc: 0.844
******** [step = 700] loss: 0.685, acc: 0.843
******** [step = 750] loss: 0.690, acc: 0.843
******** [step = 800] loss: 0.694, acc: 0.842
******** [step = 850] loss: 0.698, acc: 0.842
EPOCH = 70 loss: 0.698, acc: 0.842, val_loss: 1.165, val_acc: 0.798

================================================================================2025-08_10 23:08:41
******** [step = 50] loss: 0.625, acc: 0.853
******** [step = 100] loss: 0.625, acc: 0.853
******** [step = 150] loss: 0.626, acc: 0.853
******** [step = 200] loss: 0.636, acc: 0.851
******** [step = 250] loss: 0.642, acc: 0.850
******** [step = 300] loss: 0.648, acc: 0.849
******** [step = 350] loss: 0.657, acc: 0.847
******** [step = 400] loss: 0.662, acc: 0.846
******** [step = 450] loss: 0.666, acc: 0.846
******** [step = 500] loss: 0.670, acc: 0.845
******** [step = 550] loss: 0.673, acc: 0.845
******** [step = 600] loss: 0.677, acc: 0.845
******** [step = 650] loss: 0.681, acc: 0.844
******** [step = 700] loss: 0.684, acc: 0.844
******** [step = 750] loss: 0.687, acc: 0.844
******** [step = 800] loss: 0.691, acc: 0.843
******** [step = 850] loss: 0.694, acc: 0.843
EPOCH = 71 loss: 0.694, acc: 0.843, val_loss: 1.170, val_acc: 0.797

================================================================================2025-08_10 23:10:02
******** [step = 50] loss: 0.625, acc: 0.853
******** [step = 100] loss: 0.626, acc: 0.852
******** [step = 150] loss: 0.635, acc: 0.851
******** [step = 200] loss: 0.640, acc: 0.850
******** [step = 250] loss: 0.642, acc: 0.850
******** [step = 300] loss: 0.648, acc: 0.848
******** [step = 350] loss: 0.651, acc: 0.848
******** [step = 400] loss: 0.656, acc: 0.847
******** [step = 450] loss: 0.660, acc: 0.847
******** [step = 500] loss: 0.665, acc: 0.846
******** [step = 550] loss: 0.669, acc: 0.845
******** [step = 600] loss: 0.673, acc: 0.845
******** [step = 650] loss: 0.677, acc: 0.844
******** [step = 700] loss: 0.681, acc: 0.844
******** [step = 750] loss: 0.685, acc: 0.843
******** [step = 800] loss: 0.688, acc: 0.843
******** [step = 850] loss: 0.692, acc: 0.843
EPOCH = 72 loss: 0.692, acc: 0.843, val_loss: 1.168, val_acc: 0.798

================================================================================2025-08_10 23:11:23
******** [step = 50] loss: 0.618, acc: 0.853
******** [step = 100] loss: 0.611, acc: 0.855
******** [step = 150] loss: 0.618, acc: 0.854
******** [step = 200] loss: 0.628, acc: 0.852
******** [step = 250] loss: 0.636, acc: 0.850
******** [step = 300] loss: 0.641, acc: 0.849
******** [step = 350] loss: 0.645, acc: 0.849
******** [step = 400] loss: 0.651, acc: 0.848
******** [step = 450] loss: 0.655, acc: 0.848
******** [step = 500] loss: 0.661, acc: 0.847
******** [step = 550] loss: 0.663, acc: 0.847
******** [step = 600] loss: 0.668, acc: 0.846
******** [step = 650] loss: 0.673, acc: 0.845
******** [step = 700] loss: 0.678, acc: 0.845
******** [step = 750] loss: 0.682, acc: 0.844
******** [step = 800] loss: 0.685, acc: 0.844
******** [step = 850] loss: 0.688, acc: 0.844
EPOCH = 73 loss: 0.688, acc: 0.844, val_loss: 1.171, val_acc: 0.798

================================================================================2025-08_10 23:12:51
******** [step = 50] loss: 0.619, acc: 0.855
******** [step = 100] loss: 0.623, acc: 0.854
******** [step = 150] loss: 0.629, acc: 0.853
******** [step = 200] loss: 0.635, acc: 0.852
******** [step = 250] loss: 0.639, acc: 0.851
******** [step = 300] loss: 0.645, acc: 0.850
******** [step = 350] loss: 0.649, acc: 0.849
******** [step = 400] loss: 0.653, acc: 0.849
******** [step = 450] loss: 0.656, acc: 0.848
******** [step = 500] loss: 0.660, acc: 0.848
******** [step = 550] loss: 0.663, acc: 0.847
******** [step = 600] loss: 0.668, acc: 0.847
******** [step = 650] loss: 0.670, acc: 0.847
******** [step = 700] loss: 0.675, acc: 0.846
******** [step = 750] loss: 0.677, acc: 0.846
******** [step = 800] loss: 0.681, acc: 0.845
******** [step = 850] loss: 0.684, acc: 0.845
EPOCH = 74 loss: 0.684, acc: 0.845, val_loss: 1.167, val_acc: 0.799

================================================================================2025-08_10 23:14:21
******** [step = 50] loss: 0.602, acc: 0.858
******** [step = 100] loss: 0.608, acc: 0.856
******** [step = 150] loss: 0.621, acc: 0.853
******** [step = 200] loss: 0.624, acc: 0.853
******** [step = 250] loss: 0.634, acc: 0.851
******** [step = 300] loss: 0.640, acc: 0.850
******** [step = 350] loss: 0.643, acc: 0.850
******** [step = 400] loss: 0.644, acc: 0.850
******** [step = 450] loss: 0.649, acc: 0.849
******** [step = 500] loss: 0.653, acc: 0.848
******** [step = 550] loss: 0.657, acc: 0.848
******** [step = 600] loss: 0.661, acc: 0.847
******** [step = 650] loss: 0.665, acc: 0.847
******** [step = 700] loss: 0.670, acc: 0.846
******** [step = 750] loss: 0.675, acc: 0.846
******** [step = 800] loss: 0.678, acc: 0.845
******** [step = 850] loss: 0.683, acc: 0.845
EPOCH = 75 loss: 0.683, acc: 0.845, val_loss: 1.167, val_acc: 0.798

================================================================================2025-08_10 23:15:41
******** [step = 50] loss: 0.614, acc: 0.856
******** [step = 100] loss: 0.615, acc: 0.855
******** [step = 150] loss: 0.618, acc: 0.854
******** [step = 200] loss: 0.625, acc: 0.853
******** [step = 250] loss: 0.628, acc: 0.852
******** [step = 300] loss: 0.633, acc: 0.852
******** [step = 350] loss: 0.635, acc: 0.851
******** [step = 400] loss: 0.640, acc: 0.851
******** [step = 450] loss: 0.645, acc: 0.850
******** [step = 500] loss: 0.651, acc: 0.849
******** [step = 550] loss: 0.655, acc: 0.849
******** [step = 600] loss: 0.660, acc: 0.848
******** [step = 650] loss: 0.665, acc: 0.847
******** [step = 700] loss: 0.669, acc: 0.847
******** [step = 750] loss: 0.672, acc: 0.846
******** [step = 800] loss: 0.676, acc: 0.846
******** [step = 850] loss: 0.679, acc: 0.845
EPOCH = 76 loss: 0.679, acc: 0.845, val_loss: 1.170, val_acc: 0.799

================================================================================2025-08_10 23:17:02
******** [step = 50] loss: 0.619, acc: 0.853
******** [step = 100] loss: 0.626, acc: 0.852
******** [step = 150] loss: 0.630, acc: 0.852
******** [step = 200] loss: 0.630, acc: 0.852
******** [step = 250] loss: 0.631, acc: 0.852
******** [step = 300] loss: 0.634, acc: 0.852
******** [step = 350] loss: 0.639, acc: 0.851
******** [step = 400] loss: 0.643, acc: 0.850
******** [step = 450] loss: 0.646, acc: 0.850
******** [step = 500] loss: 0.649, acc: 0.850
******** [step = 550] loss: 0.653, acc: 0.849
******** [step = 600] loss: 0.656, acc: 0.849
******** [step = 650] loss: 0.659, acc: 0.848
******** [step = 700] loss: 0.663, acc: 0.848
******** [step = 750] loss: 0.667, acc: 0.847
******** [step = 800] loss: 0.671, acc: 0.847
******** [step = 850] loss: 0.674, acc: 0.846
EPOCH = 77 loss: 0.674, acc: 0.846, val_loss: 1.164, val_acc: 0.800

================================================================================2025-08_10 23:18:21
******** [step = 50] loss: 0.612, acc: 0.855
******** [step = 100] loss: 0.604, acc: 0.857
******** [step = 150] loss: 0.608, acc: 0.856
******** [step = 200] loss: 0.617, acc: 0.855
******** [step = 250] loss: 0.622, acc: 0.854
******** [step = 300] loss: 0.629, acc: 0.852
******** [step = 350] loss: 0.633, acc: 0.852
******** [step = 400] loss: 0.637, acc: 0.851
******** [step = 450] loss: 0.642, acc: 0.851
******** [step = 500] loss: 0.646, acc: 0.850
******** [step = 550] loss: 0.651, acc: 0.849
******** [step = 600] loss: 0.657, acc: 0.848
******** [step = 650] loss: 0.661, acc: 0.848
******** [step = 700] loss: 0.665, acc: 0.848
******** [step = 750] loss: 0.668, acc: 0.847
******** [step = 800] loss: 0.672, acc: 0.847
******** [step = 850] loss: 0.675, acc: 0.846
EPOCH = 78 loss: 0.675, acc: 0.846, val_loss: 1.163, val_acc: 0.799

================================================================================2025-08_10 23:19:42
******** [step = 50] loss: 0.600, acc: 0.858
******** [step = 100] loss: 0.606, acc: 0.856
******** [step = 150] loss: 0.611, acc: 0.855
******** [step = 200] loss: 0.621, acc: 0.854
******** [step = 250] loss: 0.628, acc: 0.853
******** [step = 300] loss: 0.630, acc: 0.852
******** [step = 350] loss: 0.634, acc: 0.852
******** [step = 400] loss: 0.639, acc: 0.851
******** [step = 450] loss: 0.643, acc: 0.851
******** [step = 500] loss: 0.648, acc: 0.850
******** [step = 550] loss: 0.651, acc: 0.850
******** [step = 600] loss: 0.654, acc: 0.849
******** [step = 650] loss: 0.657, acc: 0.849
******** [step = 700] loss: 0.661, acc: 0.848
******** [step = 750] loss: 0.665, acc: 0.848
******** [step = 800] loss: 0.668, acc: 0.847
******** [step = 850] loss: 0.671, acc: 0.847
EPOCH = 79 loss: 0.671, acc: 0.847, val_loss: 1.166, val_acc: 0.798

================================================================================2025-08_10 23:21:01
******** [step = 50] loss: 0.609, acc: 0.856
******** [step = 100] loss: 0.603, acc: 0.858
******** [step = 150] loss: 0.608, acc: 0.856
******** [step = 200] loss: 0.615, acc: 0.855
******** [step = 250] loss: 0.623, acc: 0.853
******** [step = 300] loss: 0.626, acc: 0.853
******** [step = 350] loss: 0.630, acc: 0.853
******** [step = 400] loss: 0.634, acc: 0.852
******** [step = 450] loss: 0.639, acc: 0.851
******** [step = 500] loss: 0.644, acc: 0.851
******** [step = 550] loss: 0.647, acc: 0.850
******** [step = 600] loss: 0.650, acc: 0.850
******** [step = 650] loss: 0.653, acc: 0.850
******** [step = 700] loss: 0.658, acc: 0.849
******** [step = 750] loss: 0.662, acc: 0.849
******** [step = 800] loss: 0.666, acc: 0.848
******** [step = 850] loss: 0.670, acc: 0.847
EPOCH = 80 loss: 0.670, acc: 0.847, val_loss: 1.164, val_acc: 0.800

================================================================================2025-08_10 23:22:27
finishing training...
Training complete in 110m 41s
    epoch  ...   val_acc
0     1.0  ...  0.438392
1     2.0  ...  0.503787
2     3.0  ...  0.559841
3     4.0  ...  0.602116
4     5.0  ...  0.633509
..    ...  ...       ...
75   76.0  ...  0.798583
76   77.0  ...  0.800225
77   78.0  ...  0.798991
78   79.0  ...  0.798271
79   80.0  ...  0.799641

[80 rows x 5 columns]
== Done ==
Sun Aug 10 11:22:54 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 23:22:56
Job ID:        6785413
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=07:32:40,vmem=0,walltime=01:53:10,mem=58096K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-005-29-0
---------------------------------------
