---------------------------------------
Begin Slurm Prolog: Aug-13-2025 17:36:12
Job ID:    7023009
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Wed Aug 13 05:36:12 PM EDT 2025
atl1-1-02-008-35-0.pace.gatech.edu
== GPU check ==
Wed Aug 13 17:36:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   41C    P0             30W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 109243.30it/s] 28%|██▊       | 38691/135842 [00:00<00:00, 161852.64it/s] 41%|████▏     | 56048/135842 [00:00<00:00, 123096.78it/s] 51%|█████▏    | 69722/135842 [00:00<00:00, 96194.18it/s]  66%|██████▌   | 89772/135842 [00:00<00:00, 121765.17it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 91606.30it/s] 92%|█████████▏| 125508/135842 [00:01<00:00, 110088.98it/s]100%|██████████| 135842/135842 [00:01<00:00, 113543.14it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 48325.67it/s] 33%|███▎      | 35613/108673 [00:00<00:00, 86354.18it/s] 49%|████▉     | 53757/108673 [00:00<00:00, 113803.52it/s] 66%|██████▌   | 71957/108673 [00:00<00:00, 133712.97it/s] 81%|████████▏ | 88334/108673 [00:01<00:00, 72228.35it/s]  98%|█████████▊| 106506/108673 [00:01<00:00, 91456.97it/s]100%|██████████| 108673/108673 [00:01<00:00, 90343.30it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17958/27169 [00:00<00:00, 179569.47it/s]100%|██████████| 27169/27169 [00:00<00:00, 181326.86it/s]
tensor([  3,  47,  20,  13,  17,  61,   6,  37,  34,  11,  51, 945,  14,  65,
        538, 194, 462,  45, 450,   4,   2,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,
          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,    5,   71,   13,   24,   99,  103,   52,   97,   82,  303,   17,
         388, 1472,    7,   37,   18, 1196,    4,    2,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,    5,   51,   25,  162,   23,  104,   15,    6,   76,   34,  506,
          21, 1011,    4,    2,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_13 17:36:52
******** [step = 50] loss: 9.249, acc: 0.021
******** [step = 100] loss: 8.890, acc: 0.093
******** [step = 150] loss: 8.552, acc: 0.139
******** [step = 200] loss: 8.230, acc: 0.166
******** [step = 250] loss: 7.926, acc: 0.184
******** [step = 300] loss: 7.629, acc: 0.196
******** [step = 350] loss: 7.338, acc: 0.207
******** [step = 400] loss: 7.069, acc: 0.215
******** [step = 450] loss: 6.826, acc: 0.224
******** [step = 500] loss: 6.611, acc: 0.233
******** [step = 550] loss: 6.423, acc: 0.241
******** [step = 600] loss: 6.257, acc: 0.248
******** [step = 650] loss: 6.107, acc: 0.255
******** [step = 700] loss: 5.975, acc: 0.261
******** [step = 750] loss: 5.853, acc: 0.267
******** [step = 800] loss: 5.747, acc: 0.271
******** [step = 850] loss: 5.645, acc: 0.276
EPOCH = 1 loss: 5.645, acc: 0.276, val_loss: 3.969, val_acc: 0.362

================================================================================2025-08_13 17:38:56
******** [step = 50] loss: 4.002, acc: 0.351
******** [step = 100] loss: 3.973, acc: 0.353
******** [step = 150] loss: 3.943, acc: 0.354
******** [step = 200] loss: 3.930, acc: 0.355
******** [step = 250] loss: 3.914, acc: 0.356
******** [step = 300] loss: 3.900, acc: 0.357
******** [step = 350] loss: 3.884, acc: 0.358
******** [step = 400] loss: 3.870, acc: 0.359
******** [step = 450] loss: 3.852, acc: 0.360
******** [step = 500] loss: 3.839, acc: 0.361
******** [step = 550] loss: 3.826, acc: 0.362
******** [step = 600] loss: 3.814, acc: 0.363
******** [step = 650] loss: 3.803, acc: 0.364
******** [step = 700] loss: 3.793, acc: 0.364
******** [step = 750] loss: 3.782, acc: 0.365
******** [step = 800] loss: 3.772, acc: 0.366
******** [step = 850] loss: 3.764, acc: 0.366
EPOCH = 2 loss: 3.764, acc: 0.366, val_loss: 3.537, val_acc: 0.389

================================================================================2025-08_13 17:41:00
******** [step = 50] loss: 3.536, acc: 0.378
******** [step = 100] loss: 3.523, acc: 0.379
******** [step = 150] loss: 3.517, acc: 0.381
******** [step = 200] loss: 3.512, acc: 0.381
******** [step = 250] loss: 3.515, acc: 0.381
******** [step = 300] loss: 3.511, acc: 0.381
******** [step = 350] loss: 3.511, acc: 0.381
******** [step = 400] loss: 3.506, acc: 0.382
******** [step = 450] loss: 3.504, acc: 0.382
******** [step = 500] loss: 3.501, acc: 0.382
******** [step = 550] loss: 3.499, acc: 0.383
******** [step = 600] loss: 3.495, acc: 0.383
******** [step = 650] loss: 3.493, acc: 0.384
******** [step = 700] loss: 3.490, acc: 0.384
******** [step = 750] loss: 3.485, acc: 0.384
******** [step = 800] loss: 3.482, acc: 0.385
******** [step = 850] loss: 3.478, acc: 0.385
EPOCH = 3 loss: 3.478, acc: 0.385, val_loss: 3.341, val_acc: 0.406

================================================================================2025-08_13 17:43:01
******** [step = 50] loss: 3.336, acc: 0.393
******** [step = 100] loss: 3.342, acc: 0.393
******** [step = 150] loss: 3.336, acc: 0.394
******** [step = 200] loss: 3.331, acc: 0.395
******** [step = 250] loss: 3.332, acc: 0.395
******** [step = 300] loss: 3.333, acc: 0.396
******** [step = 350] loss: 3.329, acc: 0.396
******** [step = 400] loss: 3.329, acc: 0.397
******** [step = 450] loss: 3.328, acc: 0.397
******** [step = 500] loss: 3.327, acc: 0.397
******** [step = 550] loss: 3.325, acc: 0.397
******** [step = 600] loss: 3.323, acc: 0.397
******** [step = 650] loss: 3.322, acc: 0.397
******** [step = 700] loss: 3.319, acc: 0.398
******** [step = 750] loss: 3.318, acc: 0.398
******** [step = 800] loss: 3.318, acc: 0.398
******** [step = 850] loss: 3.316, acc: 0.398
EPOCH = 4 loss: 3.316, acc: 0.398, val_loss: 3.250, val_acc: 0.411

================================================================================2025-08_13 17:45:02
******** [step = 50] loss: 3.209, acc: 0.404
******** [step = 100] loss: 3.213, acc: 0.405
******** [step = 150] loss: 3.213, acc: 0.405
******** [step = 200] loss: 3.212, acc: 0.405
******** [step = 250] loss: 3.206, acc: 0.406
******** [step = 300] loss: 3.207, acc: 0.406
******** [step = 350] loss: 3.207, acc: 0.406
******** [step = 400] loss: 3.209, acc: 0.406
******** [step = 450] loss: 3.207, acc: 0.407
******** [step = 500] loss: 3.207, acc: 0.406
******** [step = 550] loss: 3.209, acc: 0.406
******** [step = 600] loss: 3.211, acc: 0.406
******** [step = 650] loss: 3.212, acc: 0.406
******** [step = 700] loss: 3.210, acc: 0.407
******** [step = 750] loss: 3.209, acc: 0.407
******** [step = 800] loss: 3.208, acc: 0.407
******** [step = 850] loss: 3.205, acc: 0.407
EPOCH = 5 loss: 3.205, acc: 0.407, val_loss: 3.154, val_acc: 0.422

================================================================================2025-08_13 17:47:04
******** [step = 50] loss: 3.100, acc: 0.414
******** [step = 100] loss: 3.096, acc: 0.415
******** [step = 150] loss: 3.101, acc: 0.415
******** [step = 200] loss: 3.097, acc: 0.416
******** [step = 250] loss: 3.102, acc: 0.416
******** [step = 300] loss: 3.102, acc: 0.415
******** [step = 350] loss: 3.099, acc: 0.416
******** [step = 400] loss: 3.097, acc: 0.416
******** [step = 450] loss: 3.098, acc: 0.416
******** [step = 500] loss: 3.098, acc: 0.416
******** [step = 550] loss: 3.095, acc: 0.416
******** [step = 600] loss: 3.095, acc: 0.416
******** [step = 650] loss: 3.096, acc: 0.417
******** [step = 700] loss: 3.097, acc: 0.417
******** [step = 750] loss: 3.097, acc: 0.417
******** [step = 800] loss: 3.097, acc: 0.417
******** [step = 850] loss: 3.095, acc: 0.417
EPOCH = 6 loss: 3.095, acc: 0.417, val_loss: 3.067, val_acc: 0.432

================================================================================2025-08_13 17:49:05
******** [step = 50] loss: 2.976, acc: 0.425
******** [step = 100] loss: 2.971, acc: 0.426
******** [step = 150] loss: 2.974, acc: 0.426
******** [step = 200] loss: 2.973, acc: 0.427
******** [step = 250] loss: 2.975, acc: 0.427
******** [step = 300] loss: 2.981, acc: 0.427
******** [step = 350] loss: 2.984, acc: 0.427
******** [step = 400] loss: 2.986, acc: 0.427
******** [step = 450] loss: 2.987, acc: 0.427
******** [step = 500] loss: 2.987, acc: 0.427
******** [step = 550] loss: 2.988, acc: 0.428
******** [step = 600] loss: 2.990, acc: 0.428
******** [step = 650] loss: 2.992, acc: 0.428
******** [step = 700] loss: 2.993, acc: 0.428
******** [step = 750] loss: 2.993, acc: 0.428
******** [step = 800] loss: 2.995, acc: 0.428
******** [step = 850] loss: 2.995, acc: 0.428
EPOCH = 7 loss: 2.995, acc: 0.428, val_loss: 3.011, val_acc: 0.439

================================================================================2025-08_13 17:51:06
******** [step = 50] loss: 2.897, acc: 0.434
******** [step = 100] loss: 2.878, acc: 0.437
******** [step = 150] loss: 2.881, acc: 0.437
******** [step = 200] loss: 2.883, acc: 0.438
******** [step = 250] loss: 2.890, acc: 0.438
******** [step = 300] loss: 2.895, acc: 0.438
******** [step = 350] loss: 2.899, acc: 0.438
******** [step = 400] loss: 2.906, acc: 0.437
******** [step = 450] loss: 2.907, acc: 0.437
******** [step = 500] loss: 2.911, acc: 0.436
******** [step = 550] loss: 2.914, acc: 0.436
******** [step = 600] loss: 2.915, acc: 0.436
******** [step = 650] loss: 2.916, acc: 0.436
******** [step = 700] loss: 2.918, acc: 0.436
******** [step = 750] loss: 2.920, acc: 0.436
******** [step = 800] loss: 2.921, acc: 0.436
******** [step = 850] loss: 2.920, acc: 0.436
EPOCH = 8 loss: 2.920, acc: 0.436, val_loss: 2.957, val_acc: 0.446

================================================================================2025-08_13 17:53:07
******** [step = 50] loss: 2.805, acc: 0.446
******** [step = 100] loss: 2.821, acc: 0.444
******** [step = 150] loss: 2.822, acc: 0.444
******** [step = 200] loss: 2.818, acc: 0.444
******** [step = 250] loss: 2.822, acc: 0.444
******** [step = 300] loss: 2.827, acc: 0.444
******** [step = 350] loss: 2.831, acc: 0.444
******** [step = 400] loss: 2.834, acc: 0.444
******** [step = 450] loss: 2.838, acc: 0.444
******** [step = 500] loss: 2.842, acc: 0.444
******** [step = 550] loss: 2.843, acc: 0.444
******** [step = 600] loss: 2.847, acc: 0.444
******** [step = 650] loss: 2.848, acc: 0.444
******** [step = 700] loss: 2.852, acc: 0.444
******** [step = 750] loss: 2.853, acc: 0.444
******** [step = 800] loss: 2.856, acc: 0.443
******** [step = 850] loss: 2.860, acc: 0.443
EPOCH = 9 loss: 2.860, acc: 0.443, val_loss: 2.934, val_acc: 0.450

================================================================================2025-08_13 17:55:08
******** [step = 50] loss: 2.794, acc: 0.449
******** [step = 100] loss: 2.790, acc: 0.449
******** [step = 150] loss: 2.778, acc: 0.451
******** [step = 200] loss: 2.776, acc: 0.451
******** [step = 250] loss: 2.777, acc: 0.451
******** [step = 300] loss: 2.779, acc: 0.451
******** [step = 350] loss: 2.780, acc: 0.451
******** [step = 400] loss: 2.784, acc: 0.451
******** [step = 450] loss: 2.787, acc: 0.450
******** [step = 500] loss: 2.791, acc: 0.450
******** [step = 550] loss: 2.795, acc: 0.450
******** [step = 600] loss: 2.798, acc: 0.450
******** [step = 650] loss: 2.800, acc: 0.449
******** [step = 700] loss: 2.799, acc: 0.450
******** [step = 750] loss: 2.801, acc: 0.450
******** [step = 800] loss: 2.803, acc: 0.449
******** [step = 850] loss: 2.804, acc: 0.449
EPOCH = 10 loss: 2.804, acc: 0.449, val_loss: 2.926, val_acc: 0.447

================================================================================2025-08_13 17:57:10
******** [step = 50] loss: 2.725, acc: 0.453
******** [step = 100] loss: 2.728, acc: 0.455
******** [step = 150] loss: 2.726, acc: 0.456
******** [step = 200] loss: 2.728, acc: 0.456
******** [step = 250] loss: 2.730, acc: 0.456
******** [step = 300] loss: 2.734, acc: 0.456
******** [step = 350] loss: 2.737, acc: 0.456
******** [step = 400] loss: 2.741, acc: 0.455
******** [step = 450] loss: 2.745, acc: 0.455
******** [step = 500] loss: 2.746, acc: 0.455
******** [step = 550] loss: 2.750, acc: 0.455
******** [step = 600] loss: 2.752, acc: 0.455
******** [step = 650] loss: 2.755, acc: 0.455
******** [step = 700] loss: 2.755, acc: 0.455
******** [step = 750] loss: 2.759, acc: 0.455
******** [step = 800] loss: 2.759, acc: 0.455
******** [step = 850] loss: 2.763, acc: 0.455
EPOCH = 11 loss: 2.763, acc: 0.455, val_loss: 2.890, val_acc: 0.457

================================================================================2025-08_13 17:59:11
******** [step = 50] loss: 2.657, acc: 0.464
******** [step = 100] loss: 2.659, acc: 0.464
******** [step = 150] loss: 2.666, acc: 0.463
******** [step = 200] loss: 2.672, acc: 0.463
******** [step = 250] loss: 2.678, acc: 0.463
******** [step = 300] loss: 2.684, acc: 0.462
******** [step = 350] loss: 2.688, acc: 0.461
******** [step = 400] loss: 2.692, acc: 0.461
******** [step = 450] loss: 2.697, acc: 0.461
******** [step = 500] loss: 2.701, acc: 0.461
******** [step = 550] loss: 2.706, acc: 0.460
******** [step = 600] loss: 2.711, acc: 0.460
******** [step = 650] loss: 2.714, acc: 0.460
******** [step = 700] loss: 2.718, acc: 0.460
******** [step = 750] loss: 2.720, acc: 0.459
******** [step = 800] loss: 2.722, acc: 0.459
******** [step = 850] loss: 2.725, acc: 0.459
EPOCH = 12 loss: 2.725, acc: 0.459, val_loss: 2.873, val_acc: 0.459

================================================================================2025-08_13 18:01:12
******** [step = 50] loss: 2.637, acc: 0.468
******** [step = 100] loss: 2.647, acc: 0.466
******** [step = 150] loss: 2.651, acc: 0.466
******** [step = 200] loss: 2.651, acc: 0.466
******** [step = 250] loss: 2.660, acc: 0.465
******** [step = 300] loss: 2.664, acc: 0.466
******** [step = 350] loss: 2.665, acc: 0.466
******** [step = 400] loss: 2.669, acc: 0.465
******** [step = 450] loss: 2.673, acc: 0.465
******** [step = 500] loss: 2.677, acc: 0.464
******** [step = 550] loss: 2.678, acc: 0.464
******** [step = 600] loss: 2.681, acc: 0.464
******** [step = 650] loss: 2.685, acc: 0.464
******** [step = 700] loss: 2.687, acc: 0.464
******** [step = 750] loss: 2.688, acc: 0.464
******** [step = 800] loss: 2.690, acc: 0.464
******** [step = 850] loss: 2.693, acc: 0.463
EPOCH = 13 loss: 2.693, acc: 0.463, val_loss: 2.860, val_acc: 0.464

================================================================================2025-08_13 18:03:13
******** [step = 50] loss: 2.606, acc: 0.471
******** [step = 100] loss: 2.603, acc: 0.473
******** [step = 150] loss: 2.602, acc: 0.473
******** [step = 200] loss: 2.610, acc: 0.472
******** [step = 250] loss: 2.622, acc: 0.471
******** [step = 300] loss: 2.630, acc: 0.470
******** [step = 350] loss: 2.636, acc: 0.469
******** [step = 400] loss: 2.639, acc: 0.469
******** [step = 450] loss: 2.642, acc: 0.469
******** [step = 500] loss: 2.645, acc: 0.469
******** [step = 550] loss: 2.651, acc: 0.468
******** [step = 600] loss: 2.653, acc: 0.468
******** [step = 650] loss: 2.655, acc: 0.468
******** [step = 700] loss: 2.659, acc: 0.468
******** [step = 750] loss: 2.662, acc: 0.467
******** [step = 800] loss: 2.664, acc: 0.467
******** [step = 850] loss: 2.666, acc: 0.467
EPOCH = 14 loss: 2.666, acc: 0.467, val_loss: 2.847, val_acc: 0.466

================================================================================2025-08_13 18:05:14
******** [step = 50] loss: 2.586, acc: 0.472
******** [step = 100] loss: 2.594, acc: 0.473
******** [step = 150] loss: 2.593, acc: 0.474
******** [step = 200] loss: 2.599, acc: 0.474
******** [step = 250] loss: 2.603, acc: 0.473
******** [step = 300] loss: 2.604, acc: 0.473
******** [step = 350] loss: 2.607, acc: 0.473
******** [step = 400] loss: 2.614, acc: 0.472
******** [step = 450] loss: 2.618, acc: 0.472
******** [step = 500] loss: 2.621, acc: 0.472
******** [step = 550] loss: 2.624, acc: 0.472
******** [step = 600] loss: 2.626, acc: 0.472
******** [step = 650] loss: 2.630, acc: 0.472
******** [step = 700] loss: 2.633, acc: 0.471
******** [step = 750] loss: 2.635, acc: 0.471
******** [step = 800] loss: 2.637, acc: 0.471
******** [step = 850] loss: 2.640, acc: 0.471
EPOCH = 15 loss: 2.640, acc: 0.471, val_loss: 2.840, val_acc: 0.469

================================================================================2025-08_13 18:07:16
******** [step = 50] loss: 2.556, acc: 0.477
******** [step = 100] loss: 2.567, acc: 0.476
******** [step = 150] loss: 2.570, acc: 0.477
******** [step = 200] loss: 2.570, acc: 0.477
******** [step = 250] loss: 2.576, acc: 0.477
******** [step = 300] loss: 2.577, acc: 0.477
******** [step = 350] loss: 2.581, acc: 0.476
******** [step = 400] loss: 2.584, acc: 0.476
******** [step = 450] loss: 2.588, acc: 0.476
******** [step = 500] loss: 2.591, acc: 0.476
******** [step = 550] loss: 2.593, acc: 0.476
******** [step = 600] loss: 2.596, acc: 0.476
******** [step = 650] loss: 2.600, acc: 0.475
******** [step = 700] loss: 2.604, acc: 0.475
******** [step = 750] loss: 2.610, acc: 0.474
******** [step = 800] loss: 2.614, acc: 0.474
******** [step = 850] loss: 2.616, acc: 0.474
EPOCH = 16 loss: 2.616, acc: 0.474, val_loss: 2.834, val_acc: 0.470

================================================================================2025-08_13 18:09:17
******** [step = 50] loss: 2.534, acc: 0.484
******** [step = 100] loss: 2.521, acc: 0.485
******** [step = 150] loss: 2.534, acc: 0.483
******** [step = 200] loss: 2.540, acc: 0.483
******** [step = 250] loss: 2.547, acc: 0.482
******** [step = 300] loss: 2.551, acc: 0.482
******** [step = 350] loss: 2.555, acc: 0.481
******** [step = 400] loss: 2.558, acc: 0.481
******** [step = 450] loss: 2.564, acc: 0.480
******** [step = 500] loss: 2.567, acc: 0.480
******** [step = 550] loss: 2.569, acc: 0.479
******** [step = 600] loss: 2.573, acc: 0.479
******** [step = 650] loss: 2.578, acc: 0.479
******** [step = 700] loss: 2.581, acc: 0.478
******** [step = 750] loss: 2.586, acc: 0.478
******** [step = 800] loss: 2.589, acc: 0.478
******** [step = 850] loss: 2.595, acc: 0.477
EPOCH = 17 loss: 2.595, acc: 0.477, val_loss: 2.823, val_acc: 0.473

================================================================================2025-08_13 18:11:18
******** [step = 50] loss: 2.515, acc: 0.483
******** [step = 100] loss: 2.527, acc: 0.483
******** [step = 150] loss: 2.531, acc: 0.482
******** [step = 200] loss: 2.530, acc: 0.482
******** [step = 250] loss: 2.532, acc: 0.482
******** [step = 300] loss: 2.534, acc: 0.482
******** [step = 350] loss: 2.539, acc: 0.482
******** [step = 400] loss: 2.546, acc: 0.481
******** [step = 450] loss: 2.549, acc: 0.481
******** [step = 500] loss: 2.556, acc: 0.481
******** [step = 550] loss: 2.560, acc: 0.480
******** [step = 600] loss: 2.563, acc: 0.480
******** [step = 650] loss: 2.564, acc: 0.480
******** [step = 700] loss: 2.567, acc: 0.480
******** [step = 750] loss: 2.569, acc: 0.480
******** [step = 800] loss: 2.572, acc: 0.480
******** [step = 850] loss: 2.574, acc: 0.480
EPOCH = 18 loss: 2.574, acc: 0.480, val_loss: 2.811, val_acc: 0.475

================================================================================2025-08_13 18:13:19
******** [step = 50] loss: 2.514, acc: 0.484
******** [step = 100] loss: 2.501, acc: 0.486
******** [step = 150] loss: 2.503, acc: 0.487
******** [step = 200] loss: 2.510, acc: 0.486
******** [step = 250] loss: 2.511, acc: 0.485
******** [step = 300] loss: 2.518, acc: 0.485
******** [step = 350] loss: 2.523, acc: 0.484
******** [step = 400] loss: 2.525, acc: 0.484
******** [step = 450] loss: 2.529, acc: 0.484
******** [step = 500] loss: 2.534, acc: 0.484
******** [step = 550] loss: 2.538, acc: 0.484
******** [step = 600] loss: 2.540, acc: 0.484
******** [step = 650] loss: 2.543, acc: 0.484
******** [step = 700] loss: 2.548, acc: 0.483
******** [step = 750] loss: 2.552, acc: 0.483
******** [step = 800] loss: 2.555, acc: 0.482
******** [step = 850] loss: 2.557, acc: 0.482
EPOCH = 19 loss: 2.557, acc: 0.482, val_loss: 2.807, val_acc: 0.476

================================================================================2025-08_13 18:15:20
******** [step = 50] loss: 2.507, acc: 0.487
******** [step = 100] loss: 2.500, acc: 0.488
******** [step = 150] loss: 2.491, acc: 0.488
******** [step = 200] loss: 2.492, acc: 0.488
******** [step = 250] loss: 2.491, acc: 0.488
******** [step = 300] loss: 2.500, acc: 0.487
******** [step = 350] loss: 2.505, acc: 0.487
******** [step = 400] loss: 2.508, acc: 0.486
******** [step = 450] loss: 2.513, acc: 0.486
******** [step = 500] loss: 2.517, acc: 0.486
******** [step = 550] loss: 2.518, acc: 0.486
******** [step = 600] loss: 2.523, acc: 0.486
******** [step = 650] loss: 2.527, acc: 0.485
******** [step = 700] loss: 2.530, acc: 0.485
******** [step = 750] loss: 2.533, acc: 0.485
******** [step = 800] loss: 2.535, acc: 0.485
******** [step = 850] loss: 2.536, acc: 0.485
EPOCH = 20 loss: 2.536, acc: 0.485, val_loss: 2.803, val_acc: 0.477

================================================================================2025-08_13 18:17:21
******** [step = 50] loss: 2.447, acc: 0.496
******** [step = 100] loss: 2.464, acc: 0.493
******** [step = 150] loss: 2.467, acc: 0.493
******** [step = 200] loss: 2.468, acc: 0.493
******** [step = 250] loss: 2.472, acc: 0.492
******** [step = 300] loss: 2.474, acc: 0.492
******** [step = 350] loss: 2.482, acc: 0.491
******** [step = 400] loss: 2.486, acc: 0.491
******** [step = 450] loss: 2.490, acc: 0.490
******** [step = 500] loss: 2.497, acc: 0.490
******** [step = 550] loss: 2.502, acc: 0.489
******** [step = 600] loss: 2.507, acc: 0.489
******** [step = 650] loss: 2.508, acc: 0.489
******** [step = 700] loss: 2.511, acc: 0.489
******** [step = 750] loss: 2.513, acc: 0.488
******** [step = 800] loss: 2.517, acc: 0.488
******** [step = 850] loss: 2.520, acc: 0.487
EPOCH = 21 loss: 2.520, acc: 0.487, val_loss: 2.792, val_acc: 0.478

================================================================================2025-08_13 18:19:21
******** [step = 50] loss: 2.446, acc: 0.495
******** [step = 100] loss: 2.451, acc: 0.493
******** [step = 150] loss: 2.452, acc: 0.493
******** [step = 200] loss: 2.462, acc: 0.492
******** [step = 250] loss: 2.468, acc: 0.491
******** [step = 300] loss: 2.469, acc: 0.490
******** [step = 350] loss: 2.473, acc: 0.490
******** [step = 400] loss: 2.476, acc: 0.490
******** [step = 450] loss: 2.480, acc: 0.490
******** [step = 500] loss: 2.482, acc: 0.490
******** [step = 550] loss: 2.486, acc: 0.490
******** [step = 600] loss: 2.489, acc: 0.490
******** [step = 650] loss: 2.493, acc: 0.490
******** [step = 700] loss: 2.496, acc: 0.490
******** [step = 750] loss: 2.499, acc: 0.489
******** [step = 800] loss: 2.503, acc: 0.489
******** [step = 850] loss: 2.506, acc: 0.489
EPOCH = 22 loss: 2.506, acc: 0.489, val_loss: 2.794, val_acc: 0.478

================================================================================2025-08_13 18:21:22
******** [step = 50] loss: 2.439, acc: 0.495
******** [step = 100] loss: 2.424, acc: 0.497
******** [step = 150] loss: 2.430, acc: 0.497
******** [step = 200] loss: 2.437, acc: 0.496
******** [step = 250] loss: 2.442, acc: 0.495
******** [step = 300] loss: 2.447, acc: 0.495
******** [step = 350] loss: 2.453, acc: 0.495
******** [step = 400] loss: 2.455, acc: 0.494
******** [step = 450] loss: 2.461, acc: 0.494
******** [step = 500] loss: 2.465, acc: 0.493
******** [step = 550] loss: 2.469, acc: 0.493
******** [step = 600] loss: 2.473, acc: 0.493
******** [step = 650] loss: 2.477, acc: 0.492
******** [step = 700] loss: 2.482, acc: 0.492
******** [step = 750] loss: 2.487, acc: 0.492
******** [step = 800] loss: 2.489, acc: 0.492
******** [step = 850] loss: 2.492, acc: 0.491
EPOCH = 23 loss: 2.492, acc: 0.491, val_loss: 2.788, val_acc: 0.480

================================================================================2025-08_13 18:23:23
******** [step = 50] loss: 2.425, acc: 0.496
******** [step = 100] loss: 2.421, acc: 0.497
******** [step = 150] loss: 2.424, acc: 0.497
******** [step = 200] loss: 2.427, acc: 0.497
******** [step = 250] loss: 2.435, acc: 0.497
******** [step = 300] loss: 2.438, acc: 0.497
******** [step = 350] loss: 2.439, acc: 0.496
******** [step = 400] loss: 2.445, acc: 0.496
******** [step = 450] loss: 2.451, acc: 0.495
******** [step = 500] loss: 2.451, acc: 0.496
******** [step = 550] loss: 2.454, acc: 0.495
******** [step = 600] loss: 2.459, acc: 0.495
******** [step = 650] loss: 2.463, acc: 0.494
******** [step = 700] loss: 2.467, acc: 0.494
******** [step = 750] loss: 2.470, acc: 0.494
******** [step = 800] loss: 2.474, acc: 0.493
******** [step = 850] loss: 2.476, acc: 0.493
EPOCH = 24 loss: 2.476, acc: 0.493, val_loss: 2.789, val_acc: 0.482

================================================================================2025-08_13 18:25:24
******** [step = 50] loss: 2.395, acc: 0.500
******** [step = 100] loss: 2.400, acc: 0.502
******** [step = 150] loss: 2.405, acc: 0.501
******** [step = 200] loss: 2.410, acc: 0.499
******** [step = 250] loss: 2.415, acc: 0.499
******** [step = 300] loss: 2.415, acc: 0.500
******** [step = 350] loss: 2.420, acc: 0.499
******** [step = 400] loss: 2.427, acc: 0.499
******** [step = 450] loss: 2.433, acc: 0.498
******** [step = 500] loss: 2.437, acc: 0.497
******** [step = 550] loss: 2.441, acc: 0.497
******** [step = 600] loss: 2.447, acc: 0.497
******** [step = 650] loss: 2.452, acc: 0.496
******** [step = 700] loss: 2.454, acc: 0.496
******** [step = 750] loss: 2.459, acc: 0.496
******** [step = 800] loss: 2.460, acc: 0.496
******** [step = 850] loss: 2.464, acc: 0.495
EPOCH = 25 loss: 2.464, acc: 0.495, val_loss: 2.780, val_acc: 0.481

================================================================================2025-08_13 18:27:25
******** [step = 50] loss: 2.381, acc: 0.504
******** [step = 100] loss: 2.385, acc: 0.504
******** [step = 150] loss: 2.390, acc: 0.503
******** [step = 200] loss: 2.394, acc: 0.503
******** [step = 250] loss: 2.398, acc: 0.503
******** [step = 300] loss: 2.407, acc: 0.502
******** [step = 350] loss: 2.411, acc: 0.501
******** [step = 400] loss: 2.415, acc: 0.501
******** [step = 450] loss: 2.422, acc: 0.500
******** [step = 500] loss: 2.427, acc: 0.499
******** [step = 550] loss: 2.432, acc: 0.499
******** [step = 600] loss: 2.437, acc: 0.498
******** [step = 650] loss: 2.442, acc: 0.498
******** [step = 700] loss: 2.445, acc: 0.497
******** [step = 750] loss: 2.447, acc: 0.497
******** [step = 800] loss: 2.451, acc: 0.497
******** [step = 850] loss: 2.453, acc: 0.497
EPOCH = 26 loss: 2.453, acc: 0.497, val_loss: 2.780, val_acc: 0.481

================================================================================2025-08_13 18:29:26
******** [step = 50] loss: 2.385, acc: 0.502
******** [step = 100] loss: 2.391, acc: 0.503
******** [step = 150] loss: 2.392, acc: 0.503
******** [step = 200] loss: 2.394, acc: 0.502
******** [step = 250] loss: 2.396, acc: 0.502
******** [step = 300] loss: 2.402, acc: 0.501
******** [step = 350] loss: 2.408, acc: 0.501
******** [step = 400] loss: 2.412, acc: 0.500
******** [step = 450] loss: 2.416, acc: 0.500
******** [step = 500] loss: 2.420, acc: 0.500
******** [step = 550] loss: 2.424, acc: 0.500
******** [step = 600] loss: 2.427, acc: 0.499
******** [step = 650] loss: 2.430, acc: 0.499
******** [step = 700] loss: 2.434, acc: 0.499
******** [step = 750] loss: 2.437, acc: 0.498
******** [step = 800] loss: 2.439, acc: 0.498
******** [step = 850] loss: 2.440, acc: 0.498
EPOCH = 27 loss: 2.440, acc: 0.498, val_loss: 2.775, val_acc: 0.483

================================================================================2025-08_13 18:31:26
******** [step = 50] loss: 2.357, acc: 0.503
******** [step = 100] loss: 2.368, acc: 0.505
******** [step = 150] loss: 2.372, acc: 0.505
******** [step = 200] loss: 2.376, acc: 0.505
******** [step = 250] loss: 2.378, acc: 0.504
******** [step = 300] loss: 2.385, acc: 0.504
******** [step = 350] loss: 2.392, acc: 0.503
******** [step = 400] loss: 2.397, acc: 0.502
******** [step = 450] loss: 2.400, acc: 0.502
******** [step = 500] loss: 2.404, acc: 0.501
******** [step = 550] loss: 2.409, acc: 0.501
******** [step = 600] loss: 2.410, acc: 0.500
******** [step = 650] loss: 2.414, acc: 0.500
******** [step = 700] loss: 2.417, acc: 0.500
******** [step = 750] loss: 2.421, acc: 0.500
******** [step = 800] loss: 2.422, acc: 0.499
******** [step = 850] loss: 2.425, acc: 0.499
EPOCH = 28 loss: 2.425, acc: 0.499, val_loss: 2.745, val_acc: 0.486

================================================================================2025-08_13 18:33:27
******** [step = 50] loss: 2.314, acc: 0.509
******** [step = 100] loss: 2.328, acc: 0.508
******** [step = 150] loss: 2.337, acc: 0.507
******** [step = 200] loss: 2.339, acc: 0.507
******** [step = 250] loss: 2.347, acc: 0.506
******** [step = 300] loss: 2.349, acc: 0.506
******** [step = 350] loss: 2.354, acc: 0.506
******** [step = 400] loss: 2.358, acc: 0.505
******** [step = 450] loss: 2.362, acc: 0.505
******** [step = 500] loss: 2.365, acc: 0.505
******** [step = 550] loss: 2.370, acc: 0.505
******** [step = 600] loss: 2.373, acc: 0.504
******** [step = 650] loss: 2.377, acc: 0.504
******** [step = 700] loss: 2.381, acc: 0.503
******** [step = 750] loss: 2.384, acc: 0.503
******** [step = 800] loss: 2.387, acc: 0.503
******** [step = 850] loss: 2.391, acc: 0.502
EPOCH = 29 loss: 2.391, acc: 0.502, val_loss: 2.731, val_acc: 0.486

================================================================================2025-08_13 18:35:28
******** [step = 50] loss: 2.301, acc: 0.510
******** [step = 100] loss: 2.315, acc: 0.509
******** [step = 150] loss: 2.325, acc: 0.508
******** [step = 200] loss: 2.324, acc: 0.508
******** [step = 250] loss: 2.331, acc: 0.508
******** [step = 300] loss: 2.339, acc: 0.507
******** [step = 350] loss: 2.344, acc: 0.506
******** [step = 400] loss: 2.346, acc: 0.506
******** [step = 450] loss: 2.350, acc: 0.506
******** [step = 500] loss: 2.354, acc: 0.506
******** [step = 550] loss: 2.358, acc: 0.506
******** [step = 600] loss: 2.361, acc: 0.505
******** [step = 650] loss: 2.364, acc: 0.505
******** [step = 700] loss: 2.368, acc: 0.505
******** [step = 750] loss: 2.372, acc: 0.504
******** [step = 800] loss: 2.374, acc: 0.504
******** [step = 850] loss: 2.376, acc: 0.504
EPOCH = 30 loss: 2.376, acc: 0.504, val_loss: 2.728, val_acc: 0.486

================================================================================2025-08_13 18:37:29
******** [step = 50] loss: 2.301, acc: 0.513
******** [step = 100] loss: 2.300, acc: 0.513
******** [step = 150] loss: 2.300, acc: 0.512
******** [step = 200] loss: 2.306, acc: 0.511
******** [step = 250] loss: 2.311, acc: 0.510
******** [step = 300] loss: 2.320, acc: 0.509
******** [step = 350] loss: 2.326, acc: 0.509
******** [step = 400] loss: 2.330, acc: 0.509
******** [step = 450] loss: 2.335, acc: 0.508
******** [step = 500] loss: 2.339, acc: 0.508
******** [step = 550] loss: 2.344, acc: 0.507
******** [step = 600] loss: 2.347, acc: 0.507
******** [step = 650] loss: 2.352, acc: 0.507
******** [step = 700] loss: 2.356, acc: 0.506
******** [step = 750] loss: 2.359, acc: 0.506
******** [step = 800] loss: 2.363, acc: 0.506
******** [step = 850] loss: 2.366, acc: 0.506
EPOCH = 31 loss: 2.366, acc: 0.506, val_loss: 2.724, val_acc: 0.489

================================================================================2025-08_13 18:39:30
******** [step = 50] loss: 2.290, acc: 0.512
******** [step = 100] loss: 2.289, acc: 0.512
******** [step = 150] loss: 2.298, acc: 0.511
******** [step = 200] loss: 2.306, acc: 0.510
******** [step = 250] loss: 2.307, acc: 0.510
******** [step = 300] loss: 2.310, acc: 0.510
******** [step = 350] loss: 2.319, acc: 0.510
******** [step = 400] loss: 2.325, acc: 0.509
******** [step = 450] loss: 2.328, acc: 0.509
******** [step = 500] loss: 2.332, acc: 0.509
******** [step = 550] loss: 2.336, acc: 0.509
******** [step = 600] loss: 2.338, acc: 0.508
******** [step = 650] loss: 2.343, acc: 0.508
******** [step = 700] loss: 2.347, acc: 0.508
******** [step = 750] loss: 2.350, acc: 0.508
******** [step = 800] loss: 2.352, acc: 0.507
******** [step = 850] loss: 2.354, acc: 0.508
EPOCH = 32 loss: 2.354, acc: 0.508, val_loss: 2.718, val_acc: 0.490

================================================================================2025-08_13 18:41:31
******** [step = 50] loss: 2.279, acc: 0.514
******** [step = 100] loss: 2.293, acc: 0.513
******** [step = 150] loss: 2.292, acc: 0.514
******** [step = 200] loss: 2.296, acc: 0.513
******** [step = 250] loss: 2.302, acc: 0.512
******** [step = 300] loss: 2.307, acc: 0.512
******** [step = 350] loss: 2.312, acc: 0.511
******** [step = 400] loss: 2.316, acc: 0.511
******** [step = 450] loss: 2.322, acc: 0.511
******** [step = 500] loss: 2.328, acc: 0.510
******** [step = 550] loss: 2.331, acc: 0.510
******** [step = 600] loss: 2.334, acc: 0.509
******** [step = 650] loss: 2.336, acc: 0.509
******** [step = 700] loss: 2.339, acc: 0.509
******** [step = 750] loss: 2.343, acc: 0.508
******** [step = 800] loss: 2.344, acc: 0.508
******** [step = 850] loss: 2.350, acc: 0.508
EPOCH = 33 loss: 2.350, acc: 0.508, val_loss: 2.732, val_acc: 0.486

================================================================================2025-08_13 18:43:32
******** [step = 50] loss: 2.268, acc: 0.516
******** [step = 100] loss: 2.274, acc: 0.516
******** [step = 150] loss: 2.284, acc: 0.515
******** [step = 200] loss: 2.290, acc: 0.514
******** [step = 250] loss: 2.295, acc: 0.513
******** [step = 300] loss: 2.302, acc: 0.512
******** [step = 350] loss: 2.308, acc: 0.512
******** [step = 400] loss: 2.311, acc: 0.512
******** [step = 450] loss: 2.313, acc: 0.512
******** [step = 500] loss: 2.315, acc: 0.511
******** [step = 550] loss: 2.320, acc: 0.511
******** [step = 600] loss: 2.323, acc: 0.511
******** [step = 650] loss: 2.327, acc: 0.510
******** [step = 700] loss: 2.329, acc: 0.510
******** [step = 750] loss: 2.331, acc: 0.510
******** [step = 800] loss: 2.336, acc: 0.510
******** [step = 850] loss: 2.338, acc: 0.509
EPOCH = 34 loss: 2.338, acc: 0.509, val_loss: 2.711, val_acc: 0.490

================================================================================2025-08_13 18:45:33
******** [step = 50] loss: 2.261, acc: 0.518
******** [step = 100] loss: 2.271, acc: 0.515
******** [step = 150] loss: 2.274, acc: 0.515
******** [step = 200] loss: 2.283, acc: 0.514
******** [step = 250] loss: 2.284, acc: 0.515
******** [step = 300] loss: 2.287, acc: 0.515
******** [step = 350] loss: 2.292, acc: 0.514
******** [step = 400] loss: 2.297, acc: 0.513
******** [step = 450] loss: 2.303, acc: 0.513
******** [step = 500] loss: 2.306, acc: 0.512
******** [step = 550] loss: 2.308, acc: 0.512
******** [step = 600] loss: 2.312, acc: 0.512
******** [step = 650] loss: 2.316, acc: 0.511
******** [step = 700] loss: 2.320, acc: 0.511
******** [step = 750] loss: 2.323, acc: 0.511
******** [step = 800] loss: 2.326, acc: 0.511
******** [step = 850] loss: 2.329, acc: 0.511
EPOCH = 35 loss: 2.329, acc: 0.511, val_loss: 2.712, val_acc: 0.491

================================================================================2025-08_13 18:47:34
******** [step = 50] loss: 2.244, acc: 0.520
******** [step = 100] loss: 2.247, acc: 0.522
******** [step = 150] loss: 2.255, acc: 0.520
******** [step = 200] loss: 2.259, acc: 0.519
******** [step = 250] loss: 2.263, acc: 0.519
******** [step = 300] loss: 2.272, acc: 0.517
******** [step = 350] loss: 2.278, acc: 0.516
******** [step = 400] loss: 2.288, acc: 0.515
******** [step = 450] loss: 2.291, acc: 0.515
******** [step = 500] loss: 2.297, acc: 0.515
******** [step = 550] loss: 2.301, acc: 0.514
******** [step = 600] loss: 2.306, acc: 0.514
******** [step = 650] loss: 2.308, acc: 0.514
******** [step = 700] loss: 2.312, acc: 0.513
******** [step = 750] loss: 2.315, acc: 0.513
******** [step = 800] loss: 2.319, acc: 0.512
******** [step = 850] loss: 2.323, acc: 0.512
EPOCH = 36 loss: 2.323, acc: 0.512, val_loss: 2.703, val_acc: 0.493

================================================================================2025-08_13 18:49:35
******** [step = 50] loss: 2.246, acc: 0.517
******** [step = 100] loss: 2.253, acc: 0.518
******** [step = 150] loss: 2.253, acc: 0.519
******** [step = 200] loss: 2.260, acc: 0.518
******** [step = 250] loss: 2.268, acc: 0.517
******** [step = 300] loss: 2.273, acc: 0.517
******** [step = 350] loss: 2.279, acc: 0.516
******** [step = 400] loss: 2.281, acc: 0.516
******** [step = 450] loss: 2.282, acc: 0.516
******** [step = 500] loss: 2.285, acc: 0.515
******** [step = 550] loss: 2.288, acc: 0.515
******** [step = 600] loss: 2.291, acc: 0.515
******** [step = 650] loss: 2.294, acc: 0.514
******** [step = 700] loss: 2.298, acc: 0.514
******** [step = 750] loss: 2.300, acc: 0.514
******** [step = 800] loss: 2.301, acc: 0.514
******** [step = 850] loss: 2.305, acc: 0.514
EPOCH = 37 loss: 2.305, acc: 0.514, val_loss: 2.675, val_acc: 0.496

================================================================================2025-08_13 18:51:36
******** [step = 50] loss: 2.197, acc: 0.526
******** [step = 100] loss: 2.209, acc: 0.525
******** [step = 150] loss: 2.205, acc: 0.525
******** [step = 200] loss: 2.214, acc: 0.524
******** [step = 250] loss: 2.219, acc: 0.524
******** [step = 300] loss: 2.227, acc: 0.523
******** [step = 350] loss: 2.232, acc: 0.522
******** [step = 400] loss: 2.236, acc: 0.522
******** [step = 450] loss: 2.237, acc: 0.522
******** [step = 500] loss: 2.244, acc: 0.522
******** [step = 550] loss: 2.250, acc: 0.521
******** [step = 600] loss: 2.255, acc: 0.520
******** [step = 650] loss: 2.259, acc: 0.520
******** [step = 700] loss: 2.263, acc: 0.520
******** [step = 750] loss: 2.265, acc: 0.520
******** [step = 800] loss: 2.268, acc: 0.519
******** [step = 850] loss: 2.272, acc: 0.519
EPOCH = 38 loss: 2.272, acc: 0.519, val_loss: 2.660, val_acc: 0.498

================================================================================2025-08_13 18:53:36
******** [step = 50] loss: 2.204, acc: 0.522
******** [step = 100] loss: 2.201, acc: 0.524
******** [step = 150] loss: 2.199, acc: 0.526
******** [step = 200] loss: 2.203, acc: 0.525
******** [step = 250] loss: 2.208, acc: 0.525
******** [step = 300] loss: 2.214, acc: 0.524
******** [step = 350] loss: 2.219, acc: 0.524
******** [step = 400] loss: 2.222, acc: 0.524
******** [step = 450] loss: 2.228, acc: 0.523
******** [step = 500] loss: 2.234, acc: 0.522
******** [step = 550] loss: 2.237, acc: 0.522
******** [step = 600] loss: 2.240, acc: 0.522
******** [step = 650] loss: 2.245, acc: 0.522
******** [step = 700] loss: 2.249, acc: 0.522
******** [step = 750] loss: 2.253, acc: 0.521
******** [step = 800] loss: 2.258, acc: 0.521
******** [step = 850] loss: 2.260, acc: 0.521
EPOCH = 39 loss: 2.260, acc: 0.521, val_loss: 2.655, val_acc: 0.500

================================================================================2025-08_13 18:55:37
******** [step = 50] loss: 2.181, acc: 0.530
******** [step = 100] loss: 2.191, acc: 0.528
******** [step = 150] loss: 2.195, acc: 0.528
******** [step = 200] loss: 2.196, acc: 0.528
******** [step = 250] loss: 2.200, acc: 0.528
******** [step = 300] loss: 2.203, acc: 0.527
******** [step = 350] loss: 2.211, acc: 0.526
******** [step = 400] loss: 2.215, acc: 0.526
******** [step = 450] loss: 2.219, acc: 0.525
******** [step = 500] loss: 2.225, acc: 0.524
******** [step = 550] loss: 2.229, acc: 0.524
******** [step = 600] loss: 2.237, acc: 0.523
******** [step = 650] loss: 2.239, acc: 0.523
******** [step = 700] loss: 2.243, acc: 0.523
******** [step = 750] loss: 2.246, acc: 0.523
******** [step = 800] loss: 2.251, acc: 0.522
******** [step = 850] loss: 2.254, acc: 0.522
EPOCH = 40 loss: 2.254, acc: 0.522, val_loss: 2.654, val_acc: 0.500

================================================================================2025-08_13 18:57:38
******** [step = 50] loss: 2.177, acc: 0.532
******** [step = 100] loss: 2.175, acc: 0.532
******** [step = 150] loss: 2.179, acc: 0.531
******** [step = 200] loss: 2.183, acc: 0.530
******** [step = 250] loss: 2.190, acc: 0.529
******** [step = 300] loss: 2.193, acc: 0.529
******** [step = 350] loss: 2.202, acc: 0.528
******** [step = 400] loss: 2.206, acc: 0.527
******** [step = 450] loss: 2.213, acc: 0.526
******** [step = 500] loss: 2.217, acc: 0.526
******** [step = 550] loss: 2.220, acc: 0.526
******** [step = 600] loss: 2.225, acc: 0.525
******** [step = 650] loss: 2.229, acc: 0.525
******** [step = 700] loss: 2.234, acc: 0.524
******** [step = 750] loss: 2.237, acc: 0.524
******** [step = 800] loss: 2.241, acc: 0.524
******** [step = 850] loss: 2.246, acc: 0.523
EPOCH = 41 loss: 2.246, acc: 0.523, val_loss: 2.657, val_acc: 0.499

================================================================================2025-08_13 18:59:42
******** [step = 50] loss: 2.173, acc: 0.531
******** [step = 100] loss: 2.173, acc: 0.532
******** [step = 150] loss: 2.179, acc: 0.531
******** [step = 200] loss: 2.184, acc: 0.531
******** [step = 250] loss: 2.186, acc: 0.530
******** [step = 300] loss: 2.191, acc: 0.530
******** [step = 350] loss: 2.195, acc: 0.529
******** [step = 400] loss: 2.201, acc: 0.528
******** [step = 450] loss: 2.207, acc: 0.528
******** [step = 500] loss: 2.211, acc: 0.527
******** [step = 550] loss: 2.216, acc: 0.526
******** [step = 600] loss: 2.219, acc: 0.526
******** [step = 650] loss: 2.223, acc: 0.526
******** [step = 700] loss: 2.228, acc: 0.525
******** [step = 750] loss: 2.230, acc: 0.525
******** [step = 800] loss: 2.233, acc: 0.525
******** [step = 850] loss: 2.238, acc: 0.524
EPOCH = 42 loss: 2.238, acc: 0.524, val_loss: 2.647, val_acc: 0.503

================================================================================2025-08_13 19:01:43
******** [step = 50] loss: 2.164, acc: 0.532
******** [step = 100] loss: 2.161, acc: 0.533
******** [step = 150] loss: 2.174, acc: 0.532
******** [step = 200] loss: 2.178, acc: 0.531
******** [step = 250] loss: 2.180, acc: 0.531
******** [step = 300] loss: 2.186, acc: 0.531
******** [step = 350] loss: 2.191, acc: 0.530
******** [step = 400] loss: 2.196, acc: 0.529
******** [step = 450] loss: 2.201, acc: 0.529
******** [step = 500] loss: 2.204, acc: 0.528
******** [step = 550] loss: 2.208, acc: 0.528
******** [step = 600] loss: 2.213, acc: 0.528
******** [step = 650] loss: 2.218, acc: 0.527
******** [step = 700] loss: 2.222, acc: 0.526
******** [step = 750] loss: 2.224, acc: 0.526
******** [step = 800] loss: 2.226, acc: 0.526
******** [step = 850] loss: 2.229, acc: 0.526
EPOCH = 43 loss: 2.229, acc: 0.526, val_loss: 2.643, val_acc: 0.503

================================================================================2025-08_13 19:03:44
******** [step = 50] loss: 2.143, acc: 0.539
******** [step = 100] loss: 2.155, acc: 0.535
******** [step = 150] loss: 2.160, acc: 0.533
******** [step = 200] loss: 2.164, acc: 0.533
******** [step = 250] loss: 2.171, acc: 0.532
******** [step = 300] loss: 2.177, acc: 0.531
******** [step = 350] loss: 2.182, acc: 0.531
******** [step = 400] loss: 2.187, acc: 0.530
******** [step = 450] loss: 2.193, acc: 0.530
******** [step = 500] loss: 2.197, acc: 0.529
******** [step = 550] loss: 2.201, acc: 0.529
******** [step = 600] loss: 2.205, acc: 0.529
******** [step = 650] loss: 2.208, acc: 0.529
******** [step = 700] loss: 2.212, acc: 0.528
******** [step = 750] loss: 2.217, acc: 0.528
******** [step = 800] loss: 2.221, acc: 0.527
******** [step = 850] loss: 2.229, acc: 0.526
EPOCH = 44 loss: 2.229, acc: 0.526, val_loss: 2.644, val_acc: 0.502

================================================================================2025-08_13 19:05:45
******** [step = 50] loss: 2.135, acc: 0.537
******** [step = 100] loss: 2.147, acc: 0.537
******** [step = 150] loss: 2.156, acc: 0.536
******** [step = 200] loss: 2.162, acc: 0.535
******** [step = 250] loss: 2.164, acc: 0.534
******** [step = 300] loss: 2.172, acc: 0.534
******** [step = 350] loss: 2.178, acc: 0.533
******** [step = 400] loss: 2.183, acc: 0.532
******** [step = 450] loss: 2.189, acc: 0.532
******** [step = 500] loss: 2.192, acc: 0.531
******** [step = 550] loss: 2.195, acc: 0.531
******** [step = 600] loss: 2.198, acc: 0.530
******** [step = 650] loss: 2.202, acc: 0.530
******** [step = 700] loss: 2.205, acc: 0.530
******** [step = 750] loss: 2.208, acc: 0.529
******** [step = 800] loss: 2.213, acc: 0.529
******** [step = 850] loss: 2.216, acc: 0.529
EPOCH = 45 loss: 2.216, acc: 0.529, val_loss: 2.638, val_acc: 0.503

================================================================================2025-08_13 19:07:46
******** [step = 50] loss: 2.145, acc: 0.537
******** [step = 100] loss: 2.143, acc: 0.537
******** [step = 150] loss: 2.145, acc: 0.536
******** [step = 200] loss: 2.152, acc: 0.536
******** [step = 250] loss: 2.155, acc: 0.535
******** [step = 300] loss: 2.158, acc: 0.535
******** [step = 350] loss: 2.162, acc: 0.534
******** [step = 400] loss: 2.167, acc: 0.534
******** [step = 450] loss: 2.173, acc: 0.533
******** [step = 500] loss: 2.180, acc: 0.533
******** [step = 550] loss: 2.183, acc: 0.532
******** [step = 600] loss: 2.188, acc: 0.532
******** [step = 650] loss: 2.193, acc: 0.531
******** [step = 700] loss: 2.196, acc: 0.531
******** [step = 750] loss: 2.202, acc: 0.531
******** [step = 800] loss: 2.206, acc: 0.530
******** [step = 850] loss: 2.209, acc: 0.530
EPOCH = 46 loss: 2.209, acc: 0.530, val_loss: 2.634, val_acc: 0.507

================================================================================2025-08_13 19:09:46
******** [step = 50] loss: 2.139, acc: 0.537
******** [step = 100] loss: 2.145, acc: 0.537
******** [step = 150] loss: 2.144, acc: 0.538
******** [step = 200] loss: 2.145, acc: 0.538
******** [step = 250] loss: 2.149, acc: 0.537
******** [step = 300] loss: 2.157, acc: 0.536
******** [step = 350] loss: 2.161, acc: 0.536
******** [step = 400] loss: 2.164, acc: 0.536
******** [step = 450] loss: 2.169, acc: 0.535
******** [step = 500] loss: 2.175, acc: 0.535
******** [step = 550] loss: 2.177, acc: 0.535
******** [step = 600] loss: 2.182, acc: 0.534
******** [step = 650] loss: 2.185, acc: 0.534
******** [step = 700] loss: 2.188, acc: 0.534
******** [step = 750] loss: 2.192, acc: 0.533
******** [step = 800] loss: 2.196, acc: 0.533
******** [step = 850] loss: 2.200, acc: 0.533
EPOCH = 47 loss: 2.200, acc: 0.533, val_loss: 2.625, val_acc: 0.510

================================================================================2025-08_13 19:11:47
******** [step = 50] loss: 2.129, acc: 0.538
******** [step = 100] loss: 2.134, acc: 0.539
******** [step = 150] loss: 2.135, acc: 0.538
******** [step = 200] loss: 2.144, acc: 0.538
******** [step = 250] loss: 2.148, acc: 0.538
******** [step = 300] loss: 2.153, acc: 0.537
******** [step = 350] loss: 2.157, acc: 0.537
******** [step = 400] loss: 2.161, acc: 0.536
******** [step = 450] loss: 2.164, acc: 0.536
******** [step = 500] loss: 2.167, acc: 0.535
******** [step = 550] loss: 2.171, acc: 0.535
******** [step = 600] loss: 2.175, acc: 0.535
******** [step = 650] loss: 2.179, acc: 0.534
******** [step = 700] loss: 2.182, acc: 0.534
******** [step = 750] loss: 2.185, acc: 0.534
******** [step = 800] loss: 2.189, acc: 0.534
******** [step = 850] loss: 2.191, acc: 0.533
EPOCH = 48 loss: 2.191, acc: 0.533, val_loss: 2.623, val_acc: 0.509

================================================================================2025-08_13 19:13:48
******** [step = 50] loss: 2.116, acc: 0.542
******** [step = 100] loss: 2.115, acc: 0.541
******** [step = 150] loss: 2.121, acc: 0.540
******** [step = 200] loss: 2.123, acc: 0.540
******** [step = 250] loss: 2.132, acc: 0.539
******** [step = 300] loss: 2.137, acc: 0.538
******** [step = 350] loss: 2.145, acc: 0.538
******** [step = 400] loss: 2.150, acc: 0.537
******** [step = 450] loss: 2.154, acc: 0.537
******** [step = 500] loss: 2.159, acc: 0.537
******** [step = 550] loss: 2.163, acc: 0.537
******** [step = 600] loss: 2.168, acc: 0.536
******** [step = 650] loss: 2.170, acc: 0.536
******** [step = 700] loss: 2.174, acc: 0.536
******** [step = 750] loss: 2.176, acc: 0.536
******** [step = 800] loss: 2.180, acc: 0.535
******** [step = 850] loss: 2.184, acc: 0.535
EPOCH = 49 loss: 2.184, acc: 0.535, val_loss: 2.618, val_acc: 0.510

================================================================================2025-08_13 19:15:48
******** [step = 50] loss: 2.113, acc: 0.543
******** [step = 100] loss: 2.103, acc: 0.544
******** [step = 150] loss: 2.114, acc: 0.544
******** [step = 200] loss: 2.114, acc: 0.544
******** [step = 250] loss: 2.120, acc: 0.543
******** [step = 300] loss: 2.128, acc: 0.542
******** [step = 350] loss: 2.134, acc: 0.541
******** [step = 400] loss: 2.140, acc: 0.540
******** [step = 450] loss: 2.142, acc: 0.540
******** [step = 500] loss: 2.149, acc: 0.539
******** [step = 550] loss: 2.154, acc: 0.538
******** [step = 600] loss: 2.159, acc: 0.538
******** [step = 650] loss: 2.163, acc: 0.537
******** [step = 700] loss: 2.166, acc: 0.537
******** [step = 750] loss: 2.170, acc: 0.537
******** [step = 800] loss: 2.173, acc: 0.537
******** [step = 850] loss: 2.178, acc: 0.536
EPOCH = 50 loss: 2.178, acc: 0.536, val_loss: 2.616, val_acc: 0.507

================================================================================2025-08_13 19:17:49
******** [step = 50] loss: 2.097, acc: 0.545
******** [step = 100] loss: 2.111, acc: 0.544
******** [step = 150] loss: 2.116, acc: 0.542
******** [step = 200] loss: 2.122, acc: 0.542
******** [step = 250] loss: 2.125, acc: 0.542
******** [step = 300] loss: 2.130, acc: 0.541
******** [step = 350] loss: 2.132, acc: 0.541
******** [step = 400] loss: 2.137, acc: 0.540
******** [step = 450] loss: 2.141, acc: 0.540
******** [step = 500] loss: 2.146, acc: 0.539
******** [step = 550] loss: 2.149, acc: 0.539
******** [step = 600] loss: 2.153, acc: 0.539
******** [step = 650] loss: 2.158, acc: 0.538
******** [step = 700] loss: 2.160, acc: 0.538
******** [step = 750] loss: 2.164, acc: 0.537
******** [step = 800] loss: 2.169, acc: 0.537
******** [step = 850] loss: 2.170, acc: 0.537
EPOCH = 51 loss: 2.170, acc: 0.537, val_loss: 2.623, val_acc: 0.509

================================================================================2025-08_13 19:19:49
******** [step = 50] loss: 2.126, acc: 0.541
******** [step = 100] loss: 2.117, acc: 0.544
******** [step = 150] loss: 2.113, acc: 0.544
******** [step = 200] loss: 2.116, acc: 0.543
******** [step = 250] loss: 2.120, acc: 0.543
******** [step = 300] loss: 2.124, acc: 0.542
******** [step = 350] loss: 2.128, acc: 0.542
******** [step = 400] loss: 2.132, acc: 0.541
******** [step = 450] loss: 2.138, acc: 0.541
******** [step = 500] loss: 2.144, acc: 0.540
******** [step = 550] loss: 2.148, acc: 0.540
******** [step = 600] loss: 2.152, acc: 0.539
******** [step = 650] loss: 2.154, acc: 0.539
******** [step = 700] loss: 2.159, acc: 0.539
******** [step = 750] loss: 2.163, acc: 0.538
******** [step = 800] loss: 2.165, acc: 0.538
******** [step = 850] loss: 2.167, acc: 0.538
EPOCH = 52 loss: 2.167, acc: 0.538, val_loss: 2.615, val_acc: 0.509

================================================================================2025-08_13 19:21:50
******** [step = 50] loss: 2.091, acc: 0.548
******** [step = 100] loss: 2.090, acc: 0.547
******** [step = 150] loss: 2.093, acc: 0.546
******** [step = 200] loss: 2.101, acc: 0.545
******** [step = 250] loss: 2.108, acc: 0.544
******** [step = 300] loss: 2.112, acc: 0.544
******** [step = 350] loss: 2.116, acc: 0.543
******** [step = 400] loss: 2.123, acc: 0.543
******** [step = 450] loss: 2.126, acc: 0.542
******** [step = 500] loss: 2.132, acc: 0.541
******** [step = 550] loss: 2.137, acc: 0.541
******** [step = 600] loss: 2.142, acc: 0.540
******** [step = 650] loss: 2.146, acc: 0.540
******** [step = 700] loss: 2.151, acc: 0.540
******** [step = 750] loss: 2.154, acc: 0.539
******** [step = 800] loss: 2.159, acc: 0.539
******** [step = 850] loss: 2.162, acc: 0.538
EPOCH = 53 loss: 2.162, acc: 0.538, val_loss: 2.620, val_acc: 0.507

================================================================================2025-08_13 19:23:51
******** [step = 50] loss: 2.098, acc: 0.544
******** [step = 100] loss: 2.098, acc: 0.544
******** [step = 150] loss: 2.095, acc: 0.545
******** [step = 200] loss: 2.097, acc: 0.545
******** [step = 250] loss: 2.102, acc: 0.545
******** [step = 300] loss: 2.108, acc: 0.544
******** [step = 350] loss: 2.115, acc: 0.543
******** [step = 400] loss: 2.121, acc: 0.543
******** [step = 450] loss: 2.126, acc: 0.542
******** [step = 500] loss: 2.129, acc: 0.542
******** [step = 550] loss: 2.135, acc: 0.541
******** [step = 600] loss: 2.138, acc: 0.541
******** [step = 650] loss: 2.143, acc: 0.541
******** [step = 700] loss: 2.147, acc: 0.540
******** [step = 750] loss: 2.152, acc: 0.540
******** [step = 800] loss: 2.155, acc: 0.540
******** [step = 850] loss: 2.159, acc: 0.539
EPOCH = 54 loss: 2.159, acc: 0.539, val_loss: 2.608, val_acc: 0.512

================================================================================2025-08_13 19:25:51
******** [step = 50] loss: 2.083, acc: 0.545
******** [step = 100] loss: 2.086, acc: 0.546
******** [step = 150] loss: 2.089, acc: 0.546
******** [step = 200] loss: 2.100, acc: 0.545
******** [step = 250] loss: 2.099, acc: 0.545
******** [step = 300] loss: 2.105, acc: 0.545
******** [step = 350] loss: 2.112, acc: 0.544
******** [step = 400] loss: 2.118, acc: 0.543
******** [step = 450] loss: 2.122, acc: 0.543
******** [step = 500] loss: 2.127, acc: 0.542
******** [step = 550] loss: 2.132, acc: 0.542
******** [step = 600] loss: 2.137, acc: 0.541
******** [step = 650] loss: 2.140, acc: 0.541
******** [step = 700] loss: 2.143, acc: 0.541
******** [step = 750] loss: 2.147, acc: 0.540
******** [step = 800] loss: 2.150, acc: 0.540
******** [step = 850] loss: 2.154, acc: 0.540
EPOCH = 55 loss: 2.154, acc: 0.540, val_loss: 2.612, val_acc: 0.510

================================================================================2025-08_13 19:27:53
******** [step = 50] loss: 2.095, acc: 0.547
******** [step = 100] loss: 2.092, acc: 0.548
******** [step = 150] loss: 2.093, acc: 0.547
******** [step = 200] loss: 2.100, acc: 0.546
******** [step = 250] loss: 2.100, acc: 0.546
******** [step = 300] loss: 2.108, acc: 0.545
******** [step = 350] loss: 2.112, acc: 0.544
******** [step = 400] loss: 2.115, acc: 0.544
******** [step = 450] loss: 2.118, acc: 0.544
******** [step = 500] loss: 2.122, acc: 0.543
******** [step = 550] loss: 2.127, acc: 0.543
******** [step = 600] loss: 2.131, acc: 0.542
******** [step = 650] loss: 2.137, acc: 0.542
******** [step = 700] loss: 2.140, acc: 0.541
******** [step = 750] loss: 2.144, acc: 0.541
******** [step = 800] loss: 2.148, acc: 0.541
******** [step = 850] loss: 2.151, acc: 0.540
EPOCH = 56 loss: 2.151, acc: 0.540, val_loss: 2.609, val_acc: 0.512

================================================================================2025-08_13 19:29:53
******** [step = 50] loss: 2.064, acc: 0.551
******** [step = 100] loss: 2.078, acc: 0.549
******** [step = 150] loss: 2.092, acc: 0.548
******** [step = 200] loss: 2.094, acc: 0.547
******** [step = 250] loss: 2.099, acc: 0.546
******** [step = 300] loss: 2.102, acc: 0.545
******** [step = 350] loss: 2.110, acc: 0.545
******** [step = 400] loss: 2.115, acc: 0.544
******** [step = 450] loss: 2.118, acc: 0.544
******** [step = 500] loss: 2.121, acc: 0.543
******** [step = 550] loss: 2.124, acc: 0.543
******** [step = 600] loss: 2.128, acc: 0.542
******** [step = 650] loss: 2.132, acc: 0.542
******** [step = 700] loss: 2.135, acc: 0.542
******** [step = 750] loss: 2.138, acc: 0.541
******** [step = 800] loss: 2.144, acc: 0.541
******** [step = 850] loss: 2.145, acc: 0.541
EPOCH = 57 loss: 2.145, acc: 0.541, val_loss: 2.606, val_acc: 0.513

================================================================================2025-08_13 19:31:54
******** [step = 50] loss: 2.068, acc: 0.551
******** [step = 100] loss: 2.077, acc: 0.550
******** [step = 150] loss: 2.083, acc: 0.549
******** [step = 200] loss: 2.086, acc: 0.548
******** [step = 250] loss: 2.092, acc: 0.547
******** [step = 300] loss: 2.098, acc: 0.546
******** [step = 350] loss: 2.104, acc: 0.545
******** [step = 400] loss: 2.109, acc: 0.545
******** [step = 450] loss: 2.110, acc: 0.545
******** [step = 500] loss: 2.117, acc: 0.544
******** [step = 550] loss: 2.121, acc: 0.544
******** [step = 600] loss: 2.124, acc: 0.543
******** [step = 650] loss: 2.126, acc: 0.543
******** [step = 700] loss: 2.131, acc: 0.543
******** [step = 750] loss: 2.135, acc: 0.542
******** [step = 800] loss: 2.139, acc: 0.542
******** [step = 850] loss: 2.142, acc: 0.542
EPOCH = 58 loss: 2.142, acc: 0.542, val_loss: 2.608, val_acc: 0.513

================================================================================2025-08_13 19:33:55
******** [step = 50] loss: 2.065, acc: 0.552
******** [step = 100] loss: 2.073, acc: 0.550
******** [step = 150] loss: 2.076, acc: 0.550
******** [step = 200] loss: 2.081, acc: 0.549
******** [step = 250] loss: 2.083, acc: 0.549
******** [step = 300] loss: 2.091, acc: 0.548
******** [step = 350] loss: 2.097, acc: 0.547
******** [step = 400] loss: 2.100, acc: 0.546
******** [step = 450] loss: 2.104, acc: 0.546
******** [step = 500] loss: 2.110, acc: 0.545
******** [step = 550] loss: 2.114, acc: 0.545
******** [step = 600] loss: 2.119, acc: 0.544
******** [step = 650] loss: 2.122, acc: 0.544
******** [step = 700] loss: 2.125, acc: 0.544
******** [step = 750] loss: 2.131, acc: 0.543
******** [step = 800] loss: 2.134, acc: 0.543
******** [step = 850] loss: 2.136, acc: 0.543
EPOCH = 59 loss: 2.136, acc: 0.543, val_loss: 2.605, val_acc: 0.513

================================================================================2025-08_13 19:35:55
******** [step = 50] loss: 2.054, acc: 0.550
******** [step = 100] loss: 2.069, acc: 0.549
******** [step = 150] loss: 2.079, acc: 0.548
******** [step = 200] loss: 2.086, acc: 0.547
******** [step = 250] loss: 2.089, acc: 0.547
******** [step = 300] loss: 2.094, acc: 0.546
******** [step = 350] loss: 2.097, acc: 0.546
******** [step = 400] loss: 2.100, acc: 0.546
******** [step = 450] loss: 2.103, acc: 0.545
******** [step = 500] loss: 2.109, acc: 0.545
******** [step = 550] loss: 2.113, acc: 0.545
******** [step = 600] loss: 2.118, acc: 0.545
******** [step = 650] loss: 2.121, acc: 0.544
******** [step = 700] loss: 2.125, acc: 0.544
******** [step = 750] loss: 2.129, acc: 0.543
******** [step = 800] loss: 2.131, acc: 0.543
******** [step = 850] loss: 2.133, acc: 0.543
EPOCH = 60 loss: 2.133, acc: 0.543, val_loss: 2.605, val_acc: 0.515

================================================================================2025-08_13 19:37:56
******** [step = 50] loss: 2.058, acc: 0.550
******** [step = 100] loss: 2.070, acc: 0.549
******** [step = 150] loss: 2.070, acc: 0.550
******** [step = 200] loss: 2.078, acc: 0.549
******** [step = 250] loss: 2.083, acc: 0.548
******** [step = 300] loss: 2.090, acc: 0.547
******** [step = 350] loss: 2.092, acc: 0.546
******** [step = 400] loss: 2.097, acc: 0.546
******** [step = 450] loss: 2.104, acc: 0.545
******** [step = 500] loss: 2.107, acc: 0.545
******** [step = 550] loss: 2.111, acc: 0.545
******** [step = 600] loss: 2.115, acc: 0.544
******** [step = 650] loss: 2.118, acc: 0.544
******** [step = 700] loss: 2.122, acc: 0.543
******** [step = 750] loss: 2.126, acc: 0.543
******** [step = 800] loss: 2.129, acc: 0.543
******** [step = 850] loss: 2.132, acc: 0.543
EPOCH = 61 loss: 2.132, acc: 0.543, val_loss: 2.604, val_acc: 0.513

================================================================================2025-08_13 19:39:57
******** [step = 50] loss: 2.055, acc: 0.552
******** [step = 100] loss: 2.061, acc: 0.551
******** [step = 150] loss: 2.063, acc: 0.550
******** [step = 200] loss: 2.072, acc: 0.549
******** [step = 250] loss: 2.079, acc: 0.548
******** [step = 300] loss: 2.084, acc: 0.548
******** [step = 350] loss: 2.089, acc: 0.548
******** [step = 400] loss: 2.093, acc: 0.547
******** [step = 450] loss: 2.099, acc: 0.546
******** [step = 500] loss: 2.103, acc: 0.546
******** [step = 550] loss: 2.107, acc: 0.545
******** [step = 600] loss: 2.110, acc: 0.545
******** [step = 650] loss: 2.115, acc: 0.545
******** [step = 700] loss: 2.119, acc: 0.544
******** [step = 750] loss: 2.121, acc: 0.544
******** [step = 800] loss: 2.124, acc: 0.544
******** [step = 850] loss: 2.127, acc: 0.543
EPOCH = 62 loss: 2.127, acc: 0.543, val_loss: 2.601, val_acc: 0.515

================================================================================2025-08_13 19:41:57
******** [step = 50] loss: 2.066, acc: 0.550
******** [step = 100] loss: 2.062, acc: 0.552
******** [step = 150] loss: 2.068, acc: 0.550
******** [step = 200] loss: 2.065, acc: 0.551
******** [step = 250] loss: 2.073, acc: 0.550
******** [step = 300] loss: 2.080, acc: 0.549
******** [step = 350] loss: 2.086, acc: 0.548
******** [step = 400] loss: 2.090, acc: 0.547
******** [step = 450] loss: 2.093, acc: 0.547
******** [step = 500] loss: 2.098, acc: 0.547
******** [step = 550] loss: 2.101, acc: 0.547
******** [step = 600] loss: 2.105, acc: 0.546
******** [step = 650] loss: 2.108, acc: 0.546
******** [step = 700] loss: 2.113, acc: 0.546
******** [step = 750] loss: 2.116, acc: 0.545
******** [step = 800] loss: 2.120, acc: 0.545
******** [step = 850] loss: 2.124, acc: 0.545
EPOCH = 63 loss: 2.124, acc: 0.545, val_loss: 2.604, val_acc: 0.515

================================================================================2025-08_13 19:43:58
******** [step = 50] loss: 2.059, acc: 0.549
******** [step = 100] loss: 2.062, acc: 0.550
******** [step = 150] loss: 2.065, acc: 0.550
******** [step = 200] loss: 2.076, acc: 0.549
******** [step = 250] loss: 2.082, acc: 0.548
******** [step = 300] loss: 2.085, acc: 0.548
******** [step = 350] loss: 2.088, acc: 0.548
******** [step = 400] loss: 2.090, acc: 0.548
******** [step = 450] loss: 2.090, acc: 0.547
******** [step = 500] loss: 2.096, acc: 0.547
******** [step = 550] loss: 2.100, acc: 0.546
******** [step = 600] loss: 2.103, acc: 0.546
******** [step = 650] loss: 2.105, acc: 0.546
******** [step = 700] loss: 2.108, acc: 0.546
******** [step = 750] loss: 2.112, acc: 0.546
******** [step = 800] loss: 2.115, acc: 0.545
******** [step = 850] loss: 2.120, acc: 0.545
EPOCH = 64 loss: 2.120, acc: 0.545, val_loss: 2.610, val_acc: 0.512

================================================================================2025-08_13 19:45:58
******** [step = 50] loss: 2.052, acc: 0.550
******** [step = 100] loss: 2.051, acc: 0.550
******** [step = 150] loss: 2.055, acc: 0.550
******** [step = 200] loss: 2.059, acc: 0.551
******** [step = 250] loss: 2.064, acc: 0.551
******** [step = 300] loss: 2.068, acc: 0.550
******** [step = 350] loss: 2.074, acc: 0.549
******** [step = 400] loss: 2.079, acc: 0.549
******** [step = 450] loss: 2.084, acc: 0.548
******** [step = 500] loss: 2.088, acc: 0.548
******** [step = 550] loss: 2.092, acc: 0.548
******** [step = 600] loss: 2.098, acc: 0.547
******** [step = 650] loss: 2.101, acc: 0.547
******** [step = 700] loss: 2.104, acc: 0.547
******** [step = 750] loss: 2.108, acc: 0.546
******** [step = 800] loss: 2.112, acc: 0.546
******** [step = 850] loss: 2.117, acc: 0.545
EPOCH = 65 loss: 2.117, acc: 0.545, val_loss: 2.602, val_acc: 0.515

================================================================================2025-08_13 19:47:59
******** [step = 50] loss: 2.047, acc: 0.550
******** [step = 100] loss: 2.038, acc: 0.553
******** [step = 150] loss: 2.047, acc: 0.553
******** [step = 200] loss: 2.057, acc: 0.552
******** [step = 250] loss: 2.062, acc: 0.551
******** [step = 300] loss: 2.067, acc: 0.550
******** [step = 350] loss: 2.071, acc: 0.550
******** [step = 400] loss: 2.077, acc: 0.549
******** [step = 450] loss: 2.082, acc: 0.549
******** [step = 500] loss: 2.084, acc: 0.549
******** [step = 550] loss: 2.088, acc: 0.548
******** [step = 600] loss: 2.092, acc: 0.548
******** [step = 650] loss: 2.096, acc: 0.548
******** [step = 700] loss: 2.100, acc: 0.547
******** [step = 750] loss: 2.104, acc: 0.547
******** [step = 800] loss: 2.109, acc: 0.546
******** [step = 850] loss: 2.113, acc: 0.546
EPOCH = 66 loss: 2.113, acc: 0.546, val_loss: 2.599, val_acc: 0.513

================================================================================2025-08_13 19:50:00
******** [step = 50] loss: 2.031, acc: 0.556
******** [step = 100] loss: 2.042, acc: 0.555
******** [step = 150] loss: 2.049, acc: 0.554
******** [step = 200] loss: 2.055, acc: 0.553
******** [step = 250] loss: 2.063, acc: 0.552
******** [step = 300] loss: 2.067, acc: 0.551
******** [step = 350] loss: 2.072, acc: 0.550
******** [step = 400] loss: 2.075, acc: 0.550
******** [step = 450] loss: 2.081, acc: 0.549
******** [step = 500] loss: 2.085, acc: 0.549
******** [step = 550] loss: 2.089, acc: 0.548
******** [step = 600] loss: 2.093, acc: 0.548
******** [step = 650] loss: 2.094, acc: 0.548
******** [step = 700] loss: 2.098, acc: 0.547
******** [step = 750] loss: 2.102, acc: 0.547
******** [step = 800] loss: 2.105, acc: 0.547
******** [step = 850] loss: 2.108, acc: 0.547
EPOCH = 67 loss: 2.108, acc: 0.547, val_loss: 2.595, val_acc: 0.515

================================================================================2025-08_13 19:52:00
******** [step = 50] loss: 2.036, acc: 0.554
******** [step = 100] loss: 2.039, acc: 0.554
******** [step = 150] loss: 2.043, acc: 0.553
******** [step = 200] loss: 2.048, acc: 0.552
******** [step = 250] loss: 2.053, acc: 0.552
******** [step = 300] loss: 2.059, acc: 0.551
******** [step = 350] loss: 2.067, acc: 0.551
******** [step = 400] loss: 2.069, acc: 0.551
******** [step = 450] loss: 2.075, acc: 0.550
******** [step = 500] loss: 2.079, acc: 0.550
******** [step = 550] loss: 2.083, acc: 0.549
******** [step = 600] loss: 2.088, acc: 0.549
******** [step = 650] loss: 2.092, acc: 0.549
******** [step = 700] loss: 2.097, acc: 0.548
******** [step = 750] loss: 2.099, acc: 0.548
******** [step = 800] loss: 2.103, acc: 0.547
******** [step = 850] loss: 2.107, acc: 0.547
EPOCH = 68 loss: 2.107, acc: 0.547, val_loss: 2.596, val_acc: 0.517

================================================================================2025-08_13 19:54:01
******** [step = 50] loss: 2.031, acc: 0.556
******** [step = 100] loss: 2.037, acc: 0.555
******** [step = 150] loss: 2.043, acc: 0.555
******** [step = 200] loss: 2.046, acc: 0.554
******** [step = 250] loss: 2.052, acc: 0.553
******** [step = 300] loss: 2.054, acc: 0.552
******** [step = 350] loss: 2.058, acc: 0.552
******** [step = 400] loss: 2.065, acc: 0.551
******** [step = 450] loss: 2.067, acc: 0.551
******** [step = 500] loss: 2.072, acc: 0.550
******** [step = 550] loss: 2.077, acc: 0.550
******** [step = 600] loss: 2.080, acc: 0.550
******** [step = 650] loss: 2.085, acc: 0.549
******** [step = 700] loss: 2.091, acc: 0.548
******** [step = 750] loss: 2.095, acc: 0.548
******** [step = 800] loss: 2.099, acc: 0.548
******** [step = 850] loss: 2.102, acc: 0.548
EPOCH = 69 loss: 2.102, acc: 0.548, val_loss: 2.592, val_acc: 0.517

================================================================================2025-08_13 19:56:02
******** [step = 50] loss: 2.017, acc: 0.557
******** [step = 100] loss: 2.017, acc: 0.557
******** [step = 150] loss: 2.029, acc: 0.556
******** [step = 200] loss: 2.035, acc: 0.554
******** [step = 250] loss: 2.042, acc: 0.553
******** [step = 300] loss: 2.047, acc: 0.553
******** [step = 350] loss: 2.056, acc: 0.552
******** [step = 400] loss: 2.057, acc: 0.552
******** [step = 450] loss: 2.065, acc: 0.551
******** [step = 500] loss: 2.069, acc: 0.551
******** [step = 550] loss: 2.074, acc: 0.551
******** [step = 600] loss: 2.078, acc: 0.551
******** [step = 650] loss: 2.083, acc: 0.550
******** [step = 700] loss: 2.086, acc: 0.549
******** [step = 750] loss: 2.091, acc: 0.549
******** [step = 800] loss: 2.095, acc: 0.549
******** [step = 850] loss: 2.098, acc: 0.548
EPOCH = 70 loss: 2.098, acc: 0.548, val_loss: 2.592, val_acc: 0.515

================================================================================2025-08_13 19:58:02
******** [step = 50] loss: 2.013, acc: 0.556
******** [step = 100] loss: 2.023, acc: 0.555
******** [step = 150] loss: 2.028, acc: 0.555
******** [step = 200] loss: 2.033, acc: 0.555
******** [step = 250] loss: 2.039, acc: 0.554
******** [step = 300] loss: 2.045, acc: 0.554
******** [step = 350] loss: 2.051, acc: 0.554
******** [step = 400] loss: 2.057, acc: 0.553
******** [step = 450] loss: 2.063, acc: 0.553
******** [step = 500] loss: 2.068, acc: 0.552
******** [step = 550] loss: 2.072, acc: 0.552
******** [step = 600] loss: 2.076, acc: 0.551
******** [step = 650] loss: 2.080, acc: 0.551
******** [step = 700] loss: 2.085, acc: 0.550
******** [step = 750] loss: 2.089, acc: 0.550
******** [step = 800] loss: 2.093, acc: 0.549
******** [step = 850] loss: 2.095, acc: 0.549
EPOCH = 71 loss: 2.095, acc: 0.549, val_loss: 2.592, val_acc: 0.516

================================================================================2025-08_13 20:00:03
******** [step = 50] loss: 2.032, acc: 0.557
******** [step = 100] loss: 2.034, acc: 0.556
******** [step = 150] loss: 2.036, acc: 0.556
******** [step = 200] loss: 2.040, acc: 0.555
******** [step = 250] loss: 2.043, acc: 0.555
******** [step = 300] loss: 2.049, acc: 0.554
******** [step = 350] loss: 2.052, acc: 0.553
******** [step = 400] loss: 2.057, acc: 0.553
******** [step = 450] loss: 2.059, acc: 0.553
******** [step = 500] loss: 2.065, acc: 0.552
******** [step = 550] loss: 2.069, acc: 0.551
******** [step = 600] loss: 2.073, acc: 0.551
******** [step = 650] loss: 2.078, acc: 0.550
******** [step = 700] loss: 2.082, acc: 0.550
******** [step = 750] loss: 2.086, acc: 0.550
******** [step = 800] loss: 2.089, acc: 0.550
******** [step = 850] loss: 2.093, acc: 0.549
EPOCH = 72 loss: 2.093, acc: 0.549, val_loss: 2.591, val_acc: 0.517

================================================================================2025-08_13 20:02:03
******** [step = 50] loss: 1.991, acc: 0.561
******** [step = 100] loss: 2.011, acc: 0.558
******** [step = 150] loss: 2.023, acc: 0.557
******** [step = 200] loss: 2.029, acc: 0.556
******** [step = 250] loss: 2.034, acc: 0.556
******** [step = 300] loss: 2.042, acc: 0.555
******** [step = 350] loss: 2.048, acc: 0.554
******** [step = 400] loss: 2.054, acc: 0.553
******** [step = 450] loss: 2.059, acc: 0.552
******** [step = 500] loss: 2.064, acc: 0.552
******** [step = 550] loss: 2.068, acc: 0.552
******** [step = 600] loss: 2.073, acc: 0.551
******** [step = 650] loss: 2.076, acc: 0.551
******** [step = 700] loss: 2.081, acc: 0.551
******** [step = 750] loss: 2.085, acc: 0.550
******** [step = 800] loss: 2.088, acc: 0.550
******** [step = 850] loss: 2.090, acc: 0.550
EPOCH = 73 loss: 2.090, acc: 0.550, val_loss: 2.591, val_acc: 0.516

================================================================================2025-08_13 20:04:04
******** [step = 50] loss: 2.011, acc: 0.558
******** [step = 100] loss: 2.017, acc: 0.556
******** [step = 150] loss: 2.029, acc: 0.555
******** [step = 200] loss: 2.034, acc: 0.554
******** [step = 250] loss: 2.040, acc: 0.554
******** [step = 300] loss: 2.043, acc: 0.554
******** [step = 350] loss: 2.048, acc: 0.554
******** [step = 400] loss: 2.054, acc: 0.553
******** [step = 450] loss: 2.057, acc: 0.553
******** [step = 500] loss: 2.062, acc: 0.552
******** [step = 550] loss: 2.066, acc: 0.552
******** [step = 600] loss: 2.069, acc: 0.551
******** [step = 650] loss: 2.073, acc: 0.551
******** [step = 700] loss: 2.078, acc: 0.551
******** [step = 750] loss: 2.082, acc: 0.550
******** [step = 800] loss: 2.085, acc: 0.550
******** [step = 850] loss: 2.088, acc: 0.550
EPOCH = 74 loss: 2.088, acc: 0.550, val_loss: 2.590, val_acc: 0.519

================================================================================2025-08_13 20:06:05
******** [step = 50] loss: 2.003, acc: 0.560
******** [step = 100] loss: 2.003, acc: 0.559
******** [step = 150] loss: 2.015, acc: 0.558
******** [step = 200] loss: 2.027, acc: 0.556
******** [step = 250] loss: 2.034, acc: 0.555
******** [step = 300] loss: 2.039, acc: 0.555
******** [step = 350] loss: 2.045, acc: 0.554
******** [step = 400] loss: 2.051, acc: 0.554
******** [step = 450] loss: 2.053, acc: 0.553
******** [step = 500] loss: 2.058, acc: 0.553
******** [step = 550] loss: 2.063, acc: 0.553
******** [step = 600] loss: 2.069, acc: 0.552
******** [step = 650] loss: 2.072, acc: 0.552
******** [step = 700] loss: 2.076, acc: 0.551
******** [step = 750] loss: 2.079, acc: 0.551
******** [step = 800] loss: 2.083, acc: 0.551
******** [step = 850] loss: 2.084, acc: 0.550
EPOCH = 75 loss: 2.084, acc: 0.550, val_loss: 2.588, val_acc: 0.519

================================================================================2025-08_13 20:08:05
******** [step = 50] loss: 2.016, acc: 0.556
******** [step = 100] loss: 2.021, acc: 0.556
******** [step = 150] loss: 2.029, acc: 0.555
******** [step = 200] loss: 2.034, acc: 0.554
******** [step = 250] loss: 2.040, acc: 0.554
******** [step = 300] loss: 2.045, acc: 0.554
******** [step = 350] loss: 2.047, acc: 0.553
******** [step = 400] loss: 2.050, acc: 0.553
******** [step = 450] loss: 2.053, acc: 0.553
******** [step = 500] loss: 2.059, acc: 0.552
******** [step = 550] loss: 2.061, acc: 0.552
******** [step = 600] loss: 2.065, acc: 0.552
******** [step = 650] loss: 2.068, acc: 0.552
******** [step = 700] loss: 2.072, acc: 0.551
******** [step = 750] loss: 2.076, acc: 0.551
******** [step = 800] loss: 2.078, acc: 0.551
******** [step = 850] loss: 2.082, acc: 0.550
EPOCH = 76 loss: 2.082, acc: 0.550, val_loss: 2.589, val_acc: 0.517

================================================================================2025-08_13 20:10:06
******** [step = 50] loss: 1.987, acc: 0.560
******** [step = 100] loss: 1.999, acc: 0.560
******** [step = 150] loss: 2.005, acc: 0.559
******** [step = 200] loss: 2.014, acc: 0.558
******** [step = 250] loss: 2.018, acc: 0.558
******** [step = 300] loss: 2.026, acc: 0.557
******** [step = 350] loss: 2.032, acc: 0.556
******** [step = 400] loss: 2.039, acc: 0.555
******** [step = 450] loss: 2.043, acc: 0.555
******** [step = 500] loss: 2.048, acc: 0.554
******** [step = 550] loss: 2.054, acc: 0.553
******** [step = 600] loss: 2.059, acc: 0.553
******** [step = 650] loss: 2.065, acc: 0.553
******** [step = 700] loss: 2.069, acc: 0.553
******** [step = 750] loss: 2.072, acc: 0.553
******** [step = 800] loss: 2.076, acc: 0.552
******** [step = 850] loss: 2.078, acc: 0.552
EPOCH = 77 loss: 2.078, acc: 0.552, val_loss: 2.593, val_acc: 0.516

================================================================================2025-08_13 20:12:06
******** [step = 50] loss: 2.008, acc: 0.558
******** [step = 100] loss: 2.008, acc: 0.558
******** [step = 150] loss: 2.013, acc: 0.558
******** [step = 200] loss: 2.022, acc: 0.557
******** [step = 250] loss: 2.029, acc: 0.556
******** [step = 300] loss: 2.034, acc: 0.555
******** [step = 350] loss: 2.038, acc: 0.555
******** [step = 400] loss: 2.043, acc: 0.555
******** [step = 450] loss: 2.047, acc: 0.555
******** [step = 500] loss: 2.053, acc: 0.554
******** [step = 550] loss: 2.056, acc: 0.554
******** [step = 600] loss: 2.059, acc: 0.553
******** [step = 650] loss: 2.064, acc: 0.553
******** [step = 700] loss: 2.069, acc: 0.553
******** [step = 750] loss: 2.072, acc: 0.552
******** [step = 800] loss: 2.075, acc: 0.552
******** [step = 850] loss: 2.080, acc: 0.551
EPOCH = 78 loss: 2.080, acc: 0.551, val_loss: 2.590, val_acc: 0.519

================================================================================2025-08_13 20:14:07
******** [step = 50] loss: 2.016, acc: 0.557
******** [step = 100] loss: 2.012, acc: 0.559
******** [step = 150] loss: 2.015, acc: 0.559
******** [step = 200] loss: 2.017, acc: 0.559
******** [step = 250] loss: 2.022, acc: 0.558
******** [step = 300] loss: 2.026, acc: 0.558
******** [step = 350] loss: 2.032, acc: 0.557
******** [step = 400] loss: 2.037, acc: 0.556
******** [step = 450] loss: 2.044, acc: 0.555
******** [step = 500] loss: 2.047, acc: 0.555
******** [step = 550] loss: 2.053, acc: 0.554
******** [step = 600] loss: 2.056, acc: 0.554
******** [step = 650] loss: 2.061, acc: 0.553
******** [step = 700] loss: 2.064, acc: 0.553
******** [step = 750] loss: 2.067, acc: 0.553
******** [step = 800] loss: 2.069, acc: 0.553
******** [step = 850] loss: 2.072, acc: 0.552
EPOCH = 79 loss: 2.072, acc: 0.552, val_loss: 2.588, val_acc: 0.519

================================================================================2025-08_13 20:16:07
******** [step = 50] loss: 1.988, acc: 0.563
******** [step = 100] loss: 2.001, acc: 0.561
******** [step = 150] loss: 2.006, acc: 0.560
******** [step = 200] loss: 2.012, acc: 0.560
******** [step = 250] loss: 2.021, acc: 0.559
******** [step = 300] loss: 2.026, acc: 0.558
******** [step = 350] loss: 2.030, acc: 0.557
******** [step = 400] loss: 2.034, acc: 0.557
******** [step = 450] loss: 2.040, acc: 0.556
******** [step = 500] loss: 2.045, acc: 0.555
******** [step = 550] loss: 2.050, acc: 0.555
******** [step = 600] loss: 2.054, acc: 0.554
******** [step = 650] loss: 2.057, acc: 0.554
******** [step = 700] loss: 2.060, acc: 0.554
******** [step = 750] loss: 2.064, acc: 0.553
******** [step = 800] loss: 2.067, acc: 0.553
******** [step = 850] loss: 2.070, acc: 0.553
EPOCH = 80 loss: 2.070, acc: 0.553, val_loss: 2.591, val_acc: 0.516

================================================================================2025-08_13 20:18:08
finishing training...
Training complete in 161m 16s
    epoch  ...   val_acc
0     1.0  ...  0.361908
1     2.0  ...  0.388517
2     3.0  ...  0.406154
3     4.0  ...  0.410852
4     5.0  ...  0.421988
..    ...  ...       ...
75   76.0  ...  0.516873
76   77.0  ...  0.515556
77   78.0  ...  0.519022
78   79.0  ...  0.518714
79   80.0  ...  0.515970

[80 rows x 5 columns]
== Done ==
Wed Aug 13 08:18:32 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-13-2025 20:18:34
Job ID:        7023009
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=10:49:28,vmem=0,walltime=02:42:22,mem=35184K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-008-35-0
---------------------------------------
