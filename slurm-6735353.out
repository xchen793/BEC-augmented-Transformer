---------------------------------------
Begin Slurm Prolog: Aug-09-2025 12:50:41
Job ID:    6735353
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sat Aug  9 12:50:41 PM EDT 2025
atl1-1-01-003-35-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Sat Aug  9 12:50:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   43C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 90681.45it/s] 28%|██▊       | 38514/135842 [00:00<00:00, 145543.21it/s] 40%|████      | 55002/135842 [00:00<00:00, 115436.49it/s] 51%|█████     | 69378/135842 [00:00<00:00, 92948.91it/s]  66%|██████▌   | 89471/135842 [00:00<00:00, 118738.45it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90123.94it/s] 92%|█████████▏| 125462/135842 [00:01<00:00, 108474.37it/s]100%|██████████| 135842/135842 [00:01<00:00, 109764.21it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47723.53it/s] 33%|███▎      | 35644/108673 [00:00<00:00, 85668.59it/s] 50%|████▉     | 53883/108673 [00:00<00:00, 113411.68it/s] 66%|██████▋   | 72139/108673 [00:00<00:00, 133507.64it/s] 81%|████████▏ | 88562/108673 [00:01<00:00, 71468.80it/s]  98%|█████████▊| 106890/108673 [00:01<00:00, 90886.97it/s]100%|██████████| 108673/108673 [00:01<00:00, 89557.02it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▌   | 17889/27169 [00:00<00:00, 178876.95it/s]100%|██████████| 27169/27169 [00:00<00:00, 180546.87it/s]
tensor([    3,     5,    21,   220,    83,    71,    35,    12,   459,   143,
          272,     7, 18672,    28,  1171,    33,  1783,    14,   667,     4,
            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,     5,    32,     6,    14,  1307,    26,    22,   141,     6,
           74,  2275,  1139,     7,   435,    42,   316,    22,   232, 13245,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([   3,    5,  108,  250,   31,   10,  533,   55,  153,   78,    5,   29,
         232,   10,  148, 1717,  725,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_09 12:51:52
******** [step = 50] loss: 9.332, acc: 0.026
******** [step = 100] loss: 8.893, acc: 0.082
******** [step = 150] loss: 8.539, acc: 0.127
******** [step = 200] loss: 8.204, acc: 0.159
******** [step = 250] loss: 7.886, acc: 0.180
******** [step = 300] loss: 7.572, acc: 0.197
******** [step = 350] loss: 7.266, acc: 0.212
******** [step = 400] loss: 6.980, acc: 0.226
******** [step = 450] loss: 6.719, acc: 0.241
******** [step = 500] loss: 6.488, acc: 0.255
******** [step = 550] loss: 6.284, acc: 0.267
******** [step = 600] loss: 6.101, acc: 0.279
******** [step = 650] loss: 5.935, acc: 0.289
******** [step = 700] loss: 5.786, acc: 0.299
******** [step = 750] loss: 5.651, acc: 0.308
******** [step = 800] loss: 5.525, acc: 0.316
******** [step = 850] loss: 5.409, acc: 0.325
EPOCH = 1 loss: 5.409, acc: 0.325, val_loss: 3.404, val_acc: 0.474

================================================================================2025-08_09 12:53:11
******** [step = 50] loss: 3.470, acc: 0.458
******** [step = 100] loss: 3.417, acc: 0.464
******** [step = 150] loss: 3.385, acc: 0.467
******** [step = 200] loss: 3.359, acc: 0.470
******** [step = 250] loss: 3.324, acc: 0.475
******** [step = 300] loss: 3.293, acc: 0.478
******** [step = 350] loss: 3.265, acc: 0.482
******** [step = 400] loss: 3.232, acc: 0.486
******** [step = 450] loss: 3.205, acc: 0.489
******** [step = 500] loss: 3.181, acc: 0.492
******** [step = 550] loss: 3.156, acc: 0.495
******** [step = 600] loss: 3.130, acc: 0.498
******** [step = 650] loss: 3.105, acc: 0.501
******** [step = 700] loss: 3.079, acc: 0.504
******** [step = 750] loss: 3.055, acc: 0.507
******** [step = 800] loss: 3.030, acc: 0.510
******** [step = 850] loss: 3.005, acc: 0.513
EPOCH = 2 loss: 3.005, acc: 0.513, val_loss: 2.494, val_acc: 0.582

================================================================================2025-08_09 12:54:30
******** [step = 50] loss: 2.574, acc: 0.560
******** [step = 100] loss: 2.513, acc: 0.569
******** [step = 150] loss: 2.484, acc: 0.574
******** [step = 200] loss: 2.470, acc: 0.576
******** [step = 250] loss: 2.449, acc: 0.579
******** [step = 300] loss: 2.434, acc: 0.582
******** [step = 350] loss: 2.417, acc: 0.584
******** [step = 400] loss: 2.402, acc: 0.586
******** [step = 450] loss: 2.389, acc: 0.588
******** [step = 500] loss: 2.378, acc: 0.590
******** [step = 550] loss: 2.364, acc: 0.592
******** [step = 600] loss: 2.350, acc: 0.594
******** [step = 650] loss: 2.336, acc: 0.596
******** [step = 700] loss: 2.323, acc: 0.598
******** [step = 750] loss: 2.307, acc: 0.600
******** [step = 800] loss: 2.292, acc: 0.602
******** [step = 850] loss: 2.282, acc: 0.604
EPOCH = 3 loss: 2.282, acc: 0.604, val_loss: 1.963, val_acc: 0.657

================================================================================2025-08_09 12:55:50
******** [step = 50] loss: 2.010, acc: 0.638
******** [step = 100] loss: 1.981, acc: 0.640
******** [step = 150] loss: 1.960, acc: 0.643
******** [step = 200] loss: 1.947, acc: 0.645
******** [step = 250] loss: 1.939, acc: 0.647
******** [step = 300] loss: 1.934, acc: 0.648
******** [step = 350] loss: 1.930, acc: 0.648
******** [step = 400] loss: 1.924, acc: 0.648
******** [step = 450] loss: 1.913, acc: 0.650
******** [step = 500] loss: 1.911, acc: 0.650
******** [step = 550] loss: 1.907, acc: 0.651
******** [step = 600] loss: 1.902, acc: 0.652
******** [step = 650] loss: 1.896, acc: 0.653
******** [step = 700] loss: 1.891, acc: 0.653
******** [step = 750] loss: 1.886, acc: 0.654
******** [step = 800] loss: 1.880, acc: 0.655
******** [step = 850] loss: 1.874, acc: 0.656
EPOCH = 4 loss: 1.874, acc: 0.656, val_loss: 1.665, val_acc: 0.696

================================================================================2025-08_09 12:57:09
******** [step = 50] loss: 1.665, acc: 0.679
******** [step = 100] loss: 1.655, acc: 0.681
******** [step = 150] loss: 1.651, acc: 0.682
******** [step = 200] loss: 1.655, acc: 0.682
******** [step = 250] loss: 1.652, acc: 0.683
******** [step = 300] loss: 1.652, acc: 0.683
******** [step = 350] loss: 1.652, acc: 0.684
******** [step = 400] loss: 1.653, acc: 0.683
******** [step = 450] loss: 1.650, acc: 0.684
******** [step = 500] loss: 1.652, acc: 0.684
******** [step = 550] loss: 1.651, acc: 0.684
******** [step = 600] loss: 1.649, acc: 0.685
******** [step = 650] loss: 1.649, acc: 0.685
******** [step = 700] loss: 1.649, acc: 0.685
******** [step = 750] loss: 1.648, acc: 0.685
******** [step = 800] loss: 1.646, acc: 0.686
******** [step = 850] loss: 1.644, acc: 0.686
EPOCH = 5 loss: 1.644, acc: 0.686, val_loss: 1.503, val_acc: 0.719

================================================================================2025-08_09 12:58:28
******** [step = 50] loss: 1.466, acc: 0.708
******** [step = 100] loss: 1.449, acc: 0.710
******** [step = 150] loss: 1.447, acc: 0.711
******** [step = 200] loss: 1.452, acc: 0.710
******** [step = 250] loss: 1.457, acc: 0.709
******** [step = 300] loss: 1.459, acc: 0.709
******** [step = 350] loss: 1.459, acc: 0.710
******** [step = 400] loss: 1.460, acc: 0.710
******** [step = 450] loss: 1.459, acc: 0.710
******** [step = 500] loss: 1.464, acc: 0.710
******** [step = 550] loss: 1.462, acc: 0.710
******** [step = 600] loss: 1.461, acc: 0.711
******** [step = 650] loss: 1.459, acc: 0.711
******** [step = 700] loss: 1.456, acc: 0.712
******** [step = 750] loss: 1.456, acc: 0.713
******** [step = 800] loss: 1.456, acc: 0.713
******** [step = 850] loss: 1.453, acc: 0.714
EPOCH = 6 loss: 1.453, acc: 0.714, val_loss: 1.397, val_acc: 0.738

================================================================================2025-08_09 12:59:48
******** [step = 50] loss: 1.275, acc: 0.737
******** [step = 100] loss: 1.277, acc: 0.736
******** [step = 150] loss: 1.276, acc: 0.737
******** [step = 200] loss: 1.280, acc: 0.736
******** [step = 250] loss: 1.285, acc: 0.736
******** [step = 300] loss: 1.289, acc: 0.736
******** [step = 350] loss: 1.288, acc: 0.736
******** [step = 400] loss: 1.290, acc: 0.736
******** [step = 450] loss: 1.293, acc: 0.736
******** [step = 500] loss: 1.294, acc: 0.736
******** [step = 550] loss: 1.295, acc: 0.736
******** [step = 600] loss: 1.296, acc: 0.737
******** [step = 650] loss: 1.297, acc: 0.737
******** [step = 700] loss: 1.298, acc: 0.737
******** [step = 750] loss: 1.299, acc: 0.737
******** [step = 800] loss: 1.301, acc: 0.737
******** [step = 850] loss: 1.301, acc: 0.737
EPOCH = 7 loss: 1.301, acc: 0.737, val_loss: 1.294, val_acc: 0.756

================================================================================2025-08_09 13:01:07
******** [step = 50] loss: 1.153, acc: 0.758
******** [step = 100] loss: 1.131, acc: 0.762
******** [step = 150] loss: 1.142, acc: 0.760
******** [step = 200] loss: 1.149, acc: 0.759
******** [step = 250] loss: 1.151, acc: 0.759
******** [step = 300] loss: 1.157, acc: 0.759
******** [step = 350] loss: 1.161, acc: 0.758
******** [step = 400] loss: 1.166, acc: 0.758
******** [step = 450] loss: 1.172, acc: 0.757
******** [step = 500] loss: 1.176, acc: 0.757
******** [step = 550] loss: 1.180, acc: 0.756
******** [step = 600] loss: 1.184, acc: 0.756
******** [step = 650] loss: 1.186, acc: 0.756
******** [step = 700] loss: 1.189, acc: 0.755
******** [step = 750] loss: 1.191, acc: 0.755
******** [step = 800] loss: 1.194, acc: 0.755
******** [step = 850] loss: 1.195, acc: 0.755
EPOCH = 8 loss: 1.195, acc: 0.755, val_loss: 1.266, val_acc: 0.763

================================================================================2025-08_09 13:02:26
******** [step = 50] loss: 1.088, acc: 0.766
******** [step = 100] loss: 1.077, acc: 0.770
******** [step = 150] loss: 1.071, acc: 0.772
******** [step = 200] loss: 1.077, acc: 0.772
******** [step = 250] loss: 1.078, acc: 0.772
******** [step = 300] loss: 1.080, acc: 0.772
******** [step = 350] loss: 1.082, acc: 0.772
******** [step = 400] loss: 1.086, acc: 0.771
******** [step = 450] loss: 1.091, acc: 0.771
******** [step = 500] loss: 1.093, acc: 0.771
******** [step = 550] loss: 1.096, acc: 0.771
******** [step = 600] loss: 1.100, acc: 0.770
******** [step = 650] loss: 1.102, acc: 0.770
******** [step = 700] loss: 1.106, acc: 0.770
******** [step = 750] loss: 1.108, acc: 0.770
******** [step = 800] loss: 1.110, acc: 0.770
******** [step = 850] loss: 1.112, acc: 0.769
EPOCH = 9 loss: 1.112, acc: 0.769, val_loss: 1.196, val_acc: 0.776

================================================================================2025-08_09 13:03:46
******** [step = 50] loss: 0.994, acc: 0.789
******** [step = 100] loss: 1.001, acc: 0.787
******** [step = 150] loss: 1.002, acc: 0.786
******** [step = 200] loss: 1.006, acc: 0.786
******** [step = 250] loss: 1.009, acc: 0.785
******** [step = 300] loss: 1.010, acc: 0.785
******** [step = 350] loss: 1.012, acc: 0.785
******** [step = 400] loss: 1.013, acc: 0.785
******** [step = 450] loss: 1.018, acc: 0.784
******** [step = 500] loss: 1.023, acc: 0.784
******** [step = 550] loss: 1.029, acc: 0.783
******** [step = 600] loss: 1.030, acc: 0.783
******** [step = 650] loss: 1.031, acc: 0.783
******** [step = 700] loss: 1.033, acc: 0.783
******** [step = 750] loss: 1.035, acc: 0.783
******** [step = 800] loss: 1.038, acc: 0.783
******** [step = 850] loss: 1.039, acc: 0.783
EPOCH = 10 loss: 1.039, acc: 0.783, val_loss: 1.161, val_acc: 0.782

================================================================================2025-08_09 13:05:05
******** [step = 50] loss: 0.908, acc: 0.799
******** [step = 100] loss: 0.916, acc: 0.799
******** [step = 150] loss: 0.926, acc: 0.799
******** [step = 200] loss: 0.930, acc: 0.799
******** [step = 250] loss: 0.931, acc: 0.799
******** [step = 300] loss: 0.939, acc: 0.798
******** [step = 350] loss: 0.946, acc: 0.797
******** [step = 400] loss: 0.951, acc: 0.796
******** [step = 450] loss: 0.954, acc: 0.795
******** [step = 500] loss: 0.960, acc: 0.795
******** [step = 550] loss: 0.964, acc: 0.794
******** [step = 600] loss: 0.967, acc: 0.794
******** [step = 650] loss: 0.970, acc: 0.793
******** [step = 700] loss: 0.974, acc: 0.793
******** [step = 750] loss: 0.975, acc: 0.793
******** [step = 800] loss: 0.979, acc: 0.793
******** [step = 850] loss: 0.980, acc: 0.793
EPOCH = 11 loss: 0.980, acc: 0.793, val_loss: 1.142, val_acc: 0.789

================================================================================2025-08_09 13:06:24
******** [step = 50] loss: 0.870, acc: 0.808
******** [step = 100] loss: 0.872, acc: 0.809
******** [step = 150] loss: 0.883, acc: 0.807
******** [step = 200] loss: 0.886, acc: 0.807
******** [step = 250] loss: 0.889, acc: 0.806
******** [step = 300] loss: 0.891, acc: 0.806
******** [step = 350] loss: 0.897, acc: 0.805
******** [step = 400] loss: 0.901, acc: 0.804
******** [step = 450] loss: 0.904, acc: 0.804
******** [step = 500] loss: 0.907, acc: 0.804
******** [step = 550] loss: 0.912, acc: 0.803
******** [step = 600] loss: 0.917, acc: 0.803
******** [step = 650] loss: 0.920, acc: 0.802
******** [step = 700] loss: 0.925, acc: 0.802
******** [step = 750] loss: 0.927, acc: 0.802
******** [step = 800] loss: 0.929, acc: 0.801
******** [step = 850] loss: 0.931, acc: 0.801
EPOCH = 12 loss: 0.931, acc: 0.801, val_loss: 1.118, val_acc: 0.792

================================================================================2025-08_09 13:07:44
******** [step = 50] loss: 0.837, acc: 0.817
******** [step = 100] loss: 0.826, acc: 0.819
******** [step = 150] loss: 0.831, acc: 0.818
******** [step = 200] loss: 0.836, acc: 0.816
******** [step = 250] loss: 0.836, acc: 0.816
******** [step = 300] loss: 0.842, acc: 0.815
******** [step = 350] loss: 0.845, acc: 0.815
******** [step = 400] loss: 0.849, acc: 0.814
******** [step = 450] loss: 0.856, acc: 0.813
******** [step = 500] loss: 0.860, acc: 0.812
******** [step = 550] loss: 0.864, acc: 0.812
******** [step = 600] loss: 0.868, acc: 0.811
******** [step = 650] loss: 0.871, acc: 0.811
******** [step = 700] loss: 0.874, acc: 0.811
******** [step = 750] loss: 0.878, acc: 0.810
******** [step = 800] loss: 0.884, acc: 0.809
******** [step = 850] loss: 0.887, acc: 0.809
EPOCH = 13 loss: 0.887, acc: 0.809, val_loss: 1.104, val_acc: 0.797

================================================================================2025-08_09 13:09:10
******** [step = 50] loss: 0.769, acc: 0.829
******** [step = 100] loss: 0.775, acc: 0.828
******** [step = 150] loss: 0.787, acc: 0.826
******** [step = 200] loss: 0.787, acc: 0.825
******** [step = 250] loss: 0.789, acc: 0.825
******** [step = 300] loss: 0.796, acc: 0.824
******** [step = 350] loss: 0.804, acc: 0.823
******** [step = 400] loss: 0.812, acc: 0.821
******** [step = 450] loss: 0.818, acc: 0.820
******** [step = 500] loss: 0.822, acc: 0.820
******** [step = 550] loss: 0.827, acc: 0.819
******** [step = 600] loss: 0.831, acc: 0.819
******** [step = 650] loss: 0.835, acc: 0.818
******** [step = 700] loss: 0.840, acc: 0.817
******** [step = 750] loss: 0.843, acc: 0.817
******** [step = 800] loss: 0.846, acc: 0.817
******** [step = 850] loss: 0.848, acc: 0.816
EPOCH = 14 loss: 0.848, acc: 0.816, val_loss: 1.092, val_acc: 0.799

================================================================================2025-08_09 13:10:29
******** [step = 50] loss: 0.746, acc: 0.832
******** [step = 100] loss: 0.751, acc: 0.832
******** [step = 150] loss: 0.759, acc: 0.830
******** [step = 200] loss: 0.761, acc: 0.830
******** [step = 250] loss: 0.769, acc: 0.828
******** [step = 300] loss: 0.775, acc: 0.827
******** [step = 350] loss: 0.777, acc: 0.827
******** [step = 400] loss: 0.782, acc: 0.827
******** [step = 450] loss: 0.786, acc: 0.826
******** [step = 500] loss: 0.790, acc: 0.825
******** [step = 550] loss: 0.792, acc: 0.825
******** [step = 600] loss: 0.796, acc: 0.825
******** [step = 650] loss: 0.802, acc: 0.824
******** [step = 700] loss: 0.807, acc: 0.823
******** [step = 750] loss: 0.809, acc: 0.823
******** [step = 800] loss: 0.812, acc: 0.822
******** [step = 850] loss: 0.814, acc: 0.822
EPOCH = 15 loss: 0.814, acc: 0.822, val_loss: 1.075, val_acc: 0.804

================================================================================2025-08_09 13:11:49
******** [step = 50] loss: 0.705, acc: 0.837
******** [step = 100] loss: 0.715, acc: 0.836
******** [step = 150] loss: 0.722, acc: 0.837
******** [step = 200] loss: 0.725, acc: 0.836
******** [step = 250] loss: 0.731, acc: 0.835
******** [step = 300] loss: 0.738, acc: 0.834
******** [step = 350] loss: 0.745, acc: 0.833
******** [step = 400] loss: 0.750, acc: 0.832
******** [step = 450] loss: 0.755, acc: 0.832
******** [step = 500] loss: 0.759, acc: 0.831
******** [step = 550] loss: 0.765, acc: 0.830
******** [step = 600] loss: 0.769, acc: 0.830
******** [step = 650] loss: 0.774, acc: 0.829
******** [step = 700] loss: 0.777, acc: 0.829
******** [step = 750] loss: 0.780, acc: 0.828
******** [step = 800] loss: 0.782, acc: 0.828
******** [step = 850] loss: 0.784, acc: 0.828
EPOCH = 16 loss: 0.784, acc: 0.828, val_loss: 1.067, val_acc: 0.806

================================================================================2025-08_09 13:13:08
******** [step = 50] loss: 0.680, acc: 0.844
******** [step = 100] loss: 0.680, acc: 0.846
******** [step = 150] loss: 0.693, acc: 0.843
******** [step = 200] loss: 0.699, acc: 0.842
******** [step = 250] loss: 0.708, acc: 0.840
******** [step = 300] loss: 0.711, acc: 0.840
******** [step = 350] loss: 0.716, acc: 0.839
******** [step = 400] loss: 0.723, acc: 0.838
******** [step = 450] loss: 0.726, acc: 0.837
******** [step = 500] loss: 0.732, acc: 0.837
******** [step = 550] loss: 0.736, acc: 0.836
******** [step = 600] loss: 0.741, acc: 0.835
******** [step = 650] loss: 0.745, acc: 0.834
******** [step = 700] loss: 0.750, acc: 0.834
******** [step = 750] loss: 0.753, acc: 0.833
******** [step = 800] loss: 0.756, acc: 0.833
******** [step = 850] loss: 0.758, acc: 0.833
EPOCH = 17 loss: 0.758, acc: 0.833, val_loss: 1.056, val_acc: 0.809

================================================================================2025-08_09 13:14:28
******** [step = 50] loss: 0.667, acc: 0.847
******** [step = 100] loss: 0.665, acc: 0.847
******** [step = 150] loss: 0.664, acc: 0.847
******** [step = 200] loss: 0.674, acc: 0.846
******** [step = 250] loss: 0.680, acc: 0.845
******** [step = 300] loss: 0.686, acc: 0.844
******** [step = 350] loss: 0.691, acc: 0.843
******** [step = 400] loss: 0.696, acc: 0.843
******** [step = 450] loss: 0.703, acc: 0.842
******** [step = 500] loss: 0.708, acc: 0.841
******** [step = 550] loss: 0.713, acc: 0.840
******** [step = 600] loss: 0.718, acc: 0.840
******** [step = 650] loss: 0.721, acc: 0.839
******** [step = 700] loss: 0.724, acc: 0.839
******** [step = 750] loss: 0.728, acc: 0.838
******** [step = 800] loss: 0.732, acc: 0.838
******** [step = 850] loss: 0.737, acc: 0.837
EPOCH = 18 loss: 0.737, acc: 0.837, val_loss: 1.057, val_acc: 0.811

================================================================================2025-08_09 13:15:48
******** [step = 50] loss: 0.654, acc: 0.849
******** [step = 100] loss: 0.651, acc: 0.850
******** [step = 150] loss: 0.649, acc: 0.850
******** [step = 200] loss: 0.655, acc: 0.849
******** [step = 250] loss: 0.662, acc: 0.848
******** [step = 300] loss: 0.665, acc: 0.848
******** [step = 350] loss: 0.670, acc: 0.847
******** [step = 400] loss: 0.675, acc: 0.846
******** [step = 450] loss: 0.677, acc: 0.846
******** [step = 500] loss: 0.682, acc: 0.845
******** [step = 550] loss: 0.688, acc: 0.845
******** [step = 600] loss: 0.693, acc: 0.844
******** [step = 650] loss: 0.698, acc: 0.843
******** [step = 700] loss: 0.701, acc: 0.843
******** [step = 750] loss: 0.705, acc: 0.842
******** [step = 800] loss: 0.709, acc: 0.842
******** [step = 850] loss: 0.713, acc: 0.841
EPOCH = 19 loss: 0.713, acc: 0.841, val_loss: 1.050, val_acc: 0.812

================================================================================2025-08_09 13:17:08
******** [step = 50] loss: 0.622, acc: 0.855
******** [step = 100] loss: 0.626, acc: 0.855
******** [step = 150] loss: 0.630, acc: 0.855
******** [step = 200] loss: 0.634, acc: 0.854
******** [step = 250] loss: 0.641, acc: 0.853
******** [step = 300] loss: 0.646, acc: 0.852
******** [step = 350] loss: 0.653, acc: 0.851
******** [step = 400] loss: 0.658, acc: 0.850
******** [step = 450] loss: 0.663, acc: 0.850
******** [step = 500] loss: 0.668, acc: 0.849
******** [step = 550] loss: 0.673, acc: 0.848
******** [step = 600] loss: 0.678, acc: 0.848
******** [step = 650] loss: 0.680, acc: 0.847
******** [step = 700] loss: 0.684, acc: 0.847
******** [step = 750] loss: 0.686, acc: 0.846
******** [step = 800] loss: 0.689, acc: 0.846
******** [step = 850] loss: 0.692, acc: 0.846
EPOCH = 20 loss: 0.692, acc: 0.846, val_loss: 1.052, val_acc: 0.813

================================================================================2025-08_09 13:18:29
******** [step = 50] loss: 0.593, acc: 0.861
******** [step = 100] loss: 0.604, acc: 0.860
******** [step = 150] loss: 0.615, acc: 0.859
******** [step = 200] loss: 0.619, acc: 0.858
******** [step = 250] loss: 0.623, acc: 0.857
******** [step = 300] loss: 0.627, acc: 0.857
******** [step = 350] loss: 0.634, acc: 0.855
******** [step = 400] loss: 0.638, acc: 0.854
******** [step = 450] loss: 0.641, acc: 0.854
******** [step = 500] loss: 0.646, acc: 0.853
******** [step = 550] loss: 0.652, acc: 0.852
******** [step = 600] loss: 0.657, acc: 0.851
******** [step = 650] loss: 0.661, acc: 0.851
******** [step = 700] loss: 0.665, acc: 0.850
******** [step = 750] loss: 0.668, acc: 0.850
******** [step = 800] loss: 0.673, acc: 0.849
******** [step = 850] loss: 0.675, acc: 0.849
EPOCH = 21 loss: 0.675, acc: 0.849, val_loss: 1.049, val_acc: 0.814

================================================================================2025-08_09 13:19:49
******** [step = 50] loss: 0.584, acc: 0.865
******** [step = 100] loss: 0.581, acc: 0.864
******** [step = 150] loss: 0.589, acc: 0.863
******** [step = 200] loss: 0.598, acc: 0.861
******** [step = 250] loss: 0.605, acc: 0.860
******** [step = 300] loss: 0.611, acc: 0.859
******** [step = 350] loss: 0.616, acc: 0.858
******** [step = 400] loss: 0.621, acc: 0.858
******** [step = 450] loss: 0.627, acc: 0.857
******** [step = 500] loss: 0.632, acc: 0.856
******** [step = 550] loss: 0.637, acc: 0.855
******** [step = 600] loss: 0.639, acc: 0.855
******** [step = 650] loss: 0.643, acc: 0.854
******** [step = 700] loss: 0.648, acc: 0.854
******** [step = 750] loss: 0.651, acc: 0.853
******** [step = 800] loss: 0.655, acc: 0.853
******** [step = 850] loss: 0.659, acc: 0.852
EPOCH = 22 loss: 0.659, acc: 0.852, val_loss: 1.047, val_acc: 0.815

================================================================================2025-08_09 13:21:09
******** [step = 50] loss: 0.584, acc: 0.864
******** [step = 100] loss: 0.575, acc: 0.865
******** [step = 150] loss: 0.577, acc: 0.865
******** [step = 200] loss: 0.584, acc: 0.864
******** [step = 250] loss: 0.589, acc: 0.863
******** [step = 300] loss: 0.596, acc: 0.862
******** [step = 350] loss: 0.599, acc: 0.861
******** [step = 400] loss: 0.605, acc: 0.860
******** [step = 450] loss: 0.607, acc: 0.860
******** [step = 500] loss: 0.613, acc: 0.859
******** [step = 550] loss: 0.618, acc: 0.859
******** [step = 600] loss: 0.622, acc: 0.858
******** [step = 650] loss: 0.627, acc: 0.857
******** [step = 700] loss: 0.630, acc: 0.857
******** [step = 750] loss: 0.634, acc: 0.856
******** [step = 800] loss: 0.638, acc: 0.856
******** [step = 850] loss: 0.641, acc: 0.856
EPOCH = 23 loss: 0.641, acc: 0.856, val_loss: 1.039, val_acc: 0.818

================================================================================2025-08_09 13:22:30
******** [step = 50] loss: 0.553, acc: 0.871
******** [step = 100] loss: 0.552, acc: 0.872
******** [step = 150] loss: 0.557, acc: 0.871
******** [step = 200] loss: 0.566, acc: 0.868
******** [step = 250] loss: 0.572, acc: 0.867
******** [step = 300] loss: 0.578, acc: 0.866
******** [step = 350] loss: 0.583, acc: 0.866
******** [step = 400] loss: 0.588, acc: 0.865
******** [step = 450] loss: 0.592, acc: 0.864
******** [step = 500] loss: 0.598, acc: 0.863
******** [step = 550] loss: 0.602, acc: 0.863
******** [step = 600] loss: 0.607, acc: 0.862
******** [step = 650] loss: 0.612, acc: 0.861
******** [step = 700] loss: 0.617, acc: 0.860
******** [step = 750] loss: 0.621, acc: 0.859
******** [step = 800] loss: 0.624, acc: 0.859
******** [step = 850] loss: 0.627, acc: 0.859
EPOCH = 24 loss: 0.627, acc: 0.859, val_loss: 1.041, val_acc: 0.819

================================================================================2025-08_09 13:23:57
******** [step = 50] loss: 0.538, acc: 0.873
******** [step = 100] loss: 0.546, acc: 0.872
******** [step = 150] loss: 0.549, acc: 0.871
******** [step = 200] loss: 0.555, acc: 0.870
******** [step = 250] loss: 0.560, acc: 0.869
******** [step = 300] loss: 0.564, acc: 0.869
******** [step = 350] loss: 0.572, acc: 0.867
******** [step = 400] loss: 0.576, acc: 0.867
******** [step = 450] loss: 0.581, acc: 0.866
******** [step = 500] loss: 0.586, acc: 0.866
******** [step = 550] loss: 0.590, acc: 0.865
******** [step = 600] loss: 0.592, acc: 0.865
******** [step = 650] loss: 0.597, acc: 0.864
******** [step = 700] loss: 0.600, acc: 0.864
******** [step = 750] loss: 0.604, acc: 0.863
******** [step = 800] loss: 0.608, acc: 0.862
******** [step = 850] loss: 0.612, acc: 0.862
EPOCH = 25 loss: 0.612, acc: 0.862, val_loss: 1.037, val_acc: 0.819

================================================================================2025-08_09 13:25:16
******** [step = 50] loss: 0.542, acc: 0.873
******** [step = 100] loss: 0.542, acc: 0.873
******** [step = 150] loss: 0.541, acc: 0.874
******** [step = 200] loss: 0.545, acc: 0.872
******** [step = 250] loss: 0.547, acc: 0.872
******** [step = 300] loss: 0.552, acc: 0.871
******** [step = 350] loss: 0.557, acc: 0.870
******** [step = 400] loss: 0.562, acc: 0.869
******** [step = 450] loss: 0.569, acc: 0.869
******** [step = 500] loss: 0.575, acc: 0.868
******** [step = 550] loss: 0.579, acc: 0.867
******** [step = 600] loss: 0.582, acc: 0.867
******** [step = 650] loss: 0.586, acc: 0.866
******** [step = 700] loss: 0.589, acc: 0.866
******** [step = 750] loss: 0.593, acc: 0.865
******** [step = 800] loss: 0.597, acc: 0.865
******** [step = 850] loss: 0.601, acc: 0.864
EPOCH = 26 loss: 0.601, acc: 0.864, val_loss: 1.029, val_acc: 0.821

================================================================================2025-08_09 13:26:35
******** [step = 50] loss: 0.502, acc: 0.880
******** [step = 100] loss: 0.512, acc: 0.879
******** [step = 150] loss: 0.523, acc: 0.877
******** [step = 200] loss: 0.532, acc: 0.876
******** [step = 250] loss: 0.536, acc: 0.876
******** [step = 300] loss: 0.540, acc: 0.875
******** [step = 350] loss: 0.545, acc: 0.874
******** [step = 400] loss: 0.550, acc: 0.873
******** [step = 450] loss: 0.554, acc: 0.872
******** [step = 500] loss: 0.559, acc: 0.871
******** [step = 550] loss: 0.564, acc: 0.870
******** [step = 600] loss: 0.569, acc: 0.870
******** [step = 650] loss: 0.572, acc: 0.869
******** [step = 700] loss: 0.576, acc: 0.868
******** [step = 750] loss: 0.581, acc: 0.868
******** [step = 800] loss: 0.584, acc: 0.867
******** [step = 850] loss: 0.588, acc: 0.867
EPOCH = 27 loss: 0.588, acc: 0.867, val_loss: 1.031, val_acc: 0.821

================================================================================2025-08_09 13:27:56
******** [step = 50] loss: 0.508, acc: 0.881
******** [step = 100] loss: 0.501, acc: 0.882
******** [step = 150] loss: 0.515, acc: 0.879
******** [step = 200] loss: 0.517, acc: 0.879
******** [step = 250] loss: 0.523, acc: 0.877
******** [step = 300] loss: 0.529, acc: 0.876
******** [step = 350] loss: 0.534, acc: 0.876
******** [step = 400] loss: 0.539, acc: 0.874
******** [step = 450] loss: 0.545, acc: 0.874
******** [step = 500] loss: 0.549, acc: 0.873
******** [step = 550] loss: 0.555, acc: 0.872
******** [step = 600] loss: 0.559, acc: 0.871
******** [step = 650] loss: 0.563, acc: 0.871
******** [step = 700] loss: 0.566, acc: 0.870
******** [step = 750] loss: 0.570, acc: 0.870
******** [step = 800] loss: 0.574, acc: 0.869
******** [step = 850] loss: 0.577, acc: 0.869
EPOCH = 28 loss: 0.577, acc: 0.869, val_loss: 1.033, val_acc: 0.821

================================================================================2025-08_09 13:29:19
******** [step = 50] loss: 0.495, acc: 0.883
******** [step = 100] loss: 0.493, acc: 0.883
******** [step = 150] loss: 0.502, acc: 0.882
******** [step = 200] loss: 0.505, acc: 0.881
******** [step = 250] loss: 0.513, acc: 0.880
******** [step = 300] loss: 0.521, acc: 0.878
******** [step = 350] loss: 0.528, acc: 0.877
******** [step = 400] loss: 0.533, acc: 0.876
******** [step = 450] loss: 0.537, acc: 0.876
******** [step = 500] loss: 0.541, acc: 0.875
******** [step = 550] loss: 0.545, acc: 0.875
******** [step = 600] loss: 0.550, acc: 0.874
******** [step = 650] loss: 0.555, acc: 0.873
******** [step = 700] loss: 0.557, acc: 0.872
******** [step = 750] loss: 0.562, acc: 0.872
******** [step = 800] loss: 0.564, acc: 0.871
******** [step = 850] loss: 0.567, acc: 0.871
EPOCH = 29 loss: 0.567, acc: 0.871, val_loss: 1.032, val_acc: 0.822

================================================================================2025-08_09 13:30:42
******** [step = 50] loss: 0.480, acc: 0.887
******** [step = 100] loss: 0.484, acc: 0.886
******** [step = 150] loss: 0.491, acc: 0.884
******** [step = 200] loss: 0.498, acc: 0.883
******** [step = 250] loss: 0.504, acc: 0.882
******** [step = 300] loss: 0.508, acc: 0.881
******** [step = 350] loss: 0.514, acc: 0.880
******** [step = 400] loss: 0.520, acc: 0.879
******** [step = 450] loss: 0.525, acc: 0.878
******** [step = 500] loss: 0.530, acc: 0.877
******** [step = 550] loss: 0.534, acc: 0.877
******** [step = 600] loss: 0.539, acc: 0.876
******** [step = 650] loss: 0.542, acc: 0.875
******** [step = 700] loss: 0.545, acc: 0.875
******** [step = 750] loss: 0.548, acc: 0.874
******** [step = 800] loss: 0.552, acc: 0.874
******** [step = 850] loss: 0.556, acc: 0.873
EPOCH = 30 loss: 0.556, acc: 0.873, val_loss: 1.037, val_acc: 0.823

================================================================================2025-08_09 13:32:01
******** [step = 50] loss: 0.477, acc: 0.885
******** [step = 100] loss: 0.475, acc: 0.886
******** [step = 150] loss: 0.486, acc: 0.885
******** [step = 200] loss: 0.490, acc: 0.884
******** [step = 250] loss: 0.495, acc: 0.883
******** [step = 300] loss: 0.501, acc: 0.882
******** [step = 350] loss: 0.505, acc: 0.881
******** [step = 400] loss: 0.510, acc: 0.880
******** [step = 450] loss: 0.514, acc: 0.880
******** [step = 500] loss: 0.519, acc: 0.879
******** [step = 550] loss: 0.524, acc: 0.878
******** [step = 600] loss: 0.529, acc: 0.878
******** [step = 650] loss: 0.533, acc: 0.877
******** [step = 700] loss: 0.538, acc: 0.876
******** [step = 750] loss: 0.541, acc: 0.876
******** [step = 800] loss: 0.545, acc: 0.875
******** [step = 850] loss: 0.549, acc: 0.875
EPOCH = 31 loss: 0.549, acc: 0.875, val_loss: 1.029, val_acc: 0.824

================================================================================2025-08_09 13:33:20
******** [step = 50] loss: 0.473, acc: 0.887
******** [step = 100] loss: 0.472, acc: 0.888
******** [step = 150] loss: 0.474, acc: 0.887
******** [step = 200] loss: 0.478, acc: 0.887
******** [step = 250] loss: 0.486, acc: 0.885
******** [step = 300] loss: 0.492, acc: 0.884
******** [step = 350] loss: 0.495, acc: 0.884
******** [step = 400] loss: 0.499, acc: 0.883
******** [step = 450] loss: 0.505, acc: 0.882
******** [step = 500] loss: 0.509, acc: 0.882
******** [step = 550] loss: 0.514, acc: 0.881
******** [step = 600] loss: 0.519, acc: 0.880
******** [step = 650] loss: 0.523, acc: 0.880
******** [step = 700] loss: 0.526, acc: 0.879
******** [step = 750] loss: 0.530, acc: 0.879
******** [step = 800] loss: 0.533, acc: 0.878
******** [step = 850] loss: 0.536, acc: 0.878
EPOCH = 32 loss: 0.536, acc: 0.878, val_loss: 1.031, val_acc: 0.824

================================================================================2025-08_09 13:34:39
******** [step = 50] loss: 0.446, acc: 0.893
******** [step = 100] loss: 0.453, acc: 0.892
******** [step = 150] loss: 0.456, acc: 0.891
******** [step = 200] loss: 0.465, acc: 0.890
******** [step = 250] loss: 0.472, acc: 0.888
******** [step = 300] loss: 0.476, acc: 0.887
******** [step = 350] loss: 0.481, acc: 0.886
******** [step = 400] loss: 0.487, acc: 0.885
******** [step = 450] loss: 0.493, acc: 0.884
******** [step = 500] loss: 0.499, acc: 0.883
******** [step = 550] loss: 0.504, acc: 0.883
******** [step = 600] loss: 0.509, acc: 0.882
******** [step = 650] loss: 0.513, acc: 0.881
******** [step = 700] loss: 0.517, acc: 0.881
******** [step = 750] loss: 0.521, acc: 0.880
******** [step = 800] loss: 0.525, acc: 0.879
******** [step = 850] loss: 0.529, acc: 0.879
EPOCH = 33 loss: 0.529, acc: 0.879, val_loss: 1.032, val_acc: 0.826

================================================================================2025-08_09 13:35:59
******** [step = 50] loss: 0.466, acc: 0.889
******** [step = 100] loss: 0.460, acc: 0.892
******** [step = 150] loss: 0.463, acc: 0.891
******** [step = 200] loss: 0.470, acc: 0.889
******** [step = 250] loss: 0.476, acc: 0.888
******** [step = 300] loss: 0.480, acc: 0.887
******** [step = 350] loss: 0.481, acc: 0.887
******** [step = 400] loss: 0.486, acc: 0.886
******** [step = 450] loss: 0.491, acc: 0.886
******** [step = 500] loss: 0.495, acc: 0.885
******** [step = 550] loss: 0.500, acc: 0.884
******** [step = 600] loss: 0.503, acc: 0.883
******** [step = 650] loss: 0.507, acc: 0.883
******** [step = 700] loss: 0.511, acc: 0.882
******** [step = 750] loss: 0.515, acc: 0.882
******** [step = 800] loss: 0.518, acc: 0.881
******** [step = 850] loss: 0.521, acc: 0.881
EPOCH = 34 loss: 0.521, acc: 0.881, val_loss: 1.030, val_acc: 0.826

================================================================================2025-08_09 13:37:19
******** [step = 50] loss: 0.446, acc: 0.894
******** [step = 100] loss: 0.454, acc: 0.893
******** [step = 150] loss: 0.459, acc: 0.892
******** [step = 200] loss: 0.459, acc: 0.891
******** [step = 250] loss: 0.463, acc: 0.890
******** [step = 300] loss: 0.468, acc: 0.889
******** [step = 350] loss: 0.472, acc: 0.888
******** [step = 400] loss: 0.478, acc: 0.887
******** [step = 450] loss: 0.482, acc: 0.887
******** [step = 500] loss: 0.486, acc: 0.886
******** [step = 550] loss: 0.491, acc: 0.885
******** [step = 600] loss: 0.494, acc: 0.885
******** [step = 650] loss: 0.498, acc: 0.884
******** [step = 700] loss: 0.502, acc: 0.884
******** [step = 750] loss: 0.505, acc: 0.883
******** [step = 800] loss: 0.509, acc: 0.883
******** [step = 850] loss: 0.512, acc: 0.882
EPOCH = 35 loss: 0.512, acc: 0.882, val_loss: 1.031, val_acc: 0.826

================================================================================2025-08_09 13:38:51
******** [step = 50] loss: 0.418, acc: 0.900
******** [step = 100] loss: 0.431, acc: 0.897
******** [step = 150] loss: 0.438, acc: 0.895
******** [step = 200] loss: 0.442, acc: 0.895
******** [step = 250] loss: 0.449, acc: 0.894
******** [step = 300] loss: 0.455, acc: 0.892
******** [step = 350] loss: 0.460, acc: 0.892
******** [step = 400] loss: 0.464, acc: 0.891
******** [step = 450] loss: 0.468, acc: 0.890
******** [step = 500] loss: 0.473, acc: 0.889
******** [step = 550] loss: 0.479, acc: 0.889
******** [step = 600] loss: 0.485, acc: 0.888
******** [step = 650] loss: 0.490, acc: 0.887
******** [step = 700] loss: 0.494, acc: 0.886
******** [step = 750] loss: 0.497, acc: 0.886
******** [step = 800] loss: 0.501, acc: 0.885
******** [step = 850] loss: 0.505, acc: 0.885
EPOCH = 36 loss: 0.505, acc: 0.885, val_loss: 1.032, val_acc: 0.826

================================================================================2025-08_09 13:40:17
******** [step = 50] loss: 0.425, acc: 0.897
******** [step = 100] loss: 0.430, acc: 0.896
******** [step = 150] loss: 0.433, acc: 0.896
******** [step = 200] loss: 0.438, acc: 0.895
******** [step = 250] loss: 0.446, acc: 0.894
******** [step = 300] loss: 0.450, acc: 0.893
******** [step = 350] loss: 0.457, acc: 0.892
******** [step = 400] loss: 0.462, acc: 0.891
******** [step = 450] loss: 0.465, acc: 0.891
******** [step = 500] loss: 0.470, acc: 0.890
******** [step = 550] loss: 0.475, acc: 0.889
******** [step = 600] loss: 0.480, acc: 0.889
******** [step = 650] loss: 0.484, acc: 0.888
******** [step = 700] loss: 0.488, acc: 0.887
******** [step = 750] loss: 0.491, acc: 0.887
******** [step = 800] loss: 0.493, acc: 0.887
******** [step = 850] loss: 0.496, acc: 0.886
EPOCH = 37 loss: 0.496, acc: 0.886, val_loss: 1.032, val_acc: 0.827

================================================================================2025-08_09 13:41:36
******** [step = 50] loss: 0.426, acc: 0.898
******** [step = 100] loss: 0.427, acc: 0.897
******** [step = 150] loss: 0.431, acc: 0.896
******** [step = 200] loss: 0.439, acc: 0.895
******** [step = 250] loss: 0.443, acc: 0.894
******** [step = 300] loss: 0.445, acc: 0.894
******** [step = 350] loss: 0.451, acc: 0.893
******** [step = 400] loss: 0.455, acc: 0.893
******** [step = 450] loss: 0.462, acc: 0.891
******** [step = 500] loss: 0.467, acc: 0.890
******** [step = 550] loss: 0.470, acc: 0.890
******** [step = 600] loss: 0.473, acc: 0.890
******** [step = 650] loss: 0.477, acc: 0.889
******** [step = 700] loss: 0.481, acc: 0.889
******** [step = 750] loss: 0.484, acc: 0.888
******** [step = 800] loss: 0.487, acc: 0.888
******** [step = 850] loss: 0.490, acc: 0.887
EPOCH = 38 loss: 0.490, acc: 0.887, val_loss: 1.026, val_acc: 0.827

================================================================================2025-08_09 13:42:56
******** [step = 50] loss: 0.406, acc: 0.903
******** [step = 100] loss: 0.417, acc: 0.901
******** [step = 150] loss: 0.426, acc: 0.899
******** [step = 200] loss: 0.427, acc: 0.898
******** [step = 250] loss: 0.433, acc: 0.897
******** [step = 300] loss: 0.438, acc: 0.896
******** [step = 350] loss: 0.442, acc: 0.896
******** [step = 400] loss: 0.446, acc: 0.895
******** [step = 450] loss: 0.452, acc: 0.894
******** [step = 500] loss: 0.456, acc: 0.893
******** [step = 550] loss: 0.460, acc: 0.892
******** [step = 600] loss: 0.466, acc: 0.892
******** [step = 650] loss: 0.470, acc: 0.891
******** [step = 700] loss: 0.474, acc: 0.890
******** [step = 750] loss: 0.477, acc: 0.890
******** [step = 800] loss: 0.481, acc: 0.889
******** [step = 850] loss: 0.484, acc: 0.889
EPOCH = 39 loss: 0.484, acc: 0.889, val_loss: 1.026, val_acc: 0.828

================================================================================2025-08_09 13:44:25
******** [step = 50] loss: 0.409, acc: 0.903
******** [step = 100] loss: 0.407, acc: 0.903
******** [step = 150] loss: 0.415, acc: 0.901
******** [step = 200] loss: 0.417, acc: 0.900
******** [step = 250] loss: 0.423, acc: 0.899
******** [step = 300] loss: 0.429, acc: 0.898
******** [step = 350] loss: 0.435, acc: 0.897
******** [step = 400] loss: 0.441, acc: 0.896
******** [step = 450] loss: 0.446, acc: 0.895
******** [step = 500] loss: 0.449, acc: 0.895
******** [step = 550] loss: 0.454, acc: 0.894
******** [step = 600] loss: 0.457, acc: 0.894
******** [step = 650] loss: 0.462, acc: 0.893
******** [step = 700] loss: 0.466, acc: 0.892
******** [step = 750] loss: 0.470, acc: 0.891
******** [step = 800] loss: 0.474, acc: 0.891
******** [step = 850] loss: 0.477, acc: 0.890
EPOCH = 40 loss: 0.477, acc: 0.890, val_loss: 1.035, val_acc: 0.827

================================================================================2025-08_09 13:45:50
******** [step = 50] loss: 0.402, acc: 0.905
******** [step = 100] loss: 0.406, acc: 0.904
******** [step = 150] loss: 0.412, acc: 0.902
******** [step = 200] loss: 0.420, acc: 0.901
******** [step = 250] loss: 0.426, acc: 0.900
******** [step = 300] loss: 0.428, acc: 0.899
******** [step = 350] loss: 0.432, acc: 0.898
******** [step = 400] loss: 0.436, acc: 0.897
******** [step = 450] loss: 0.438, acc: 0.897
******** [step = 500] loss: 0.443, acc: 0.896
******** [step = 550] loss: 0.448, acc: 0.895
******** [step = 600] loss: 0.452, acc: 0.895
******** [step = 650] loss: 0.455, acc: 0.894
******** [step = 700] loss: 0.459, acc: 0.894
******** [step = 750] loss: 0.463, acc: 0.893
******** [step = 800] loss: 0.467, acc: 0.893
******** [step = 850] loss: 0.471, acc: 0.892
EPOCH = 41 loss: 0.471, acc: 0.892, val_loss: 1.032, val_acc: 0.827

================================================================================2025-08_09 13:47:09
******** [step = 50] loss: 0.397, acc: 0.905
******** [step = 100] loss: 0.406, acc: 0.903
******** [step = 150] loss: 0.410, acc: 0.902
******** [step = 200] loss: 0.416, acc: 0.901
******** [step = 250] loss: 0.419, acc: 0.900
******** [step = 300] loss: 0.423, acc: 0.900
******** [step = 350] loss: 0.426, acc: 0.899
******** [step = 400] loss: 0.430, acc: 0.898
******** [step = 450] loss: 0.434, acc: 0.898
******** [step = 500] loss: 0.439, acc: 0.897
******** [step = 550] loss: 0.443, acc: 0.896
******** [step = 600] loss: 0.448, acc: 0.896
******** [step = 650] loss: 0.452, acc: 0.895
******** [step = 700] loss: 0.455, acc: 0.895
******** [step = 750] loss: 0.458, acc: 0.894
******** [step = 800] loss: 0.462, acc: 0.893
******** [step = 850] loss: 0.464, acc: 0.893
EPOCH = 42 loss: 0.464, acc: 0.893, val_loss: 1.035, val_acc: 0.828

================================================================================2025-08_09 13:48:28
******** [step = 50] loss: 0.399, acc: 0.905
******** [step = 100] loss: 0.398, acc: 0.906
******** [step = 150] loss: 0.401, acc: 0.905
******** [step = 200] loss: 0.402, acc: 0.904
******** [step = 250] loss: 0.408, acc: 0.903
******** [step = 300] loss: 0.414, acc: 0.902
******** [step = 350] loss: 0.421, acc: 0.900
******** [step = 400] loss: 0.426, acc: 0.900
******** [step = 450] loss: 0.428, acc: 0.899
******** [step = 500] loss: 0.434, acc: 0.898
******** [step = 550] loss: 0.438, acc: 0.898
******** [step = 600] loss: 0.440, acc: 0.897
******** [step = 650] loss: 0.445, acc: 0.897
******** [step = 700] loss: 0.449, acc: 0.896
******** [step = 750] loss: 0.452, acc: 0.896
******** [step = 800] loss: 0.455, acc: 0.895
******** [step = 850] loss: 0.458, acc: 0.895
EPOCH = 43 loss: 0.458, acc: 0.895, val_loss: 1.034, val_acc: 0.828

================================================================================2025-08_09 13:49:51
******** [step = 50] loss: 0.389, acc: 0.906
******** [step = 100] loss: 0.388, acc: 0.907
******** [step = 150] loss: 0.392, acc: 0.906
******** [step = 200] loss: 0.399, acc: 0.905
******** [step = 250] loss: 0.402, acc: 0.904
******** [step = 300] loss: 0.408, acc: 0.903
******** [step = 350] loss: 0.415, acc: 0.902
******** [step = 400] loss: 0.421, acc: 0.901
******** [step = 450] loss: 0.425, acc: 0.900
******** [step = 500] loss: 0.429, acc: 0.899
******** [step = 550] loss: 0.433, acc: 0.899
******** [step = 600] loss: 0.436, acc: 0.898
******** [step = 650] loss: 0.440, acc: 0.898
******** [step = 700] loss: 0.443, acc: 0.897
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 6735353 ON atl1-1-01-003-35-0 CANCELLED AT 2025-08-09T13:50:54 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 6735353.0 ON atl1-1-01-003-35-0 CANCELLED AT 2025-08-09T13:50:54 DUE TO TIME LIMIT ***
---------------------------------------
Begin Slurm Epilog: Aug-09-2025 13:50:57
Job ID:        6735353
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=04:00:52,vmem=0,walltime=01:00:13,mem=36556K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-01-003-35-0
---------------------------------------
