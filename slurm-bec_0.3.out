---------------------------------------
Begin Slurm Prolog: Aug-10-2025 19:30:08
Job ID:    6784851
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Sun Aug 10 07:30:08 PM EDT 2025
atl1-1-03-006-31-0.pace.gatech.edu
== GPU check ==
Sun Aug 10 19:30:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:D8:00.0 Off |                    0 |
| N/A   33C    P0             24W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 108141.98it/s] 28%|██▊       | 38624/135842 [00:00<00:00, 160681.79it/s] 41%|████      | 55896/135842 [00:00<00:00, 122012.80it/s] 51%|█████     | 69479/135842 [00:00<00:00, 95073.14it/s]  66%|██████▌   | 89553/135842 [00:00<00:00, 120833.60it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 90884.66it/s] 92%|█████████▏| 125565/135842 [00:01<00:00, 109465.57it/s]100%|██████████| 135842/135842 [00:01<00:00, 112736.17it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 47630.81it/s] 33%|███▎      | 35688/108673 [00:00<00:00, 85672.38it/s] 50%|████▉     | 53915/108673 [00:00<00:00, 113352.91it/s] 66%|██████▋   | 72187/108673 [00:00<00:00, 133497.89it/s] 82%|████████▏ | 88621/108673 [00:01<00:00, 71580.65it/s]  98%|█████████▊| 106995/108673 [00:01<00:00, 91074.28it/s]100%|██████████| 108673/108673 [00:01<00:00, 89598.30it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 66%|██████▋   | 18009/27169 [00:00<00:00, 180079.87it/s]100%|██████████| 27169/27169 [00:00<00:00, 181792.29it/s]
tensor([    3,    12,    15,   258,    13,   495,   475,   195,    13,    16,
          108,    15,  2355,    65,    30,    15,    38,    11,     9, 11084,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([    3,  6743, 16945,     6,  5310,    16, 10526,    40,    28,  2764,
           33,  5780,    14, 17756,    45,  7392,   177, 18580,    18, 14713,
            7,    14,   997,     4,     2,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
tensor([    3,  6627, 10404,  4973,  5067,    21,    81,  2138,     7,  2321,
         5274, 10846,   224, 11171,   956,  9313,     4,     2,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1]) torch.int64
*************************** start training...

================================================================================2025-08_10 19:31:25
******** [step = 50] loss: 9.343, acc: 0.009
******** [step = 100] loss: 8.930, acc: 0.098
******** [step = 150] loss: 8.591, acc: 0.144
******** [step = 200] loss: 8.267, acc: 0.170
******** [step = 250] loss: 7.955, acc: 0.186
******** [step = 300] loss: 7.648, acc: 0.197
******** [step = 350] loss: 7.347, acc: 0.208
******** [step = 400] loss: 7.064, acc: 0.219
******** [step = 450] loss: 6.810, acc: 0.231
******** [step = 500] loss: 6.585, acc: 0.242
******** [step = 550] loss: 6.387, acc: 0.252
******** [step = 600] loss: 6.210, acc: 0.261
******** [step = 650] loss: 6.053, acc: 0.270
******** [step = 700] loss: 5.909, acc: 0.278
******** [step = 750] loss: 5.779, acc: 0.285
******** [step = 800] loss: 5.661, acc: 0.292
******** [step = 850] loss: 5.552, acc: 0.298
EPOCH = 1 loss: 5.552, acc: 0.298, val_loss: 3.685, val_acc: 0.416

================================================================================2025-08_10 19:32:47
******** [step = 50] loss: 3.721, acc: 0.407
******** [step = 100] loss: 3.691, acc: 0.408
******** [step = 150] loss: 3.665, acc: 0.410
******** [step = 200] loss: 3.646, acc: 0.413
******** [step = 250] loss: 3.623, acc: 0.415
******** [step = 300] loss: 3.601, acc: 0.416
******** [step = 350] loss: 3.577, acc: 0.418
******** [step = 400] loss: 3.556, acc: 0.420
******** [step = 450] loss: 3.535, acc: 0.422
******** [step = 500] loss: 3.518, acc: 0.424
******** [step = 550] loss: 3.498, acc: 0.426
******** [step = 600] loss: 3.481, acc: 0.427
******** [step = 650] loss: 3.464, acc: 0.429
******** [step = 700] loss: 3.447, acc: 0.431
******** [step = 750] loss: 3.432, acc: 0.432
******** [step = 800] loss: 3.418, acc: 0.434
******** [step = 850] loss: 3.405, acc: 0.435
EPOCH = 2 loss: 3.405, acc: 0.435, val_loss: 3.102, val_acc: 0.473

================================================================================2025-08_10 19:34:09
******** [step = 50] loss: 3.093, acc: 0.464
******** [step = 100] loss: 3.073, acc: 0.467
******** [step = 150] loss: 3.065, acc: 0.468
******** [step = 200] loss: 3.061, acc: 0.468
******** [step = 250] loss: 3.056, acc: 0.469
******** [step = 300] loss: 3.051, acc: 0.470
******** [step = 350] loss: 3.041, acc: 0.472
******** [step = 400] loss: 3.034, acc: 0.473
******** [step = 450] loss: 3.027, acc: 0.474
******** [step = 500] loss: 3.019, acc: 0.475
******** [step = 550] loss: 3.010, acc: 0.475
******** [step = 600] loss: 3.001, acc: 0.477
******** [step = 650] loss: 2.994, acc: 0.478
******** [step = 700] loss: 2.985, acc: 0.479
******** [step = 750] loss: 2.979, acc: 0.480
******** [step = 800] loss: 2.969, acc: 0.481
******** [step = 850] loss: 2.960, acc: 0.482
EPOCH = 3 loss: 2.960, acc: 0.482, val_loss: 2.762, val_acc: 0.515

================================================================================2025-08_10 19:35:30
******** [step = 50] loss: 2.795, acc: 0.498
******** [step = 100] loss: 2.769, acc: 0.501
******** [step = 150] loss: 2.750, acc: 0.504
******** [step = 200] loss: 2.742, acc: 0.505
******** [step = 250] loss: 2.731, acc: 0.506
******** [step = 300] loss: 2.726, acc: 0.508
******** [step = 350] loss: 2.720, acc: 0.509
******** [step = 400] loss: 2.713, acc: 0.510
******** [step = 450] loss: 2.706, acc: 0.511
******** [step = 500] loss: 2.700, acc: 0.512
******** [step = 550] loss: 2.697, acc: 0.512
******** [step = 600] loss: 2.688, acc: 0.514
******** [step = 650] loss: 2.685, acc: 0.514
******** [step = 700] loss: 2.679, acc: 0.515
******** [step = 750] loss: 2.672, acc: 0.517
******** [step = 800] loss: 2.666, acc: 0.518
******** [step = 850] loss: 2.661, acc: 0.519
EPOCH = 4 loss: 2.661, acc: 0.519, val_loss: 2.493, val_acc: 0.554

================================================================================2025-08_10 19:36:51
******** [step = 50] loss: 2.488, acc: 0.538
******** [step = 100] loss: 2.469, acc: 0.541
******** [step = 150] loss: 2.460, acc: 0.542
******** [step = 200] loss: 2.459, acc: 0.543
******** [step = 250] loss: 2.454, acc: 0.544
******** [step = 300] loss: 2.454, acc: 0.544
******** [step = 350] loss: 2.447, acc: 0.545
******** [step = 400] loss: 2.444, acc: 0.546
******** [step = 450] loss: 2.441, acc: 0.547
******** [step = 500] loss: 2.440, acc: 0.547
******** [step = 550] loss: 2.436, acc: 0.548
******** [step = 600] loss: 2.429, acc: 0.549
******** [step = 650] loss: 2.425, acc: 0.550
******** [step = 700] loss: 2.420, acc: 0.551
******** [step = 750] loss: 2.413, acc: 0.552
******** [step = 800] loss: 2.409, acc: 0.553
******** [step = 850] loss: 2.405, acc: 0.553
EPOCH = 5 loss: 2.405, acc: 0.553, val_loss: 2.245, val_acc: 0.590

================================================================================2025-08_10 19:38:11
******** [step = 50] loss: 2.215, acc: 0.577
******** [step = 100] loss: 2.205, acc: 0.577
******** [step = 150] loss: 2.201, acc: 0.579
******** [step = 200] loss: 2.196, acc: 0.580
******** [step = 250] loss: 2.188, acc: 0.582
******** [step = 300] loss: 2.186, acc: 0.583
******** [step = 350] loss: 2.189, acc: 0.583
******** [step = 400] loss: 2.186, acc: 0.583
******** [step = 450] loss: 2.182, acc: 0.584
******** [step = 500] loss: 2.181, acc: 0.585
******** [step = 550] loss: 2.179, acc: 0.585
******** [step = 600] loss: 2.175, acc: 0.586
******** [step = 650] loss: 2.173, acc: 0.587
******** [step = 700] loss: 2.170, acc: 0.587
******** [step = 750] loss: 2.167, acc: 0.588
******** [step = 800] loss: 2.163, acc: 0.589
******** [step = 850] loss: 2.158, acc: 0.590
EPOCH = 6 loss: 2.158, acc: 0.590, val_loss: 2.006, val_acc: 0.625

================================================================================2025-08_10 19:39:32
******** [step = 50] loss: 1.987, acc: 0.608
******** [step = 100] loss: 1.991, acc: 0.609
******** [step = 150] loss: 1.986, acc: 0.611
******** [step = 200] loss: 1.982, acc: 0.612
******** [step = 250] loss: 1.980, acc: 0.613
******** [step = 300] loss: 1.981, acc: 0.613
******** [step = 350] loss: 1.985, acc: 0.613
******** [step = 400] loss: 1.983, acc: 0.613
******** [step = 450] loss: 1.980, acc: 0.614
******** [step = 500] loss: 1.980, acc: 0.614
******** [step = 550] loss: 1.980, acc: 0.615
******** [step = 600] loss: 1.977, acc: 0.615
******** [step = 650] loss: 1.975, acc: 0.616
******** [step = 700] loss: 1.974, acc: 0.616
******** [step = 750] loss: 1.973, acc: 0.617
******** [step = 800] loss: 1.971, acc: 0.617
******** [step = 850] loss: 1.970, acc: 0.618
EPOCH = 7 loss: 1.970, acc: 0.618, val_loss: 1.881, val_acc: 0.646

================================================================================2025-08_10 19:40:52
******** [step = 50] loss: 1.846, acc: 0.633
******** [step = 100] loss: 1.826, acc: 0.634
******** [step = 150] loss: 1.817, acc: 0.636
******** [step = 200] loss: 1.822, acc: 0.635
******** [step = 250] loss: 1.820, acc: 0.636
******** [step = 300] loss: 1.822, acc: 0.636
******** [step = 350] loss: 1.825, acc: 0.636
******** [step = 400] loss: 1.826, acc: 0.636
******** [step = 450] loss: 1.829, acc: 0.636
******** [step = 500] loss: 1.831, acc: 0.637
******** [step = 550] loss: 1.832, acc: 0.637
******** [step = 600] loss: 1.834, acc: 0.637
******** [step = 650] loss: 1.831, acc: 0.638
******** [step = 700] loss: 1.831, acc: 0.638
******** [step = 750] loss: 1.831, acc: 0.638
******** [step = 800] loss: 1.832, acc: 0.638
******** [step = 850] loss: 1.832, acc: 0.639
EPOCH = 8 loss: 1.832, acc: 0.639, val_loss: 1.788, val_acc: 0.662

================================================================================2025-08_10 19:42:12
******** [step = 50] loss: 1.704, acc: 0.654
******** [step = 100] loss: 1.714, acc: 0.652
******** [step = 150] loss: 1.711, acc: 0.653
******** [step = 200] loss: 1.713, acc: 0.653
******** [step = 250] loss: 1.710, acc: 0.654
******** [step = 300] loss: 1.709, acc: 0.654
******** [step = 350] loss: 1.713, acc: 0.654
******** [step = 400] loss: 1.717, acc: 0.654
******** [step = 450] loss: 1.717, acc: 0.654
******** [step = 500] loss: 1.720, acc: 0.654
******** [step = 550] loss: 1.720, acc: 0.655
******** [step = 600] loss: 1.721, acc: 0.655
******** [step = 650] loss: 1.722, acc: 0.655
******** [step = 700] loss: 1.722, acc: 0.655
******** [step = 750] loss: 1.721, acc: 0.656
******** [step = 800] loss: 1.725, acc: 0.655
******** [step = 850] loss: 1.724, acc: 0.656
EPOCH = 9 loss: 1.724, acc: 0.656, val_loss: 1.712, val_acc: 0.676

================================================================================2025-08_10 19:43:32
******** [step = 50] loss: 1.607, acc: 0.671
******** [step = 100] loss: 1.602, acc: 0.672
******** [step = 150] loss: 1.602, acc: 0.672
******** [step = 200] loss: 1.601, acc: 0.672
******** [step = 250] loss: 1.604, acc: 0.672
******** [step = 300] loss: 1.608, acc: 0.672
******** [step = 350] loss: 1.612, acc: 0.671
******** [step = 400] loss: 1.615, acc: 0.671
******** [step = 450] loss: 1.620, acc: 0.670
******** [step = 500] loss: 1.623, acc: 0.670
******** [step = 550] loss: 1.623, acc: 0.671
******** [step = 600] loss: 1.626, acc: 0.671
******** [step = 650] loss: 1.628, acc: 0.670
******** [step = 700] loss: 1.632, acc: 0.670
******** [step = 750] loss: 1.634, acc: 0.670
******** [step = 800] loss: 1.635, acc: 0.670
******** [step = 850] loss: 1.636, acc: 0.670
EPOCH = 10 loss: 1.636, acc: 0.670, val_loss: 1.666, val_acc: 0.687

================================================================================2025-08_10 19:44:52
******** [step = 50] loss: 1.518, acc: 0.687
******** [step = 100] loss: 1.518, acc: 0.686
******** [step = 150] loss: 1.518, acc: 0.686
******** [step = 200] loss: 1.524, acc: 0.686
******** [step = 250] loss: 1.533, acc: 0.685
******** [step = 300] loss: 1.536, acc: 0.684
******** [step = 350] loss: 1.541, acc: 0.683
******** [step = 400] loss: 1.544, acc: 0.683
******** [step = 450] loss: 1.549, acc: 0.683
******** [step = 500] loss: 1.554, acc: 0.682
******** [step = 550] loss: 1.555, acc: 0.682
******** [step = 600] loss: 1.558, acc: 0.682
******** [step = 650] loss: 1.560, acc: 0.682
******** [step = 700] loss: 1.562, acc: 0.682
******** [step = 750] loss: 1.564, acc: 0.682
******** [step = 800] loss: 1.567, acc: 0.682
******** [step = 850] loss: 1.569, acc: 0.682
EPOCH = 11 loss: 1.569, acc: 0.682, val_loss: 1.617, val_acc: 0.697

================================================================================2025-08_10 19:46:12
******** [step = 50] loss: 1.485, acc: 0.689
******** [step = 100] loss: 1.470, acc: 0.692
******** [step = 150] loss: 1.464, acc: 0.695
******** [step = 200] loss: 1.472, acc: 0.695
******** [step = 250] loss: 1.472, acc: 0.694
******** [step = 300] loss: 1.478, acc: 0.694
******** [step = 350] loss: 1.480, acc: 0.694
******** [step = 400] loss: 1.483, acc: 0.693
******** [step = 450] loss: 1.486, acc: 0.693
******** [step = 500] loss: 1.488, acc: 0.693
******** [step = 550] loss: 1.493, acc: 0.692
******** [step = 600] loss: 1.496, acc: 0.692
******** [step = 650] loss: 1.501, acc: 0.692
******** [step = 700] loss: 1.502, acc: 0.692
******** [step = 750] loss: 1.505, acc: 0.692
******** [step = 800] loss: 1.508, acc: 0.692
******** [step = 850] loss: 1.510, acc: 0.691
EPOCH = 12 loss: 1.510, acc: 0.691, val_loss: 1.592, val_acc: 0.704

================================================================================2025-08_10 19:47:33
******** [step = 50] loss: 1.423, acc: 0.703
******** [step = 100] loss: 1.419, acc: 0.703
******** [step = 150] loss: 1.419, acc: 0.703
******** [step = 200] loss: 1.423, acc: 0.703
******** [step = 250] loss: 1.429, acc: 0.702
******** [step = 300] loss: 1.430, acc: 0.702
******** [step = 350] loss: 1.432, acc: 0.703
******** [step = 400] loss: 1.434, acc: 0.703
******** [step = 450] loss: 1.440, acc: 0.702
******** [step = 500] loss: 1.441, acc: 0.702
******** [step = 550] loss: 1.444, acc: 0.701
******** [step = 600] loss: 1.447, acc: 0.701
******** [step = 650] loss: 1.449, acc: 0.701
******** [step = 700] loss: 1.452, acc: 0.701
******** [step = 750] loss: 1.455, acc: 0.701
******** [step = 800] loss: 1.456, acc: 0.701
******** [step = 850] loss: 1.457, acc: 0.700
EPOCH = 13 loss: 1.457, acc: 0.700, val_loss: 1.556, val_acc: 0.708

================================================================================2025-08_10 19:48:53
******** [step = 50] loss: 1.348, acc: 0.714
******** [step = 100] loss: 1.351, acc: 0.715
******** [step = 150] loss: 1.355, acc: 0.714
******** [step = 200] loss: 1.354, acc: 0.714
******** [step = 250] loss: 1.360, acc: 0.713
******** [step = 300] loss: 1.370, acc: 0.712
******** [step = 350] loss: 1.376, acc: 0.711
******** [step = 400] loss: 1.380, acc: 0.711
******** [step = 450] loss: 1.383, acc: 0.711
******** [step = 500] loss: 1.389, acc: 0.710
******** [step = 550] loss: 1.391, acc: 0.710
******** [step = 600] loss: 1.394, acc: 0.710
******** [step = 650] loss: 1.398, acc: 0.709
******** [step = 700] loss: 1.401, acc: 0.709
******** [step = 750] loss: 1.404, acc: 0.709
******** [step = 800] loss: 1.408, acc: 0.708
******** [step = 850] loss: 1.411, acc: 0.708
EPOCH = 14 loss: 1.411, acc: 0.708, val_loss: 1.525, val_acc: 0.714

================================================================================2025-08_10 19:50:17
******** [step = 50] loss: 1.329, acc: 0.718
******** [step = 100] loss: 1.316, acc: 0.721
******** [step = 150] loss: 1.314, acc: 0.722
******** [step = 200] loss: 1.323, acc: 0.720
******** [step = 250] loss: 1.329, acc: 0.720
******** [step = 300] loss: 1.334, acc: 0.719
******** [step = 350] loss: 1.336, acc: 0.718
******** [step = 400] loss: 1.340, acc: 0.718
******** [step = 450] loss: 1.342, acc: 0.718
******** [step = 500] loss: 1.345, acc: 0.718
******** [step = 550] loss: 1.348, acc: 0.718
******** [step = 600] loss: 1.353, acc: 0.717
******** [step = 650] loss: 1.357, acc: 0.717
******** [step = 700] loss: 1.362, acc: 0.717
******** [step = 750] loss: 1.364, acc: 0.716
******** [step = 800] loss: 1.366, acc: 0.716
******** [step = 850] loss: 1.374, acc: 0.715
EPOCH = 15 loss: 1.374, acc: 0.715, val_loss: 1.502, val_acc: 0.720

================================================================================2025-08_10 19:51:41
******** [step = 50] loss: 1.283, acc: 0.726
******** [step = 100] loss: 1.279, acc: 0.727
******** [step = 150] loss: 1.279, acc: 0.727
******** [step = 200] loss: 1.285, acc: 0.726
******** [step = 250] loss: 1.286, acc: 0.726
******** [step = 300] loss: 1.294, acc: 0.725
******** [step = 350] loss: 1.296, acc: 0.725
******** [step = 400] loss: 1.298, acc: 0.725
******** [step = 450] loss: 1.304, acc: 0.724
******** [step = 500] loss: 1.309, acc: 0.723
******** [step = 550] loss: 1.313, acc: 0.723
******** [step = 600] loss: 1.317, acc: 0.723
******** [step = 650] loss: 1.323, acc: 0.722
******** [step = 700] loss: 1.326, acc: 0.722
******** [step = 750] loss: 1.327, acc: 0.722
******** [step = 800] loss: 1.329, acc: 0.722
******** [step = 850] loss: 1.331, acc: 0.722
EPOCH = 16 loss: 1.331, acc: 0.722, val_loss: 1.480, val_acc: 0.724

================================================================================2025-08_10 19:53:03
******** [step = 50] loss: 1.253, acc: 0.732
******** [step = 100] loss: 1.232, acc: 0.735
******** [step = 150] loss: 1.235, acc: 0.735
******** [step = 200] loss: 1.240, acc: 0.734
******** [step = 250] loss: 1.248, acc: 0.733
******** [step = 300] loss: 1.252, acc: 0.732
******** [step = 350] loss: 1.260, acc: 0.731
******** [step = 400] loss: 1.265, acc: 0.731
******** [step = 450] loss: 1.268, acc: 0.730
******** [step = 500] loss: 1.271, acc: 0.730
******** [step = 550] loss: 1.278, acc: 0.729
******** [step = 600] loss: 1.283, acc: 0.729
******** [step = 650] loss: 1.285, acc: 0.729
******** [step = 700] loss: 1.290, acc: 0.728
******** [step = 750] loss: 1.294, acc: 0.728
******** [step = 800] loss: 1.296, acc: 0.728
******** [step = 850] loss: 1.298, acc: 0.728
EPOCH = 17 loss: 1.298, acc: 0.728, val_loss: 1.467, val_acc: 0.728

================================================================================2025-08_10 19:54:31
******** [step = 50] loss: 1.193, acc: 0.742
******** [step = 100] loss: 1.200, acc: 0.741
******** [step = 150] loss: 1.203, acc: 0.740
******** [step = 200] loss: 1.214, acc: 0.739
******** [step = 250] loss: 1.219, acc: 0.738
******** [step = 300] loss: 1.225, acc: 0.737
******** [step = 350] loss: 1.230, acc: 0.736
******** [step = 400] loss: 1.235, acc: 0.735
******** [step = 450] loss: 1.239, acc: 0.735
******** [step = 500] loss: 1.244, acc: 0.734
******** [step = 550] loss: 1.248, acc: 0.734
******** [step = 600] loss: 1.251, acc: 0.734
******** [step = 650] loss: 1.256, acc: 0.733
******** [step = 700] loss: 1.260, acc: 0.733
******** [step = 750] loss: 1.263, acc: 0.733
******** [step = 800] loss: 1.266, acc: 0.732
******** [step = 850] loss: 1.268, acc: 0.732
EPOCH = 18 loss: 1.268, acc: 0.732, val_loss: 1.444, val_acc: 0.732

================================================================================2025-08_10 19:55:55
******** [step = 50] loss: 1.167, acc: 0.744
******** [step = 100] loss: 1.172, acc: 0.745
******** [step = 150] loss: 1.177, acc: 0.744
******** [step = 200] loss: 1.182, acc: 0.744
******** [step = 250] loss: 1.187, acc: 0.744
******** [step = 300] loss: 1.197, acc: 0.742
******** [step = 350] loss: 1.201, acc: 0.742
******** [step = 400] loss: 1.206, acc: 0.741
******** [step = 450] loss: 1.214, acc: 0.740
******** [step = 500] loss: 1.217, acc: 0.739
******** [step = 550] loss: 1.221, acc: 0.739
******** [step = 600] loss: 1.223, acc: 0.739
******** [step = 650] loss: 1.228, acc: 0.738
******** [step = 700] loss: 1.230, acc: 0.738
******** [step = 750] loss: 1.235, acc: 0.738
******** [step = 800] loss: 1.239, acc: 0.737
******** [step = 850] loss: 1.241, acc: 0.737
EPOCH = 19 loss: 1.241, acc: 0.737, val_loss: 1.444, val_acc: 0.732

================================================================================2025-08_10 19:57:27
******** [step = 50] loss: 1.144, acc: 0.750
******** [step = 100] loss: 1.147, acc: 0.751
******** [step = 150] loss: 1.150, acc: 0.750
******** [step = 200] loss: 1.167, acc: 0.747
******** [step = 250] loss: 1.174, acc: 0.746
******** [step = 300] loss: 1.180, acc: 0.745
******** [step = 350] loss: 1.183, acc: 0.745
******** [step = 400] loss: 1.186, acc: 0.744
******** [step = 450] loss: 1.190, acc: 0.744
******** [step = 500] loss: 1.193, acc: 0.744
******** [step = 550] loss: 1.196, acc: 0.743
******** [step = 600] loss: 1.198, acc: 0.743
******** [step = 650] loss: 1.204, acc: 0.742
******** [step = 700] loss: 1.208, acc: 0.742
******** [step = 750] loss: 1.211, acc: 0.742
******** [step = 800] loss: 1.213, acc: 0.742
******** [step = 850] loss: 1.215, acc: 0.741
EPOCH = 20 loss: 1.215, acc: 0.741, val_loss: 1.417, val_acc: 0.737

================================================================================2025-08_10 19:58:51
******** [step = 50] loss: 1.125, acc: 0.755
******** [step = 100] loss: 1.123, acc: 0.754
******** [step = 150] loss: 1.135, acc: 0.752
******** [step = 200] loss: 1.143, acc: 0.750
******** [step = 250] loss: 1.151, acc: 0.749
******** [step = 300] loss: 1.154, acc: 0.749
******** [step = 350] loss: 1.159, acc: 0.748
******** [step = 400] loss: 1.164, acc: 0.748
******** [step = 450] loss: 1.168, acc: 0.747
******** [step = 500] loss: 1.172, acc: 0.747
******** [step = 550] loss: 1.177, acc: 0.746
******** [step = 600] loss: 1.179, acc: 0.746
******** [step = 650] loss: 1.183, acc: 0.746
******** [step = 700] loss: 1.186, acc: 0.746
******** [step = 750] loss: 1.189, acc: 0.746
******** [step = 800] loss: 1.191, acc: 0.746
******** [step = 850] loss: 1.192, acc: 0.746
EPOCH = 21 loss: 1.192, acc: 0.746, val_loss: 1.411, val_acc: 0.739

================================================================================2025-08_10 20:00:11
******** [step = 50] loss: 1.092, acc: 0.757
******** [step = 100] loss: 1.101, acc: 0.757
******** [step = 150] loss: 1.113, acc: 0.755
******** [step = 200] loss: 1.126, acc: 0.753
******** [step = 250] loss: 1.130, acc: 0.752
******** [step = 300] loss: 1.132, acc: 0.752
******** [step = 350] loss: 1.137, acc: 0.752
******** [step = 400] loss: 1.140, acc: 0.752
******** [step = 450] loss: 1.142, acc: 0.752
******** [step = 500] loss: 1.145, acc: 0.752
******** [step = 550] loss: 1.149, acc: 0.751
******** [step = 600] loss: 1.152, acc: 0.751
******** [step = 650] loss: 1.157, acc: 0.750
******** [step = 700] loss: 1.161, acc: 0.750
******** [step = 750] loss: 1.164, acc: 0.750
******** [step = 800] loss: 1.168, acc: 0.749
******** [step = 850] loss: 1.171, acc: 0.749
EPOCH = 22 loss: 1.171, acc: 0.749, val_loss: 1.393, val_acc: 0.743

================================================================================2025-08_10 20:01:32
******** [step = 50] loss: 1.066, acc: 0.763
******** [step = 100] loss: 1.080, acc: 0.761
******** [step = 150] loss: 1.085, acc: 0.761
******** [step = 200] loss: 1.093, acc: 0.760
******** [step = 250] loss: 1.098, acc: 0.759
******** [step = 300] loss: 1.107, acc: 0.757
******** [step = 350] loss: 1.111, acc: 0.757
******** [step = 400] loss: 1.116, acc: 0.756
******** [step = 450] loss: 1.121, acc: 0.756
******** [step = 500] loss: 1.124, acc: 0.756
******** [step = 550] loss: 1.129, acc: 0.755
******** [step = 600] loss: 1.133, acc: 0.754
******** [step = 650] loss: 1.137, acc: 0.754
******** [step = 700] loss: 1.141, acc: 0.754
******** [step = 750] loss: 1.144, acc: 0.753
******** [step = 800] loss: 1.147, acc: 0.753
******** [step = 850] loss: 1.152, acc: 0.752
EPOCH = 23 loss: 1.152, acc: 0.752, val_loss: 1.381, val_acc: 0.745

================================================================================2025-08_10 20:02:53
******** [step = 50] loss: 1.054, acc: 0.765
******** [step = 100] loss: 1.071, acc: 0.762
******** [step = 150] loss: 1.082, acc: 0.760
******** [step = 200] loss: 1.088, acc: 0.760
******** [step = 250] loss: 1.090, acc: 0.759
******** [step = 300] loss: 1.091, acc: 0.759
******** [step = 350] loss: 1.096, acc: 0.759
******** [step = 400] loss: 1.101, acc: 0.759
******** [step = 450] loss: 1.106, acc: 0.758
******** [step = 500] loss: 1.110, acc: 0.758
******** [step = 550] loss: 1.113, acc: 0.758
******** [step = 600] loss: 1.118, acc: 0.757
******** [step = 650] loss: 1.120, acc: 0.757
******** [step = 700] loss: 1.124, acc: 0.757
******** [step = 750] loss: 1.128, acc: 0.756
******** [step = 800] loss: 1.131, acc: 0.756
******** [step = 850] loss: 1.135, acc: 0.756
EPOCH = 24 loss: 1.135, acc: 0.756, val_loss: 1.385, val_acc: 0.745

================================================================================2025-08_10 20:04:14
******** [step = 50] loss: 1.052, acc: 0.766
******** [step = 100] loss: 1.066, acc: 0.764
******** [step = 150] loss: 1.074, acc: 0.763
******** [step = 200] loss: 1.071, acc: 0.763
******** [step = 250] loss: 1.075, acc: 0.763
******** [step = 300] loss: 1.077, acc: 0.763
******** [step = 350] loss: 1.080, acc: 0.763
******** [step = 400] loss: 1.084, acc: 0.762
******** [step = 450] loss: 1.088, acc: 0.762
******** [step = 500] loss: 1.092, acc: 0.762
******** [step = 550] loss: 1.099, acc: 0.761
******** [step = 600] loss: 1.104, acc: 0.760
******** [step = 650] loss: 1.105, acc: 0.760
******** [step = 700] loss: 1.107, acc: 0.760
******** [step = 750] loss: 1.112, acc: 0.759
******** [step = 800] loss: 1.115, acc: 0.759
******** [step = 850] loss: 1.118, acc: 0.759
EPOCH = 25 loss: 1.118, acc: 0.759, val_loss: 1.380, val_acc: 0.745

================================================================================2025-08_10 20:05:34
******** [step = 50] loss: 1.048, acc: 0.766
******** [step = 100] loss: 1.042, acc: 0.767
******** [step = 150] loss: 1.047, acc: 0.767
******** [step = 200] loss: 1.051, acc: 0.767
******** [step = 250] loss: 1.052, acc: 0.767
******** [step = 300] loss: 1.056, acc: 0.767
******** [step = 350] loss: 1.061, acc: 0.766
******** [step = 400] loss: 1.068, acc: 0.765
******** [step = 450] loss: 1.071, acc: 0.765
******** [step = 500] loss: 1.076, acc: 0.764
******** [step = 550] loss: 1.080, acc: 0.764
******** [step = 600] loss: 1.082, acc: 0.763
******** [step = 650] loss: 1.086, acc: 0.763
******** [step = 700] loss: 1.090, acc: 0.763
******** [step = 750] loss: 1.094, acc: 0.762
******** [step = 800] loss: 1.099, acc: 0.762
******** [step = 850] loss: 1.101, acc: 0.761
EPOCH = 26 loss: 1.101, acc: 0.761, val_loss: 1.361, val_acc: 0.749

================================================================================2025-08_10 20:06:56
******** [step = 50] loss: 1.009, acc: 0.772
******** [step = 100] loss: 1.023, acc: 0.771
******** [step = 150] loss: 1.030, acc: 0.770
******** [step = 200] loss: 1.035, acc: 0.769
******** [step = 250] loss: 1.037, acc: 0.769
******** [step = 300] loss: 1.045, acc: 0.768
******** [step = 350] loss: 1.047, acc: 0.768
******** [step = 400] loss: 1.049, acc: 0.768
******** [step = 450] loss: 1.056, acc: 0.767
******** [step = 500] loss: 1.060, acc: 0.767
******** [step = 550] loss: 1.063, acc: 0.766
******** [step = 600] loss: 1.067, acc: 0.766
******** [step = 650] loss: 1.071, acc: 0.766
******** [step = 700] loss: 1.073, acc: 0.765
******** [step = 750] loss: 1.078, acc: 0.765
******** [step = 800] loss: 1.082, acc: 0.764
******** [step = 850] loss: 1.086, acc: 0.764
EPOCH = 27 loss: 1.086, acc: 0.764, val_loss: 1.357, val_acc: 0.750

================================================================================2025-08_10 20:08:18
******** [step = 50] loss: 1.024, acc: 0.770
******** [step = 100] loss: 1.006, acc: 0.774
******** [step = 150] loss: 1.006, acc: 0.774
******** [step = 200] loss: 1.015, acc: 0.773
******** [step = 250] loss: 1.023, acc: 0.773
******** [step = 300] loss: 1.029, acc: 0.772
******** [step = 350] loss: 1.034, acc: 0.771
******** [step = 400] loss: 1.039, acc: 0.771
******** [step = 450] loss: 1.045, acc: 0.770
******** [step = 500] loss: 1.051, acc: 0.769
******** [step = 550] loss: 1.055, acc: 0.769
******** [step = 600] loss: 1.057, acc: 0.769
******** [step = 650] loss: 1.060, acc: 0.768
******** [step = 700] loss: 1.063, acc: 0.768
******** [step = 750] loss: 1.065, acc: 0.768
******** [step = 800] loss: 1.067, acc: 0.767
******** [step = 850] loss: 1.073, acc: 0.767
EPOCH = 28 loss: 1.073, acc: 0.767, val_loss: 1.355, val_acc: 0.752

================================================================================2025-08_10 20:09:38
******** [step = 50] loss: 0.975, acc: 0.779
******** [step = 100] loss: 0.989, acc: 0.778
******** [step = 150] loss: 1.001, acc: 0.776
******** [step = 200] loss: 1.001, acc: 0.776
******** [step = 250] loss: 1.007, acc: 0.775
******** [step = 300] loss: 1.015, acc: 0.774
******** [step = 350] loss: 1.022, acc: 0.773
******** [step = 400] loss: 1.026, acc: 0.773
******** [step = 450] loss: 1.031, acc: 0.772
******** [step = 500] loss: 1.034, acc: 0.772
******** [step = 550] loss: 1.039, acc: 0.771
******** [step = 600] loss: 1.043, acc: 0.771
******** [step = 650] loss: 1.046, acc: 0.770
******** [step = 700] loss: 1.049, acc: 0.770
******** [step = 750] loss: 1.054, acc: 0.770
******** [step = 800] loss: 1.057, acc: 0.769
******** [step = 850] loss: 1.061, acc: 0.769
EPOCH = 29 loss: 1.061, acc: 0.769, val_loss: 1.345, val_acc: 0.754

================================================================================2025-08_10 20:11:02
******** [step = 50] loss: 0.992, acc: 0.776
******** [step = 100] loss: 0.978, acc: 0.780
******** [step = 150] loss: 0.980, acc: 0.780
******** [step = 200] loss: 0.986, acc: 0.779
******** [step = 250] loss: 0.993, acc: 0.778
******** [step = 300] loss: 0.996, acc: 0.778
******** [step = 350] loss: 1.003, acc: 0.777
******** [step = 400] loss: 1.011, acc: 0.776
******** [step = 450] loss: 1.015, acc: 0.775
******** [step = 500] loss: 1.021, acc: 0.774
******** [step = 550] loss: 1.024, acc: 0.774
******** [step = 600] loss: 1.029, acc: 0.774
******** [step = 650] loss: 1.032, acc: 0.773
******** [step = 700] loss: 1.036, acc: 0.772
******** [step = 750] loss: 1.040, acc: 0.772
******** [step = 800] loss: 1.044, acc: 0.772
******** [step = 850] loss: 1.048, acc: 0.771
EPOCH = 30 loss: 1.048, acc: 0.771, val_loss: 1.338, val_acc: 0.755

================================================================================2025-08_10 20:12:31
******** [step = 50] loss: 0.980, acc: 0.780
******** [step = 100] loss: 0.984, acc: 0.780
******** [step = 150] loss: 0.977, acc: 0.781
******** [step = 200] loss: 0.985, acc: 0.779
******** [step = 250] loss: 0.983, acc: 0.780
******** [step = 300] loss: 0.989, acc: 0.779
******** [step = 350] loss: 0.995, acc: 0.778
******** [step = 400] loss: 1.000, acc: 0.778
******** [step = 450] loss: 1.004, acc: 0.777
******** [step = 500] loss: 1.007, acc: 0.777
******** [step = 550] loss: 1.011, acc: 0.776
******** [step = 600] loss: 1.016, acc: 0.776
******** [step = 650] loss: 1.020, acc: 0.776
******** [step = 700] loss: 1.024, acc: 0.775
******** [step = 750] loss: 1.027, acc: 0.775
******** [step = 800] loss: 1.031, acc: 0.774
******** [step = 850] loss: 1.037, acc: 0.774
EPOCH = 31 loss: 1.037, acc: 0.774, val_loss: 1.333, val_acc: 0.756

================================================================================2025-08_10 20:14:03
******** [step = 50] loss: 0.974, acc: 0.782
******** [step = 100] loss: 0.969, acc: 0.782
******** [step = 150] loss: 0.977, acc: 0.781
******** [step = 200] loss: 0.978, acc: 0.781
******** [step = 250] loss: 0.981, acc: 0.781
******** [step = 300] loss: 0.985, acc: 0.780
******** [step = 350] loss: 0.988, acc: 0.780
******** [step = 400] loss: 0.993, acc: 0.779
******** [step = 450] loss: 0.996, acc: 0.779
******** [step = 500] loss: 1.001, acc: 0.778
******** [step = 550] loss: 1.006, acc: 0.777
******** [step = 600] loss: 1.010, acc: 0.777
******** [step = 650] loss: 1.014, acc: 0.777
******** [step = 700] loss: 1.014, acc: 0.777
******** [step = 750] loss: 1.018, acc: 0.776
******** [step = 800] loss: 1.021, acc: 0.776
******** [step = 850] loss: 1.023, acc: 0.776
EPOCH = 32 loss: 1.023, acc: 0.776, val_loss: 1.330, val_acc: 0.759

================================================================================2025-08_10 20:15:33
******** [step = 50] loss: 0.952, acc: 0.786
******** [step = 100] loss: 0.960, acc: 0.783
******** [step = 150] loss: 0.965, acc: 0.783
******** [step = 200] loss: 0.971, acc: 0.782
******** [step = 250] loss: 0.974, acc: 0.782
******** [step = 300] loss: 0.976, acc: 0.781
******** [step = 350] loss: 0.978, acc: 0.782
******** [step = 400] loss: 0.982, acc: 0.781
******** [step = 450] loss: 0.985, acc: 0.781
******** [step = 500] loss: 0.988, acc: 0.781
******** [step = 550] loss: 0.992, acc: 0.780
******** [step = 600] loss: 0.996, acc: 0.780
******** [step = 650] loss: 1.001, acc: 0.779
******** [step = 700] loss: 1.007, acc: 0.778
******** [step = 750] loss: 1.010, acc: 0.778
******** [step = 800] loss: 1.013, acc: 0.778
******** [step = 850] loss: 1.016, acc: 0.777
EPOCH = 33 loss: 1.016, acc: 0.777, val_loss: 1.324, val_acc: 0.759

================================================================================2025-08_10 20:16:53
******** [step = 50] loss: 0.945, acc: 0.786
******** [step = 100] loss: 0.939, acc: 0.788
******** [step = 150] loss: 0.947, acc: 0.787
******** [step = 200] loss: 0.948, acc: 0.787
******** [step = 250] loss: 0.952, acc: 0.787
******** [step = 300] loss: 0.956, acc: 0.786
******** [step = 350] loss: 0.958, acc: 0.785
******** [step = 400] loss: 0.966, acc: 0.784
******** [step = 450] loss: 0.969, acc: 0.784
******** [step = 500] loss: 0.975, acc: 0.783
******** [step = 550] loss: 0.981, acc: 0.782
******** [step = 600] loss: 0.985, acc: 0.782
******** [step = 650] loss: 0.989, acc: 0.781
******** [step = 700] loss: 0.992, acc: 0.781
******** [step = 750] loss: 0.995, acc: 0.781
******** [step = 800] loss: 0.998, acc: 0.780
******** [step = 850] loss: 1.003, acc: 0.780
EPOCH = 34 loss: 1.003, acc: 0.780, val_loss: 1.334, val_acc: 0.754

================================================================================2025-08_10 20:18:15
******** [step = 50] loss: 0.933, acc: 0.789
******** [step = 100] loss: 0.937, acc: 0.788
******** [step = 150] loss: 0.940, acc: 0.788
******** [step = 200] loss: 0.938, acc: 0.788
******** [step = 250] loss: 0.941, acc: 0.788
******** [step = 300] loss: 0.947, acc: 0.787
******** [step = 350] loss: 0.954, acc: 0.786
******** [step = 400] loss: 0.961, acc: 0.785
******** [step = 450] loss: 0.966, acc: 0.785
******** [step = 500] loss: 0.968, acc: 0.785
******** [step = 550] loss: 0.972, acc: 0.784
******** [step = 600] loss: 0.976, acc: 0.784
******** [step = 650] loss: 0.979, acc: 0.783
******** [step = 700] loss: 0.983, acc: 0.783
******** [step = 750] loss: 0.987, acc: 0.782
******** [step = 800] loss: 0.991, acc: 0.782
******** [step = 850] loss: 0.996, acc: 0.781
EPOCH = 35 loss: 0.996, acc: 0.781, val_loss: 1.311, val_acc: 0.761

================================================================================2025-08_10 20:19:35
******** [step = 50] loss: 0.940, acc: 0.789
******** [step = 100] loss: 0.927, acc: 0.792
******** [step = 150] loss: 0.932, acc: 0.791
******** [step = 200] loss: 0.936, acc: 0.790
******** [step = 250] loss: 0.937, acc: 0.790
******** [step = 300] loss: 0.945, acc: 0.789
******** [step = 350] loss: 0.949, acc: 0.788
******** [step = 400] loss: 0.955, acc: 0.787
******** [step = 450] loss: 0.960, acc: 0.787
******** [step = 500] loss: 0.962, acc: 0.786
******** [step = 550] loss: 0.966, acc: 0.786
******** [step = 600] loss: 0.969, acc: 0.785
******** [step = 650] loss: 0.972, acc: 0.785
******** [step = 700] loss: 0.977, acc: 0.784
******** [step = 750] loss: 0.980, acc: 0.784
******** [step = 800] loss: 0.982, acc: 0.783
******** [step = 850] loss: 0.985, acc: 0.783
EPOCH = 36 loss: 0.985, acc: 0.783, val_loss: 1.310, val_acc: 0.762

================================================================================2025-08_10 20:20:55
******** [step = 50] loss: 0.934, acc: 0.787
******** [step = 100] loss: 0.915, acc: 0.792
******** [step = 150] loss: 0.914, acc: 0.792
******** [step = 200] loss: 0.920, acc: 0.791
******** [step = 250] loss: 0.927, acc: 0.790
******** [step = 300] loss: 0.935, acc: 0.789
******** [step = 350] loss: 0.942, acc: 0.788
******** [step = 400] loss: 0.946, acc: 0.788
******** [step = 450] loss: 0.951, acc: 0.787
******** [step = 500] loss: 0.956, acc: 0.787
******** [step = 550] loss: 0.959, acc: 0.786
******** [step = 600] loss: 0.962, acc: 0.786
******** [step = 650] loss: 0.966, acc: 0.786
******** [step = 700] loss: 0.969, acc: 0.785
******** [step = 750] loss: 0.972, acc: 0.785
******** [step = 800] loss: 0.974, acc: 0.785
******** [step = 850] loss: 0.978, acc: 0.784
EPOCH = 37 loss: 0.978, acc: 0.784, val_loss: 1.310, val_acc: 0.761

================================================================================2025-08_10 20:22:15
******** [step = 50] loss: 0.895, acc: 0.796
******** [step = 100] loss: 0.909, acc: 0.794
******** [step = 150] loss: 0.907, acc: 0.794
******** [step = 200] loss: 0.913, acc: 0.793
******** [step = 250] loss: 0.914, acc: 0.793
******** [step = 300] loss: 0.923, acc: 0.791
******** [step = 350] loss: 0.926, acc: 0.791
******** [step = 400] loss: 0.932, acc: 0.790
******** [step = 450] loss: 0.936, acc: 0.790
******** [step = 500] loss: 0.940, acc: 0.790
******** [step = 550] loss: 0.945, acc: 0.789
******** [step = 600] loss: 0.950, acc: 0.788
******** [step = 650] loss: 0.955, acc: 0.788
******** [step = 700] loss: 0.959, acc: 0.787
******** [step = 750] loss: 0.963, acc: 0.787
******** [step = 800] loss: 0.966, acc: 0.787
******** [step = 850] loss: 0.970, acc: 0.786
EPOCH = 38 loss: 0.970, acc: 0.786, val_loss: 1.304, val_acc: 0.764

================================================================================2025-08_10 20:23:38
******** [step = 50] loss: 0.903, acc: 0.794
******** [step = 100] loss: 0.903, acc: 0.794
******** [step = 150] loss: 0.904, acc: 0.794
******** [step = 200] loss: 0.914, acc: 0.792
******** [step = 250] loss: 0.916, acc: 0.792
******** [step = 300] loss: 0.924, acc: 0.791
******** [step = 350] loss: 0.927, acc: 0.791
******** [step = 400] loss: 0.930, acc: 0.791
******** [step = 450] loss: 0.934, acc: 0.790
******** [step = 500] loss: 0.937, acc: 0.790
******** [step = 550] loss: 0.941, acc: 0.790
******** [step = 600] loss: 0.945, acc: 0.789
******** [step = 650] loss: 0.948, acc: 0.789
******** [step = 700] loss: 0.953, acc: 0.788
******** [step = 750] loss: 0.956, acc: 0.788
******** [step = 800] loss: 0.958, acc: 0.788
******** [step = 850] loss: 0.960, acc: 0.788
EPOCH = 39 loss: 0.960, acc: 0.788, val_loss: 1.298, val_acc: 0.765

================================================================================2025-08_10 20:25:10
******** [step = 50] loss: 0.899, acc: 0.795
******** [step = 100] loss: 0.893, acc: 0.796
******** [step = 150] loss: 0.893, acc: 0.796
******** [step = 200] loss: 0.899, acc: 0.796
******** [step = 250] loss: 0.901, acc: 0.795
******** [step = 300] loss: 0.908, acc: 0.795
******** [step = 350] loss: 0.914, acc: 0.794
******** [step = 400] loss: 0.921, acc: 0.793
******** [step = 450] loss: 0.925, acc: 0.792
******** [step = 500] loss: 0.927, acc: 0.792
******** [step = 550] loss: 0.929, acc: 0.792
******** [step = 600] loss: 0.932, acc: 0.792
******** [step = 650] loss: 0.937, acc: 0.791
******** [step = 700] loss: 0.941, acc: 0.790
******** [step = 750] loss: 0.943, acc: 0.790
******** [step = 800] loss: 0.949, acc: 0.789
******** [step = 850] loss: 0.953, acc: 0.789
EPOCH = 40 loss: 0.953, acc: 0.789, val_loss: 1.302, val_acc: 0.763

================================================================================2025-08_10 20:26:40
******** [step = 50] loss: 0.853, acc: 0.803
******** [step = 100] loss: 0.878, acc: 0.798
******** [step = 150] loss: 0.883, acc: 0.798
******** [step = 200] loss: 0.889, acc: 0.797
******** [step = 250] loss: 0.897, acc: 0.796
******** [step = 300] loss: 0.901, acc: 0.796
******** [step = 350] loss: 0.907, acc: 0.795
******** [step = 400] loss: 0.911, acc: 0.794
******** [step = 450] loss: 0.916, acc: 0.794
******** [step = 500] loss: 0.920, acc: 0.793
******** [step = 550] loss: 0.925, acc: 0.792
******** [step = 600] loss: 0.929, acc: 0.792
******** [step = 650] loss: 0.932, acc: 0.792
******** [step = 700] loss: 0.935, acc: 0.792
******** [step = 750] loss: 0.939, acc: 0.791
******** [step = 800] loss: 0.943, acc: 0.791
******** [step = 850] loss: 0.945, acc: 0.791
EPOCH = 41 loss: 0.945, acc: 0.791, val_loss: 1.295, val_acc: 0.765

================================================================================2025-08_10 20:28:09
******** [step = 50] loss: 0.874, acc: 0.799
******** [step = 100] loss: 0.879, acc: 0.799
******** [step = 150] loss: 0.881, acc: 0.799
******** [step = 200] loss: 0.885, acc: 0.798
******** [step = 250] loss: 0.888, acc: 0.798
******** [step = 300] loss: 0.889, acc: 0.798
******** [step = 350] loss: 0.893, acc: 0.798
******** [step = 400] loss: 0.898, acc: 0.797
******** [step = 450] loss: 0.904, acc: 0.796
******** [step = 500] loss: 0.910, acc: 0.796
******** [step = 550] loss: 0.916, acc: 0.795
******** [step = 600] loss: 0.921, acc: 0.794
******** [step = 650] loss: 0.926, acc: 0.793
******** [step = 700] loss: 0.929, acc: 0.793
******** [step = 750] loss: 0.934, acc: 0.792
******** [step = 800] loss: 0.937, acc: 0.792
******** [step = 850] loss: 0.938, acc: 0.792
EPOCH = 42 loss: 0.938, acc: 0.792, val_loss: 1.292, val_acc: 0.767

================================================================================2025-08_10 20:29:29
******** [step = 50] loss: 0.868, acc: 0.802
******** [step = 100] loss: 0.875, acc: 0.801
******** [step = 150] loss: 0.876, acc: 0.800
******** [step = 200] loss: 0.879, acc: 0.800
******** [step = 250] loss: 0.887, acc: 0.798
******** [step = 300] loss: 0.889, acc: 0.798
******** [step = 350] loss: 0.894, acc: 0.797
******** [step = 400] loss: 0.901, acc: 0.796
******** [step = 450] loss: 0.903, acc: 0.796
******** [step = 500] loss: 0.909, acc: 0.796
******** [step = 550] loss: 0.912, acc: 0.795
******** [step = 600] loss: 0.916, acc: 0.795
******** [step = 650] loss: 0.918, acc: 0.795
******** [step = 700] loss: 0.921, acc: 0.794
******** [step = 750] loss: 0.926, acc: 0.794
******** [step = 800] loss: 0.929, acc: 0.793
******** [step = 850] loss: 0.934, acc: 0.793
EPOCH = 43 loss: 0.934, acc: 0.793, val_loss: 1.294, val_acc: 0.765

================================================================================2025-08_10 20:30:50
******** [step = 50] loss: 0.854, acc: 0.805
******** [step = 100] loss: 0.857, acc: 0.805
******** [step = 150] loss: 0.865, acc: 0.803
******** [step = 200] loss: 0.870, acc: 0.802
******** [step = 250] loss: 0.878, acc: 0.801
******** [step = 300] loss: 0.882, acc: 0.800
******** [step = 350] loss: 0.885, acc: 0.800
******** [step = 400] loss: 0.891, acc: 0.799
******** [step = 450] loss: 0.896, acc: 0.798
******** [step = 500] loss: 0.900, acc: 0.797
******** [step = 550] loss: 0.905, acc: 0.797
******** [step = 600] loss: 0.907, acc: 0.797
******** [step = 650] loss: 0.911, acc: 0.796
******** [step = 700] loss: 0.916, acc: 0.796
******** [step = 750] loss: 0.919, acc: 0.795
******** [step = 800] loss: 0.923, acc: 0.795
******** [step = 850] loss: 0.929, acc: 0.794
EPOCH = 44 loss: 0.929, acc: 0.794, val_loss: 1.290, val_acc: 0.767

================================================================================2025-08_10 20:32:09
******** [step = 50] loss: 0.860, acc: 0.803
******** [step = 100] loss: 0.857, acc: 0.804
******** [step = 150] loss: 0.858, acc: 0.804
******** [step = 200] loss: 0.863, acc: 0.803
******** [step = 250] loss: 0.864, acc: 0.803
******** [step = 300] loss: 0.867, acc: 0.802
******** [step = 350] loss: 0.874, acc: 0.801
******** [step = 400] loss: 0.876, acc: 0.801
******** [step = 450] loss: 0.883, acc: 0.800
******** [step = 500] loss: 0.887, acc: 0.800
******** [step = 550] loss: 0.891, acc: 0.799
******** [step = 600] loss: 0.896, acc: 0.799
******** [step = 650] loss: 0.901, acc: 0.798
******** [step = 700] loss: 0.906, acc: 0.797
******** [step = 750] loss: 0.911, acc: 0.797
******** [step = 800] loss: 0.917, acc: 0.796
******** [step = 850] loss: 0.920, acc: 0.796
EPOCH = 45 loss: 0.920, acc: 0.796, val_loss: 1.281, val_acc: 0.768

================================================================================2025-08_10 20:33:29
******** [step = 50] loss: 0.844, acc: 0.806
******** [step = 100] loss: 0.855, acc: 0.806
******** [step = 150] loss: 0.856, acc: 0.805
******** [step = 200] loss: 0.865, acc: 0.803
******** [step = 250] loss: 0.874, acc: 0.802
******** [step = 300] loss: 0.878, acc: 0.801
******** [step = 350] loss: 0.880, acc: 0.800
******** [step = 400] loss: 0.883, acc: 0.800
******** [step = 450] loss: 0.885, acc: 0.800
******** [step = 500] loss: 0.888, acc: 0.800
******** [step = 550] loss: 0.894, acc: 0.799
******** [step = 600] loss: 0.897, acc: 0.799
******** [step = 650] loss: 0.899, acc: 0.799
******** [step = 700] loss: 0.903, acc: 0.798
******** [step = 750] loss: 0.906, acc: 0.798
******** [step = 800] loss: 0.911, acc: 0.797
******** [step = 850] loss: 0.914, acc: 0.797
EPOCH = 46 loss: 0.914, acc: 0.797, val_loss: 1.283, val_acc: 0.770

================================================================================2025-08_10 20:34:50
******** [step = 50] loss: 0.819, acc: 0.811
******** [step = 100] loss: 0.841, acc: 0.808
******** [step = 150] loss: 0.846, acc: 0.806
******** [step = 200] loss: 0.851, acc: 0.806
******** [step = 250] loss: 0.856, acc: 0.805
******** [step = 300] loss: 0.860, acc: 0.804
******** [step = 350] loss: 0.866, acc: 0.803
******** [step = 400] loss: 0.870, acc: 0.803
******** [step = 450] loss: 0.876, acc: 0.802
******** [step = 500] loss: 0.879, acc: 0.801
******** [step = 550] loss: 0.885, acc: 0.801
******** [step = 600] loss: 0.888, acc: 0.801
******** [step = 650] loss: 0.891, acc: 0.800
******** [step = 700] loss: 0.896, acc: 0.799
******** [step = 750] loss: 0.901, acc: 0.799
******** [step = 800] loss: 0.904, acc: 0.798
******** [step = 850] loss: 0.910, acc: 0.798
EPOCH = 47 loss: 0.910, acc: 0.798, val_loss: 1.277, val_acc: 0.769

================================================================================2025-08_10 20:36:10
******** [step = 50] loss: 0.833, acc: 0.808
******** [step = 100] loss: 0.837, acc: 0.807
******** [step = 150] loss: 0.838, acc: 0.807
******** [step = 200] loss: 0.843, acc: 0.806
******** [step = 250] loss: 0.848, acc: 0.805
******** [step = 300] loss: 0.855, acc: 0.804
******** [step = 350] loss: 0.861, acc: 0.804
******** [step = 400] loss: 0.867, acc: 0.803
******** [step = 450] loss: 0.873, acc: 0.802
******** [step = 500] loss: 0.875, acc: 0.802
******** [step = 550] loss: 0.879, acc: 0.802
******** [step = 600] loss: 0.883, acc: 0.801
******** [step = 650] loss: 0.886, acc: 0.801
******** [step = 700] loss: 0.890, acc: 0.801
******** [step = 750] loss: 0.894, acc: 0.800
******** [step = 800] loss: 0.898, acc: 0.800
******** [step = 850] loss: 0.900, acc: 0.799
EPOCH = 48 loss: 0.900, acc: 0.799, val_loss: 1.282, val_acc: 0.768

================================================================================2025-08_10 20:37:31
******** [step = 50] loss: 0.815, acc: 0.809
******** [step = 100] loss: 0.827, acc: 0.808
******** [step = 150] loss: 0.838, acc: 0.807
******** [step = 200] loss: 0.842, acc: 0.807
******** [step = 250] loss: 0.848, acc: 0.806
******** [step = 300] loss: 0.849, acc: 0.806
******** [step = 350] loss: 0.856, acc: 0.805
******** [step = 400] loss: 0.863, acc: 0.804
******** [step = 450] loss: 0.867, acc: 0.803
******** [step = 500] loss: 0.870, acc: 0.803
******** [step = 550] loss: 0.874, acc: 0.802
******** [step = 600] loss: 0.879, acc: 0.802
******** [step = 650] loss: 0.883, acc: 0.801
******** [step = 700] loss: 0.889, acc: 0.800
******** [step = 750] loss: 0.891, acc: 0.800
******** [step = 800] loss: 0.896, acc: 0.800
******** [step = 850] loss: 0.902, acc: 0.799
EPOCH = 49 loss: 0.902, acc: 0.799, val_loss: 1.277, val_acc: 0.769

================================================================================2025-08_10 20:38:51
******** [step = 50] loss: 0.828, acc: 0.810
******** [step = 100] loss: 0.815, acc: 0.812
******** [step = 150] loss: 0.819, acc: 0.812
******** [step = 200] loss: 0.828, acc: 0.810
******** [step = 250] loss: 0.836, acc: 0.809
******** [step = 300] loss: 0.842, acc: 0.807
******** [step = 350] loss: 0.847, acc: 0.807
******** [step = 400] loss: 0.851, acc: 0.806
******** [step = 450] loss: 0.859, acc: 0.805
******** [step = 500] loss: 0.863, acc: 0.805
******** [step = 550] loss: 0.867, acc: 0.804
******** [step = 600] loss: 0.871, acc: 0.804
******** [step = 650] loss: 0.875, acc: 0.804
******** [step = 700] loss: 0.881, acc: 0.803
******** [step = 750] loss: 0.885, acc: 0.802
******** [step = 800] loss: 0.888, acc: 0.802
******** [step = 850] loss: 0.891, acc: 0.802
EPOCH = 50 loss: 0.891, acc: 0.802, val_loss: 1.276, val_acc: 0.771

================================================================================2025-08_10 20:40:20
******** [step = 50] loss: 0.822, acc: 0.811
******** [step = 100] loss: 0.822, acc: 0.812
******** [step = 150] loss: 0.829, acc: 0.810
******** [step = 200] loss: 0.831, acc: 0.810
******** [step = 250] loss: 0.836, acc: 0.809
******** [step = 300] loss: 0.840, acc: 0.808
******** [step = 350] loss: 0.844, acc: 0.807
******** [step = 400] loss: 0.849, acc: 0.807
******** [step = 450] loss: 0.855, acc: 0.806
******** [step = 500] loss: 0.860, acc: 0.805
******** [step = 550] loss: 0.863, acc: 0.805
******** [step = 600] loss: 0.867, acc: 0.804
******** [step = 650] loss: 0.873, acc: 0.804
******** [step = 700] loss: 0.877, acc: 0.803
******** [step = 750] loss: 0.879, acc: 0.803
******** [step = 800] loss: 0.882, acc: 0.803
******** [step = 850] loss: 0.886, acc: 0.802
EPOCH = 51 loss: 0.886, acc: 0.802, val_loss: 1.269, val_acc: 0.771

================================================================================2025-08_10 20:41:54
******** [step = 50] loss: 0.810, acc: 0.815
******** [step = 100] loss: 0.811, acc: 0.814
******** [step = 150] loss: 0.818, acc: 0.812
******** [step = 200] loss: 0.822, acc: 0.811
******** [step = 250] loss: 0.830, acc: 0.810
******** [step = 300] loss: 0.838, acc: 0.808
******** [step = 350] loss: 0.843, acc: 0.808
******** [step = 400] loss: 0.847, acc: 0.807
******** [step = 450] loss: 0.850, acc: 0.807
******** [step = 500] loss: 0.857, acc: 0.806
******** [step = 550] loss: 0.861, acc: 0.806
******** [step = 600] loss: 0.864, acc: 0.805
******** [step = 650] loss: 0.869, acc: 0.805
******** [step = 700] loss: 0.872, acc: 0.804
******** [step = 750] loss: 0.875, acc: 0.804
******** [step = 800] loss: 0.879, acc: 0.803
******** [step = 850] loss: 0.882, acc: 0.803
EPOCH = 52 loss: 0.882, acc: 0.803, val_loss: 1.270, val_acc: 0.772

================================================================================2025-08_10 20:43:26
******** [step = 50] loss: 0.810, acc: 0.813
******** [step = 100] loss: 0.806, acc: 0.815
******** [step = 150] loss: 0.814, acc: 0.813
******** [step = 200] loss: 0.819, acc: 0.812
******** [step = 250] loss: 0.824, acc: 0.811
******** [step = 300] loss: 0.829, acc: 0.810
******** [step = 350] loss: 0.836, acc: 0.809
******** [step = 400] loss: 0.839, acc: 0.809
******** [step = 450] loss: 0.845, acc: 0.808
******** [step = 500] loss: 0.847, acc: 0.808
******** [step = 550] loss: 0.853, acc: 0.807
******** [step = 600] loss: 0.859, acc: 0.806
******** [step = 650] loss: 0.862, acc: 0.806
******** [step = 700] loss: 0.866, acc: 0.805
******** [step = 750] loss: 0.869, acc: 0.805
******** [step = 800] loss: 0.874, acc: 0.804
******** [step = 850] loss: 0.876, acc: 0.804
EPOCH = 53 loss: 0.876, acc: 0.804, val_loss: 1.270, val_acc: 0.772

================================================================================2025-08_10 20:44:47
******** [step = 50] loss: 0.780, acc: 0.819
******** [step = 100] loss: 0.782, acc: 0.819
******** [step = 150] loss: 0.798, acc: 0.816
******** [step = 200] loss: 0.802, acc: 0.814
******** [step = 250] loss: 0.809, acc: 0.813
******** [step = 300] loss: 0.819, acc: 0.812
******** [step = 350] loss: 0.828, acc: 0.810
******** [step = 400] loss: 0.834, acc: 0.810
******** [step = 450] loss: 0.839, acc: 0.809
******** [step = 500] loss: 0.843, acc: 0.808
******** [step = 550] loss: 0.847, acc: 0.808
******** [step = 600] loss: 0.852, acc: 0.807
******** [step = 650] loss: 0.858, acc: 0.807
******** [step = 700] loss: 0.862, acc: 0.806
******** [step = 750] loss: 0.865, acc: 0.806
******** [step = 800] loss: 0.869, acc: 0.805
******** [step = 850] loss: 0.873, acc: 0.805
EPOCH = 54 loss: 0.873, acc: 0.805, val_loss: 1.269, val_acc: 0.771

================================================================================2025-08_10 20:46:08
******** [step = 50] loss: 0.812, acc: 0.810
******** [step = 100] loss: 0.810, acc: 0.812
******** [step = 150] loss: 0.814, acc: 0.812
******** [step = 200] loss: 0.816, acc: 0.811
******** [step = 250] loss: 0.821, acc: 0.811
******** [step = 300] loss: 0.823, acc: 0.811
******** [step = 350] loss: 0.827, acc: 0.810
******** [step = 400] loss: 0.833, acc: 0.810
******** [step = 450] loss: 0.837, acc: 0.809
******** [step = 500] loss: 0.842, acc: 0.809
******** [step = 550] loss: 0.846, acc: 0.808
******** [step = 600] loss: 0.850, acc: 0.808
******** [step = 650] loss: 0.854, acc: 0.807
******** [step = 700] loss: 0.857, acc: 0.807
******** [step = 750] loss: 0.862, acc: 0.806
******** [step = 800] loss: 0.864, acc: 0.806
******** [step = 850] loss: 0.869, acc: 0.805
EPOCH = 55 loss: 0.869, acc: 0.805, val_loss: 1.264, val_acc: 0.774

================================================================================2025-08_10 20:47:30
******** [step = 50] loss: 0.790, acc: 0.817
******** [step = 100] loss: 0.798, acc: 0.815
******** [step = 150] loss: 0.806, acc: 0.814
******** [step = 200] loss: 0.813, acc: 0.813
******** [step = 250] loss: 0.814, acc: 0.813
******** [step = 300] loss: 0.816, acc: 0.813
******** [step = 350] loss: 0.819, acc: 0.812
******** [step = 400] loss: 0.825, acc: 0.811
******** [step = 450] loss: 0.828, acc: 0.811
******** [step = 500] loss: 0.835, acc: 0.810
******** [step = 550] loss: 0.840, acc: 0.810
******** [step = 600] loss: 0.844, acc: 0.809
******** [step = 650] loss: 0.848, acc: 0.809
******** [step = 700] loss: 0.853, acc: 0.808
******** [step = 750] loss: 0.857, acc: 0.808
******** [step = 800] loss: 0.860, acc: 0.807
******** [step = 850] loss: 0.864, acc: 0.807
EPOCH = 56 loss: 0.864, acc: 0.807, val_loss: 1.265, val_acc: 0.775

================================================================================2025-08_10 20:48:51
******** [step = 50] loss: 0.763, acc: 0.821
******** [step = 100] loss: 0.780, acc: 0.817
******** [step = 150] loss: 0.788, acc: 0.816
******** [step = 200] loss: 0.801, acc: 0.814
******** [step = 250] loss: 0.811, acc: 0.813
******** [step = 300] loss: 0.818, acc: 0.812
******** [step = 350] loss: 0.820, acc: 0.812
******** [step = 400] loss: 0.823, acc: 0.812
******** [step = 450] loss: 0.830, acc: 0.811
******** [step = 500] loss: 0.834, acc: 0.810
******** [step = 550] loss: 0.838, acc: 0.810
******** [step = 600] loss: 0.841, acc: 0.809
******** [step = 650] loss: 0.844, acc: 0.809
******** [step = 700] loss: 0.847, acc: 0.809
******** [step = 750] loss: 0.851, acc: 0.808
******** [step = 800] loss: 0.855, acc: 0.808
******** [step = 850] loss: 0.861, acc: 0.807
EPOCH = 57 loss: 0.861, acc: 0.807, val_loss: 1.265, val_acc: 0.776

================================================================================2025-08_10 20:50:12
******** [step = 50] loss: 0.781, acc: 0.819
******** [step = 100] loss: 0.789, acc: 0.817
******** [step = 150] loss: 0.797, acc: 0.816
******** [step = 200] loss: 0.799, acc: 0.816
******** [step = 250] loss: 0.805, acc: 0.815
******** [step = 300] loss: 0.810, acc: 0.814
******** [step = 350] loss: 0.815, acc: 0.813
******** [step = 400] loss: 0.817, acc: 0.813
******** [step = 450] loss: 0.821, acc: 0.812
******** [step = 500] loss: 0.826, acc: 0.812
******** [step = 550] loss: 0.830, acc: 0.811
******** [step = 600] loss: 0.834, acc: 0.811
******** [step = 650] loss: 0.837, acc: 0.810
******** [step = 700] loss: 0.841, acc: 0.810
******** [step = 750] loss: 0.846, acc: 0.809
******** [step = 800] loss: 0.851, acc: 0.809
******** [step = 850] loss: 0.854, acc: 0.808
EPOCH = 58 loss: 0.854, acc: 0.808, val_loss: 1.259, val_acc: 0.775

================================================================================2025-08_10 20:51:32
******** [step = 50] loss: 0.780, acc: 0.819
******** [step = 100] loss: 0.785, acc: 0.818
******** [step = 150] loss: 0.791, acc: 0.817
******** [step = 200] loss: 0.800, acc: 0.816
******** [step = 250] loss: 0.805, acc: 0.815
******** [step = 300] loss: 0.808, acc: 0.814
******** [step = 350] loss: 0.811, acc: 0.814
******** [step = 400] loss: 0.816, acc: 0.813
******** [step = 450] loss: 0.820, acc: 0.813
******** [step = 500] loss: 0.824, acc: 0.812
******** [step = 550] loss: 0.828, acc: 0.812
******** [step = 600] loss: 0.833, acc: 0.811
******** [step = 650] loss: 0.838, acc: 0.810
******** [step = 700] loss: 0.841, acc: 0.810
******** [step = 750] loss: 0.843, acc: 0.810
******** [step = 800] loss: 0.846, acc: 0.810
******** [step = 850] loss: 0.849, acc: 0.809
EPOCH = 59 loss: 0.849, acc: 0.809, val_loss: 1.261, val_acc: 0.775

================================================================================2025-08_10 20:52:52
******** [step = 50] loss: 0.773, acc: 0.820
******** [step = 100] loss: 0.790, acc: 0.818
******** [step = 150] loss: 0.792, acc: 0.818
******** [step = 200] loss: 0.797, acc: 0.817
******** [step = 250] loss: 0.801, acc: 0.816
******** [step = 300] loss: 0.806, acc: 0.815
******** [step = 350] loss: 0.809, acc: 0.815
******** [step = 400] loss: 0.812, acc: 0.815
******** [step = 450] loss: 0.815, acc: 0.814
******** [step = 500] loss: 0.820, acc: 0.813
******** [step = 550] loss: 0.825, acc: 0.813
******** [step = 600] loss: 0.828, acc: 0.812
******** [step = 650] loss: 0.831, acc: 0.812
******** [step = 700] loss: 0.834, acc: 0.812
******** [step = 750] loss: 0.838, acc: 0.811
******** [step = 800] loss: 0.842, acc: 0.811
******** [step = 850] loss: 0.847, acc: 0.810
EPOCH = 60 loss: 0.847, acc: 0.810, val_loss: 1.255, val_acc: 0.775

================================================================================2025-08_10 20:54:20
******** [step = 50] loss: 0.767, acc: 0.820
******** [step = 100] loss: 0.769, acc: 0.819
******** [step = 150] loss: 0.775, acc: 0.819
******** [step = 200] loss: 0.780, acc: 0.818
******** [step = 250] loss: 0.786, acc: 0.817
******** [step = 300] loss: 0.794, acc: 0.816
******** [step = 350] loss: 0.800, acc: 0.815
******** [step = 400] loss: 0.804, acc: 0.814
******** [step = 450] loss: 0.809, acc: 0.814
******** [step = 500] loss: 0.814, acc: 0.813
******** [step = 550] loss: 0.820, acc: 0.813
******** [step = 600] loss: 0.824, acc: 0.812
******** [step = 650] loss: 0.827, acc: 0.812
******** [step = 700] loss: 0.831, acc: 0.812
******** [step = 750] loss: 0.835, acc: 0.811
******** [step = 800] loss: 0.838, acc: 0.811
******** [step = 850] loss: 0.842, acc: 0.810
EPOCH = 61 loss: 0.842, acc: 0.810, val_loss: 1.259, val_acc: 0.775

================================================================================2025-08_10 20:55:44
******** [step = 50] loss: 0.765, acc: 0.819
******** [step = 100] loss: 0.768, acc: 0.820
******** [step = 150] loss: 0.776, acc: 0.819
******** [step = 200] loss: 0.779, acc: 0.819
******** [step = 250] loss: 0.784, acc: 0.818
******** [step = 300] loss: 0.789, acc: 0.817
******** [step = 350] loss: 0.794, acc: 0.817
******** [step = 400] loss: 0.797, acc: 0.816
******** [step = 450] loss: 0.801, acc: 0.816
******** [step = 500] loss: 0.806, acc: 0.815
******** [step = 550] loss: 0.812, acc: 0.815
******** [step = 600] loss: 0.815, acc: 0.814
******** [step = 650] loss: 0.820, acc: 0.814
******** [step = 700] loss: 0.824, acc: 0.813
******** [step = 750] loss: 0.830, acc: 0.812
******** [step = 800] loss: 0.835, acc: 0.812
******** [step = 850] loss: 0.840, acc: 0.811
EPOCH = 62 loss: 0.840, acc: 0.811, val_loss: 1.258, val_acc: 0.776

================================================================================2025-08_10 20:57:03
******** [step = 50] loss: 0.764, acc: 0.822
******** [step = 100] loss: 0.769, acc: 0.821
******** [step = 150] loss: 0.770, acc: 0.821
******** [step = 200] loss: 0.772, acc: 0.820
******** [step = 250] loss: 0.777, acc: 0.820
******** [step = 300] loss: 0.787, acc: 0.818
******** [step = 350] loss: 0.794, acc: 0.817
******** [step = 400] loss: 0.796, acc: 0.817
******** [step = 450] loss: 0.800, acc: 0.816
******** [step = 500] loss: 0.802, acc: 0.816
******** [step = 550] loss: 0.807, acc: 0.815
******** [step = 600] loss: 0.812, acc: 0.815
******** [step = 650] loss: 0.816, acc: 0.814
******** [step = 700] loss: 0.822, acc: 0.814
******** [step = 750] loss: 0.826, acc: 0.813
******** [step = 800] loss: 0.831, acc: 0.812
******** [step = 850] loss: 0.833, acc: 0.812
EPOCH = 63 loss: 0.833, acc: 0.812, val_loss: 1.254, val_acc: 0.776

================================================================================2025-08_10 20:58:24
******** [step = 50] loss: 0.753, acc: 0.824
******** [step = 100] loss: 0.756, acc: 0.824
******** [step = 150] loss: 0.761, acc: 0.823
******** [step = 200] loss: 0.770, acc: 0.821
******** [step = 250] loss: 0.778, acc: 0.820
******** [step = 300] loss: 0.786, acc: 0.819
******** [step = 350] loss: 0.789, acc: 0.818
******** [step = 400] loss: 0.794, acc: 0.818
******** [step = 450] loss: 0.798, acc: 0.817
******** [step = 500] loss: 0.801, acc: 0.817
******** [step = 550] loss: 0.803, acc: 0.817
******** [step = 600] loss: 0.807, acc: 0.816
******** [step = 650] loss: 0.812, acc: 0.816
******** [step = 700] loss: 0.817, acc: 0.815
******** [step = 750] loss: 0.823, acc: 0.814
******** [step = 800] loss: 0.828, acc: 0.814
******** [step = 850] loss: 0.831, acc: 0.813
EPOCH = 64 loss: 0.831, acc: 0.813, val_loss: 1.250, val_acc: 0.777

================================================================================2025-08_10 20:59:47
******** [step = 50] loss: 0.750, acc: 0.825
******** [step = 100] loss: 0.763, acc: 0.823
******** [step = 150] loss: 0.764, acc: 0.823
******** [step = 200] loss: 0.771, acc: 0.822
******** [step = 250] loss: 0.777, acc: 0.820
******** [step = 300] loss: 0.780, acc: 0.820
******** [step = 350] loss: 0.785, acc: 0.819
******** [step = 400] loss: 0.789, acc: 0.818
******** [step = 450] loss: 0.794, acc: 0.817
******** [step = 500] loss: 0.798, acc: 0.817
******** [step = 550] loss: 0.802, acc: 0.816
******** [step = 600] loss: 0.806, acc: 0.816
******** [step = 650] loss: 0.810, acc: 0.816
******** [step = 700] loss: 0.814, acc: 0.815
******** [step = 750] loss: 0.819, acc: 0.815
******** [step = 800] loss: 0.823, acc: 0.814
******** [step = 850] loss: 0.826, acc: 0.814
EPOCH = 65 loss: 0.826, acc: 0.814, val_loss: 1.251, val_acc: 0.777

================================================================================2025-08_10 21:01:07
******** [step = 50] loss: 0.757, acc: 0.826
******** [step = 100] loss: 0.766, acc: 0.824
******** [step = 150] loss: 0.767, acc: 0.823
******** [step = 200] loss: 0.771, acc: 0.822
******** [step = 250] loss: 0.775, acc: 0.821
******** [step = 300] loss: 0.779, acc: 0.820
******** [step = 350] loss: 0.784, acc: 0.819
******** [step = 400] loss: 0.787, acc: 0.819
******** [step = 450] loss: 0.792, acc: 0.818
******** [step = 500] loss: 0.795, acc: 0.818
******** [step = 550] loss: 0.799, acc: 0.817
******** [step = 600] loss: 0.804, acc: 0.817
******** [step = 650] loss: 0.808, acc: 0.816
******** [step = 700] loss: 0.813, acc: 0.816
******** [step = 750] loss: 0.817, acc: 0.815
******** [step = 800] loss: 0.821, acc: 0.815
******** [step = 850] loss: 0.824, acc: 0.814
EPOCH = 66 loss: 0.824, acc: 0.814, val_loss: 1.249, val_acc: 0.778

================================================================================2025-08_10 21:02:31
******** [step = 50] loss: 0.750, acc: 0.825
******** [step = 100] loss: 0.759, acc: 0.824
******** [step = 150] loss: 0.765, acc: 0.822
******** [step = 200] loss: 0.772, acc: 0.821
******** [step = 250] loss: 0.777, acc: 0.820
******** [step = 300] loss: 0.781, acc: 0.820
******** [step = 350] loss: 0.785, acc: 0.819
******** [step = 400] loss: 0.788, acc: 0.819
******** [step = 450] loss: 0.790, acc: 0.819
******** [step = 500] loss: 0.792, acc: 0.818
******** [step = 550] loss: 0.797, acc: 0.817
******** [step = 600] loss: 0.801, acc: 0.817
******** [step = 650] loss: 0.806, acc: 0.816
******** [step = 700] loss: 0.811, acc: 0.816
******** [step = 750] loss: 0.815, acc: 0.816
******** [step = 800] loss: 0.818, acc: 0.815
******** [step = 850] loss: 0.822, acc: 0.815
EPOCH = 67 loss: 0.822, acc: 0.815, val_loss: 1.245, val_acc: 0.780

================================================================================2025-08_10 21:04:01
******** [step = 50] loss: 0.738, acc: 0.828
******** [step = 100] loss: 0.746, acc: 0.825
******** [step = 150] loss: 0.756, acc: 0.823
******** [step = 200] loss: 0.758, acc: 0.822
******** [step = 250] loss: 0.766, acc: 0.821
******** [step = 300] loss: 0.772, acc: 0.820
******** [step = 350] loss: 0.776, acc: 0.819
******** [step = 400] loss: 0.781, acc: 0.819
******** [step = 450] loss: 0.786, acc: 0.819
******** [step = 500] loss: 0.788, acc: 0.818
******** [step = 550] loss: 0.794, acc: 0.818
******** [step = 600] loss: 0.797, acc: 0.818
******** [step = 650] loss: 0.801, acc: 0.817
******** [step = 700] loss: 0.805, acc: 0.817
******** [step = 750] loss: 0.808, acc: 0.816
******** [step = 800] loss: 0.812, acc: 0.816
******** [step = 850] loss: 0.817, acc: 0.815
EPOCH = 68 loss: 0.817, acc: 0.815, val_loss: 1.251, val_acc: 0.778

================================================================================2025-08_10 21:05:28
******** [step = 50] loss: 0.745, acc: 0.826
******** [step = 100] loss: 0.743, acc: 0.826
******** [step = 150] loss: 0.749, acc: 0.825
******** [step = 200] loss: 0.757, acc: 0.823
******** [step = 250] loss: 0.763, acc: 0.822
******** [step = 300] loss: 0.767, acc: 0.822
******** [step = 350] loss: 0.773, acc: 0.821
******** [step = 400] loss: 0.777, acc: 0.820
******** [step = 450] loss: 0.780, acc: 0.820
******** [step = 500] loss: 0.785, acc: 0.819
******** [step = 550] loss: 0.789, acc: 0.819
******** [step = 600] loss: 0.792, acc: 0.819
******** [step = 650] loss: 0.796, acc: 0.818
******** [step = 700] loss: 0.798, acc: 0.818
******** [step = 750] loss: 0.803, acc: 0.818
******** [step = 800] loss: 0.809, acc: 0.817
******** [step = 850] loss: 0.813, acc: 0.816
EPOCH = 69 loss: 0.813, acc: 0.816, val_loss: 1.248, val_acc: 0.779

================================================================================2025-08_10 21:06:56
******** [step = 50] loss: 0.741, acc: 0.827
******** [step = 100] loss: 0.739, acc: 0.827
******** [step = 150] loss: 0.746, acc: 0.825
******** [step = 200] loss: 0.750, acc: 0.824
******** [step = 250] loss: 0.759, acc: 0.823
******** [step = 300] loss: 0.761, acc: 0.823
******** [step = 350] loss: 0.767, acc: 0.821
******** [step = 400] loss: 0.774, acc: 0.820
******** [step = 450] loss: 0.779, acc: 0.820
******** [step = 500] loss: 0.784, acc: 0.819
******** [step = 550] loss: 0.790, acc: 0.819
******** [step = 600] loss: 0.794, acc: 0.818
******** [step = 650] loss: 0.798, acc: 0.818
******** [step = 700] loss: 0.800, acc: 0.818
******** [step = 750] loss: 0.805, acc: 0.817
******** [step = 800] loss: 0.808, acc: 0.817
******** [step = 850] loss: 0.811, acc: 0.817
EPOCH = 70 loss: 0.811, acc: 0.817, val_loss: 1.244, val_acc: 0.780

================================================================================2025-08_10 21:08:16
******** [step = 50] loss: 0.739, acc: 0.827
******** [step = 100] loss: 0.742, acc: 0.826
******** [step = 150] loss: 0.747, acc: 0.825
******** [step = 200] loss: 0.753, acc: 0.824
******** [step = 250] loss: 0.754, acc: 0.824
******** [step = 300] loss: 0.759, acc: 0.823
******** [step = 350] loss: 0.764, acc: 0.823
******** [step = 400] loss: 0.770, acc: 0.822
******** [step = 450] loss: 0.775, acc: 0.821
******** [step = 500] loss: 0.780, acc: 0.821
******** [step = 550] loss: 0.784, acc: 0.820
******** [step = 600] loss: 0.789, acc: 0.820
******** [step = 650] loss: 0.794, acc: 0.819
******** [step = 700] loss: 0.797, acc: 0.819
******** [step = 750] loss: 0.801, acc: 0.818
******** [step = 800] loss: 0.804, acc: 0.818
******** [step = 850] loss: 0.808, acc: 0.818
EPOCH = 71 loss: 0.808, acc: 0.818, val_loss: 1.242, val_acc: 0.780

================================================================================2025-08_10 21:09:35
******** [step = 50] loss: 0.728, acc: 0.829
******** [step = 100] loss: 0.745, acc: 0.827
******** [step = 150] loss: 0.746, acc: 0.826
******** [step = 200] loss: 0.751, acc: 0.825
******** [step = 250] loss: 0.756, acc: 0.824
******** [step = 300] loss: 0.761, acc: 0.823
******** [step = 350] loss: 0.766, acc: 0.823
******** [step = 400] loss: 0.772, acc: 0.822
******** [step = 450] loss: 0.776, acc: 0.821
******** [step = 500] loss: 0.781, acc: 0.821
******** [step = 550] loss: 0.782, acc: 0.821
******** [step = 600] loss: 0.785, acc: 0.820
******** [step = 650] loss: 0.789, acc: 0.820
******** [step = 700] loss: 0.793, acc: 0.820
******** [step = 750] loss: 0.797, acc: 0.819
******** [step = 800] loss: 0.799, acc: 0.819
******** [step = 850] loss: 0.805, acc: 0.818
EPOCH = 72 loss: 0.805, acc: 0.818, val_loss: 1.244, val_acc: 0.779

================================================================================2025-08_10 21:11:03
******** [step = 50] loss: 0.732, acc: 0.829
******** [step = 100] loss: 0.731, acc: 0.828
******** [step = 150] loss: 0.739, acc: 0.827
******** [step = 200] loss: 0.745, acc: 0.826
******** [step = 250] loss: 0.751, acc: 0.825
******** [step = 300] loss: 0.754, acc: 0.825
******** [step = 350] loss: 0.759, acc: 0.824
******** [step = 400] loss: 0.762, acc: 0.824
******** [step = 450] loss: 0.768, acc: 0.823
******** [step = 500] loss: 0.775, acc: 0.822
******** [step = 550] loss: 0.781, acc: 0.821
******** [step = 600] loss: 0.785, acc: 0.821
******** [step = 650] loss: 0.787, acc: 0.820
******** [step = 700] loss: 0.790, acc: 0.820
******** [step = 750] loss: 0.794, acc: 0.820
******** [step = 800] loss: 0.799, acc: 0.819
******** [step = 850] loss: 0.802, acc: 0.819
EPOCH = 73 loss: 0.802, acc: 0.819, val_loss: 1.240, val_acc: 0.781

================================================================================2025-08_10 21:12:27
******** [step = 50] loss: 0.712, acc: 0.834
******** [step = 100] loss: 0.726, acc: 0.830
******** [step = 150] loss: 0.737, acc: 0.828
******** [step = 200] loss: 0.742, acc: 0.827
******** [step = 250] loss: 0.749, acc: 0.826
******** [step = 300] loss: 0.750, acc: 0.825
******** [step = 350] loss: 0.755, acc: 0.825
******** [step = 400] loss: 0.761, acc: 0.824
******** [step = 450] loss: 0.766, acc: 0.823
******** [step = 500] loss: 0.769, acc: 0.823
******** [step = 550] loss: 0.773, acc: 0.822
******** [step = 600] loss: 0.778, acc: 0.822
******** [step = 650] loss: 0.783, acc: 0.821
******** [step = 700] loss: 0.788, acc: 0.820
******** [step = 750] loss: 0.791, acc: 0.820
******** [step = 800] loss: 0.795, acc: 0.820
******** [step = 850] loss: 0.797, acc: 0.820
EPOCH = 74 loss: 0.797, acc: 0.820, val_loss: 1.246, val_acc: 0.780

================================================================================2025-08_10 21:13:57
******** [step = 50] loss: 0.715, acc: 0.834
******** [step = 100] loss: 0.723, acc: 0.832
******** [step = 150] loss: 0.732, acc: 0.830
******** [step = 200] loss: 0.736, acc: 0.829
******** [step = 250] loss: 0.744, acc: 0.827
******** [step = 300] loss: 0.751, acc: 0.826
******** [step = 350] loss: 0.757, acc: 0.825
******** [step = 400] loss: 0.760, acc: 0.824
******** [step = 450] loss: 0.764, acc: 0.824
******** [step = 500] loss: 0.767, acc: 0.824
******** [step = 550] loss: 0.771, acc: 0.823
******** [step = 600] loss: 0.776, acc: 0.823
******** [step = 650] loss: 0.779, acc: 0.823
******** [step = 700] loss: 0.783, acc: 0.822
******** [step = 750] loss: 0.787, acc: 0.822
******** [step = 800] loss: 0.790, acc: 0.821
******** [step = 850] loss: 0.792, acc: 0.821
EPOCH = 75 loss: 0.792, acc: 0.821, val_loss: 1.243, val_acc: 0.780

================================================================================2025-08_10 21:15:16
******** [step = 50] loss: 0.722, acc: 0.830
******** [step = 100] loss: 0.729, acc: 0.829
******** [step = 150] loss: 0.732, acc: 0.829
******** [step = 200] loss: 0.735, acc: 0.828
******** [step = 250] loss: 0.745, acc: 0.827
******** [step = 300] loss: 0.748, acc: 0.826
******** [step = 350] loss: 0.753, acc: 0.825
******** [step = 400] loss: 0.757, acc: 0.825
******** [step = 450] loss: 0.761, acc: 0.824
******** [step = 500] loss: 0.766, acc: 0.823
******** [step = 550] loss: 0.770, acc: 0.823
******** [step = 600] loss: 0.774, acc: 0.822
******** [step = 650] loss: 0.778, acc: 0.822
******** [step = 700] loss: 0.782, acc: 0.822
******** [step = 750] loss: 0.786, acc: 0.821
******** [step = 800] loss: 0.789, acc: 0.821
******** [step = 850] loss: 0.794, acc: 0.821
EPOCH = 76 loss: 0.794, acc: 0.821, val_loss: 1.247, val_acc: 0.779

================================================================================2025-08_10 21:16:40
******** [step = 50] loss: 0.721, acc: 0.830
******** [step = 100] loss: 0.716, acc: 0.832
******** [step = 150] loss: 0.724, acc: 0.830
******** [step = 200] loss: 0.733, acc: 0.828
******** [step = 250] loss: 0.737, acc: 0.828
******** [step = 300] loss: 0.742, acc: 0.827
******** [step = 350] loss: 0.747, acc: 0.826
******** [step = 400] loss: 0.753, acc: 0.826
******** [step = 450] loss: 0.758, acc: 0.825
******** [step = 500] loss: 0.763, acc: 0.824
******** [step = 550] loss: 0.766, acc: 0.824
******** [step = 600] loss: 0.770, acc: 0.823
******** [step = 650] loss: 0.774, acc: 0.823
******** [step = 700] loss: 0.778, acc: 0.823
******** [step = 750] loss: 0.782, acc: 0.822
******** [step = 800] loss: 0.786, acc: 0.822
******** [step = 850] loss: 0.788, acc: 0.822
EPOCH = 77 loss: 0.788, acc: 0.822, val_loss: 1.242, val_acc: 0.781

================================================================================2025-08_10 21:18:05
******** [step = 50] loss: 0.702, acc: 0.833
******** [step = 100] loss: 0.711, acc: 0.832
******** [step = 150] loss: 0.717, acc: 0.831
******** [step = 200] loss: 0.728, acc: 0.829
******** [step = 250] loss: 0.733, acc: 0.828
******** [step = 300] loss: 0.738, acc: 0.828
******** [step = 350] loss: 0.742, acc: 0.827
******** [step = 400] loss: 0.747, acc: 0.826
******** [step = 450] loss: 0.751, acc: 0.826
******** [step = 500] loss: 0.756, acc: 0.825
******** [step = 550] loss: 0.760, acc: 0.825
******** [step = 600] loss: 0.764, acc: 0.824
******** [step = 650] loss: 0.768, acc: 0.824
******** [step = 700] loss: 0.771, acc: 0.824
******** [step = 750] loss: 0.776, acc: 0.823
******** [step = 800] loss: 0.780, acc: 0.823
******** [step = 850] loss: 0.784, acc: 0.822
EPOCH = 78 loss: 0.784, acc: 0.822, val_loss: 1.236, val_acc: 0.782

================================================================================2025-08_10 21:19:30
******** [step = 50] loss: 0.710, acc: 0.833
******** [step = 100] loss: 0.715, acc: 0.832
******** [step = 150] loss: 0.714, acc: 0.831
******** [step = 200] loss: 0.722, acc: 0.830
******** [step = 250] loss: 0.729, acc: 0.829
******** [step = 300] loss: 0.738, acc: 0.828
******** [step = 350] loss: 0.744, acc: 0.827
******** [step = 400] loss: 0.748, acc: 0.827
******** [step = 450] loss: 0.752, acc: 0.826
******** [step = 500] loss: 0.758, acc: 0.826
******** [step = 550] loss: 0.762, acc: 0.825
******** [step = 600] loss: 0.766, acc: 0.825
******** [step = 650] loss: 0.769, acc: 0.824
******** [step = 700] loss: 0.771, acc: 0.824
******** [step = 750] loss: 0.775, acc: 0.824
******** [step = 800] loss: 0.779, acc: 0.823
******** [step = 850] loss: 0.782, acc: 0.823
EPOCH = 79 loss: 0.782, acc: 0.823, val_loss: 1.243, val_acc: 0.781

================================================================================2025-08_10 21:20:55
******** [step = 50] loss: 0.713, acc: 0.833
******** [step = 100] loss: 0.710, acc: 0.832
******** [step = 150] loss: 0.717, acc: 0.831
******** [step = 200] loss: 0.721, acc: 0.831
******** [step = 250] loss: 0.725, acc: 0.830
******** [step = 300] loss: 0.730, acc: 0.830
******** [step = 350] loss: 0.732, acc: 0.829
******** [step = 400] loss: 0.737, acc: 0.828
******** [step = 450] loss: 0.743, acc: 0.827
******** [step = 500] loss: 0.750, acc: 0.826
******** [step = 550] loss: 0.754, acc: 0.826
******** [step = 600] loss: 0.759, acc: 0.825
******** [step = 650] loss: 0.763, acc: 0.825
******** [step = 700] loss: 0.769, acc: 0.825
******** [step = 750] loss: 0.773, acc: 0.824
******** [step = 800] loss: 0.777, acc: 0.823
******** [step = 850] loss: 0.780, acc: 0.823
EPOCH = 80 loss: 0.780, acc: 0.823, val_loss: 1.240, val_acc: 0.782

================================================================================2025-08_10 21:22:14
finishing training...
Training complete in 110m 50s
    epoch  ...   val_acc
0     1.0  ...  0.415611
1     2.0  ...  0.472622
2     3.0  ...  0.514993
3     4.0  ...  0.553561
4     5.0  ...  0.589700
..    ...  ...       ...
75   76.0  ...  0.778649
76   77.0  ...  0.780924
77   78.0  ...  0.781610
78   79.0  ...  0.780891
79   80.0  ...  0.782068

[80 rows x 5 columns]
== Done ==
Sun Aug 10 09:22:42 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-10-2025 21:22:43
Job ID:        6784851
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=07:30:16,vmem=0,walltime=01:52:34,mem=35004K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-03-006-31-0
---------------------------------------
