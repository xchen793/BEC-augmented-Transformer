---------------------------------------
Begin Slurm Prolog: Aug-12-2025 17:06:28
Job ID:    6974795
User ID:   xchen920
Account:   gts-apm7
Job name:  channel_trans
Partition: gpu-v100
QOS:       inferno
---------------------------------------
== Job info ==
Tue Aug 12 05:06:29 PM EDT 2025
atl1-1-02-009-36-0.pace.gatech.edu
/usr/local/pace-apps/lmod/lmod/init/bash: line 200: conda: command not found
== GPU check ==
Tue Aug 12 17:06:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-16GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   38C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
== Launch ==
/storage/scratch1/4/xchen920/project
0.10.0
device= cuda:0
  0%|          | 0/135842 [00:00<?, ?it/s] 12%|█▏        | 16831/135842 [00:00<00:01, 108036.23it/s] 28%|██▊       | 38232/135842 [00:00<00:00, 158702.48it/s] 41%|████      | 55266/135842 [00:00<00:00, 120174.94it/s] 51%|█████     | 69378/135842 [00:00<00:00, 94391.05it/s]  66%|██████▌   | 89147/135842 [00:00<00:00, 119361.77it/s] 79%|███████▉  | 107136/135842 [00:01<00:00, 86920.32it/s] 92%|█████████▏| 125320/135842 [00:01<00:00, 105060.17it/s]100%|██████████| 135842/135842 [00:01<00:00, 109584.09it/s]
  0%|          | 0/108673 [00:00<?, ?it/s] 16%|█▌        | 17395/108673 [00:00<00:01, 46975.57it/s] 33%|███▎      | 35401/108673 [00:00<00:00, 84022.53it/s] 49%|████▉     | 53394/108673 [00:00<00:00, 111309.79it/s] 66%|██████▌   | 71584/108673 [00:00<00:00, 131707.49it/s] 81%|████████  | 87856/108673 [00:01<00:00, 70454.07it/s]  97%|█████████▋| 105865/108673 [00:01<00:00, 89400.73it/s]100%|██████████| 108673/108673 [00:01<00:00, 88548.56it/s]
  0%|          | 0/27169 [00:00<?, ?it/s] 65%|██████▌   | 17740/27169 [00:00<00:00, 177391.71it/s]100%|██████████| 27169/27169 [00:00<00:00, 179341.25it/s]
tensor([   3,   27,   42,    6, 1038,   13,   20,  212,   12, 9685,   70,  330,
         648,   35,   12,    6, 2681,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
torch.Size([52, 128]) torch.Size([52, 128])
++++++++++++++++ 213
len(train_dataloader): 850
torch.Size([128, 52]) torch.Size([128, 52])
tensor([   3,   59,   17,   51,   41,  906,   89,   16,  336,   17,   75,   41,
          52,   40,  975,   16, 5727,   45,   74,    4,    2,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
tensor([   3,   67,    6,   49,   83,  325,    6,   29,   59,   52,   21,   54,
          72, 1597,   17,    8,   55,    4,    2,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1]) torch.int64
*************************** start training...

================================================================================2025-08_12 17:07:32
******** [step = 50] loss: 9.372, acc: 0.005
******** [step = 100] loss: 8.939, acc: 0.086
******** [step = 150] loss: 8.568, acc: 0.130
******** [step = 200] loss: 8.225, acc: 0.160
******** [step = 250] loss: 7.910, acc: 0.178
******** [step = 300] loss: 7.608, acc: 0.190
******** [step = 350] loss: 7.314, acc: 0.201
******** [step = 400] loss: 7.045, acc: 0.210
******** [step = 450] loss: 6.803, acc: 0.219
******** [step = 500] loss: 6.589, acc: 0.228
******** [step = 550] loss: 6.400, acc: 0.237
******** [step = 600] loss: 6.231, acc: 0.245
******** [step = 650] loss: 6.079, acc: 0.253
******** [step = 700] loss: 5.944, acc: 0.260
******** [step = 750] loss: 5.821, acc: 0.266
******** [step = 800] loss: 5.708, acc: 0.272
******** [step = 850] loss: 5.605, acc: 0.277
EPOCH = 1 loss: 5.605, acc: 0.277, val_loss: 3.817, val_acc: 0.380

================================================================================2025-08_12 17:09:39
******** [step = 50] loss: 3.899, acc: 0.364
******** [step = 100] loss: 3.855, acc: 0.367
******** [step = 150] loss: 3.828, acc: 0.370
******** [step = 200] loss: 3.809, acc: 0.372
******** [step = 250] loss: 3.789, acc: 0.373
******** [step = 300] loss: 3.768, acc: 0.375
******** [step = 350] loss: 3.751, acc: 0.377
******** [step = 400] loss: 3.738, acc: 0.378
******** [step = 450] loss: 3.722, acc: 0.379
******** [step = 500] loss: 3.705, acc: 0.380
******** [step = 550] loss: 3.690, acc: 0.382
******** [step = 600] loss: 3.676, acc: 0.382
******** [step = 650] loss: 3.664, acc: 0.383
******** [step = 700] loss: 3.652, acc: 0.384
******** [step = 750] loss: 3.641, acc: 0.385
******** [step = 800] loss: 3.629, acc: 0.386
******** [step = 850] loss: 3.617, acc: 0.387
EPOCH = 2 loss: 3.617, acc: 0.387, val_loss: 3.363, val_acc: 0.412

================================================================================2025-08_12 17:11:41
******** [step = 50] loss: 3.392, acc: 0.403
******** [step = 100] loss: 3.373, acc: 0.405
******** [step = 150] loss: 3.366, acc: 0.405
******** [step = 200] loss: 3.356, acc: 0.407
******** [step = 250] loss: 3.356, acc: 0.407
******** [step = 300] loss: 3.348, acc: 0.408
******** [step = 350] loss: 3.338, acc: 0.409
******** [step = 400] loss: 3.331, acc: 0.410
******** [step = 450] loss: 3.327, acc: 0.411
******** [step = 500] loss: 3.322, acc: 0.411
******** [step = 550] loss: 3.317, acc: 0.412
******** [step = 600] loss: 3.313, acc: 0.413
******** [step = 650] loss: 3.310, acc: 0.413
******** [step = 700] loss: 3.306, acc: 0.414
******** [step = 750] loss: 3.301, acc: 0.414
******** [step = 800] loss: 3.297, acc: 0.415
******** [step = 850] loss: 3.290, acc: 0.416
EPOCH = 3 loss: 3.290, acc: 0.416, val_loss: 3.158, val_acc: 0.436

================================================================================2025-08_12 17:13:45
******** [step = 50] loss: 3.163, acc: 0.423
******** [step = 100] loss: 3.151, acc: 0.427
******** [step = 150] loss: 3.143, acc: 0.428
******** [step = 200] loss: 3.136, acc: 0.429
******** [step = 250] loss: 3.131, acc: 0.430
******** [step = 300] loss: 3.127, acc: 0.430
******** [step = 350] loss: 3.127, acc: 0.430
******** [step = 400] loss: 3.123, acc: 0.431
******** [step = 450] loss: 3.122, acc: 0.432
******** [step = 500] loss: 3.120, acc: 0.432
******** [step = 550] loss: 3.118, acc: 0.432
******** [step = 600] loss: 3.115, acc: 0.433
******** [step = 650] loss: 3.112, acc: 0.433
******** [step = 700] loss: 3.110, acc: 0.434
******** [step = 750] loss: 3.108, acc: 0.434
******** [step = 800] loss: 3.106, acc: 0.435
******** [step = 850] loss: 3.107, acc: 0.435
EPOCH = 4 loss: 3.107, acc: 0.435, val_loss: 3.048, val_acc: 0.445

================================================================================2025-08_12 17:15:50
******** [step = 50] loss: 3.041, acc: 0.434
******** [step = 100] loss: 3.016, acc: 0.439
******** [step = 150] loss: 3.005, acc: 0.441
******** [step = 200] loss: 2.992, acc: 0.443
******** [step = 250] loss: 2.987, acc: 0.444
******** [step = 300] loss: 2.987, acc: 0.444
******** [step = 350] loss: 2.988, acc: 0.445
******** [step = 400] loss: 2.986, acc: 0.445
******** [step = 450] loss: 2.986, acc: 0.446
******** [step = 500] loss: 2.986, acc: 0.446
******** [step = 550] loss: 2.987, acc: 0.446
******** [step = 600] loss: 2.990, acc: 0.446
******** [step = 650] loss: 2.997, acc: 0.445
******** [step = 700] loss: 2.999, acc: 0.445
******** [step = 750] loss: 2.998, acc: 0.446
******** [step = 800] loss: 3.000, acc: 0.446
******** [step = 850] loss: 2.999, acc: 0.446
EPOCH = 5 loss: 2.999, acc: 0.446, val_loss: 2.917, val_acc: 0.467

================================================================================2025-08_12 17:17:51
******** [step = 50] loss: 2.925, acc: 0.449
******** [step = 100] loss: 2.921, acc: 0.451
******** [step = 150] loss: 2.913, acc: 0.452
******** [step = 200] loss: 2.912, acc: 0.453
******** [step = 250] loss: 2.910, acc: 0.453
******** [step = 300] loss: 2.906, acc: 0.454
******** [step = 350] loss: 2.905, acc: 0.455
******** [step = 400] loss: 2.904, acc: 0.455
******** [step = 450] loss: 2.906, acc: 0.456
******** [step = 500] loss: 2.907, acc: 0.456
******** [step = 550] loss: 2.906, acc: 0.456
******** [step = 600] loss: 2.906, acc: 0.456
******** [step = 650] loss: 2.904, acc: 0.456
******** [step = 700] loss: 2.901, acc: 0.457
******** [step = 750] loss: 2.899, acc: 0.457
******** [step = 800] loss: 2.898, acc: 0.458
******** [step = 850] loss: 2.896, acc: 0.458
EPOCH = 6 loss: 2.896, acc: 0.458, val_loss: 2.822, val_acc: 0.478

================================================================================2025-08_12 17:19:53
******** [step = 50] loss: 2.785, acc: 0.464
******** [step = 100] loss: 2.792, acc: 0.465
******** [step = 150] loss: 2.804, acc: 0.466
******** [step = 200] loss: 2.804, acc: 0.465
******** [step = 250] loss: 2.804, acc: 0.466
******** [step = 300] loss: 2.802, acc: 0.466
******** [step = 350] loss: 2.799, acc: 0.466
******** [step = 400] loss: 2.799, acc: 0.466
******** [step = 450] loss: 2.804, acc: 0.466
******** [step = 500] loss: 2.805, acc: 0.466
******** [step = 550] loss: 2.807, acc: 0.466
******** [step = 600] loss: 2.806, acc: 0.467
******** [step = 650] loss: 2.806, acc: 0.467
******** [step = 700] loss: 2.804, acc: 0.467
******** [step = 750] loss: 2.804, acc: 0.468
******** [step = 800] loss: 2.800, acc: 0.468
******** [step = 850] loss: 2.797, acc: 0.469
EPOCH = 7 loss: 2.797, acc: 0.469, val_loss: 2.749, val_acc: 0.490

================================================================================2025-08_12 17:22:02
******** [step = 50] loss: 2.674, acc: 0.480
******** [step = 100] loss: 2.659, acc: 0.482
******** [step = 150] loss: 2.664, acc: 0.482
******** [step = 200] loss: 2.671, acc: 0.481
******** [step = 250] loss: 2.679, acc: 0.480
******** [step = 300] loss: 2.683, acc: 0.480
******** [step = 350] loss: 2.696, acc: 0.479
******** [step = 400] loss: 2.696, acc: 0.479
******** [step = 450] loss: 2.696, acc: 0.480
******** [step = 500] loss: 2.696, acc: 0.480
******** [step = 550] loss: 2.696, acc: 0.481
******** [step = 600] loss: 2.697, acc: 0.481
******** [step = 650] loss: 2.697, acc: 0.481
******** [step = 700] loss: 2.698, acc: 0.481
******** [step = 750] loss: 2.697, acc: 0.481
******** [step = 800] loss: 2.697, acc: 0.481
******** [step = 850] loss: 2.696, acc: 0.482
EPOCH = 8 loss: 2.696, acc: 0.482, val_loss: 2.699, val_acc: 0.497

================================================================================2025-08_12 17:24:05
******** [step = 50] loss: 2.585, acc: 0.492
******** [step = 100] loss: 2.583, acc: 0.492
******** [step = 150] loss: 2.589, acc: 0.492
******** [step = 200] loss: 2.599, acc: 0.492
******** [step = 250] loss: 2.615, acc: 0.490
******** [step = 300] loss: 2.623, acc: 0.488
******** [step = 350] loss: 2.626, acc: 0.488
******** [step = 400] loss: 2.628, acc: 0.488
******** [step = 450] loss: 2.631, acc: 0.488
******** [step = 500] loss: 2.631, acc: 0.489
******** [step = 550] loss: 2.636, acc: 0.488
******** [step = 600] loss: 2.638, acc: 0.488
******** [step = 650] loss: 2.640, acc: 0.488
******** [step = 700] loss: 2.642, acc: 0.488
******** [step = 750] loss: 2.644, acc: 0.488
******** [step = 800] loss: 2.644, acc: 0.488
******** [step = 850] loss: 2.643, acc: 0.488
EPOCH = 9 loss: 2.643, acc: 0.488, val_loss: 2.693, val_acc: 0.497

================================================================================2025-08_12 17:26:07
******** [step = 50] loss: 2.606, acc: 0.490
******** [step = 100] loss: 2.590, acc: 0.492
******** [step = 150] loss: 2.597, acc: 0.491
******** [step = 200] loss: 2.596, acc: 0.491
******** [step = 250] loss: 2.590, acc: 0.491
******** [step = 300] loss: 2.584, acc: 0.492
******** [step = 350] loss: 2.585, acc: 0.492
******** [step = 400] loss: 2.587, acc: 0.492
******** [step = 450] loss: 2.589, acc: 0.492
******** [step = 500] loss: 2.590, acc: 0.492
******** [step = 550] loss: 2.590, acc: 0.492
******** [step = 600] loss: 2.590, acc: 0.493
******** [step = 650] loss: 2.592, acc: 0.493
******** [step = 700] loss: 2.593, acc: 0.493
******** [step = 750] loss: 2.595, acc: 0.493
******** [step = 800] loss: 2.596, acc: 0.493
******** [step = 850] loss: 2.598, acc: 0.493
EPOCH = 10 loss: 2.598, acc: 0.493, val_loss: 2.638, val_acc: 0.507

================================================================================2025-08_12 17:28:12
******** [step = 50] loss: 2.506, acc: 0.500
******** [step = 100] loss: 2.500, acc: 0.502
******** [step = 150] loss: 2.495, acc: 0.503
******** [step = 200] loss: 2.505, acc: 0.502
******** [step = 250] loss: 2.507, acc: 0.503
******** [step = 300] loss: 2.511, acc: 0.502
******** [step = 350] loss: 2.513, acc: 0.502
******** [step = 400] loss: 2.515, acc: 0.502
******** [step = 450] loss: 2.520, acc: 0.502
******** [step = 500] loss: 2.525, acc: 0.502
******** [step = 550] loss: 2.530, acc: 0.501
******** [step = 600] loss: 2.534, acc: 0.501
******** [step = 650] loss: 2.536, acc: 0.501
******** [step = 700] loss: 2.536, acc: 0.501
******** [step = 750] loss: 2.537, acc: 0.501
******** [step = 800] loss: 2.539, acc: 0.501
******** [step = 850] loss: 2.542, acc: 0.501
EPOCH = 11 loss: 2.542, acc: 0.501, val_loss: 2.634, val_acc: 0.509

================================================================================2025-08_12 17:30:13
******** [step = 50] loss: 2.461, acc: 0.507
******** [step = 100] loss: 2.455, acc: 0.509
******** [step = 150] loss: 2.452, acc: 0.510
******** [step = 200] loss: 2.448, acc: 0.510
******** [step = 250] loss: 2.455, acc: 0.510
******** [step = 300] loss: 2.455, acc: 0.510
******** [step = 350] loss: 2.456, acc: 0.510
******** [step = 400] loss: 2.456, acc: 0.510
******** [step = 450] loss: 2.459, acc: 0.510
******** [step = 500] loss: 2.462, acc: 0.510
******** [step = 550] loss: 2.467, acc: 0.510
******** [step = 600] loss: 2.471, acc: 0.510
******** [step = 650] loss: 2.475, acc: 0.509
******** [step = 700] loss: 2.476, acc: 0.509
******** [step = 750] loss: 2.479, acc: 0.509
******** [step = 800] loss: 2.481, acc: 0.509
******** [step = 850] loss: 2.485, acc: 0.509
EPOCH = 12 loss: 2.485, acc: 0.509, val_loss: 2.607, val_acc: 0.514

================================================================================2025-08_12 17:32:28
******** [step = 50] loss: 2.391, acc: 0.518
******** [step = 100] loss: 2.397, acc: 0.519
******** [step = 150] loss: 2.415, acc: 0.517
******** [step = 200] loss: 2.414, acc: 0.516
******** [step = 250] loss: 2.417, acc: 0.517
******** [step = 300] loss: 2.425, acc: 0.515
******** [step = 350] loss: 2.429, acc: 0.515
******** [step = 400] loss: 2.432, acc: 0.515
******** [step = 450] loss: 2.433, acc: 0.515
******** [step = 500] loss: 2.437, acc: 0.515
******** [step = 550] loss: 2.441, acc: 0.514
******** [step = 600] loss: 2.445, acc: 0.514
******** [step = 650] loss: 2.450, acc: 0.513
******** [step = 700] loss: 2.450, acc: 0.513
******** [step = 750] loss: 2.454, acc: 0.513
******** [step = 800] loss: 2.458, acc: 0.512
******** [step = 850] loss: 2.463, acc: 0.512
EPOCH = 13 loss: 2.463, acc: 0.512, val_loss: 2.609, val_acc: 0.514

================================================================================2025-08_12 17:34:40
******** [step = 50] loss: 2.409, acc: 0.513
******** [step = 100] loss: 2.401, acc: 0.515
******** [step = 150] loss: 2.397, acc: 0.516
******** [step = 200] loss: 2.400, acc: 0.517
******** [step = 250] loss: 2.397, acc: 0.518
******** [step = 300] loss: 2.403, acc: 0.517
******** [step = 350] loss: 2.407, acc: 0.517
******** [step = 400] loss: 2.408, acc: 0.517
******** [step = 450] loss: 2.413, acc: 0.517
******** [step = 500] loss: 2.416, acc: 0.517
******** [step = 550] loss: 2.420, acc: 0.517
******** [step = 600] loss: 2.421, acc: 0.516
******** [step = 650] loss: 2.425, acc: 0.517
******** [step = 700] loss: 2.427, acc: 0.517
******** [step = 750] loss: 2.427, acc: 0.517
******** [step = 800] loss: 2.431, acc: 0.517
******** [step = 850] loss: 2.432, acc: 0.517
EPOCH = 14 loss: 2.432, acc: 0.517, val_loss: 2.566, val_acc: 0.521

================================================================================2025-08_12 17:36:41
******** [step = 50] loss: 2.374, acc: 0.521
******** [step = 100] loss: 2.364, acc: 0.522
******** [step = 150] loss: 2.359, acc: 0.523
******** [step = 200] loss: 2.359, acc: 0.523
******** [step = 250] loss: 2.361, acc: 0.523
******** [step = 300] loss: 2.362, acc: 0.523
******** [step = 350] loss: 2.364, acc: 0.523
******** [step = 400] loss: 2.364, acc: 0.524
******** [step = 450] loss: 2.369, acc: 0.523
******** [step = 500] loss: 2.373, acc: 0.523
******** [step = 550] loss: 2.378, acc: 0.522
******** [step = 600] loss: 2.380, acc: 0.522
******** [step = 650] loss: 2.384, acc: 0.522
******** [step = 700] loss: 2.390, acc: 0.522
******** [step = 750] loss: 2.393, acc: 0.521
******** [step = 800] loss: 2.399, acc: 0.521
******** [step = 850] loss: 2.404, acc: 0.520
EPOCH = 15 loss: 2.404, acc: 0.520, val_loss: 2.619, val_acc: 0.513

================================================================================2025-08_12 17:38:43
******** [step = 50] loss: 2.362, acc: 0.523
******** [step = 100] loss: 2.346, acc: 0.524
******** [step = 150] loss: 2.347, acc: 0.524
******** [step = 200] loss: 2.356, acc: 0.523
******** [step = 250] loss: 2.361, acc: 0.522
******** [step = 300] loss: 2.359, acc: 0.523
******** [step = 350] loss: 2.365, acc: 0.522
******** [step = 400] loss: 2.365, acc: 0.523
******** [step = 450] loss: 2.367, acc: 0.523
******** [step = 500] loss: 2.370, acc: 0.522
******** [step = 550] loss: 2.372, acc: 0.522
******** [step = 600] loss: 2.376, acc: 0.522
******** [step = 650] loss: 2.380, acc: 0.522
******** [step = 700] loss: 2.385, acc: 0.521
******** [step = 750] loss: 2.387, acc: 0.521
******** [step = 800] loss: 2.391, acc: 0.521
******** [step = 850] loss: 2.393, acc: 0.521
EPOCH = 16 loss: 2.393, acc: 0.521, val_loss: 2.556, val_acc: 0.524

================================================================================2025-08_12 17:40:44
******** [step = 50] loss: 2.321, acc: 0.533
******** [step = 100] loss: 2.311, acc: 0.533
******** [step = 150] loss: 2.313, acc: 0.531
******** [step = 200] loss: 2.314, acc: 0.531
******** [step = 250] loss: 2.317, acc: 0.530
******** [step = 300] loss: 2.320, acc: 0.530
******** [step = 350] loss: 2.328, acc: 0.529
******** [step = 400] loss: 2.330, acc: 0.529
******** [step = 450] loss: 2.334, acc: 0.529
******** [step = 500] loss: 2.341, acc: 0.528
******** [step = 550] loss: 2.345, acc: 0.527
******** [step = 600] loss: 2.347, acc: 0.527
******** [step = 650] loss: 2.350, acc: 0.527
******** [step = 700] loss: 2.351, acc: 0.527
******** [step = 750] loss: 2.353, acc: 0.527
******** [step = 800] loss: 2.356, acc: 0.526
******** [step = 850] loss: 2.361, acc: 0.526
EPOCH = 17 loss: 2.361, acc: 0.526, val_loss: 2.535, val_acc: 0.528

================================================================================2025-08_12 17:42:45
******** [step = 50] loss: 2.292, acc: 0.533
******** [step = 100] loss: 2.288, acc: 0.533
******** [step = 150] loss: 2.282, acc: 0.534
******** [step = 200] loss: 2.291, acc: 0.533
******** [step = 250] loss: 2.296, acc: 0.533
******** [step = 300] loss: 2.299, acc: 0.533
******** [step = 350] loss: 2.301, acc: 0.533
******** [step = 400] loss: 2.303, acc: 0.533
******** [step = 450] loss: 2.305, acc: 0.533
******** [step = 500] loss: 2.308, acc: 0.533
******** [step = 550] loss: 2.309, acc: 0.533
******** [step = 600] loss: 2.311, acc: 0.533
******** [step = 650] loss: 2.313, acc: 0.532
******** [step = 700] loss: 2.318, acc: 0.532
******** [step = 750] loss: 2.321, acc: 0.532
******** [step = 800] loss: 2.324, acc: 0.532
******** [step = 850] loss: 2.328, acc: 0.531
EPOCH = 18 loss: 2.328, acc: 0.531, val_loss: 2.529, val_acc: 0.531

================================================================================2025-08_12 17:44:46
******** [step = 50] loss: 2.258, acc: 0.538
******** [step = 100] loss: 2.251, acc: 0.539
******** [step = 150] loss: 2.250, acc: 0.540
******** [step = 200] loss: 2.254, acc: 0.539
******** [step = 250] loss: 2.262, acc: 0.538
******** [step = 300] loss: 2.265, acc: 0.538
******** [step = 350] loss: 2.269, acc: 0.538
******** [step = 400] loss: 2.273, acc: 0.537
******** [step = 450] loss: 2.277, acc: 0.537
******** [step = 500] loss: 2.281, acc: 0.536
******** [step = 550] loss: 2.285, acc: 0.536
******** [step = 600] loss: 2.288, acc: 0.536
******** [step = 650] loss: 2.293, acc: 0.536
******** [step = 700] loss: 2.297, acc: 0.535
******** [step = 750] loss: 2.297, acc: 0.535
******** [step = 800] loss: 2.300, acc: 0.535
******** [step = 850] loss: 2.303, acc: 0.535
EPOCH = 19 loss: 2.303, acc: 0.535, val_loss: 2.512, val_acc: 0.533

================================================================================2025-08_12 17:46:51
******** [step = 50] loss: 2.199, acc: 0.545
******** [step = 100] loss: 2.219, acc: 0.542
******** [step = 150] loss: 2.232, acc: 0.540
******** [step = 200] loss: 2.237, acc: 0.540
******** [step = 250] loss: 2.239, acc: 0.541
******** [step = 300] loss: 2.243, acc: 0.541
******** [step = 350] loss: 2.246, acc: 0.541
******** [step = 400] loss: 2.250, acc: 0.540
******** [step = 450] loss: 2.254, acc: 0.540
******** [step = 500] loss: 2.259, acc: 0.539
******** [step = 550] loss: 2.261, acc: 0.539
******** [step = 600] loss: 2.265, acc: 0.539
******** [step = 650] loss: 2.269, acc: 0.539
******** [step = 700] loss: 2.272, acc: 0.539
******** [step = 750] loss: 2.275, acc: 0.538
******** [step = 800] loss: 2.279, acc: 0.538
******** [step = 850] loss: 2.281, acc: 0.538
EPOCH = 20 loss: 2.281, acc: 0.538, val_loss: 2.507, val_acc: 0.532

================================================================================2025-08_12 17:48:53
******** [step = 50] loss: 2.186, acc: 0.545
******** [step = 100] loss: 2.190, acc: 0.547
******** [step = 150] loss: 2.197, acc: 0.546
******** [step = 200] loss: 2.207, acc: 0.546
******** [step = 250] loss: 2.217, acc: 0.544
******** [step = 300] loss: 2.223, acc: 0.544
******** [step = 350] loss: 2.228, acc: 0.543
******** [step = 400] loss: 2.232, acc: 0.543
******** [step = 450] loss: 2.239, acc: 0.542
******** [step = 500] loss: 2.243, acc: 0.541
******** [step = 550] loss: 2.247, acc: 0.541
******** [step = 600] loss: 2.250, acc: 0.541
******** [step = 650] loss: 2.255, acc: 0.540
******** [step = 700] loss: 2.258, acc: 0.540
******** [step = 750] loss: 2.260, acc: 0.540
******** [step = 800] loss: 2.264, acc: 0.540
******** [step = 850] loss: 2.267, acc: 0.539
EPOCH = 21 loss: 2.267, acc: 0.539, val_loss: 2.515, val_acc: 0.534

================================================================================2025-08_12 17:50:54
******** [step = 50] loss: 2.221, acc: 0.540
******** [step = 100] loss: 2.219, acc: 0.542
******** [step = 150] loss: 2.214, acc: 0.544
******** [step = 200] loss: 2.216, acc: 0.544
******** [step = 250] loss: 2.213, acc: 0.544
******** [step = 300] loss: 2.216, acc: 0.544
******** [step = 350] loss: 2.219, acc: 0.544
******** [step = 400] loss: 2.223, acc: 0.544
******** [step = 450] loss: 2.226, acc: 0.544
******** [step = 500] loss: 2.228, acc: 0.544
******** [step = 550] loss: 2.230, acc: 0.544
******** [step = 600] loss: 2.231, acc: 0.544
******** [step = 650] loss: 2.235, acc: 0.544
******** [step = 700] loss: 2.239, acc: 0.544
******** [step = 750] loss: 2.244, acc: 0.543
******** [step = 800] loss: 2.248, acc: 0.543
******** [step = 850] loss: 2.249, acc: 0.543
EPOCH = 22 loss: 2.249, acc: 0.543, val_loss: 2.496, val_acc: 0.537

================================================================================2025-08_12 17:52:54
******** [step = 50] loss: 2.181, acc: 0.548
******** [step = 100] loss: 2.177, acc: 0.550
******** [step = 150] loss: 2.177, acc: 0.550
******** [step = 200] loss: 2.180, acc: 0.549
******** [step = 250] loss: 2.183, acc: 0.550
******** [step = 300] loss: 2.187, acc: 0.549
******** [step = 350] loss: 2.191, acc: 0.549
******** [step = 400] loss: 2.193, acc: 0.549
******** [step = 450] loss: 2.196, acc: 0.549
******** [step = 500] loss: 2.200, acc: 0.548
******** [step = 550] loss: 2.209, acc: 0.547
******** [step = 600] loss: 2.214, acc: 0.547
******** [step = 650] loss: 2.217, acc: 0.546
******** [step = 700] loss: 2.220, acc: 0.546
******** [step = 750] loss: 2.222, acc: 0.546
******** [step = 800] loss: 2.225, acc: 0.546
******** [step = 850] loss: 2.228, acc: 0.546
EPOCH = 23 loss: 2.228, acc: 0.546, val_loss: 2.482, val_acc: 0.540

================================================================================2025-08_12 17:54:55
******** [step = 50] loss: 2.146, acc: 0.552
******** [step = 100] loss: 2.145, acc: 0.553
******** [step = 150] loss: 2.161, acc: 0.551
******** [step = 200] loss: 2.161, acc: 0.552
******** [step = 250] loss: 2.168, acc: 0.551
******** [step = 300] loss: 2.175, acc: 0.550
******** [step = 350] loss: 2.178, acc: 0.550
******** [step = 400] loss: 2.181, acc: 0.550
******** [step = 450] loss: 2.185, acc: 0.550
******** [step = 500] loss: 2.190, acc: 0.549
******** [step = 550] loss: 2.193, acc: 0.549
******** [step = 600] loss: 2.195, acc: 0.549
******** [step = 650] loss: 2.199, acc: 0.549
******** [step = 700] loss: 2.202, acc: 0.549
******** [step = 750] loss: 2.204, acc: 0.548
******** [step = 800] loss: 2.208, acc: 0.548
******** [step = 850] loss: 2.210, acc: 0.548
EPOCH = 24 loss: 2.210, acc: 0.548, val_loss: 2.488, val_acc: 0.539

================================================================================2025-08_12 17:56:56
******** [step = 50] loss: 2.139, acc: 0.553
******** [step = 100] loss: 2.148, acc: 0.554
******** [step = 150] loss: 2.151, acc: 0.553
******** [step = 200] loss: 2.155, acc: 0.553
******** [step = 250] loss: 2.161, acc: 0.552
******** [step = 300] loss: 2.165, acc: 0.552
******** [step = 350] loss: 2.173, acc: 0.551
******** [step = 400] loss: 2.179, acc: 0.550
******** [step = 450] loss: 2.184, acc: 0.550
******** [step = 500] loss: 2.187, acc: 0.549
******** [step = 550] loss: 2.190, acc: 0.549
******** [step = 600] loss: 2.197, acc: 0.548
******** [step = 650] loss: 2.202, acc: 0.548
******** [step = 700] loss: 2.208, acc: 0.547
******** [step = 750] loss: 2.211, acc: 0.547
******** [step = 800] loss: 2.212, acc: 0.547
******** [step = 850] loss: 2.215, acc: 0.547
EPOCH = 25 loss: 2.215, acc: 0.547, val_loss: 2.497, val_acc: 0.540

================================================================================2025-08_12 17:58:57
******** [step = 50] loss: 2.142, acc: 0.554
******** [step = 100] loss: 2.139, acc: 0.555
******** [step = 150] loss: 2.138, acc: 0.555
******** [step = 200] loss: 2.140, acc: 0.555
******** [step = 250] loss: 2.145, acc: 0.554
******** [step = 300] loss: 2.150, acc: 0.554
******** [step = 350] loss: 2.158, acc: 0.553
******** [step = 400] loss: 2.165, acc: 0.552
******** [step = 450] loss: 2.170, acc: 0.552
******** [step = 500] loss: 2.174, acc: 0.551
******** [step = 550] loss: 2.176, acc: 0.551
******** [step = 600] loss: 2.179, acc: 0.551
******** [step = 650] loss: 2.184, acc: 0.551
******** [step = 700] loss: 2.188, acc: 0.550
******** [step = 750] loss: 2.193, acc: 0.550
******** [step = 800] loss: 2.196, acc: 0.549
******** [step = 850] loss: 2.204, acc: 0.548
EPOCH = 26 loss: 2.204, acc: 0.548, val_loss: 2.503, val_acc: 0.538

================================================================================2025-08_12 18:00:59
******** [step = 50] loss: 2.161, acc: 0.551
******** [step = 100] loss: 2.142, acc: 0.554
******** [step = 150] loss: 2.144, acc: 0.554
******** [step = 200] loss: 2.146, acc: 0.554
******** [step = 250] loss: 2.152, acc: 0.553
******** [step = 300] loss: 2.153, acc: 0.553
******** [step = 350] loss: 2.155, acc: 0.553
******** [step = 400] loss: 2.155, acc: 0.554
******** [step = 450] loss: 2.159, acc: 0.553
******** [step = 500] loss: 2.162, acc: 0.553
******** [step = 550] loss: 2.166, acc: 0.553
******** [step = 600] loss: 2.169, acc: 0.552
******** [step = 650] loss: 2.175, acc: 0.552
******** [step = 700] loss: 2.178, acc: 0.551
******** [step = 750] loss: 2.180, acc: 0.551
******** [step = 800] loss: 2.184, acc: 0.551
******** [step = 850] loss: 2.187, acc: 0.550
EPOCH = 27 loss: 2.187, acc: 0.550, val_loss: 2.484, val_acc: 0.539

================================================================================2025-08_12 18:02:59
******** [step = 50] loss: 2.118, acc: 0.557
******** [step = 100] loss: 2.122, acc: 0.557
******** [step = 150] loss: 2.125, acc: 0.557
******** [step = 200] loss: 2.134, acc: 0.556
******** [step = 250] loss: 2.137, acc: 0.556
******** [step = 300] loss: 2.141, acc: 0.555
******** [step = 350] loss: 2.142, acc: 0.555
******** [step = 400] loss: 2.145, acc: 0.555
******** [step = 450] loss: 2.149, acc: 0.555
******** [step = 500] loss: 2.153, acc: 0.554
******** [step = 550] loss: 2.156, acc: 0.554
******** [step = 600] loss: 2.158, acc: 0.554
******** [step = 650] loss: 2.163, acc: 0.554
******** [step = 700] loss: 2.165, acc: 0.554
******** [step = 750] loss: 2.168, acc: 0.554
******** [step = 800] loss: 2.169, acc: 0.554
******** [step = 850] loss: 2.169, acc: 0.554
EPOCH = 28 loss: 2.169, acc: 0.554, val_loss: 2.461, val_acc: 0.546

================================================================================2025-08_12 18:05:06
******** [step = 50] loss: 2.082, acc: 0.562
******** [step = 100] loss: 2.091, acc: 0.561
******** [step = 150] loss: 2.091, acc: 0.562
******** [step = 200] loss: 2.098, acc: 0.562
******** [step = 250] loss: 2.107, acc: 0.561
******** [step = 300] loss: 2.116, acc: 0.560
******** [step = 350] loss: 2.128, acc: 0.558
******** [step = 400] loss: 2.136, acc: 0.557
******** [step = 450] loss: 2.142, acc: 0.556
******** [step = 500] loss: 2.147, acc: 0.556
******** [step = 550] loss: 2.155, acc: 0.555
******** [step = 600] loss: 2.159, acc: 0.555
******** [step = 650] loss: 2.164, acc: 0.554
******** [step = 700] loss: 2.167, acc: 0.554
******** [step = 750] loss: 2.170, acc: 0.554
******** [step = 800] loss: 2.173, acc: 0.553
******** [step = 850] loss: 2.175, acc: 0.553
EPOCH = 29 loss: 2.175, acc: 0.553, val_loss: 2.495, val_acc: 0.541

================================================================================2025-08_12 18:07:07
******** [step = 50] loss: 2.110, acc: 0.560
******** [step = 100] loss: 2.108, acc: 0.561
******** [step = 150] loss: 2.108, acc: 0.561
******** [step = 200] loss: 2.105, acc: 0.561
******** [step = 250] loss: 2.111, acc: 0.560
******** [step = 300] loss: 2.113, acc: 0.560
******** [step = 350] loss: 2.115, acc: 0.561
******** [step = 400] loss: 2.117, acc: 0.561
******** [step = 450] loss: 2.121, acc: 0.560
******** [step = 500] loss: 2.123, acc: 0.560
******** [step = 550] loss: 2.129, acc: 0.559
******** [step = 600] loss: 2.133, acc: 0.559
******** [step = 650] loss: 2.136, acc: 0.558
******** [step = 700] loss: 2.139, acc: 0.558
******** [step = 750] loss: 2.139, acc: 0.558
******** [step = 800] loss: 2.142, acc: 0.558
******** [step = 850] loss: 2.147, acc: 0.557
EPOCH = 30 loss: 2.147, acc: 0.557, val_loss: 2.462, val_acc: 0.546

================================================================================2025-08_12 18:09:07
******** [step = 50] loss: 2.071, acc: 0.564
******** [step = 100] loss: 2.073, acc: 0.565
******** [step = 150] loss: 2.079, acc: 0.564
******** [step = 200] loss: 2.082, acc: 0.564
******** [step = 250] loss: 2.084, acc: 0.565
******** [step = 300] loss: 2.086, acc: 0.564
******** [step = 350] loss: 2.093, acc: 0.564
******** [step = 400] loss: 2.100, acc: 0.563
******** [step = 450] loss: 2.103, acc: 0.563
******** [step = 500] loss: 2.108, acc: 0.562
******** [step = 550] loss: 2.111, acc: 0.562
******** [step = 600] loss: 2.116, acc: 0.561
******** [step = 650] loss: 2.121, acc: 0.561
******** [step = 700] loss: 2.122, acc: 0.561
******** [step = 750] loss: 2.125, acc: 0.561
******** [step = 800] loss: 2.128, acc: 0.560
******** [step = 850] loss: 2.132, acc: 0.560
EPOCH = 31 loss: 2.132, acc: 0.560, val_loss: 2.445, val_acc: 0.548

================================================================================2025-08_12 18:11:08
******** [step = 50] loss: 2.066, acc: 0.565
******** [step = 100] loss: 2.062, acc: 0.565
******** [step = 150] loss: 2.058, acc: 0.566
******** [step = 200] loss: 2.070, acc: 0.565
******** [step = 250] loss: 2.078, acc: 0.564
******** [step = 300] loss: 2.081, acc: 0.564
******** [step = 350] loss: 2.086, acc: 0.564
******** [step = 400] loss: 2.090, acc: 0.564
******** [step = 450] loss: 2.095, acc: 0.563
******** [step = 500] loss: 2.097, acc: 0.563
******** [step = 550] loss: 2.099, acc: 0.563
******** [step = 600] loss: 2.100, acc: 0.563
******** [step = 650] loss: 2.102, acc: 0.563
******** [step = 700] loss: 2.109, acc: 0.562
******** [step = 750] loss: 2.112, acc: 0.562
******** [step = 800] loss: 2.116, acc: 0.562
******** [step = 850] loss: 2.118, acc: 0.562
EPOCH = 32 loss: 2.118, acc: 0.562, val_loss: 2.444, val_acc: 0.548

================================================================================2025-08_12 18:13:08
******** [step = 50] loss: 2.061, acc: 0.569
******** [step = 100] loss: 2.050, acc: 0.568
******** [step = 150] loss: 2.054, acc: 0.568
******** [step = 200] loss: 2.053, acc: 0.568
******** [step = 250] loss: 2.062, acc: 0.568
******** [step = 300] loss: 2.062, acc: 0.568
******** [step = 350] loss: 2.067, acc: 0.567
******** [step = 400] loss: 2.070, acc: 0.567
******** [step = 450] loss: 2.077, acc: 0.566
******** [step = 500] loss: 2.081, acc: 0.566
******** [step = 550] loss: 2.086, acc: 0.565
******** [step = 600] loss: 2.090, acc: 0.565
******** [step = 650] loss: 2.093, acc: 0.565
******** [step = 700] loss: 2.095, acc: 0.565
******** [step = 750] loss: 2.099, acc: 0.564
******** [step = 800] loss: 2.103, acc: 0.564
******** [step = 850] loss: 2.108, acc: 0.563
EPOCH = 33 loss: 2.108, acc: 0.563, val_loss: 2.444, val_acc: 0.549

================================================================================2025-08_12 18:15:10
******** [step = 50] loss: 2.029, acc: 0.572
******** [step = 100] loss: 2.023, acc: 0.573
******** [step = 150] loss: 2.029, acc: 0.573
******** [step = 200] loss: 2.034, acc: 0.572
******** [step = 250] loss: 2.041, acc: 0.571
******** [step = 300] loss: 2.046, acc: 0.570
******** [step = 350] loss: 2.050, acc: 0.570
******** [step = 400] loss: 2.056, acc: 0.569
******** [step = 450] loss: 2.061, acc: 0.569
******** [step = 500] loss: 2.066, acc: 0.568
******** [step = 550] loss: 2.071, acc: 0.568
******** [step = 600] loss: 2.076, acc: 0.567
******** [step = 650] loss: 2.078, acc: 0.567
******** [step = 700] loss: 2.081, acc: 0.567
******** [step = 750] loss: 2.085, acc: 0.567
******** [step = 800] loss: 2.087, acc: 0.566
******** [step = 850] loss: 2.091, acc: 0.566
EPOCH = 34 loss: 2.091, acc: 0.566, val_loss: 2.435, val_acc: 0.551

================================================================================2025-08_12 18:17:16
******** [step = 50] loss: 2.012, acc: 0.573
******** [step = 100] loss: 2.013, acc: 0.573
******** [step = 150] loss: 2.021, acc: 0.572
******** [step = 200] loss: 2.030, acc: 0.571
******** [step = 250] loss: 2.030, acc: 0.571
******** [step = 300] loss: 2.035, acc: 0.571
******** [step = 350] loss: 2.041, acc: 0.570
******** [step = 400] loss: 2.047, acc: 0.570
******** [step = 450] loss: 2.051, acc: 0.570
******** [step = 500] loss: 2.056, acc: 0.570
******** [step = 550] loss: 2.062, acc: 0.569
******** [step = 600] loss: 2.065, acc: 0.569
******** [step = 650] loss: 2.071, acc: 0.568
******** [step = 700] loss: 2.075, acc: 0.567
******** [step = 750] loss: 2.079, acc: 0.567
******** [step = 800] loss: 2.084, acc: 0.566
******** [step = 850] loss: 2.086, acc: 0.567
EPOCH = 35 loss: 2.086, acc: 0.567, val_loss: 2.439, val_acc: 0.550

================================================================================2025-08_12 18:19:25
******** [step = 50] loss: 2.016, acc: 0.573
******** [step = 100] loss: 2.008, acc: 0.575
******** [step = 150] loss: 2.018, acc: 0.573
******** [step = 200] loss: 2.028, acc: 0.572
******** [step = 250] loss: 2.040, acc: 0.570
******** [step = 300] loss: 2.049, acc: 0.569
******** [step = 350] loss: 2.053, acc: 0.569
******** [step = 400] loss: 2.057, acc: 0.568
******** [step = 450] loss: 2.065, acc: 0.568
******** [step = 500] loss: 2.068, acc: 0.567
******** [step = 550] loss: 2.072, acc: 0.567
******** [step = 600] loss: 2.075, acc: 0.567
******** [step = 650] loss: 2.078, acc: 0.567
******** [step = 700] loss: 2.081, acc: 0.566
******** [step = 750] loss: 2.085, acc: 0.566
******** [step = 800] loss: 2.088, acc: 0.566
******** [step = 850] loss: 2.093, acc: 0.565
EPOCH = 36 loss: 2.093, acc: 0.565, val_loss: 2.441, val_acc: 0.550

================================================================================2025-08_12 18:21:28
******** [step = 50] loss: 2.022, acc: 0.573
******** [step = 100] loss: 2.025, acc: 0.571
******** [step = 150] loss: 2.023, acc: 0.572
******** [step = 200] loss: 2.032, acc: 0.572
******** [step = 250] loss: 2.037, acc: 0.571
******** [step = 300] loss: 2.040, acc: 0.571
******** [step = 350] loss: 2.046, acc: 0.570
******** [step = 400] loss: 2.054, acc: 0.569
******** [step = 450] loss: 2.057, acc: 0.569
******** [step = 500] loss: 2.064, acc: 0.568
******** [step = 550] loss: 2.068, acc: 0.568
******** [step = 600] loss: 2.071, acc: 0.568
******** [step = 650] loss: 2.074, acc: 0.568
******** [step = 700] loss: 2.077, acc: 0.567
******** [step = 750] loss: 2.080, acc: 0.567
******** [step = 800] loss: 2.083, acc: 0.567
******** [step = 850] loss: 2.089, acc: 0.566
EPOCH = 37 loss: 2.089, acc: 0.566, val_loss: 2.445, val_acc: 0.549

================================================================================2025-08_12 18:23:29
******** [step = 50] loss: 1.993, acc: 0.576
******** [step = 100] loss: 2.008, acc: 0.575
******** [step = 150] loss: 2.012, acc: 0.574
******** [step = 200] loss: 2.020, acc: 0.574
******** [step = 250] loss: 2.024, acc: 0.573
******** [step = 300] loss: 2.036, acc: 0.571
******** [step = 350] loss: 2.043, acc: 0.570
******** [step = 400] loss: 2.049, acc: 0.570
******** [step = 450] loss: 2.055, acc: 0.569
******** [step = 500] loss: 2.060, acc: 0.569
******** [step = 550] loss: 2.063, acc: 0.569
******** [step = 600] loss: 2.067, acc: 0.568
******** [step = 650] loss: 2.071, acc: 0.568
******** [step = 700] loss: 2.075, acc: 0.568
******** [step = 750] loss: 2.080, acc: 0.567
******** [step = 800] loss: 2.083, acc: 0.567
******** [step = 850] loss: 2.085, acc: 0.567
EPOCH = 38 loss: 2.085, acc: 0.567, val_loss: 2.439, val_acc: 0.550

================================================================================2025-08_12 18:25:29
******** [step = 50] loss: 2.001, acc: 0.575
******** [step = 100] loss: 2.005, acc: 0.575
******** [step = 150] loss: 2.010, acc: 0.574
******** [step = 200] loss: 2.019, acc: 0.573
******** [step = 250] loss: 2.024, acc: 0.572
******** [step = 300] loss: 2.028, acc: 0.572
******** [step = 350] loss: 2.033, acc: 0.572
******** [step = 400] loss: 2.037, acc: 0.571
******** [step = 450] loss: 2.043, acc: 0.571
******** [step = 500] loss: 2.046, acc: 0.571
******** [step = 550] loss: 2.049, acc: 0.571
******** [step = 600] loss: 2.053, acc: 0.570
******** [step = 650] loss: 2.057, acc: 0.570
******** [step = 700] loss: 2.060, acc: 0.570
******** [step = 750] loss: 2.064, acc: 0.569
******** [step = 800] loss: 2.066, acc: 0.569
******** [step = 850] loss: 2.070, acc: 0.569
EPOCH = 39 loss: 2.070, acc: 0.569, val_loss: 2.429, val_acc: 0.553

================================================================================2025-08_12 18:27:30
******** [step = 50] loss: 1.969, acc: 0.580
******** [step = 100] loss: 1.989, acc: 0.577
******** [step = 150] loss: 1.997, acc: 0.576
******** [step = 200] loss: 2.004, acc: 0.575
******** [step = 250] loss: 2.012, acc: 0.575
******** [step = 300] loss: 2.018, acc: 0.574
******** [step = 350] loss: 2.023, acc: 0.573
******** [step = 400] loss: 2.029, acc: 0.573
******** [step = 450] loss: 2.031, acc: 0.572
******** [step = 500] loss: 2.036, acc: 0.572
******** [step = 550] loss: 2.042, acc: 0.572
******** [step = 600] loss: 2.047, acc: 0.571
******** [step = 650] loss: 2.051, acc: 0.571
******** [step = 700] loss: 2.056, acc: 0.571
******** [step = 750] loss: 2.059, acc: 0.570
******** [step = 800] loss: 2.062, acc: 0.570
******** [step = 850] loss: 2.066, acc: 0.570
EPOCH = 40 loss: 2.066, acc: 0.570, val_loss: 2.436, val_acc: 0.551

================================================================================2025-08_12 18:29:35
******** [step = 50] loss: 2.008, acc: 0.572
******** [step = 100] loss: 1.995, acc: 0.575
******** [step = 150] loss: 2.002, acc: 0.575
******** [step = 200] loss: 2.006, acc: 0.574
******** [step = 250] loss: 2.007, acc: 0.574
******** [step = 300] loss: 2.011, acc: 0.574
******** [step = 350] loss: 2.013, acc: 0.574
******** [step = 400] loss: 2.018, acc: 0.574
******** [step = 450] loss: 2.022, acc: 0.574
******** [step = 500] loss: 2.027, acc: 0.573
******** [step = 550] loss: 2.033, acc: 0.573
******** [step = 600] loss: 2.034, acc: 0.573
******** [step = 650] loss: 2.039, acc: 0.573
******** [step = 700] loss: 2.043, acc: 0.572
******** [step = 750] loss: 2.047, acc: 0.572
******** [step = 800] loss: 2.051, acc: 0.572
******** [step = 850] loss: 2.054, acc: 0.571
EPOCH = 41 loss: 2.054, acc: 0.571, val_loss: 2.427, val_acc: 0.552

================================================================================2025-08_12 18:31:36
******** [step = 50] loss: 1.971, acc: 0.577
******** [step = 100] loss: 1.981, acc: 0.577
******** [step = 150] loss: 1.988, acc: 0.577
******** [step = 200] loss: 2.001, acc: 0.576
******** [step = 250] loss: 2.006, acc: 0.576
******** [step = 300] loss: 2.014, acc: 0.575
******** [step = 350] loss: 2.023, acc: 0.574
******** [step = 400] loss: 2.025, acc: 0.573
******** [step = 450] loss: 2.029, acc: 0.573
******** [step = 500] loss: 2.033, acc: 0.573
******** [step = 550] loss: 2.037, acc: 0.572
******** [step = 600] loss: 2.041, acc: 0.572
******** [step = 650] loss: 2.044, acc: 0.572
******** [step = 700] loss: 2.048, acc: 0.571
******** [step = 750] loss: 2.050, acc: 0.571
******** [step = 800] loss: 2.054, acc: 0.571
******** [step = 850] loss: 2.056, acc: 0.571
EPOCH = 42 loss: 2.056, acc: 0.571, val_loss: 2.423, val_acc: 0.553

================================================================================2025-08_12 18:33:36
******** [step = 50] loss: 1.967, acc: 0.580
******** [step = 100] loss: 1.982, acc: 0.578
******** [step = 150] loss: 1.992, acc: 0.578
******** [step = 200] loss: 1.995, acc: 0.578
******** [step = 250] loss: 1.998, acc: 0.578
******** [step = 300] loss: 2.001, acc: 0.577
******** [step = 350] loss: 2.005, acc: 0.577
******** [step = 400] loss: 2.009, acc: 0.576
******** [step = 450] loss: 2.013, acc: 0.576
******** [step = 500] loss: 2.017, acc: 0.575
******** [step = 550] loss: 2.020, acc: 0.575
******** [step = 600] loss: 2.022, acc: 0.575
******** [step = 650] loss: 2.026, acc: 0.575
******** [step = 700] loss: 2.030, acc: 0.574
******** [step = 750] loss: 2.033, acc: 0.574
******** [step = 800] loss: 2.035, acc: 0.574
******** [step = 850] loss: 2.041, acc: 0.573
EPOCH = 43 loss: 2.041, acc: 0.573, val_loss: 2.425, val_acc: 0.554

================================================================================2025-08_12 18:35:37
******** [step = 50] loss: 1.957, acc: 0.581
******** [step = 100] loss: 1.968, acc: 0.581
******** [step = 150] loss: 1.971, acc: 0.581
******** [step = 200] loss: 1.972, acc: 0.580
******** [step = 250] loss: 1.981, acc: 0.579
******** [step = 300] loss: 1.990, acc: 0.578
******** [step = 350] loss: 1.993, acc: 0.578
******** [step = 400] loss: 1.999, acc: 0.577
******** [step = 450] loss: 2.003, acc: 0.577
******** [step = 500] loss: 2.008, acc: 0.577
******** [step = 550] loss: 2.015, acc: 0.576
******** [step = 600] loss: 2.019, acc: 0.575
******** [step = 650] loss: 2.021, acc: 0.575
******** [step = 700] loss: 2.025, acc: 0.575
******** [step = 750] loss: 2.027, acc: 0.575
******** [step = 800] loss: 2.031, acc: 0.574
******** [step = 850] loss: 2.035, acc: 0.574
EPOCH = 44 loss: 2.035, acc: 0.574, val_loss: 2.419, val_acc: 0.554

================================================================================2025-08_12 18:37:38
******** [step = 50] loss: 1.977, acc: 0.583
******** [step = 100] loss: 1.977, acc: 0.582
******** [step = 150] loss: 1.972, acc: 0.581
******** [step = 200] loss: 1.983, acc: 0.580
******** [step = 250] loss: 1.991, acc: 0.578
******** [step = 300] loss: 1.993, acc: 0.578
******** [step = 350] loss: 1.997, acc: 0.578
******** [step = 400] loss: 2.001, acc: 0.577
******** [step = 450] loss: 2.007, acc: 0.577
******** [step = 500] loss: 2.012, acc: 0.576
******** [step = 550] loss: 2.017, acc: 0.576
******** [step = 600] loss: 2.022, acc: 0.575
******** [step = 650] loss: 2.026, acc: 0.574
******** [step = 700] loss: 2.030, acc: 0.574
******** [step = 750] loss: 2.033, acc: 0.574
******** [step = 800] loss: 2.036, acc: 0.574
******** [step = 850] loss: 2.039, acc: 0.573
EPOCH = 45 loss: 2.039, acc: 0.573, val_loss: 2.429, val_acc: 0.555

================================================================================2025-08_12 18:39:39
******** [step = 50] loss: 1.995, acc: 0.576
******** [step = 100] loss: 1.989, acc: 0.577
******** [step = 150] loss: 1.993, acc: 0.577
******** [step = 200] loss: 1.992, acc: 0.578
******** [step = 250] loss: 1.994, acc: 0.577
******** [step = 300] loss: 1.996, acc: 0.577
******** [step = 350] loss: 1.997, acc: 0.578
******** [step = 400] loss: 2.001, acc: 0.578
******** [step = 450] loss: 2.003, acc: 0.578
******** [step = 500] loss: 2.005, acc: 0.577
******** [step = 550] loss: 2.007, acc: 0.577
******** [step = 600] loss: 2.011, acc: 0.577
******** [step = 650] loss: 2.015, acc: 0.576
******** [step = 700] loss: 2.019, acc: 0.576
******** [step = 750] loss: 2.022, acc: 0.576
******** [step = 800] loss: 2.025, acc: 0.576
******** [step = 850] loss: 2.029, acc: 0.575
EPOCH = 46 loss: 2.029, acc: 0.575, val_loss: 2.412, val_acc: 0.556

================================================================================2025-08_12 18:41:40
******** [step = 50] loss: 1.970, acc: 0.579
******** [step = 100] loss: 1.956, acc: 0.582
******** [step = 150] loss: 1.958, acc: 0.581
******** [step = 200] loss: 1.960, acc: 0.581
******** [step = 250] loss: 1.964, acc: 0.581
******** [step = 300] loss: 1.968, acc: 0.580
******** [step = 350] loss: 1.972, acc: 0.580
******** [step = 400] loss: 1.976, acc: 0.580
******** [step = 450] loss: 1.981, acc: 0.579
******** [step = 500] loss: 1.984, acc: 0.579
******** [step = 550] loss: 1.990, acc: 0.579
******** [step = 600] loss: 1.994, acc: 0.578
******** [step = 650] loss: 2.001, acc: 0.578
******** [step = 700] loss: 2.006, acc: 0.577
******** [step = 750] loss: 2.011, acc: 0.577
******** [step = 800] loss: 2.014, acc: 0.577
******** [step = 850] loss: 2.018, acc: 0.576
EPOCH = 47 loss: 2.018, acc: 0.576, val_loss: 2.414, val_acc: 0.557

================================================================================2025-08_12 18:43:41
******** [step = 50] loss: 1.953, acc: 0.582
******** [step = 100] loss: 1.953, acc: 0.583
******** [step = 150] loss: 1.950, acc: 0.583
******** [step = 200] loss: 1.953, acc: 0.583
******** [step = 250] loss: 1.958, acc: 0.583
******** [step = 300] loss: 1.965, acc: 0.582
******** [step = 350] loss: 1.971, acc: 0.582
******** [step = 400] loss: 1.976, acc: 0.581
******** [step = 450] loss: 1.980, acc: 0.581
******** [step = 500] loss: 1.986, acc: 0.580
******** [step = 550] loss: 1.990, acc: 0.580
******** [step = 600] loss: 1.992, acc: 0.580
******** [step = 650] loss: 1.996, acc: 0.579
******** [step = 700] loss: 2.000, acc: 0.579
******** [step = 750] loss: 2.004, acc: 0.579
******** [step = 800] loss: 2.008, acc: 0.578
******** [step = 850] loss: 2.013, acc: 0.578
EPOCH = 48 loss: 2.013, acc: 0.578, val_loss: 2.408, val_acc: 0.557

================================================================================2025-08_12 18:45:41
******** [step = 50] loss: 1.915, acc: 0.589
******** [step = 100] loss: 1.930, acc: 0.587
******** [step = 150] loss: 1.925, acc: 0.588
******** [step = 200] loss: 1.936, acc: 0.587
******** [step = 250] loss: 1.945, acc: 0.585
******** [step = 300] loss: 1.957, acc: 0.584
******** [step = 350] loss: 1.962, acc: 0.583
******** [step = 400] loss: 1.965, acc: 0.583
******** [step = 450] loss: 1.972, acc: 0.582
******** [step = 500] loss: 1.976, acc: 0.582
******** [step = 550] loss: 1.981, acc: 0.582
******** [step = 600] loss: 1.985, acc: 0.581
******** [step = 650] loss: 1.988, acc: 0.580
******** [step = 700] loss: 1.993, acc: 0.580
******** [step = 750] loss: 1.998, acc: 0.580
******** [step = 800] loss: 2.001, acc: 0.579
******** [step = 850] loss: 2.004, acc: 0.579
EPOCH = 49 loss: 2.004, acc: 0.579, val_loss: 2.405, val_acc: 0.557

================================================================================2025-08_12 18:47:42
******** [step = 50] loss: 1.930, acc: 0.586
******** [step = 100] loss: 1.928, acc: 0.588
******** [step = 150] loss: 1.939, acc: 0.586
******** [step = 200] loss: 1.940, acc: 0.586
******** [step = 250] loss: 1.941, acc: 0.586
******** [step = 300] loss: 1.949, acc: 0.585
******** [step = 350] loss: 1.955, acc: 0.584
******** [step = 400] loss: 1.962, acc: 0.583
******** [step = 450] loss: 1.966, acc: 0.583
******** [step = 500] loss: 1.972, acc: 0.582
******** [step = 550] loss: 1.977, acc: 0.582
******** [step = 600] loss: 1.983, acc: 0.581
******** [step = 650] loss: 1.987, acc: 0.580
******** [step = 700] loss: 1.991, acc: 0.580
******** [step = 750] loss: 1.993, acc: 0.580
******** [step = 800] loss: 1.996, acc: 0.580
******** [step = 850] loss: 2.000, acc: 0.580
EPOCH = 50 loss: 2.000, acc: 0.580, val_loss: 2.408, val_acc: 0.558

================================================================================2025-08_12 18:49:46
******** [step = 50] loss: 1.925, acc: 0.588
******** [step = 100] loss: 1.925, acc: 0.587
******** [step = 150] loss: 1.938, acc: 0.585
******** [step = 200] loss: 1.941, acc: 0.585
******** [step = 250] loss: 1.949, acc: 0.585
******** [step = 300] loss: 1.953, acc: 0.584
******** [step = 350] loss: 1.957, acc: 0.584
******** [step = 400] loss: 1.960, acc: 0.583
******** [step = 450] loss: 1.966, acc: 0.583
******** [step = 500] loss: 1.969, acc: 0.582
******** [step = 550] loss: 1.973, acc: 0.582
******** [step = 600] loss: 1.978, acc: 0.582
******** [step = 650] loss: 1.982, acc: 0.581
******** [step = 700] loss: 1.988, acc: 0.581
******** [step = 750] loss: 1.991, acc: 0.581
******** [step = 800] loss: 1.993, acc: 0.580
******** [step = 850] loss: 1.996, acc: 0.580
EPOCH = 51 loss: 1.996, acc: 0.580, val_loss: 2.401, val_acc: 0.558

================================================================================2025-08_12 18:51:53
******** [step = 50] loss: 1.902, acc: 0.591
******** [step = 100] loss: 1.908, acc: 0.591
******** [step = 150] loss: 1.913, acc: 0.590
******** [step = 200] loss: 1.923, acc: 0.588
******** [step = 250] loss: 1.929, acc: 0.587
******** [step = 300] loss: 1.936, acc: 0.587
******** [step = 350] loss: 1.942, acc: 0.586
******** [step = 400] loss: 1.951, acc: 0.585
******** [step = 450] loss: 1.956, acc: 0.584
******** [step = 500] loss: 1.962, acc: 0.583
******** [step = 550] loss: 1.967, acc: 0.583
******** [step = 600] loss: 1.970, acc: 0.582
******** [step = 650] loss: 1.976, acc: 0.582
******** [step = 700] loss: 1.980, acc: 0.581
******** [step = 750] loss: 1.983, acc: 0.581
******** [step = 800] loss: 1.987, acc: 0.581
******** [step = 850] loss: 1.993, acc: 0.580
EPOCH = 52 loss: 1.993, acc: 0.580, val_loss: 2.414, val_acc: 0.556

================================================================================2025-08_12 18:53:54
******** [step = 50] loss: 1.918, acc: 0.587
******** [step = 100] loss: 1.929, acc: 0.587
******** [step = 150] loss: 1.931, acc: 0.587
******** [step = 200] loss: 1.932, acc: 0.587
******** [step = 250] loss: 1.938, acc: 0.586
******** [step = 300] loss: 1.946, acc: 0.585
******** [step = 350] loss: 1.951, acc: 0.584
******** [step = 400] loss: 1.957, acc: 0.584
******** [step = 450] loss: 1.962, acc: 0.583
******** [step = 500] loss: 1.963, acc: 0.583
******** [step = 550] loss: 1.967, acc: 0.583
******** [step = 600] loss: 1.971, acc: 0.582
******** [step = 650] loss: 1.976, acc: 0.582
******** [step = 700] loss: 1.980, acc: 0.582
******** [step = 750] loss: 1.984, acc: 0.581
******** [step = 800] loss: 1.986, acc: 0.581
******** [step = 850] loss: 1.989, acc: 0.581
EPOCH = 53 loss: 1.989, acc: 0.581, val_loss: 2.408, val_acc: 0.558

================================================================================2025-08_12 18:56:03
******** [step = 50] loss: 1.917, acc: 0.587
******** [step = 100] loss: 1.916, acc: 0.588
******** [step = 150] loss: 1.922, acc: 0.588
******** [step = 200] loss: 1.931, acc: 0.587
******** [step = 250] loss: 1.935, acc: 0.586
******** [step = 300] loss: 1.939, acc: 0.586
******** [step = 350] loss: 1.945, acc: 0.585
******** [step = 400] loss: 1.950, acc: 0.584
******** [step = 450] loss: 1.952, acc: 0.584
******** [step = 500] loss: 1.954, acc: 0.584
******** [step = 550] loss: 1.959, acc: 0.584
******** [step = 600] loss: 1.963, acc: 0.584
******** [step = 650] loss: 1.968, acc: 0.583
******** [step = 700] loss: 1.971, acc: 0.583
******** [step = 750] loss: 1.974, acc: 0.583
******** [step = 800] loss: 1.976, acc: 0.583
******** [step = 850] loss: 1.978, acc: 0.583
EPOCH = 54 loss: 1.978, acc: 0.583, val_loss: 2.396, val_acc: 0.559

================================================================================2025-08_12 18:58:05
******** [step = 50] loss: 1.891, acc: 0.592
******** [step = 100] loss: 1.899, acc: 0.591
******** [step = 150] loss: 1.903, acc: 0.592
******** [step = 200] loss: 1.908, acc: 0.591
******** [step = 250] loss: 1.916, acc: 0.590
******** [step = 300] loss: 1.922, acc: 0.589
******** [step = 350] loss: 1.931, acc: 0.588
******** [step = 400] loss: 1.936, acc: 0.587
******** [step = 450] loss: 1.940, acc: 0.587
******** [step = 500] loss: 1.943, acc: 0.587
******** [step = 550] loss: 1.948, acc: 0.586
******** [step = 600] loss: 1.953, acc: 0.586
******** [step = 650] loss: 1.957, acc: 0.585
******** [step = 700] loss: 1.961, acc: 0.585
******** [step = 750] loss: 1.964, acc: 0.584
******** [step = 800] loss: 1.967, acc: 0.584
******** [step = 850] loss: 1.970, acc: 0.584
EPOCH = 55 loss: 1.970, acc: 0.584, val_loss: 2.391, val_acc: 0.561

================================================================================2025-08_12 19:00:06
******** [step = 50] loss: 1.900, acc: 0.593
******** [step = 100] loss: 1.905, acc: 0.591
******** [step = 150] loss: 1.912, acc: 0.591
******** [step = 200] loss: 1.919, acc: 0.590
******** [step = 250] loss: 1.919, acc: 0.590
******** [step = 300] loss: 1.923, acc: 0.589
******** [step = 350] loss: 1.926, acc: 0.589
******** [step = 400] loss: 1.930, acc: 0.589
******** [step = 450] loss: 1.936, acc: 0.588
******** [step = 500] loss: 1.939, acc: 0.587
******** [step = 550] loss: 1.944, acc: 0.587
******** [step = 600] loss: 1.948, acc: 0.586
******** [step = 650] loss: 1.952, acc: 0.586
******** [step = 700] loss: 1.955, acc: 0.586
******** [step = 750] loss: 1.959, acc: 0.586
******** [step = 800] loss: 1.963, acc: 0.585
******** [step = 850] loss: 1.966, acc: 0.585
EPOCH = 56 loss: 1.966, acc: 0.585, val_loss: 2.396, val_acc: 0.561

================================================================================2025-08_12 19:02:07
******** [step = 50] loss: 1.867, acc: 0.596
******** [step = 100] loss: 1.878, acc: 0.594
******** [step = 150] loss: 1.885, acc: 0.594
******** [step = 200] loss: 1.893, acc: 0.593
******** [step = 250] loss: 1.905, acc: 0.591
******** [step = 300] loss: 1.911, acc: 0.591
******** [step = 350] loss: 1.916, acc: 0.590
******** [step = 400] loss: 1.922, acc: 0.590
******** [step = 450] loss: 1.929, acc: 0.589
******** [step = 500] loss: 1.932, acc: 0.588
******** [step = 550] loss: 1.935, acc: 0.588
******** [step = 600] loss: 1.940, acc: 0.588
******** [step = 650] loss: 1.945, acc: 0.587
******** [step = 700] loss: 1.949, acc: 0.587
******** [step = 750] loss: 1.951, acc: 0.586
******** [step = 800] loss: 1.957, acc: 0.586
******** [step = 850] loss: 1.959, acc: 0.586
EPOCH = 57 loss: 1.959, acc: 0.586, val_loss: 2.397, val_acc: 0.560

================================================================================2025-08_12 19:04:08
******** [step = 50] loss: 1.881, acc: 0.595
******** [step = 100] loss: 1.894, acc: 0.593
******** [step = 150] loss: 1.903, acc: 0.591
******** [step = 200] loss: 1.908, acc: 0.590
******** [step = 250] loss: 1.910, acc: 0.591
******** [step = 300] loss: 1.918, acc: 0.589
******** [step = 350] loss: 1.923, acc: 0.589
******** [step = 400] loss: 1.924, acc: 0.589
******** [step = 450] loss: 1.930, acc: 0.588
******** [step = 500] loss: 1.933, acc: 0.588
******** [step = 550] loss: 1.937, acc: 0.587
******** [step = 600] loss: 1.941, acc: 0.587
******** [step = 650] loss: 1.945, acc: 0.587
******** [step = 700] loss: 1.950, acc: 0.586
******** [step = 750] loss: 1.952, acc: 0.586
******** [step = 800] loss: 1.957, acc: 0.586
******** [step = 850] loss: 1.959, acc: 0.586
EPOCH = 58 loss: 1.959, acc: 0.586, val_loss: 2.389, val_acc: 0.561

================================================================================2025-08_12 19:06:08
******** [step = 50] loss: 1.880, acc: 0.594
******** [step = 100] loss: 1.890, acc: 0.593
******** [step = 150] loss: 1.895, acc: 0.592
******** [step = 200] loss: 1.900, acc: 0.592
******** [step = 250] loss: 1.905, acc: 0.591
******** [step = 300] loss: 1.910, acc: 0.590
******** [step = 350] loss: 1.914, acc: 0.591
******** [step = 400] loss: 1.917, acc: 0.590
******** [step = 450] loss: 1.920, acc: 0.590
******** [step = 500] loss: 1.924, acc: 0.589
******** [step = 550] loss: 1.927, acc: 0.589
******** [step = 600] loss: 1.932, acc: 0.589
******** [step = 650] loss: 1.938, acc: 0.588
******** [step = 700] loss: 1.941, acc: 0.588
******** [step = 750] loss: 1.945, acc: 0.588
******** [step = 800] loss: 1.949, acc: 0.587
******** [step = 850] loss: 1.955, acc: 0.587
EPOCH = 59 loss: 1.955, acc: 0.587, val_loss: 2.396, val_acc: 0.561

================================================================================2025-08_12 19:08:09
******** [step = 50] loss: 1.885, acc: 0.593
******** [step = 100] loss: 1.882, acc: 0.595
******** [step = 150] loss: 1.888, acc: 0.594
******** [step = 200] loss: 1.888, acc: 0.594
******** [step = 250] loss: 1.896, acc: 0.593
******** [step = 300] loss: 1.900, acc: 0.592
******** [step = 350] loss: 1.907, acc: 0.591
******** [step = 400] loss: 1.913, acc: 0.590
******** [step = 450] loss: 1.920, acc: 0.589
******** [step = 500] loss: 1.927, acc: 0.589
******** [step = 550] loss: 1.932, acc: 0.588
******** [step = 600] loss: 1.938, acc: 0.587
******** [step = 650] loss: 1.942, acc: 0.587
******** [step = 700] loss: 1.948, acc: 0.586
******** [step = 750] loss: 1.952, acc: 0.586
******** [step = 800] loss: 1.956, acc: 0.585
******** [step = 850] loss: 1.960, acc: 0.585
EPOCH = 60 loss: 1.960, acc: 0.585, val_loss: 2.397, val_acc: 0.559

================================================================================2025-08_12 19:10:09
******** [step = 50] loss: 1.868, acc: 0.595
******** [step = 100] loss: 1.876, acc: 0.595
******** [step = 150] loss: 1.886, acc: 0.593
******** [step = 200] loss: 1.893, acc: 0.592
******** [step = 250] loss: 1.903, acc: 0.591
******** [step = 300] loss: 1.905, acc: 0.590
******** [step = 350] loss: 1.911, acc: 0.590
******** [step = 400] loss: 1.915, acc: 0.590
******** [step = 450] loss: 1.919, acc: 0.590
******** [step = 500] loss: 1.924, acc: 0.589
******** [step = 550] loss: 1.932, acc: 0.588
******** [step = 600] loss: 1.938, acc: 0.587
******** [step = 650] loss: 1.945, acc: 0.587
******** [step = 700] loss: 1.950, acc: 0.586
******** [step = 750] loss: 1.956, acc: 0.585
******** [step = 800] loss: 1.959, acc: 0.585
******** [step = 850] loss: 1.963, acc: 0.585
EPOCH = 61 loss: 1.963, acc: 0.585, val_loss: 2.403, val_acc: 0.560

================================================================================2025-08_12 19:12:09
******** [step = 50] loss: 1.878, acc: 0.594
******** [step = 100] loss: 1.891, acc: 0.593
******** [step = 150] loss: 1.891, acc: 0.593
******** [step = 200] loss: 1.900, acc: 0.592
******** [step = 250] loss: 1.901, acc: 0.592
******** [step = 300] loss: 1.907, acc: 0.591
******** [step = 350] loss: 1.914, acc: 0.591
******** [step = 400] loss: 1.917, acc: 0.590
******** [step = 450] loss: 1.922, acc: 0.590
******** [step = 500] loss: 1.926, acc: 0.589
******** [step = 550] loss: 1.931, acc: 0.589
******** [step = 600] loss: 1.935, acc: 0.589
******** [step = 650] loss: 1.939, acc: 0.588
******** [step = 700] loss: 1.943, acc: 0.588
******** [step = 750] loss: 1.947, acc: 0.587
******** [step = 800] loss: 1.951, acc: 0.587
******** [step = 850] loss: 1.953, acc: 0.587
EPOCH = 62 loss: 1.953, acc: 0.587, val_loss: 2.402, val_acc: 0.559

================================================================================2025-08_12 19:14:10
******** [step = 50] loss: 1.869, acc: 0.595
******** [step = 100] loss: 1.872, acc: 0.595
******** [step = 150] loss: 1.882, acc: 0.594
******** [step = 200] loss: 1.891, acc: 0.593
******** [step = 250] loss: 1.901, acc: 0.592
******** [step = 300] loss: 1.906, acc: 0.591
******** [step = 350] loss: 1.908, acc: 0.591
******** [step = 400] loss: 1.913, acc: 0.590
******** [step = 450] loss: 1.916, acc: 0.590
******** [step = 500] loss: 1.919, acc: 0.590
******** [step = 550] loss: 1.924, acc: 0.589
******** [step = 600] loss: 1.929, acc: 0.589
******** [step = 650] loss: 1.931, acc: 0.588
******** [step = 700] loss: 1.936, acc: 0.588
******** [step = 750] loss: 1.940, acc: 0.588
******** [step = 800] loss: 1.944, acc: 0.587
******** [step = 850] loss: 1.946, acc: 0.587
EPOCH = 63 loss: 1.946, acc: 0.587, val_loss: 2.393, val_acc: 0.562

================================================================================2025-08_12 19:16:10
******** [step = 50] loss: 1.871, acc: 0.595
******** [step = 100] loss: 1.872, acc: 0.597
******** [step = 150] loss: 1.882, acc: 0.595
******** [step = 200] loss: 1.883, acc: 0.596
******** [step = 250] loss: 1.884, acc: 0.595
******** [step = 300] loss: 1.887, acc: 0.595
******** [step = 350] loss: 1.894, acc: 0.594
******** [step = 400] loss: 1.902, acc: 0.593
******** [step = 450] loss: 1.907, acc: 0.592
******** [step = 500] loss: 1.913, acc: 0.591
******** [step = 550] loss: 1.920, acc: 0.590
******** [step = 600] loss: 1.926, acc: 0.590
******** [step = 650] loss: 1.930, acc: 0.589
******** [step = 700] loss: 1.933, acc: 0.589
******** [step = 750] loss: 1.937, acc: 0.589
******** [step = 800] loss: 1.941, acc: 0.588
******** [step = 850] loss: 1.945, acc: 0.588
EPOCH = 64 loss: 1.945, acc: 0.588, val_loss: 2.405, val_acc: 0.560

================================================================================2025-08_12 19:18:15
******** [step = 50] loss: 1.887, acc: 0.591
******** [step = 100] loss: 1.884, acc: 0.592
******** [step = 150] loss: 1.882, acc: 0.591
******** [step = 200] loss: 1.884, acc: 0.592
******** [step = 250] loss: 1.892, acc: 0.591
******** [step = 300] loss: 1.896, acc: 0.591
******** [step = 350] loss: 1.902, acc: 0.590
******** [step = 400] loss: 1.907, acc: 0.590
******** [step = 450] loss: 1.914, acc: 0.590
******** [step = 500] loss: 1.918, acc: 0.589
******** [step = 550] loss: 1.924, acc: 0.588
******** [step = 600] loss: 1.928, acc: 0.588
******** [step = 650] loss: 1.931, acc: 0.588
******** [step = 700] loss: 1.933, acc: 0.588
******** [step = 750] loss: 1.937, acc: 0.588
******** [step = 800] loss: 1.941, acc: 0.588
******** [step = 850] loss: 1.946, acc: 0.587
EPOCH = 65 loss: 1.946, acc: 0.587, val_loss: 2.393, val_acc: 0.562

================================================================================2025-08_12 19:20:24
******** [step = 50] loss: 1.861, acc: 0.596
******** [step = 100] loss: 1.871, acc: 0.595
******** [step = 150] loss: 1.880, acc: 0.594
******** [step = 200] loss: 1.886, acc: 0.593
******** [step = 250] loss: 1.890, acc: 0.593
******** [step = 300] loss: 1.896, acc: 0.592
******** [step = 350] loss: 1.899, acc: 0.592
******** [step = 400] loss: 1.903, acc: 0.592
******** [step = 450] loss: 1.907, acc: 0.591
******** [step = 500] loss: 1.913, acc: 0.591
******** [step = 550] loss: 1.919, acc: 0.590
******** [step = 600] loss: 1.923, acc: 0.589
******** [step = 650] loss: 1.928, acc: 0.589
******** [step = 700] loss: 1.930, acc: 0.589
******** [step = 750] loss: 1.933, acc: 0.589
******** [step = 800] loss: 1.937, acc: 0.588
******** [step = 850] loss: 1.941, acc: 0.588
EPOCH = 66 loss: 1.941, acc: 0.588, val_loss: 2.397, val_acc: 0.560

================================================================================2025-08_12 19:22:25
******** [step = 50] loss: 1.885, acc: 0.593
******** [step = 100] loss: 1.885, acc: 0.592
******** [step = 150] loss: 1.887, acc: 0.593
******** [step = 200] loss: 1.889, acc: 0.593
******** [step = 250] loss: 1.892, acc: 0.592
******** [step = 300] loss: 1.895, acc: 0.593
******** [step = 350] loss: 1.897, acc: 0.593
******** [step = 400] loss: 1.901, acc: 0.593
******** [step = 450] loss: 1.907, acc: 0.592
******** [step = 500] loss: 1.912, acc: 0.591
******** [step = 550] loss: 1.916, acc: 0.591
******** [step = 600] loss: 1.920, acc: 0.591
******** [step = 650] loss: 1.924, acc: 0.590
******** [step = 700] loss: 1.929, acc: 0.589
******** [step = 750] loss: 1.933, acc: 0.589
******** [step = 800] loss: 1.936, acc: 0.589
******** [step = 850] loss: 1.938, acc: 0.589
EPOCH = 67 loss: 1.938, acc: 0.589, val_loss: 2.396, val_acc: 0.560

================================================================================2025-08_12 19:24:25
******** [step = 50] loss: 1.873, acc: 0.595
******** [step = 100] loss: 1.878, acc: 0.595
******** [step = 150] loss: 1.879, acc: 0.595
******** [step = 200] loss: 1.884, acc: 0.595
******** [step = 250] loss: 1.885, acc: 0.595
******** [step = 300] loss: 1.888, acc: 0.594
******** [step = 350] loss: 1.893, acc: 0.593
******** [step = 400] loss: 1.897, acc: 0.593
******** [step = 450] loss: 1.902, acc: 0.592
******** [step = 500] loss: 1.908, acc: 0.592
******** [step = 550] loss: 1.910, acc: 0.592
******** [step = 600] loss: 1.913, acc: 0.591
******** [step = 650] loss: 1.917, acc: 0.591
******** [step = 700] loss: 1.921, acc: 0.590
******** [step = 750] loss: 1.925, acc: 0.590
******** [step = 800] loss: 1.928, acc: 0.590
******** [step = 850] loss: 1.931, acc: 0.589
EPOCH = 68 loss: 1.931, acc: 0.589, val_loss: 2.393, val_acc: 0.561

================================================================================2025-08_12 19:26:30
******** [step = 50] loss: 1.869, acc: 0.598
******** [step = 100] loss: 1.869, acc: 0.597
******** [step = 150] loss: 1.872, acc: 0.596
******** [step = 200] loss: 1.874, acc: 0.595
******** [step = 250] loss: 1.875, acc: 0.595
******** [step = 300] loss: 1.881, acc: 0.595
******** [step = 350] loss: 1.886, acc: 0.594
******** [step = 400] loss: 1.889, acc: 0.594
******** [step = 450] loss: 1.896, acc: 0.593
******** [step = 500] loss: 1.900, acc: 0.593
******** [step = 550] loss: 1.905, acc: 0.592
******** [step = 600] loss: 1.908, acc: 0.592
******** [step = 650] loss: 1.911, acc: 0.592
******** [step = 700] loss: 1.915, acc: 0.591
******** [step = 750] loss: 1.918, acc: 0.591
******** [step = 800] loss: 1.922, acc: 0.591
******** [step = 850] loss: 1.925, acc: 0.591
EPOCH = 69 loss: 1.925, acc: 0.591, val_loss: 2.390, val_acc: 0.565

================================================================================2025-08_12 19:28:30
******** [step = 50] loss: 1.863, acc: 0.599
******** [step = 100] loss: 1.860, acc: 0.598
******** [step = 150] loss: 1.858, acc: 0.598
******** [step = 200] loss: 1.862, acc: 0.598
******** [step = 250] loss: 1.868, acc: 0.597
******** [step = 300] loss: 1.877, acc: 0.596
******** [step = 350] loss: 1.884, acc: 0.595
******** [step = 400] loss: 1.887, acc: 0.594
******** [step = 450] loss: 1.891, acc: 0.594
******** [step = 500] loss: 1.893, acc: 0.594
******** [step = 550] loss: 1.897, acc: 0.593
******** [step = 600] loss: 1.901, acc: 0.593
******** [step = 650] loss: 1.907, acc: 0.593
******** [step = 700] loss: 1.911, acc: 0.592
******** [step = 750] loss: 1.915, acc: 0.592
******** [step = 800] loss: 1.918, acc: 0.592
******** [step = 850] loss: 1.920, acc: 0.592
EPOCH = 70 loss: 1.920, acc: 0.592, val_loss: 2.387, val_acc: 0.564

================================================================================2025-08_12 19:30:35
******** [step = 50] loss: 1.856, acc: 0.598
******** [step = 100] loss: 1.845, acc: 0.600
******** [step = 150] loss: 1.852, acc: 0.600
******** [step = 200] loss: 1.853, acc: 0.600
******** [step = 250] loss: 1.856, acc: 0.600
******** [step = 300] loss: 1.867, acc: 0.598
******** [step = 350] loss: 1.872, acc: 0.597
******** [step = 400] loss: 1.875, acc: 0.597
******** [step = 450] loss: 1.881, acc: 0.596
******** [step = 500] loss: 1.887, acc: 0.595
******** [step = 550] loss: 1.891, acc: 0.595
******** [step = 600] loss: 1.895, acc: 0.595
******** [step = 650] loss: 1.900, acc: 0.594
******** [step = 700] loss: 1.903, acc: 0.594
******** [step = 750] loss: 1.908, acc: 0.593
******** [step = 800] loss: 1.911, acc: 0.593
******** [step = 850] loss: 1.917, acc: 0.593
EPOCH = 71 loss: 1.917, acc: 0.593, val_loss: 2.393, val_acc: 0.562

================================================================================2025-08_12 19:32:36
******** [step = 50] loss: 1.836, acc: 0.599
******** [step = 100] loss: 1.848, acc: 0.599
******** [step = 150] loss: 1.852, acc: 0.599
******** [step = 200] loss: 1.857, acc: 0.599
******** [step = 250] loss: 1.861, acc: 0.598
******** [step = 300] loss: 1.864, acc: 0.598
******** [step = 350] loss: 1.867, acc: 0.598
******** [step = 400] loss: 1.871, acc: 0.597
******** [step = 450] loss: 1.876, acc: 0.597
******** [step = 500] loss: 1.881, acc: 0.596
******** [step = 550] loss: 1.887, acc: 0.595
******** [step = 600] loss: 1.893, acc: 0.595
******** [step = 650] loss: 1.896, acc: 0.595
******** [step = 700] loss: 1.901, acc: 0.594
******** [step = 750] loss: 1.905, acc: 0.594
******** [step = 800] loss: 1.910, acc: 0.593
******** [step = 850] loss: 1.912, acc: 0.593
EPOCH = 72 loss: 1.912, acc: 0.593, val_loss: 2.385, val_acc: 0.564

================================================================================2025-08_12 19:34:39
******** [step = 50] loss: 1.826, acc: 0.602
******** [step = 100] loss: 1.837, acc: 0.600
******** [step = 150] loss: 1.847, acc: 0.600
******** [step = 200] loss: 1.857, acc: 0.598
******** [step = 250] loss: 1.855, acc: 0.599
******** [step = 300] loss: 1.863, acc: 0.597
******** [step = 350] loss: 1.870, acc: 0.597
******** [step = 400] loss: 1.877, acc: 0.596
******** [step = 450] loss: 1.881, acc: 0.596
******** [step = 500] loss: 1.884, acc: 0.595
******** [step = 550] loss: 1.889, acc: 0.595
******** [step = 600] loss: 1.894, acc: 0.594
******** [step = 650] loss: 1.899, acc: 0.594
******** [step = 700] loss: 1.902, acc: 0.594
******** [step = 750] loss: 1.907, acc: 0.593
******** [step = 800] loss: 1.911, acc: 0.593
******** [step = 850] loss: 1.913, acc: 0.593
EPOCH = 73 loss: 1.913, acc: 0.593, val_loss: 2.391, val_acc: 0.563

================================================================================2025-08_12 19:36:47
******** [step = 50] loss: 1.835, acc: 0.599
******** [step = 100] loss: 1.845, acc: 0.600
******** [step = 150] loss: 1.844, acc: 0.601
******** [step = 200] loss: 1.855, acc: 0.599
******** [step = 250] loss: 1.860, acc: 0.598
******** [step = 300] loss: 1.866, acc: 0.598
******** [step = 350] loss: 1.872, acc: 0.597
******** [step = 400] loss: 1.875, acc: 0.597
******** [step = 450] loss: 1.880, acc: 0.596
******** [step = 500] loss: 1.884, acc: 0.596
******** [step = 550] loss: 1.889, acc: 0.595
******** [step = 600] loss: 1.892, acc: 0.595
******** [step = 650] loss: 1.895, acc: 0.595
******** [step = 700] loss: 1.899, acc: 0.594
******** [step = 750] loss: 1.902, acc: 0.594
******** [step = 800] loss: 1.905, acc: 0.594
******** [step = 850] loss: 1.908, acc: 0.594
EPOCH = 74 loss: 1.908, acc: 0.594, val_loss: 2.383, val_acc: 0.564

================================================================================2025-08_12 19:38:50
******** [step = 50] loss: 1.830, acc: 0.601
******** [step = 100] loss: 1.836, acc: 0.600
******** [step = 150] loss: 1.836, acc: 0.600
******** [step = 200] loss: 1.842, acc: 0.600
******** [step = 250] loss: 1.850, acc: 0.599
******** [step = 300] loss: 1.857, acc: 0.598
******** [step = 350] loss: 1.862, acc: 0.598
******** [step = 400] loss: 1.863, acc: 0.598
******** [step = 450] loss: 1.868, acc: 0.598
******** [step = 500] loss: 1.874, acc: 0.597
******** [step = 550] loss: 1.877, acc: 0.596
******** [step = 600] loss: 1.882, acc: 0.596
******** [step = 650] loss: 1.887, acc: 0.595
******** [step = 700] loss: 1.891, acc: 0.595
******** [step = 750] loss: 1.894, acc: 0.595
******** [step = 800] loss: 1.898, acc: 0.595
******** [step = 850] loss: 1.905, acc: 0.594
EPOCH = 75 loss: 1.905, acc: 0.594, val_loss: 2.380, val_acc: 0.565

================================================================================2025-08_12 19:41:01
******** [step = 50] loss: 1.840, acc: 0.603
******** [step = 100] loss: 1.837, acc: 0.602
******** [step = 150] loss: 1.839, acc: 0.601
******** [step = 200] loss: 1.847, acc: 0.601
******** [step = 250] loss: 1.852, acc: 0.600
******** [step = 300] loss: 1.854, acc: 0.600
******** [step = 350] loss: 1.860, acc: 0.599
******** [step = 400] loss: 1.867, acc: 0.598
******** [step = 450] loss: 1.870, acc: 0.597
******** [step = 500] loss: 1.874, acc: 0.597
******** [step = 550] loss: 1.877, acc: 0.597
******** [step = 600] loss: 1.881, acc: 0.597
******** [step = 650] loss: 1.885, acc: 0.596
******** [step = 700] loss: 1.889, acc: 0.596
******** [step = 750] loss: 1.894, acc: 0.595
******** [step = 800] loss: 1.899, acc: 0.595
******** [step = 850] loss: 1.903, acc: 0.594
EPOCH = 76 loss: 1.903, acc: 0.594, val_loss: 2.383, val_acc: 0.563

================================================================================2025-08_12 19:43:02
******** [step = 50] loss: 1.828, acc: 0.600
******** [step = 100] loss: 1.837, acc: 0.599
******** [step = 150] loss: 1.840, acc: 0.600
******** [step = 200] loss: 1.846, acc: 0.599
******** [step = 250] loss: 1.851, acc: 0.599
******** [step = 300] loss: 1.854, acc: 0.599
******** [step = 350] loss: 1.860, acc: 0.598
******** [step = 400] loss: 1.866, acc: 0.598
******** [step = 450] loss: 1.869, acc: 0.597
******** [step = 500] loss: 1.872, acc: 0.597
******** [step = 550] loss: 1.876, acc: 0.597
******** [step = 600] loss: 1.879, acc: 0.597
******** [step = 650] loss: 1.883, acc: 0.596
******** [step = 700] loss: 1.887, acc: 0.596
******** [step = 750] loss: 1.891, acc: 0.595
******** [step = 800] loss: 1.893, acc: 0.595
******** [step = 850] loss: 1.896, acc: 0.595
EPOCH = 77 loss: 1.896, acc: 0.595, val_loss: 2.376, val_acc: 0.565

================================================================================2025-08_12 19:45:02
******** [step = 50] loss: 1.810, acc: 0.605
******** [step = 100] loss: 1.825, acc: 0.603
******** [step = 150] loss: 1.822, acc: 0.604
******** [step = 200] loss: 1.828, acc: 0.602
******** [step = 250] loss: 1.835, acc: 0.602
******** [step = 300] loss: 1.840, acc: 0.602
******** [step = 350] loss: 1.846, acc: 0.601
******** [step = 400] loss: 1.853, acc: 0.600
******** [step = 450] loss: 1.858, acc: 0.599
******** [step = 500] loss: 1.864, acc: 0.599
******** [step = 550] loss: 1.869, acc: 0.598
******** [step = 600] loss: 1.873, acc: 0.598
******** [step = 650] loss: 1.876, acc: 0.598
******** [step = 700] loss: 1.880, acc: 0.597
******** [step = 750] loss: 1.883, acc: 0.597
******** [step = 800] loss: 1.888, acc: 0.597
******** [step = 850] loss: 1.892, acc: 0.597
EPOCH = 78 loss: 1.892, acc: 0.597, val_loss: 2.375, val_acc: 0.566

================================================================================2025-08_12 19:47:06
******** [step = 50] loss: 1.822, acc: 0.604
******** [step = 100] loss: 1.826, acc: 0.604
******** [step = 150] loss: 1.826, acc: 0.603
******** [step = 200] loss: 1.830, acc: 0.603
******** [step = 250] loss: 1.836, acc: 0.602
******** [step = 300] loss: 1.844, acc: 0.601
******** [step = 350] loss: 1.846, acc: 0.601
******** [step = 400] loss: 1.851, acc: 0.600
******** [step = 450] loss: 1.857, acc: 0.599
******** [step = 500] loss: 1.862, acc: 0.599
******** [step = 550] loss: 1.866, acc: 0.598
******** [step = 600] loss: 1.872, acc: 0.598
******** [step = 650] loss: 1.877, acc: 0.597
******** [step = 700] loss: 1.881, acc: 0.597
******** [step = 750] loss: 1.883, acc: 0.597
******** [step = 800] loss: 1.886, acc: 0.597
******** [step = 850] loss: 1.890, acc: 0.597
EPOCH = 79 loss: 1.890, acc: 0.597, val_loss: 2.375, val_acc: 0.566

================================================================================2025-08_12 19:49:08
******** [step = 50] loss: 1.813, acc: 0.605
******** [step = 100] loss: 1.817, acc: 0.605
******** [step = 150] loss: 1.825, acc: 0.603
******** [step = 200] loss: 1.831, acc: 0.603
******** [step = 250] loss: 1.837, acc: 0.602
******** [step = 300] loss: 1.845, acc: 0.601
******** [step = 350] loss: 1.847, acc: 0.601
******** [step = 400] loss: 1.851, acc: 0.601
******** [step = 450] loss: 1.856, acc: 0.600
******** [step = 500] loss: 1.861, acc: 0.600
******** [step = 550] loss: 1.865, acc: 0.599
******** [step = 600] loss: 1.867, acc: 0.599
******** [step = 650] loss: 1.871, acc: 0.599
******** [step = 700] loss: 1.874, acc: 0.598
******** [step = 750] loss: 1.878, acc: 0.598
******** [step = 800] loss: 1.881, acc: 0.598
******** [step = 850] loss: 1.884, acc: 0.598
EPOCH = 80 loss: 1.884, acc: 0.598, val_loss: 2.372, val_acc: 0.567

================================================================================2025-08_12 19:51:09
finishing training...
Training complete in 163m 37s
    epoch  ...   val_acc
0     1.0  ...  0.380427
1     2.0  ...  0.411748
2     3.0  ...  0.435625
3     4.0  ...  0.445039
4     5.0  ...  0.467168
..    ...  ...       ...
75   76.0  ...  0.562612
76   77.0  ...  0.565103
77   78.0  ...  0.566261
78   79.0  ...  0.565992
79   80.0  ...  0.566859

[80 rows x 5 columns]
== Done ==
Tue Aug 12 07:51:36 PM EDT 2025
---------------------------------------
Begin Slurm Epilog: Aug-12-2025 19:51:36
Job ID:        6974795
User ID:       xchen920
Account:       gts-apm7
Job name:      channel_trans
Resources:     cpu=4,gres/gpu:v100=1,mem=16G,node=1
Rsrc Used:     cput=11:00:32,vmem=0,walltime=02:45:08,mem=36784K,energy_used=0
Partition:     gpu-v100
QOS:           inferno
Nodes:         atl1-1-02-009-36-0
---------------------------------------
